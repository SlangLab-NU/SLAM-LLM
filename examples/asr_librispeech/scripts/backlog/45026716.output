/work/van-speech-nlp/jindaznb/slamenv/bin/python
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: w2v2
speech encoder2 path: vitouphy/wav2vec2-xls-r-300m-timit-phoneme
llm_path: 
Identifier: 
use_peft: true
use_fp16: 
Final identifier: psst_phoneme_wavlm_llama32_1b_dual_peft
/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft does not exist
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/
Resume epoch: 1
Resume step: 0
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/
[2024-11-08 00:22:18][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-08 00:22:18][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-08 00:22:18][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-08 00:22:21][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-08 00:22:26][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 00:22:26][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-08 00:22:26][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 00:22:26][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-08 00:22:28][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-08 00:22:28][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-08 00:22:28][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-08 00:22:28][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-08 00:22:39][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 00:22:39][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-08 00:22:39][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-08 00:22:40][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 00:22:40][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-08 00:22:40][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-08 00:22:40][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-08 00:22:40][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft//model.pt
No GT file matching pattern '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/decode_test_beam4_*_gt' found.
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
llm_path: 
use_peft: true
use_fp16: 
Final identifier: psst_phoneme_wavlm_llama32_1b_linear_peft
/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft does not exist
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/
Resume epoch: 1
Resume step: 0
[2024-11-08 00:23:05][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-08 00:23:05][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-08 00:23:05][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-08 00:23:05][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-11-08_00-23-04.txt', 'log_interval': 5}
[2024-11-08 00:23:29][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-08 00:23:36][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 00:23:36][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-08 00:23:36][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 00:23:36][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-08 00:23:42][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 00:23:42][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-08 00:23:42][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-08 00:23:42][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 00:23:42][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-08 00:23:43][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-08 00:23:43][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-08 00:23:43][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-08 00:23:43][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-08 00:23:47][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-08 00:23:50][root][INFO] - --> Training Set Length = 2298
[2024-11-08 00:23:50][root][INFO] - --> Validation Set Length = 341
[2024-11-08 00:23:50][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-08 00:23:50][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-08 00:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:23:56][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-11-08 00:23:59][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 7.947604656219482, acc: 0.0357142873108387)
[2024-11-08 00:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:00][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 8.088130950927734, acc: 0.03703703731298447)
[2024-11-08 00:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:00][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 7.8554487228393555, acc: 0.0)
[2024-11-08 00:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:00][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 8.20986557006836, acc: 0.0)
[2024-11-08 00:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:01][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 7.860559940338135, acc: 0.0)
[2024-11-08 00:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:01][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 8.403226852416992, acc: 0.10526315867900848)
[2024-11-08 00:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:02][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 9.102214813232422, acc: 0.0)
[2024-11-08 00:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:02][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 8.463139533996582, acc: 0.0)
[2024-11-08 00:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:02][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 9.067090034484863, acc: 0.0416666679084301)
[2024-11-08 00:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:03][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 7.422235488891602, acc: 0.032258063554763794)
[2024-11-08 00:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:03][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 8.424070358276367, acc: 0.0)
[2024-11-08 00:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:04][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 7.286851406097412, acc: 0.0)
[2024-11-08 00:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:04][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 7.353127479553223, acc: 0.10000000149011612)
[2024-11-08 00:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:04][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 8.039188385009766, acc: 0.05263157933950424)
[2024-11-08 00:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:05][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 8.64094066619873, acc: 0.0)
[2024-11-08 00:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:05][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 8.323009490966797, acc: 0.0)
[2024-11-08 00:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:05][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 7.886688709259033, acc: 0.0357142873108387)
[2024-11-08 00:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:06][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 8.295414924621582, acc: 0.0)
[2024-11-08 00:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:06][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 7.142056465148926, acc: 0.0)
[2024-11-08 00:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:07][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 8.74642276763916, acc: 0.05263157933950424)
[2024-11-08 00:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:07][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 8.067055702209473, acc: 0.10526315867900848)
[2024-11-08 00:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:07][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 8.321099281311035, acc: 0.0)
[2024-11-08 00:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:08][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 8.47517204284668, acc: 0.0)
[2024-11-08 00:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:08][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 8.440641403198242, acc: 0.0)
[2024-11-08 00:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:08][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 8.060158729553223, acc: 0.06666667014360428)
[2024-11-08 00:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:09][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 7.426005840301514, acc: 0.0)
[2024-11-08 00:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:09][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 7.413356781005859, acc: 0.0476190485060215)
[2024-11-08 00:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:11][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 7.793929100036621, acc: 0.05263157933950424)
[2024-11-08 00:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:11][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 7.623058795928955, acc: 0.0)
[2024-11-08 00:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:11][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 7.777623176574707, acc: 0.05000000074505806)
[2024-11-08 00:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:12][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 7.882206439971924, acc: 0.04545454680919647)
[2024-11-08 00:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:12][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 7.426916122436523, acc: 0.0)
[2024-11-08 00:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:12][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 7.050841331481934, acc: 0.0)
[2024-11-08 00:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:13][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 7.364468097686768, acc: 0.0)
[2024-11-08 00:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:13][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 6.911843299865723, acc: 0.0476190485060215)
[2024-11-08 00:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:14][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 7.92087984085083, acc: 0.10526315867900848)
[2024-11-08 00:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:14][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 7.5130510330200195, acc: 0.0)
[2024-11-08 00:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:14][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 8.222193717956543, acc: 0.0)
[2024-11-08 00:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:15][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 7.4829936027526855, acc: 0.0)
[2024-11-08 00:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:15][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 7.189054012298584, acc: 0.0)
[2024-11-08 00:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:16][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 6.517695903778076, acc: 0.0)
[2024-11-08 00:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:16][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 6.329058647155762, acc: 0.0)
[2024-11-08 00:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:17][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 6.347604274749756, acc: 0.0)
[2024-11-08 00:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:17][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 7.372229099273682, acc: 0.0)
[2024-11-08 00:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:17][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 7.21855354309082, acc: 0.0)
[2024-11-08 00:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:18][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 6.180222988128662, acc: 0.0)
[2024-11-08 00:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:18][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 7.628971099853516, acc: 0.0416666679084301)
[2024-11-08 00:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:19][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 6.345128536224365, acc: 0.0)
[2024-11-08 00:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:19][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 6.6951117515563965, acc: 0.0)
[2024-11-08 00:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:19][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 5.792221546173096, acc: 0.03999999910593033)
[2024-11-08 00:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:20][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 5.565186023712158, acc: 0.095238097012043)
[2024-11-08 00:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:20][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 6.274376392364502, acc: 0.15789473056793213)
[2024-11-08 00:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:20][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 6.0141191482543945, acc: 0.10000000149011612)
[2024-11-08 00:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:21][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 7.27454137802124, acc: 0.095238097012043)
[2024-11-08 00:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:21][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 5.46662712097168, acc: 0.09090909361839294)
[2024-11-08 00:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:22][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 6.167999267578125, acc: 0.0714285746216774)
[2024-11-08 00:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:24][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 6.062131404876709, acc: 0.0476190485060215)
[2024-11-08 00:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:26][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 5.214450836181641, acc: 0.1428571492433548)
[2024-11-08 00:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:26][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 5.797534942626953, acc: 0.10526315867900848)
[2024-11-08 00:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:27][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 5.763888835906982, acc: 0.09090909361839294)
[2024-11-08 00:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:27][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 5.659824371337891, acc: 0.3684210479259491)
[2024-11-08 00:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:28][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 5.67388391494751, acc: 0.2380952388048172)
[2024-11-08 00:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:28][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 5.295855522155762, acc: 0.1388888955116272)
[2024-11-08 00:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:29][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 5.445302486419678, acc: 0.0714285746216774)
[2024-11-08 00:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:29][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 4.41820764541626, acc: 0.2380952388048172)
[2024-11-08 00:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:30][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 5.570509910583496, acc: 0.25)
[2024-11-08 00:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:30][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 4.780205726623535, acc: 0.31578946113586426)
[2024-11-08 00:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:30][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 4.972931861877441, acc: 0.30000001192092896)
[2024-11-08 00:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:31][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 5.704770565032959, acc: 0.09090909361839294)
[2024-11-08 00:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:31][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 5.068165302276611, acc: 0.1818181872367859)
[2024-11-08 00:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:31][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 5.563843250274658, acc: 0.2222222238779068)
[2024-11-08 00:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:32][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 4.944694519042969, acc: 0.1666666716337204)
[2024-11-08 00:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:32][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 3.9587061405181885, acc: 0.2380952388048172)
[2024-11-08 00:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:33][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 4.449914455413818, acc: 0.31578946113586426)
[2024-11-08 00:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:33][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 4.293195724487305, acc: 0.30000001192092896)
[2024-11-08 00:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:33][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 4.689733028411865, acc: 0.3333333432674408)
[2024-11-08 00:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:34][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 4.095514297485352, acc: 0.3181818127632141)
[2024-11-08 00:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:34][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 4.786952495574951, acc: 0.25)
[2024-11-08 00:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:35][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 5.127197265625, acc: 0.29629629850387573)
[2024-11-08 00:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:35][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 4.8171305656433105, acc: 0.17142857611179352)
[2024-11-08 00:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:35][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 3.9294722080230713, acc: 0.26923078298568726)
[2024-11-08 00:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:36][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 3.117560863494873, acc: 0.2857142984867096)
[2024-11-08 00:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:36][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 4.286716938018799, acc: 0.31578946113586426)
[2024-11-08 00:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:37][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 3.9878246784210205, acc: 0.3684210479259491)
[2024-11-08 00:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:37][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 4.025278568267822, acc: 0.22727273404598236)
[2024-11-08 00:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:37][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 3.3264694213867188, acc: 0.4285714328289032)
[2024-11-08 00:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:38][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 5.177685737609863, acc: 0.25)
[2024-11-08 00:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:38][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 4.228473663330078, acc: 0.3461538553237915)
[2024-11-08 00:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:39][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.1426162719726562, acc: 0.2857142984867096)
[2024-11-08 00:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:40][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 4.084886074066162, acc: 0.3684210479259491)
[2024-11-08 00:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:41][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 3.5362465381622314, acc: 0.42105263471603394)
[2024-11-08 00:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:42][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 3.978856086730957, acc: 0.3636363744735718)
[2024-11-08 00:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:42][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 3.0314934253692627, acc: 0.380952388048172)
[2024-11-08 00:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:43][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 3.8542304039001465, acc: 0.3076923191547394)
[2024-11-08 00:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:44][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 3.4664533138275146, acc: 0.3214285671710968)
[2024-11-08 00:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:44][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 2.97624135017395, acc: 0.4285714328289032)
[2024-11-08 00:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:45][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 3.2263522148132324, acc: 0.42105263471603394)
[2024-11-08 00:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:45][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 3.26741361618042, acc: 0.4000000059604645)
[2024-11-08 00:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:45][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 3.245651960372925, acc: 0.380952388048172)
[2024-11-08 00:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:46][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 2.9903619289398193, acc: 0.3636363744735718)
[2024-11-08 00:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:46][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 3.5291388034820557, acc: 0.3333333432674408)
[2024-11-08 00:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:46][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 3.1579668521881104, acc: 0.3199999928474426)
[2024-11-08 00:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:47][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 2.6775295734405518, acc: 0.4285714328289032)
[2024-11-08 00:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:47][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 3.32232928276062, acc: 0.4736842215061188)
[2024-11-08 00:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:48][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 2.842317819595337, acc: 0.4545454680919647)
[2024-11-08 00:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:48][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 2.760047674179077, acc: 0.4736842215061188)
[2024-11-08 00:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:48][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 2.877810478210449, acc: 0.3636363744735718)
[2024-11-08 00:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:49][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 3.7414779663085938, acc: 0.2857142984867096)
[2024-11-08 00:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:49][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 3.4194283485412598, acc: 0.23333333432674408)
[2024-11-08 00:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:50][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 1.9340314865112305, acc: 0.4761904776096344)
[2024-11-08 00:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:50][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 2.4268908500671387, acc: 0.4285714328289032)
[2024-11-08 00:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:51][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 2.142958879470825, acc: 0.5789473652839661)
[2024-11-08 00:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:51][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 2.5018413066864014, acc: 0.4545454680919647)
[2024-11-08 00:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:51][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 2.187410593032837, acc: 0.4736842215061188)
[2024-11-08 00:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:52][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 2.673696756362915, acc: 0.3636363744735718)
[2024-11-08 00:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:52][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 2.9218549728393555, acc: 0.42307692766189575)
[2024-11-08 00:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:53][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 2.761120080947876, acc: 0.36000001430511475)
[2024-11-08 00:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:53][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 2.582913637161255, acc: 0.4000000059604645)
[2024-11-08 00:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:53][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 2.095674991607666, acc: 0.6315789222717285)
[2024-11-08 00:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:54][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 2.183824300765991, acc: 0.5454545617103577)
[2024-11-08 00:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:55][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 1.9911904335021973, acc: 0.5263158082962036)
[2024-11-08 00:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:55][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 2.250030279159546, acc: 0.4545454680919647)
[2024-11-08 00:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:55][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 3.0169804096221924, acc: 0.3571428656578064)
[2024-11-08 00:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:56][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 3.0474355220794678, acc: 0.42307692766189575)
[2024-11-08 00:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:56][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 1.863208532333374, acc: 0.5)
[2024-11-08 00:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:56][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 1.9741227626800537, acc: 0.5)
[2024-11-08 00:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:57][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 1.5696980953216553, acc: 0.6315789222717285)
[2024-11-08 00:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:57][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 1.9910838603973389, acc: 0.5454545617103577)
[2024-11-08 00:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:58][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 1.7769540548324585, acc: 0.44999998807907104)
[2024-11-08 00:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:58][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 2.2216908931732178, acc: 0.4545454680919647)
[2024-11-08 00:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:58][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 2.88625431060791, acc: 0.39393940567970276)
[2024-11-08 00:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:59][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.5352916717529297, acc: 0.4444444477558136)
[2024-11-08 00:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:59][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 3.2269699573516846, acc: 0.3333333432674408)
[2024-11-08 00:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:24:59][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 1.4632923603057861, acc: 0.6499999761581421)
[2024-11-08 00:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:00][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 1.6360143423080444, acc: 0.5)
[2024-11-08 00:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:00][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 1.6425292491912842, acc: 0.6315789222717285)
[2024-11-08 00:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:01][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 1.8253566026687622, acc: 0.6363636255264282)
[2024-11-08 00:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:01][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 1.8100521564483643, acc: 0.5)
[2024-11-08 00:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:01][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 1.701310396194458, acc: 0.5)
[2024-11-08 00:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:02][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 2.871021032333374, acc: 0.42424243688583374)
[2024-11-08 00:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:02][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 2.297578811645508, acc: 0.48148149251937866)
[2024-11-08 00:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:02][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 3.2369697093963623, acc: 0.27272728085517883)
[2024-11-08 00:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:03][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 1.4132760763168335, acc: 0.6000000238418579)
[2024-11-08 00:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:35][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(6.6890, device='cuda:0') eval_epoch_loss=tensor(1.9005, device='cuda:0') eval_epoch_acc=tensor(0.5350, device='cuda:0')
[2024-11-08 00:25:35][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:25:35][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:25:35][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_1_step_143_loss_1.9004685878753662/model.pt
[2024-11-08 00:25:35][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:25:35][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.9004685878753662
[2024-11-08 00:25:35][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.535007119178772
[2024-11-08 00:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:35][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 1.6755774021148682, acc: 0.5)
[2024-11-08 00:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:36][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 1.3220583200454712, acc: 0.7368420958518982)
[2024-11-08 00:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:36][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 1.4312957525253296, acc: 0.5454545617103577)
[2024-11-08 00:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:37][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 1.250998616218567, acc: 0.699999988079071)
[2024-11-08 00:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:37][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 1.8147801160812378, acc: 0.5454545617103577)
[2024-11-08 00:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:37][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 2.535604476928711, acc: 0.4545454680919647)
[2024-11-08 00:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:37][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 2.209836959838867, acc: 0.48148149251937866)
[2024-11-08 00:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:38][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 2.70060658454895, acc: 0.3870967626571655)
[2024-11-08 00:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:38][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 1.0395013093948364, acc: 0.800000011920929)
[2024-11-08 00:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:38][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 1.0224730968475342, acc: 0.699999988079071)
[2024-11-08 00:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:39][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 1.1696195602416992, acc: 0.7894737124443054)
[2024-11-08 00:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:39][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 1.4871723651885986, acc: 0.5454545617103577)
[2024-11-08 00:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:40][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 1.1497739553451538, acc: 0.699999988079071)
[2024-11-08 00:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:40][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 1.3207348585128784, acc: 0.6363636255264282)
[2024-11-08 00:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:40][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 2.326528549194336, acc: 0.39393940567970276)
[2024-11-08 00:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:42][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 1.5374820232391357, acc: 0.699999988079071)
[2024-11-08 00:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:42][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 0.5660444498062134, acc: 0.8500000238418579)
[2024-11-08 00:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:43][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 0.9251865744590759, acc: 0.7368420958518982)
[2024-11-08 00:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:43][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 1.4812736511230469, acc: 0.5454545617103577)
[2024-11-08 00:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:44][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 1.0311254262924194, acc: 0.800000011920929)
[2024-11-08 00:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:44][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 1.7833819389343262, acc: 0.5199999809265137)
[2024-11-08 00:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:45][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 1.8480134010314941, acc: 0.5185185074806213)
[2024-11-08 00:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:45][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 2.9056925773620605, acc: 0.3888888955116272)
[2024-11-08 00:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:45][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 1.6502765417099, acc: 0.5652173757553101)
[2024-11-08 00:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:46][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 0.8219731450080872, acc: 0.7142857313156128)
[2024-11-08 00:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:46][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 0.7406309843063354, acc: 0.7368420958518982)
[2024-11-08 00:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:46][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 1.2324485778808594, acc: 0.5454545617103577)
[2024-11-08 00:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:47][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 0.9170110821723938, acc: 0.75)
[2024-11-08 00:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:48][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 0.8615999817848206, acc: 0.7272727489471436)
[2024-11-08 00:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:48][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 1.7083379030227661, acc: 0.5757575631141663)
[2024-11-08 00:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:48][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 1.4167983531951904, acc: 0.6333333253860474)
[2024-11-08 00:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:49][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 1.1192659139633179, acc: 0.6666666865348816)
[2024-11-08 00:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:49][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 0.7332956194877625, acc: 0.8095238208770752)
[2024-11-08 00:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:50][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 0.9370843768119812, acc: 0.5789473652839661)
[2024-11-08 00:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:50][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 1.1050214767456055, acc: 0.6818181872367859)
[2024-11-08 00:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:51][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 0.7323905825614929, acc: 0.7894737124443054)
[2024-11-08 00:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:52][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 0.5898019075393677, acc: 0.8181818127632141)
[2024-11-08 00:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:52][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 1.1588503122329712, acc: 0.7857142686843872)
[2024-11-08 00:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:52][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 1.2422711849212646, acc: 0.6666666865348816)
[2024-11-08 00:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:53][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 2.602656602859497, acc: 0.3142857253551483)
[2024-11-08 00:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:53][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 1.2883877754211426, acc: 0.6086956262588501)
[2024-11-08 00:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:53][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 0.8296351432800293, acc: 0.7142857313156128)
[2024-11-08 00:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:54][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 0.6709856390953064, acc: 0.7894737124443054)
[2024-11-08 00:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:54][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 0.6075390577316284, acc: 0.75)
[2024-11-08 00:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:55][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 0.7513260841369629, acc: 0.6666666865348816)
[2024-11-08 00:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:55][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 0.6142590045928955, acc: 0.7727272510528564)
[2024-11-08 00:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:55][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 1.7492026090621948, acc: 0.5199999809265137)
[2024-11-08 00:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:56][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 0.8852675557136536, acc: 0.699999988079071)
[2024-11-08 00:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:57][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 0.7468295097351074, acc: 0.7142857313156128)
[2024-11-08 00:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:58][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 0.7009879946708679, acc: 0.7368420958518982)
[2024-11-08 00:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:25:59][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 0.41634175181388855, acc: 0.9090909361839294)
[2024-11-08 00:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:00][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 0.5592311024665833, acc: 0.8571428656578064)
[2024-11-08 00:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:01][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 0.9730088114738464, acc: 0.7916666865348816)
[2024-11-08 00:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:01][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 1.2517932653427124, acc: 0.6451612710952759)
[2024-11-08 00:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:01][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 1.2539000511169434, acc: 0.5666666626930237)
[2024-11-08 00:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:01][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 0.5064729452133179, acc: 0.800000011920929)
[2024-11-08 00:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:02][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 0.34852835536003113, acc: 0.8999999761581421)
[2024-11-08 00:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:02][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 0.6233446598052979, acc: 0.7894737124443054)
[2024-11-08 00:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:03][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 0.941195547580719, acc: 0.6363636255264282)
[2024-11-08 00:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:03][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 0.5404542684555054, acc: 0.800000011920929)
[2024-11-08 00:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:03][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 0.6192551255226135, acc: 0.8695651888847351)
[2024-11-08 00:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:04][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 1.5482772588729858, acc: 0.5714285969734192)
[2024-11-08 00:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:04][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 0.32425427436828613, acc: 0.8571428656578064)
[2024-11-08 00:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:04][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 0.416151762008667, acc: 0.8421052694320679)
[2024-11-08 00:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:05][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 0.4300474226474762, acc: 0.7894737124443054)
[2024-11-08 00:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:05][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 0.606006383895874, acc: 0.7727272510528564)
[2024-11-08 00:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:05][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 0.48230117559432983, acc: 0.8571428656578064)
[2024-11-08 00:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:06][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 0.8703622221946716, acc: 0.7083333134651184)
[2024-11-08 00:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:06][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 1.0999044179916382, acc: 0.6774193644523621)
[2024-11-08 00:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:06][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 1.4282219409942627, acc: 0.6129032373428345)
[2024-11-08 00:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:07][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 1.5318458080291748, acc: 0.6538461446762085)
[2024-11-08 00:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:07][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 0.7366499304771423, acc: 0.7142857313156128)
[2024-11-08 00:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:08][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 0.8117680549621582, acc: 0.7368420958518982)
[2024-11-08 00:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:08][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 0.5474599003791809, acc: 0.8421052694320679)
[2024-11-08 00:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:09][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 0.42250630259513855, acc: 0.8181818127632141)
[2024-11-08 00:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:09][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 0.4993794858455658, acc: 0.8095238208770752)
[2024-11-08 00:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:10][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 0.552807629108429, acc: 0.75)
[2024-11-08 00:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:10][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 0.9362258911132812, acc: 0.7096773982048035)
[2024-11-08 00:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:10][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 0.9839510321617126, acc: 0.5483871102333069)
[2024-11-08 00:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:11][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 1.0915027856826782, acc: 0.6538461446762085)
[2024-11-08 00:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:11][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 0.5380445718765259, acc: 0.761904776096344)
[2024-11-08 00:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:12][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 0.6067670583724976, acc: 0.7894737124443054)
[2024-11-08 00:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:12][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 0.575670599937439, acc: 0.7894737124443054)
[2024-11-08 00:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:13][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 0.6314132213592529, acc: 0.7272727489471436)
[2024-11-08 00:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:13][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 0.6652963161468506, acc: 0.8095238208770752)
[2024-11-08 00:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:13][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 0.598516583442688, acc: 0.7083333134651184)
[2024-11-08 00:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:14][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 0.575387716293335, acc: 0.782608687877655)
[2024-11-08 00:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:14][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 0.34431806206703186, acc: 0.8571428656578064)
[2024-11-08 00:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:14][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 0.33542612195014954, acc: 0.8947368264198303)
[2024-11-08 00:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:15][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 0.44396018981933594, acc: 0.7894737124443054)
[2024-11-08 00:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:15][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 0.46768587827682495, acc: 0.8181818127632141)
[2024-11-08 00:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:16][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 0.6779100298881531, acc: 0.7272727489471436)
[2024-11-08 00:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:16][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 0.6381721496582031, acc: 0.7857142686843872)
[2024-11-08 00:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:16][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 0.5463724136352539, acc: 0.8518518805503845)
[2024-11-08 00:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:17][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 1.0616905689239502, acc: 0.6363636255264282)
[2024-11-08 00:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:17][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 0.3632872402667999, acc: 0.9090909361839294)
[2024-11-08 00:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:18][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 0.38406112790107727, acc: 0.8571428656578064)
[2024-11-08 00:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:18][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 0.36175212264060974, acc: 0.8421052694320679)
[2024-11-08 00:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:18][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 0.40531647205352783, acc: 0.8181818127632141)
[2024-11-08 00:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:19][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 0.6161535382270813, acc: 0.7894737124443054)
[2024-11-08 00:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:19][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 0.42854803800582886, acc: 0.8636363744735718)
[2024-11-08 00:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:20][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 0.32332950830459595, acc: 0.9285714030265808)
[2024-11-08 00:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:20][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 0.6298841834068298, acc: 0.8333333134651184)
[2024-11-08 00:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:20][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 0.6626254916191101, acc: 0.6857143044471741)
[2024-11-08 00:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:21][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 0.4543810188770294, acc: 0.8695651888847351)
[2024-11-08 00:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:21][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 0.6002063155174255, acc: 0.7142857313156128)
[2024-11-08 00:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:22][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 0.5549513697624207, acc: 0.7894737124443054)
[2024-11-08 00:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:22][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 0.48143941164016724, acc: 0.75)
[2024-11-08 00:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:22][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 0.5008360743522644, acc: 0.8571428656578064)
[2024-11-08 00:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:23][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 0.45893555879592896, acc: 0.7727272510528564)
[2024-11-08 00:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:23][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 0.3267383575439453, acc: 0.9642857313156128)
[2024-11-08 00:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:23][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 0.5455877780914307, acc: 0.8275862336158752)
[2024-11-08 00:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:23][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 0.5464153289794922, acc: 0.800000011920929)
[2024-11-08 00:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:24][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 0.4192322790622711, acc: 0.761904776096344)
[2024-11-08 00:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:24][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 0.8601407408714294, acc: 0.761904776096344)
[2024-11-08 00:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:25][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 0.39137354493141174, acc: 0.8421052694320679)
[2024-11-08 00:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:25][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 0.4347068965435028, acc: 0.7727272510528564)
[2024-11-08 00:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:26][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 0.5655955672264099, acc: 0.7894737124443054)
[2024-11-08 00:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:26][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 0.554908812046051, acc: 0.7727272510528564)
[2024-11-08 00:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:26][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 0.4237864315509796, acc: 0.8823529481887817)
[2024-11-08 00:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:27][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 0.3011620342731476, acc: 0.9545454382896423)
[2024-11-08 00:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:27][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 0.5457838773727417, acc: 0.800000011920929)
[2024-11-08 00:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:28][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 0.46965253353118896, acc: 0.7368420958518982)
[2024-11-08 00:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:28][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 0.33115115761756897, acc: 0.9130434989929199)
[2024-11-08 00:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:29][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 0.28291764855384827, acc: 0.9523809552192688)
[2024-11-08 00:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:29][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 0.468340128660202, acc: 0.8275862336158752)
[2024-11-08 00:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:29][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 0.4556567370891571, acc: 0.8888888955116272)
[2024-11-08 00:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:30][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 0.5882402658462524, acc: 0.807692289352417)
[2024-11-08 00:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:30][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 0.22750645875930786, acc: 0.9047619104385376)
[2024-11-08 00:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:31][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 0.5312938094139099, acc: 0.7894737124443054)
[2024-11-08 00:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:31][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 0.3684100806713104, acc: 0.7894737124443054)
[2024-11-08 00:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:31][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 0.38722851872444153, acc: 0.8181818127632141)
[2024-11-08 00:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:32][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 0.3919072449207306, acc: 0.9047619104385376)
[2024-11-08 00:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:32][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 0.36850282549858093, acc: 0.8620689511299133)
[2024-11-08 00:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:32][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 0.45246702432632446, acc: 0.8518518805503845)
[2024-11-08 00:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:33][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 0.33856064081192017, acc: 0.761904776096344)
[2024-11-08 00:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:33][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 0.5060330629348755, acc: 0.8571428656578064)
[2024-11-08 00:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:33][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 0.34284457564353943, acc: 0.8888888955116272)
[2024-11-08 00:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:34][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 0.45123738050460815, acc: 0.8181818127632141)
[2024-11-08 00:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:34][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 0.35363703966140747, acc: 0.9047619104385376)
[2024-11-08 00:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:34][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 0.3264223337173462, acc: 0.8620689511299133)
[2024-11-08 00:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:35][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 0.5289267301559448, acc: 0.8333333134651184)
[2024-11-08 00:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:35][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 0.6266530156135559, acc: 0.8965517282485962)
[2024-11-08 00:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:04][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.5651, device='cuda:0') eval_epoch_loss=tensor(0.4480, device='cuda:0') eval_epoch_acc=tensor(0.8407, device='cuda:0')
[2024-11-08 00:27:04][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:27:04][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:27:04][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_1_step_286_loss_0.44796013832092285/model.pt
[2024-11-08 00:27:04][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:27:04][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.44796013832092285
[2024-11-08 00:27:04][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8406923413276672
[2024-11-08 00:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:04][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 0.312828928232193, acc: 0.8571428656578064)
[2024-11-08 00:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:04][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 0.6646785736083984, acc: 0.7894737124443054)
[2024-11-08 00:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:05][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 0.3945278525352478, acc: 0.8421052694320679)
[2024-11-08 00:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:05][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 0.4094787836074829, acc: 0.8636363744735718)
[2024-11-08 00:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:06][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 0.3224571943283081, acc: 0.9090909361839294)
[2024-11-08 00:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:06][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 0.6153892874717712, acc: 0.800000011920929)
[2024-11-08 00:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:06][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 0.549230694770813, acc: 0.800000011920929)
[2024-11-08 00:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:07][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 0.4697408080101013, acc: 0.761904776096344)
[2024-11-08 00:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:07][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 0.33762428164482117, acc: 0.8500000238418579)
[2024-11-08 00:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:08][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 0.4825865626335144, acc: 0.7894737124443054)
[2024-11-08 00:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:08][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 0.43230387568473816, acc: 0.7727272510528564)
[2024-11-08 00:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:08][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 0.6992461681365967, acc: 0.6666666865348816)
[2024-11-08 00:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:09][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 0.3802221715450287, acc: 0.875)
[2024-11-08 00:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:09][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 0.46412310004234314, acc: 0.8709677457809448)
[2024-11-08 00:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:09][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 0.5607355237007141, acc: 0.8064516186714172)
[2024-11-08 00:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:10][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 0.5503348112106323, acc: 0.807692289352417)
[2024-11-08 00:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:10][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 0.3024049401283264, acc: 0.8571428656578064)
[2024-11-08 00:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:10][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 0.44039827585220337, acc: 0.7368420958518982)
[2024-11-08 00:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:11][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 0.4712572693824768, acc: 0.7894737124443054)
[2024-11-08 00:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:11][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 0.3986012041568756, acc: 0.8260869383811951)
[2024-11-08 00:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:11][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 0.4637480676174164, acc: 0.8181818127632141)
[2024-11-08 00:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:12][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 0.44663843512535095, acc: 0.8846153616905212)
[2024-11-08 00:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:12][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 0.22585895657539368, acc: 0.8571428656578064)
[2024-11-08 00:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:12][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 0.6128573417663574, acc: 0.7368420958518982)
[2024-11-08 00:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:13][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 0.2988360524177551, acc: 0.8421052694320679)
[2024-11-08 00:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:13][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 0.44994229078292847, acc: 0.7727272510528564)
[2024-11-08 00:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:14][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 0.4515188932418823, acc: 0.8181818127632141)
[2024-11-08 00:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:14][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 0.3228316009044647, acc: 0.8928571343421936)
[2024-11-08 00:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:14][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 0.5150666236877441, acc: 0.7777777910232544)
[2024-11-08 00:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:15][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 0.774048924446106, acc: 0.7714285850524902)
[2024-11-08 00:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:15][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 0.4930790066719055, acc: 0.807692289352417)
[2024-11-08 00:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:15][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 0.2281588762998581, acc: 0.9047619104385376)
[2024-11-08 00:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:16][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 0.46351560950279236, acc: 0.75)
[2024-11-08 00:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:16][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 0.3257315754890442, acc: 0.949999988079071)
[2024-11-08 00:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:16][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 0.5787932276725769, acc: 0.761904776096344)
[2024-11-08 00:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:17][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 0.3642105758190155, acc: 0.7727272510528564)
[2024-11-08 00:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:17][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 0.42818644642829895, acc: 0.782608687877655)
[2024-11-08 00:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:17][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 0.33773303031921387, acc: 0.8571428656578064)
[2024-11-08 00:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:18][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 0.35695257782936096, acc: 0.7894737124443054)
[2024-11-08 00:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:18][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 0.28714412450790405, acc: 0.8636363744735718)
[2024-11-08 00:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:18][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 0.4023633897304535, acc: 0.7894737124443054)
[2024-11-08 00:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:19][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 0.5871631503105164, acc: 0.7916666865348816)
[2024-11-08 00:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:19][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 0.5486839413642883, acc: 0.8275862336158752)
[2024-11-08 00:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:19][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 0.574000895023346, acc: 0.8148148059844971)
[2024-11-08 00:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:20][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 0.41300496459007263, acc: 0.761904776096344)
[2024-11-08 00:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:20][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 0.3619261682033539, acc: 0.8421052694320679)
[2024-11-08 00:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:20][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 0.301899790763855, acc: 0.8636363744735718)
[2024-11-08 00:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:21][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 0.5147247910499573, acc: 0.7894737124443054)
[2024-11-08 00:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:21][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 0.42164069414138794, acc: 0.7727272510528564)
[2024-11-08 00:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:21][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 0.46544408798217773, acc: 0.8461538553237915)
[2024-11-08 00:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:22][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 0.42118170857429504, acc: 0.9166666865348816)
[2024-11-08 00:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:22][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 0.4398532211780548, acc: 0.761904776096344)
[2024-11-08 00:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:22][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 0.2516486346721649, acc: 0.8695651888847351)
[2024-11-08 00:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:23][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 0.2882903814315796, acc: 0.8636363744735718)
[2024-11-08 00:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:23][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 0.5809364318847656, acc: 0.8333333134651184)
[2024-11-08 00:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:23][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 0.3965580463409424, acc: 0.8181818127632141)
[2024-11-08 00:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:24][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 0.46701550483703613, acc: 0.800000011920929)
[2024-11-08 00:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:24][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 0.42285043001174927, acc: 0.8500000238418579)
[2024-11-08 00:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:25][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 0.5619904398918152, acc: 0.8421052694320679)
[2024-11-08 00:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:25][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 0.32304513454437256, acc: 0.8636363744735718)
[2024-11-08 00:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:25][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 0.5786481499671936, acc: 0.699999988079071)
[2024-11-08 00:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:25][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 0.3644808530807495, acc: 0.875)
[2024-11-08 00:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:26][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 0.4944239854812622, acc: 0.8260869383811951)
[2024-11-08 00:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:26][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 0.19965860247612, acc: 0.8999999761581421)
[2024-11-08 00:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:27][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 0.4093611240386963, acc: 0.7368420958518982)
[2024-11-08 00:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:27][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 0.29607632756233215, acc: 0.8181818127632141)
[2024-11-08 00:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:27][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 0.4526239037513733, acc: 0.800000011920929)
[2024-11-08 00:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:28][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 0.4851265847682953, acc: 0.8799999952316284)
[2024-11-08 00:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:28][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 0.4683611989021301, acc: 0.8399999737739563)
[2024-11-08 00:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:28][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 0.4278540313243866, acc: 0.761904776096344)
[2024-11-08 00:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:29][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 0.3338358998298645, acc: 0.8500000238418579)
[2024-11-08 00:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:29][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 0.2835036814212799, acc: 0.9047619104385376)
[2024-11-08 00:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:30][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 0.33962950110435486, acc: 0.8636363744735718)
[2024-11-08 00:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:30][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 0.3161124587059021, acc: 0.8928571343421936)
[2024-11-08 00:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:30][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 0.6221924424171448, acc: 0.7931034564971924)
[2024-11-08 00:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:31][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 0.479227215051651, acc: 0.8857142925262451)
[2024-11-08 00:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:31][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 0.3067992031574249, acc: 0.8636363744735718)
[2024-11-08 00:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:31][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 0.4821767807006836, acc: 0.7894737124443054)
[2024-11-08 00:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:32][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 0.22876393795013428, acc: 0.9523809552192688)
[2024-11-08 00:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:32][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 0.5963353514671326, acc: 0.8421052694320679)
[2024-11-08 00:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:32][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 0.3908199369907379, acc: 0.8636363744735718)
[2024-11-08 00:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:33][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 0.4018273949623108, acc: 0.8888888955116272)
[2024-11-08 00:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:33][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 0.3575059175491333, acc: 0.84375)
[2024-11-08 00:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:34][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 0.4588983952999115, acc: 0.8095238208770752)
[2024-11-08 00:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:34][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 0.4377376437187195, acc: 0.761904776096344)
[2024-11-08 00:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:35][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 0.27784979343414307, acc: 0.8947368264198303)
[2024-11-08 00:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:36][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 0.29482653737068176, acc: 0.8636363744735718)
[2024-11-08 00:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:36][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 0.3937930166721344, acc: 0.8947368264198303)
[2024-11-08 00:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:36][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 0.3805128037929535, acc: 0.8181818127632141)
[2024-11-08 00:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:37][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.30516722798347473, acc: 0.8965517282485962)
[2024-11-08 00:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:37][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 0.39320075511932373, acc: 0.8387096524238586)
[2024-11-08 00:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:37][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 0.36884206533432007, acc: 0.8571428656578064)
[2024-11-08 00:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:38][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 0.34949949383735657, acc: 0.761904776096344)
[2024-11-08 00:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:38][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 0.23659813404083252, acc: 0.8421052694320679)
[2024-11-08 00:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:39][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 0.2670954465866089, acc: 0.8181818127632141)
[2024-11-08 00:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:40][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 0.4427962005138397, acc: 0.8947368264198303)
[2024-11-08 00:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:40][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 0.40072375535964966, acc: 0.8636363744735718)
[2024-11-08 00:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:41][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 0.19664117693901062, acc: 0.9285714030265808)
[2024-11-08 00:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:41][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 0.25097885727882385, acc: 0.9333333373069763)
[2024-11-08 00:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:41][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 0.3768443465232849, acc: 0.8571428656578064)
[2024-11-08 00:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:42][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 0.4416387975215912, acc: 0.8500000238418579)
[2024-11-08 00:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:42][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 0.7817451357841492, acc: 0.7894737124443054)
[2024-11-08 00:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:42][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 0.32866936922073364, acc: 0.8500000238418579)
[2024-11-08 00:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:43][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 0.5576862692832947, acc: 0.8500000238418579)
[2024-11-08 00:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:43][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 0.3440973162651062, acc: 0.9090909361839294)
[2024-11-08 00:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:43][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 0.13615453243255615, acc: 0.9523809552192688)
[2024-11-08 00:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:44][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 0.5044756531715393, acc: 0.8947368264198303)
[2024-11-08 00:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:44][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 0.40957507491111755, acc: 0.7894737124443054)
[2024-11-08 00:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:45][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 0.4266936480998993, acc: 0.8181818127632141)
[2024-11-08 00:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:45][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 0.3408198356628418, acc: 0.8571428656578064)
[2024-11-08 00:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:45][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 0.4725870192050934, acc: 0.8620689511299133)
[2024-11-08 00:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:46][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 0.7731425762176514, acc: 0.7666666507720947)
[2024-11-08 00:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:46][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 0.27377310395240784, acc: 0.8095238208770752)
[2024-11-08 00:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:46][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 0.35049960017204285, acc: 0.8333333134651184)
[2024-11-08 00:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:47][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 0.2730856239795685, acc: 0.8636363744735718)
[2024-11-08 00:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:47][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 0.22644193470478058, acc: 0.9047619104385376)
[2024-11-08 00:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:48][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 0.610137403011322, acc: 0.7916666865348816)
[2024-11-08 00:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:48][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 0.3782246708869934, acc: 0.90625)
[2024-11-08 00:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:48][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 0.4324191212654114, acc: 0.8571428656578064)
[2024-11-08 00:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:49][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 0.4070946276187897, acc: 0.8461538553237915)
[2024-11-08 00:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:49][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 0.43053144216537476, acc: 0.8095238208770752)
[2024-11-08 00:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:49][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 0.5083986520767212, acc: 0.7894737124443054)
[2024-11-08 00:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:50][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 0.4221814274787903, acc: 0.800000011920929)
[2024-11-08 00:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:50][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 0.5736597776412964, acc: 0.761904776096344)
[2024-11-08 00:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:50][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 0.36467015743255615, acc: 0.8636363744735718)
[2024-11-08 00:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:51][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 0.3089890778064728, acc: 0.8928571343421936)
[2024-11-08 00:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:51][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 0.4160623550415039, acc: 0.8518518805503845)
[2024-11-08 00:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:51][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 0.305637925863266, acc: 0.9142857193946838)
[2024-11-08 00:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:51][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 0.36644446849823, acc: 0.807692289352417)
[2024-11-08 00:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:52][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 0.28229984641075134, acc: 0.8571428656578064)
[2024-11-08 00:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:52][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 0.5585743188858032, acc: 0.7894737124443054)
[2024-11-08 00:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:52][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 0.421989381313324, acc: 0.8947368264198303)
[2024-11-08 00:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:53][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 0.34499791264533997, acc: 0.8636363744735718)
[2024-11-08 00:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:53][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 0.435994952917099, acc: 0.761904776096344)
[2024-11-08 00:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:53][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.3145754039287567, acc: 0.9166666865348816)
[2024-11-08 00:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:54][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 0.40564456582069397, acc: 0.8387096524238586)
[2024-11-08 00:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:54][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 0.35159894824028015, acc: 0.8709677457809448)
[2024-11-08 00:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:54][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 0.4042142331600189, acc: 0.807692289352417)
[2024-11-08 00:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:55][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 0.2484617531299591, acc: 0.8095238208770752)
[2024-11-08 00:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:55][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 0.42835190892219543, acc: 0.7894737124443054)
[2024-11-08 00:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:55][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 0.2997263967990875, acc: 0.8947368264198303)
[2024-11-08 00:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:56][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 0.3917733430862427, acc: 0.8181818127632141)
[2024-11-08 00:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:56][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 0.35604968667030334, acc: 0.9047619104385376)
[2024-11-08 00:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:25][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3844, device='cuda:0') eval_epoch_loss=tensor(0.3253, device='cuda:0') eval_epoch_acc=tensor(0.8685, device='cuda:0')
[2024-11-08 00:28:25][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:28:25][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:28:25][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_1_step_429_loss_0.3252936899662018/model.pt
[2024-11-08 00:28:25][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:28:25][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.3252936899662018
[2024-11-08 00:28:25][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8684862852096558
[2024-11-08 00:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:26][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 0.37819337844848633, acc: 0.7916666865348816)
[2024-11-08 00:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:26][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.2810586392879486, acc: 0.9032257795333862)
[2024-11-08 00:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:26][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 0.3367217183113098, acc: 0.8387096524238586)
[2024-11-08 00:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:27][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 0.25401172041893005, acc: 0.8846153616905212)
[2024-11-08 00:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:27][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 0.18820813298225403, acc: 0.9047619104385376)
[2024-11-08 00:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:27][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.3488546311855316, acc: 0.8421052694320679)
[2024-11-08 00:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:28][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 0.3835771083831787, acc: 0.7894737124443054)
[2024-11-08 00:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:28][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 0.30771467089653015, acc: 0.8636363744735718)
[2024-11-08 00:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:29][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 0.4274173378944397, acc: 0.8571428656578064)
[2024-11-08 00:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:29][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.26944872736930847, acc: 0.9090909361839294)
[2024-11-08 00:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:29][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 0.3377767503261566, acc: 0.8571428656578064)
[2024-11-08 00:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:30][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 0.20585797727108002, acc: 0.9047619104385376)
[2024-11-08 00:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:30][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 0.47342801094055176, acc: 0.7894737124443054)
[2024-11-08 00:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:31][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 0.3363478481769562, acc: 0.7368420958518982)
[2024-11-08 00:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:32][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 0.269416868686676, acc: 0.9545454382896423)
[2024-11-08 00:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:32][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 0.25774797797203064, acc: 0.9523809552192688)
[2024-11-08 00:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:32][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 0.2324453592300415, acc: 0.875)
[2024-11-08 00:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:33][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 0.5568806529045105, acc: 0.8709677457809448)
[2024-11-08 00:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:33][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 0.2377084195613861, acc: 0.9032257795333862)
[2024-11-08 00:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:33][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 0.2786422073841095, acc: 0.8709677457809448)
[2024-11-08 00:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:34][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 0.43016189336776733, acc: 0.800000011920929)
[2024-11-08 00:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:34][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 0.2393103539943695, acc: 0.8999999761581421)
[2024-11-08 00:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:34][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 0.7834076881408691, acc: 0.7894737124443054)
[2024-11-08 00:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:35][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 0.36657917499542236, acc: 0.7727272510528564)
[2024-11-08 00:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:35][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 0.48034805059432983, acc: 0.75)
[2024-11-08 00:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:36][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 0.23180800676345825, acc: 0.9090909361839294)
[2024-11-08 00:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:36][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 0.12710979580879211, acc: 0.9677419066429138)
[2024-11-08 00:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:36][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 0.33109575510025024, acc: 0.8500000238418579)
[2024-11-08 00:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:37][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 0.34347230195999146, acc: 0.800000011920929)
[2024-11-08 00:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:37][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 0.32226499915122986, acc: 0.9473684430122375)
[2024-11-08 00:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:37][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 0.30618780851364136, acc: 0.8181818127632141)
[2024-11-08 00:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:38][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 0.4056953489780426, acc: 0.8999999761581421)
[2024-11-08 00:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:38][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 0.24415577948093414, acc: 0.9545454382896423)
[2024-11-08 00:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:38][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 0.2598232626914978, acc: 0.9090909361839294)
[2024-11-08 00:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:39][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 0.5147904753684998, acc: 0.7777777910232544)
[2024-11-08 00:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:39][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 0.2256525754928589, acc: 0.939393937587738)
[2024-11-08 00:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:39][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 0.31764155626296997, acc: 0.8500000238418579)
[2024-11-08 00:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:40][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 0.32689064741134644, acc: 0.8999999761581421)
[2024-11-08 00:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:40][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 0.4551437795162201, acc: 0.7894737124443054)
[2024-11-08 00:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:40][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 0.3760220408439636, acc: 0.800000011920929)
[2024-11-08 00:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:41][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 0.335114061832428, acc: 0.8571428656578064)
[2024-11-08 00:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:41][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 0.3378562033176422, acc: 0.875)
[2024-11-08 00:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:42][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 0.24765095114707947, acc: 0.8999999761581421)
[2024-11-08 00:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:42][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 0.5361472964286804, acc: 0.7916666865348816)
[2024-11-08 00:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:42][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 0.4174478054046631, acc: 0.8095238208770752)
[2024-11-08 00:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:43][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 0.317865252494812, acc: 0.8947368264198303)
[2024-11-08 00:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:43][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 0.34535226225852966, acc: 0.800000011920929)
[2024-11-08 00:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:44][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 0.5847324132919312, acc: 0.761904776096344)
[2024-11-08 00:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:44][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 0.40392133593559265, acc: 0.9090909361839294)
[2024-11-08 00:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:44][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 0.3340323269367218, acc: 0.8928571343421936)
[2024-11-08 00:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:45][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 0.5461984872817993, acc: 0.774193525314331)
[2024-11-08 00:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:45][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 0.4008866846561432, acc: 0.8709677457809448)
[2024-11-08 00:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:45][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 0.5943629145622253, acc: 0.8095238208770752)
[2024-11-08 00:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:45][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 0.5690168738365173, acc: 0.8421052694320679)
[2024-11-08 00:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:46][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.2685099840164185, acc: 0.5909090638160706)
[2024-11-08 00:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:46][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 0.47764357924461365, acc: 0.8181818127632141)
[2024-11-08 00:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:47][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 0.23193998634815216, acc: 0.9642857313156128)
[2024-11-08 00:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:47][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 0.4433335065841675, acc: 0.8518518805503845)
[2024-11-08 00:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:47][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 0.4460318684577942, acc: 0.8285714387893677)
[2024-11-08 00:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:48][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 0.7710987329483032, acc: 0.7692307829856873)
[2024-11-08 00:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:48][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 0.22118139266967773, acc: 0.9047619104385376)
[2024-11-08 00:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:48][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 0.479447603225708, acc: 0.7894737124443054)
[2024-11-08 00:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:49][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 0.5156856179237366, acc: 0.8421052694320679)
[2024-11-08 00:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:49][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 0.31223830580711365, acc: 0.8636363744735718)
[2024-11-08 00:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:49][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 0.3923911452293396, acc: 0.8571428656578064)
[2024-11-08 00:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:50][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.594566285610199, acc: 0.7916666865348816)
[2024-11-08 00:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:50][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 0.5285693407058716, acc: 0.8214285969734192)
[2024-11-08 00:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:50][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 0.254504919052124, acc: 0.8999999761581421)
[2024-11-08 00:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:51][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 0.23189620673656464, acc: 0.949999988079071)
[2024-11-08 00:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:51][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 0.3967958688735962, acc: 0.8421052694320679)
[2024-11-08 00:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:52][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 0.2593111991882324, acc: 0.8636363744735718)
[2024-11-08 00:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:52][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 0.5660487413406372, acc: 0.800000011920929)
[2024-11-08 00:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:52][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.2398538738489151, acc: 0.9583333134651184)
[2024-11-08 00:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:53][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 0.3927527964115143, acc: 0.8709677457809448)
[2024-11-08 00:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:53][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 0.36739566922187805, acc: 0.8387096524238586)
[2024-11-08 00:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:53][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 0.3572685718536377, acc: 0.8695651888847351)
[2024-11-08 00:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:54][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 0.4681892991065979, acc: 0.8095238208770752)
[2024-11-08 00:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:54][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 0.5776622891426086, acc: 0.7368420958518982)
[2024-11-08 00:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:54][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 0.19768989086151123, acc: 0.8500000238418579)
[2024-11-08 00:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:55][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 0.5806127786636353, acc: 0.699999988079071)
[2024-11-08 00:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:55][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.46198710799217224, acc: 0.7727272510528564)
[2024-11-08 00:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:56][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.23126398026943207, acc: 0.9090909361839294)
[2024-11-08 00:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:56][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 0.6763234734535217, acc: 0.7333333492279053)
[2024-11-08 00:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:58][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 0.3437138497829437, acc: 0.8214285969734192)
[2024-11-08 00:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:59][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 0.29242634773254395, acc: 0.8571428656578064)
[2024-11-08 00:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:28:59][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 0.3722480237483978, acc: 0.7894737124443054)
[2024-11-08 00:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:00][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 0.4179378151893616, acc: 0.8500000238418579)
[2024-11-08 00:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:00][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 0.39231735467910767, acc: 0.8571428656578064)
[2024-11-08 00:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:01][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.28904467821121216, acc: 0.8636363744735718)
[2024-11-08 00:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:01][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 0.5832552909851074, acc: 0.8571428656578064)
[2024-11-08 00:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:01][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 0.3841088116168976, acc: 0.8888888955116272)
[2024-11-08 00:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:02][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 0.3713022768497467, acc: 0.8857142925262451)
[2024-11-08 00:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:03][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 0.17841285467147827, acc: 0.9130434989929199)
[2024-11-08 00:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:03][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 0.3530479073524475, acc: 0.8095238208770752)
[2024-11-08 00:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:04][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 0.7976880669593811, acc: 0.7894737124443054)
[2024-11-08 00:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:04][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 0.2482142299413681, acc: 0.8636363744735718)
[2024-11-08 00:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:05][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 0.4561385214328766, acc: 0.8500000238418579)
[2024-11-08 00:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:05][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 0.4943958520889282, acc: 0.8181818127632141)
[2024-11-08 00:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:05][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 0.3572799265384674, acc: 0.8787878751754761)
[2024-11-08 00:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:06][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 0.6339423060417175, acc: 0.8214285969734192)
[2024-11-08 00:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:06][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 0.27068445086479187, acc: 0.8571428656578064)
[2024-11-08 00:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:06][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 0.5667518377304077, acc: 0.7894737124443054)
[2024-11-08 00:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:07][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 0.319847971200943, acc: 0.8999999761581421)
[2024-11-08 00:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:07][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 0.4832490384578705, acc: 0.8095238208770752)
[2024-11-08 00:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:07][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 0.2977217733860016, acc: 0.8181818127632141)
[2024-11-08 00:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:08][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 0.26416271924972534, acc: 0.9285714030265808)
[2024-11-08 00:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:08][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 0.29623353481292725, acc: 0.8888888955116272)
[2024-11-08 00:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:09][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 0.7100399732589722, acc: 0.7599999904632568)
[2024-11-08 00:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:09][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 0.35151761770248413, acc: 0.8095238208770752)
[2024-11-08 00:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:10][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 0.368864506483078, acc: 0.8421052694320679)
[2024-11-08 00:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:10][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 0.29291996359825134, acc: 0.8999999761581421)
[2024-11-08 00:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:10][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 0.45991575717926025, acc: 0.8571428656578064)
[2024-11-08 00:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:11][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.36888590455055237, acc: 0.8571428656578064)
[2024-11-08 00:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:11][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 0.18598341941833496, acc: 0.939393937587738)
[2024-11-08 00:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:11][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.3457624912261963, acc: 0.8518518805503845)
[2024-11-08 00:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:12][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 0.39179331064224243, acc: 0.8484848737716675)
[2024-11-08 00:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:12][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 0.36442506313323975, acc: 0.8500000238418579)
[2024-11-08 00:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:12][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 0.290641725063324, acc: 0.8500000238418579)
[2024-11-08 00:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:13][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 0.35316160321235657, acc: 0.9047619104385376)
[2024-11-08 00:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:13][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 0.3077896237373352, acc: 0.8571428656578064)
[2024-11-08 00:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:13][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.554405152797699, acc: 0.8636363744735718)
[2024-11-08 00:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:14][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 0.20769862830638885, acc: 0.931034505367279)
[2024-11-08 00:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:14][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 0.29081711173057556, acc: 0.8387096524238586)
[2024-11-08 00:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:14][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 0.30803191661834717, acc: 0.9259259104728699)
[2024-11-08 00:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:15][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 0.271538108587265, acc: 0.9047619104385376)
[2024-11-08 00:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:15][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 0.42039355635643005, acc: 0.7894737124443054)
[2024-11-08 00:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:16][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 0.34693652391433716, acc: 0.8500000238418579)
[2024-11-08 00:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:16][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 0.424550324678421, acc: 0.8571428656578064)
[2024-11-08 00:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:16][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 0.3711912930011749, acc: 0.8636363744735718)
[2024-11-08 00:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:17][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.2542083263397217, acc: 0.9285714030265808)
[2024-11-08 00:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:17][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 0.21016179025173187, acc: 0.9259259104728699)
[2024-11-08 00:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:17][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 0.2771828770637512, acc: 0.939393937587738)
[2024-11-08 00:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:17][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 0.44431641697883606, acc: 0.8095238208770752)
[2024-11-08 00:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:18][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 0.3597707450389862, acc: 0.8571428656578064)
[2024-11-08 00:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:18][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 0.5384390950202942, acc: 0.8333333134651184)
[2024-11-08 00:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:18][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 0.3685862720012665, acc: 0.8636363744735718)
[2024-11-08 00:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:19][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 0.47360658645629883, acc: 0.75)
[2024-11-08 00:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:19][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 0.1826164573431015, acc: 0.9545454382896423)
[2024-11-08 00:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:19][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 0.2672359049320221, acc: 0.8888888955116272)
[2024-11-08 00:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:20][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 0.2278161346912384, acc: 0.9032257795333862)
[2024-11-08 00:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:20][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 0.40000277757644653, acc: 0.800000011920929)
[2024-11-08 00:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:20][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 0.2477661669254303, acc: 0.8500000238418579)
[2024-11-08 00:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:21][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 0.4893361032009125, acc: 0.7368420958518982)
[2024-11-08 00:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:49][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4146, device='cuda:0') eval_epoch_loss=tensor(0.3468, device='cuda:0') eval_epoch_acc=tensor(0.8626, device='cuda:0')
[2024-11-08 00:29:49][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:29:49][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:29:49][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_1_step_572_loss_0.3468300402164459/model.pt
[2024-11-08 00:29:49][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:50][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 0.3131556808948517, acc: 0.9090909361839294)
[2024-11-08 00:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:50][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 0.39557456970214844, acc: 0.8500000238418579)
[2024-11-08 00:29:51][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=5.2638, train_epoch_loss=1.6608, epoch time 360.29812774062157s
[2024-11-08 00:29:51][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 10 GB
[2024-11-08 00:29:51][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 00:29:51][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 10 GB
[2024-11-08 00:29:51][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 00:29:51][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 00:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:51][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 0.38796713948249817, acc: 0.8928571343421936)
[2024-11-08 00:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:52][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 0.40461859107017517, acc: 0.8148148059844971)
[2024-11-08 00:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:52][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 0.4894302487373352, acc: 0.8857142925262451)
[2024-11-08 00:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:52][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 0.3701309561729431, acc: 0.8571428656578064)
[2024-11-08 00:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:53][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 0.6041182279586792, acc: 0.8095238208770752)
[2024-11-08 00:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:53][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 0.6196938157081604, acc: 0.7894737124443054)
[2024-11-08 00:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:53][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 0.4189990758895874, acc: 0.8636363744735718)
[2024-11-08 00:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:54][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 0.49753689765930176, acc: 0.8095238208770752)
[2024-11-08 00:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:54][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.4894595444202423, acc: 0.75)
[2024-11-08 00:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:54][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.3661666512489319, acc: 0.9032257795333862)
[2024-11-08 00:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:55][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 0.3662499189376831, acc: 0.8709677457809448)
[2024-11-08 00:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:55][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 0.371101975440979, acc: 0.8799999952316284)
[2024-11-08 00:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:55][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 0.20028583705425262, acc: 0.8999999761581421)
[2024-11-08 00:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:56][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 0.3276340961456299, acc: 0.8421052694320679)
[2024-11-08 00:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:56][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 0.4050101041793823, acc: 0.9130434989929199)
[2024-11-08 00:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:56][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 0.29583460092544556, acc: 0.8636363744735718)
[2024-11-08 00:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:56][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.22526893019676208, acc: 0.9642857313156128)
[2024-11-08 00:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:57][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 0.2847960889339447, acc: 0.8928571343421936)
[2024-11-08 00:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:57][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 0.2813390791416168, acc: 0.8636363744735718)
[2024-11-08 00:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:57][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 0.532453179359436, acc: 0.7894737124443054)
[2024-11-08 00:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:58][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 0.35696548223495483, acc: 0.8947368264198303)
[2024-11-08 00:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:58][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 0.4186447262763977, acc: 0.8181818127632141)
[2024-11-08 00:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:58][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 0.3815118372440338, acc: 0.8571428656578064)
[2024-11-08 00:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:59][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 0.22656527161598206, acc: 0.9523809552192688)
[2024-11-08 00:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:59][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 0.49407678842544556, acc: 0.8333333134651184)
[2024-11-08 00:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:29:59][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 0.4360060393810272, acc: 0.7142857313156128)
[2024-11-08 00:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:00][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 0.49530360102653503, acc: 0.8095238208770752)
[2024-11-08 00:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:01][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 0.2919226884841919, acc: 0.8421052694320679)
[2024-11-08 00:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:01][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 0.3025076687335968, acc: 0.8181818127632141)
[2024-11-08 00:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:02][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 0.4339194893836975, acc: 0.8500000238418579)
[2024-11-08 00:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:02][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 0.6261699199676514, acc: 0.8636363744735718)
[2024-11-08 00:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:02][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 0.45152097940444946, acc: 0.8275862336158752)
[2024-11-08 00:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:02][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 0.5275543332099915, acc: 0.8484848737716675)
[2024-11-08 00:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:03][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 0.2985351085662842, acc: 0.9130434989929199)
[2024-11-08 00:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:03][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 0.20116740465164185, acc: 0.8571428656578064)
[2024-11-08 00:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:03][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 0.4692474603652954, acc: 0.7894737124443054)
[2024-11-08 00:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:04][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 0.3883746266365051, acc: 0.800000011920929)
[2024-11-08 00:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:04][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 0.5176612138748169, acc: 0.761904776096344)
[2024-11-08 00:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:04][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 0.3198400139808655, acc: 0.8636363744735718)
[2024-11-08 00:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:05][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 0.255808025598526, acc: 0.9285714030265808)
[2024-11-08 00:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:05][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 0.35799941420555115, acc: 0.8571428656578064)
[2024-11-08 00:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:05][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 0.28157758712768555, acc: 0.8999999761581421)
[2024-11-08 00:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:06][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 0.25622016191482544, acc: 0.8999999761581421)
[2024-11-08 00:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:06][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 0.5371060371398926, acc: 0.7894737124443054)
[2024-11-08 00:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:06][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 0.3288210928440094, acc: 0.8181818127632141)
[2024-11-08 00:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:07][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 0.3588690757751465, acc: 0.8571428656578064)
[2024-11-08 00:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:07][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.2520529627799988, acc: 0.9166666865348816)
[2024-11-08 00:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:07][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.3394038677215576, acc: 0.8387096524238586)
[2024-11-08 00:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:08][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 0.31451812386512756, acc: 0.8571428656578064)
[2024-11-08 00:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:08][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 0.24339494109153748, acc: 0.8799999952316284)
[2024-11-08 00:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:08][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 0.3944213092327118, acc: 0.761904776096344)
[2024-11-08 00:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:09][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 0.41190218925476074, acc: 0.8421052694320679)
[2024-11-08 00:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:09][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 0.3277852237224579, acc: 0.8999999761581421)
[2024-11-08 00:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:09][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 0.36870381236076355, acc: 0.8571428656578064)
[2024-11-08 00:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:10][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 0.5366640090942383, acc: 0.8181818127632141)
[2024-11-08 00:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:10][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.16550573706626892, acc: 0.9642857313156128)
[2024-11-08 00:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:13][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 0.3813009262084961, acc: 0.8571428656578064)
[2024-11-08 00:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:14][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 0.27365046739578247, acc: 0.8095238208770752)
[2024-11-08 00:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:14][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 0.3869864046573639, acc: 0.7894737124443054)
[2024-11-08 00:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:15][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 0.25969943404197693, acc: 0.8181818127632141)
[2024-11-08 00:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:15][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 0.4116581678390503, acc: 0.8947368264198303)
[2024-11-08 00:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:16][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 0.2550774812698364, acc: 0.8571428656578064)
[2024-11-08 00:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:16][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 0.27641621232032776, acc: 0.8888888955116272)
[2024-11-08 00:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:16][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 0.2981841266155243, acc: 0.8571428656578064)
[2024-11-08 00:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:17][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 0.3041296899318695, acc: 0.8095238208770752)
[2024-11-08 00:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:17][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.45195889472961426, acc: 0.800000011920929)
[2024-11-08 00:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:17][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 0.27908027172088623, acc: 0.9473684430122375)
[2024-11-08 00:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:18][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 0.4075475335121155, acc: 0.800000011920929)
[2024-11-08 00:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:18][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.11525499075651169, acc: 1.0)
[2024-11-08 00:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:18][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 0.1695372313261032, acc: 0.939393937587738)
[2024-11-08 00:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:19][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 0.4193986654281616, acc: 0.9259259104728699)
[2024-11-08 00:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:19][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 0.44838589429855347, acc: 0.8333333134651184)
[2024-11-08 00:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:19][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 0.22188572585582733, acc: 0.8571428656578064)
[2024-11-08 00:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:20][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 0.46557939052581787, acc: 0.7894737124443054)
[2024-11-08 00:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:20][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 0.30271637439727783, acc: 0.8500000238418579)
[2024-11-08 00:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:20][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 0.3678757846355438, acc: 0.8095238208770752)
[2024-11-08 00:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:21][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 0.3710569143295288, acc: 0.8636363744735718)
[2024-11-08 00:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:21][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.12390226125717163, acc: 0.9642857313156128)
[2024-11-08 00:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:21][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.23142802715301514, acc: 0.9259259104728699)
[2024-11-08 00:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:22][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 0.3188514709472656, acc: 0.9142857193946838)
[2024-11-08 00:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:22][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 0.30529701709747314, acc: 0.8461538553237915)
[2024-11-08 00:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:22][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 0.20601190626621246, acc: 0.8571428656578064)
[2024-11-08 00:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:23][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 0.42125341296195984, acc: 0.7894737124443054)
[2024-11-08 00:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:23][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 0.3070213198661804, acc: 0.8421052694320679)
[2024-11-08 00:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:23][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 0.333098441362381, acc: 0.9090909361839294)
[2024-11-08 00:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:24][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 0.3563859462738037, acc: 0.8095238208770752)
[2024-11-08 00:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:24][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 0.14586971700191498, acc: 1.0)
[2024-11-08 00:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:24][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 0.2504540681838989, acc: 0.8846153616905212)
[2024-11-08 00:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:25][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 0.17164012789726257, acc: 0.8571428656578064)
[2024-11-08 00:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:26][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 0.35602402687072754, acc: 0.8421052694320679)
[2024-11-08 00:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:26][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 0.40114808082580566, acc: 0.7894737124443054)
[2024-11-08 00:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:27][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 0.41502806544303894, acc: 0.8181818127632141)
[2024-11-08 00:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:28][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 0.3081414997577667, acc: 0.9047619104385376)
[2024-11-08 00:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:29][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 0.770313024520874, acc: 0.7692307829856873)
[2024-11-08 00:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:29][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 0.29291316866874695, acc: 0.8214285969734192)
[2024-11-08 00:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:30][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 0.3355570435523987, acc: 0.8095238208770752)
[2024-11-08 00:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:30][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 0.4322938323020935, acc: 0.8421052694320679)
[2024-11-08 00:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:30][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 0.2757108211517334, acc: 0.8999999761581421)
[2024-11-08 00:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:31][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 0.38014212250709534, acc: 0.8095238208770752)
[2024-11-08 00:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:31][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 0.33197346329689026, acc: 0.7727272510528564)
[2024-11-08 00:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:31][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 0.24217090010643005, acc: 0.9523809552192688)
[2024-11-08 00:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:32][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.811931312084198, acc: 0.7200000286102295)
[2024-11-08 00:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:32][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 0.394096314907074, acc: 0.8571428656578064)
[2024-11-08 00:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:32][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 0.32197844982147217, acc: 0.8421052694320679)
[2024-11-08 00:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:32][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 0.19515633583068848, acc: 1.0)
[2024-11-08 00:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:33][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 0.3688448965549469, acc: 0.8421052694320679)
[2024-11-08 00:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:33][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 0.2855466902256012, acc: 0.9090909361839294)
[2024-11-08 00:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:33][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.1637948751449585, acc: 0.9285714030265808)
[2024-11-08 00:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:34][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.5043387413024902, acc: 0.8333333134651184)
[2024-11-08 00:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:34][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 0.4558042883872986, acc: 0.8095238208770752)
[2024-11-08 00:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:34][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 0.4240139424800873, acc: 0.8095238208770752)
[2024-11-08 00:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:35][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 0.22038686275482178, acc: 0.9473684430122375)
[2024-11-08 00:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:35][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 0.20934580266475677, acc: 0.9545454382896423)
[2024-11-08 00:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:35][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 0.32236984372138977, acc: 0.8947368264198303)
[2024-11-08 00:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:36][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 0.24459391832351685, acc: 0.9090909361839294)
[2024-11-08 00:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:36][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.2804334759712219, acc: 0.8846153616905212)
[2024-11-08 00:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:36][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 0.42584455013275146, acc: 0.8799999952316284)
[2024-11-08 00:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:37][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 0.3629169166088104, acc: 0.8500000238418579)
[2024-11-08 00:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:37][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 0.3410051167011261, acc: 0.7894737124443054)
[2024-11-08 00:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:38][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 0.21489819884300232, acc: 0.9090909361839294)
[2024-11-08 00:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:38][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 0.5409815311431885, acc: 0.7368420958518982)
[2024-11-08 00:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:39][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 0.666570246219635, acc: 0.8181818127632141)
[2024-11-08 00:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:39][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.18248669803142548, acc: 0.9285714030265808)
[2024-11-08 00:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:39][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 0.4761090874671936, acc: 0.7692307829856873)
[2024-11-08 00:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:40][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 0.4160003066062927, acc: 0.800000011920929)
[2024-11-08 00:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:40][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 0.3525189459323883, acc: 0.8999999761581421)
[2024-11-08 00:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:40][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 0.3590019941329956, acc: 0.8421052694320679)
[2024-11-08 00:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:41][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 0.30921366810798645, acc: 0.8181818127632141)
[2024-11-08 00:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:41][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 0.38141971826553345, acc: 0.8999999761581421)
[2024-11-08 00:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:41][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 0.23563018441200256, acc: 0.9090909361839294)
[2024-11-08 00:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:42][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 0.30904433131217957, acc: 0.8787878751754761)
[2024-11-08 00:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:42][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.4868413209915161, acc: 0.8518518805503845)
[2024-11-08 00:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:42][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 0.3885807693004608, acc: 0.8787878751754761)
[2024-11-08 00:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:43][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 0.42756766080856323, acc: 0.800000011920929)
[2024-11-08 00:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:43][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 0.29948586225509644, acc: 0.8999999761581421)
[2024-11-08 00:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:43][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 0.5561437010765076, acc: 0.7368420958518982)
[2024-11-08 00:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:44][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 0.23684373497962952, acc: 0.9090909361839294)
[2024-11-08 00:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:44][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 0.4142548143863678, acc: 0.800000011920929)
[2024-11-08 00:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:44][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 0.33018410205841064, acc: 0.9545454382896423)
[2024-11-08 00:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:44][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 0.2711765468120575, acc: 0.939393937587738)
[2024-11-08 00:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:45][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 0.47668787837028503, acc: 0.8148148059844971)
[2024-11-08 00:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:13][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4487, device='cuda:0') eval_epoch_loss=tensor(0.3707, device='cuda:0') eval_epoch_acc=tensor(0.8694, device='cuda:0')
[2024-11-08 00:31:13][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:31:13][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:31:14][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_141_loss_0.370682030916214/model.pt
[2024-11-08 00:31:14][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:31:14][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8693848848342896
[2024-11-08 00:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:14][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 0.4548773467540741, acc: 0.8484848737716675)
[2024-11-08 00:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:14][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 0.43798357248306274, acc: 0.800000011920929)
[2024-11-08 00:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:15][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 0.5160672068595886, acc: 0.800000011920929)
[2024-11-08 00:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:15][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 0.3227494955062866, acc: 0.8947368264198303)
[2024-11-08 00:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:16][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 0.22068604826927185, acc: 0.9545454382896423)
[2024-11-08 00:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:16][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 0.6263042688369751, acc: 0.75)
[2024-11-08 00:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:16][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 0.314046710729599, acc: 0.8636363744735718)
[2024-11-08 00:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:17][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 0.305213063955307, acc: 0.9090909361839294)
[2024-11-08 00:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:17][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 0.49601608514785767, acc: 0.7777777910232544)
[2024-11-08 00:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:17][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 0.39978763461112976, acc: 0.8387096524238586)
[2024-11-08 00:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:18][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 0.42118778824806213, acc: 0.800000011920929)
[2024-11-08 00:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:18][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 0.19776397943496704, acc: 0.949999988079071)
[2024-11-08 00:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:18][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 0.38444435596466064, acc: 0.8947368264198303)
[2024-11-08 00:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:19][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 0.2884249687194824, acc: 0.8636363744735718)
[2024-11-08 00:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:19][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 0.42574959993362427, acc: 0.800000011920929)
[2024-11-08 00:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:19][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 0.29798734188079834, acc: 0.9090909361839294)
[2024-11-08 00:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:20][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 0.47996804118156433, acc: 0.8181818127632141)
[2024-11-08 00:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:21][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 0.6098312735557556, acc: 0.8500000238418579)
[2024-11-08 00:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:22][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 0.2975709140300751, acc: 0.8500000238418579)
[2024-11-08 00:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:22][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 0.49424973130226135, acc: 0.7894737124443054)
[2024-11-08 00:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:22][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 0.32248762249946594, acc: 0.9090909361839294)
[2024-11-08 00:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:23][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 0.45071786642074585, acc: 0.800000011920929)
[2024-11-08 00:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:23][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 0.2902546226978302, acc: 0.8799999952316284)
[2024-11-08 00:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:24][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 0.3394733965396881, acc: 0.8888888955116272)
[2024-11-08 00:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:24][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 0.35557472705841064, acc: 0.9166666865348816)
[2024-11-08 00:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:24][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 0.27642562985420227, acc: 0.8695651888847351)
[2024-11-08 00:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:24][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 0.34695619344711304, acc: 0.8571428656578064)
[2024-11-08 00:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:25][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 0.37510812282562256, acc: 0.7894737124443054)
[2024-11-08 00:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:25][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 0.2954115569591522, acc: 0.8636363744735718)
[2024-11-08 00:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:26][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 0.48147130012512207, acc: 0.800000011920929)
[2024-11-08 00:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:26][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 0.15486234426498413, acc: 0.9545454382896423)
[2024-11-08 00:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:27][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 0.28000012040138245, acc: 0.8787878751754761)
[2024-11-08 00:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:27][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 0.191310316324234, acc: 0.9333333373069763)
[2024-11-08 00:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:28][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 0.3044930696487427, acc: 0.8571428656578064)
[2024-11-08 00:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:28][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 0.46026721596717834, acc: 0.8095238208770752)
[2024-11-08 00:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:28][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 0.293152779340744, acc: 0.8421052694320679)
[2024-11-08 00:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:29][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 0.2704823315143585, acc: 0.9090909361839294)
[2024-11-08 00:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:30][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 0.5674424171447754, acc: 0.7894737124443054)
[2024-11-08 00:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:30][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 0.3714672327041626, acc: 0.8181818127632141)
[2024-11-08 00:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:31][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.1406337469816208, acc: 0.9642857313156128)
[2024-11-08 00:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:31][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 0.20981410145759583, acc: 0.9333333373069763)
[2024-11-08 00:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:31][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 0.41944414377212524, acc: 0.800000011920929)
[2024-11-08 00:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:31][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 0.25881311297416687, acc: 0.95652174949646)
[2024-11-08 00:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:32][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 0.3956144452095032, acc: 0.761904776096344)
[2024-11-08 00:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:32][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 0.4284522533416748, acc: 0.8421052694320679)
[2024-11-08 00:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:32][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 0.3390722870826721, acc: 0.8500000238418579)
[2024-11-08 00:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:33][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 0.3340238332748413, acc: 0.8095238208770752)
[2024-11-08 00:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:33][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 0.3180960416793823, acc: 0.8636363744735718)
[2024-11-08 00:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:34][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 0.5115854144096375, acc: 0.8799999952316284)
[2024-11-08 00:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:34][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 0.408475399017334, acc: 0.8500000238418579)
[2024-11-08 00:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:35][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 0.49759307503700256, acc: 0.8095238208770752)
[2024-11-08 00:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:36][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 0.37255334854125977, acc: 0.8947368264198303)
[2024-11-08 00:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:37][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 0.4818362295627594, acc: 0.8181818127632141)
[2024-11-08 00:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:38][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 0.3099960684776306, acc: 0.8095238208770752)
[2024-11-08 00:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:38][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 0.13835467398166656, acc: 0.9166666865348816)
[2024-11-08 00:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:39][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.3493099510669708, acc: 0.8387096524238586)
[2024-11-08 00:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:39][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 0.2508746385574341, acc: 0.8666666746139526)
[2024-11-08 00:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:39][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 0.28920263051986694, acc: 0.8999999761581421)
[2024-11-08 00:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:40][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 0.26981320977211, acc: 0.8500000238418579)
[2024-11-08 00:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:40][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 0.4294308125972748, acc: 0.7894737124443054)
[2024-11-08 00:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:40][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 0.2716619670391083, acc: 0.7727272510528564)
[2024-11-08 00:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:41][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 0.44475698471069336, acc: 0.8500000238418579)
[2024-11-08 00:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:41][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 0.20161999762058258, acc: 0.95652174949646)
[2024-11-08 00:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:41][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 0.39818981289863586, acc: 0.9285714030265808)
[2024-11-08 00:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:42][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 0.23550346493721008, acc: 0.8095238208770752)
[2024-11-08 00:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:42][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 0.29833146929740906, acc: 0.9473684430122375)
[2024-11-08 00:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:42][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 0.3112475872039795, acc: 0.8421052694320679)
[2024-11-08 00:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:43][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 0.4197225570678711, acc: 0.8181818127632141)
[2024-11-08 00:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:43][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 0.30185821652412415, acc: 0.8571428656578064)
[2024-11-08 00:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:43][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 0.24179707467556, acc: 0.875)
[2024-11-08 00:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:44][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 0.24739335477352142, acc: 0.9354838728904724)
[2024-11-08 00:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:44][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 0.40880221128463745, acc: 0.8709677457809448)
[2024-11-08 00:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:44][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 0.414129376411438, acc: 0.8846153616905212)
[2024-11-08 00:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:45][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 0.2897539734840393, acc: 0.8571428656578064)
[2024-11-08 00:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:45][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 0.32816967368125916, acc: 0.8947368264198303)
[2024-11-08 00:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:45][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 0.24820147454738617, acc: 0.9473684430122375)
[2024-11-08 00:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:46][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 0.28597795963287354, acc: 0.8181818127632141)
[2024-11-08 00:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:46][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 0.3122744560241699, acc: 0.8095238208770752)
[2024-11-08 00:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:47][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.1167924627661705, acc: 0.9583333134651184)
[2024-11-08 00:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:47][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.21570008993148804, acc: 0.9032257795333862)
[2024-11-08 00:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:47][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.1823732554912567, acc: 0.9032257795333862)
[2024-11-08 00:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:48][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 0.1937827318906784, acc: 0.8846153616905212)
[2024-11-08 00:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:48][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 0.18876631557941437, acc: 0.9047619104385376)
[2024-11-08 00:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:49][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 0.44119828939437866, acc: 0.7894737124443054)
[2024-11-08 00:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:49][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 0.40706026554107666, acc: 0.8421052694320679)
[2024-11-08 00:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:50][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 0.3331123888492584, acc: 0.8636363744735718)
[2024-11-08 00:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:50][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 0.4022989869117737, acc: 0.8571428656578064)
[2024-11-08 00:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:50][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 0.46544280648231506, acc: 0.875)
[2024-11-08 00:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:50][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 0.40574488043785095, acc: 0.782608687877655)
[2024-11-08 00:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:51][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 0.22408172488212585, acc: 0.9523809552192688)
[2024-11-08 00:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:51][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 0.44697070121765137, acc: 0.7894737124443054)
[2024-11-08 00:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:52][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 0.354704350233078, acc: 0.7894737124443054)
[2024-11-08 00:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:52][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 0.3754716217517853, acc: 0.8181818127632141)
[2024-11-08 00:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:53][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 0.36861932277679443, acc: 0.8181818127632141)
[2024-11-08 00:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:53][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 0.3146784007549286, acc: 0.9285714030265808)
[2024-11-08 00:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:53][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 0.7331914305686951, acc: 0.8518518805503845)
[2024-11-08 00:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:54][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 0.5278360843658447, acc: 0.8484848737716675)
[2024-11-08 00:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:54][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 0.27043330669403076, acc: 0.9090909361839294)
[2024-11-08 00:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:54][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 0.49655383825302124, acc: 0.8571428656578064)
[2024-11-08 00:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:55][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 0.21640701591968536, acc: 0.9473684430122375)
[2024-11-08 00:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:55][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 0.24587176740169525, acc: 0.9090909361839294)
[2024-11-08 00:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:55][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 0.4870319366455078, acc: 0.7894737124443054)
[2024-11-08 00:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:56][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 0.43801915645599365, acc: 0.8181818127632141)
[2024-11-08 00:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:56][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.20134849846363068, acc: 0.9285714030265808)
[2024-11-08 00:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:57][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 0.36176589131355286, acc: 0.8666666746139526)
[2024-11-08 00:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:57][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 0.28570684790611267, acc: 0.8285714387893677)
[2024-11-08 00:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:57][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.2468557357788086, acc: 0.9130434989929199)
[2024-11-08 00:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:58][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 0.41584497690200806, acc: 0.8571428656578064)
[2024-11-08 00:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:58][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 0.5572993755340576, acc: 0.7894737124443054)
[2024-11-08 00:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:58][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 0.5475342869758606, acc: 0.75)
[2024-11-08 00:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:59][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 0.4653201401233673, acc: 0.8095238208770752)
[2024-11-08 00:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:59][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.28520095348358154, acc: 0.9090909361839294)
[2024-11-08 00:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:59][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.2323905974626541, acc: 0.9285714030265808)
[2024-11-08 00:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:31:59][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.3242933452129364, acc: 0.8965517282485962)
[2024-11-08 00:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:00][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.3239995837211609, acc: 0.8857142925262451)
[2024-11-08 00:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:00][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 0.4434310793876648, acc: 0.8095238208770752)
[2024-11-08 00:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:00][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 0.40695101022720337, acc: 0.8095238208770752)
[2024-11-08 00:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:01][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 0.307996541261673, acc: 0.8947368264198303)
[2024-11-08 00:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:01][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 0.32616087794303894, acc: 0.8181818127632141)
[2024-11-08 00:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:02][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 0.2659604549407959, acc: 0.8947368264198303)
[2024-11-08 00:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:02][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 0.24516621232032776, acc: 0.9090909361839294)
[2024-11-08 00:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:02][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 0.35645124316215515, acc: 0.8823529481887817)
[2024-11-08 00:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:03][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 0.23359717428684235, acc: 0.9545454382896423)
[2024-11-08 00:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:03][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 0.3743278384208679, acc: 0.8500000238418579)
[2024-11-08 00:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:04][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 0.28478869795799255, acc: 0.9473684430122375)
[2024-11-08 00:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:04][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 0.27879586815834045, acc: 0.8695651888847351)
[2024-11-08 00:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:05][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 0.29807552695274353, acc: 0.8571428656578064)
[2024-11-08 00:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:05][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 0.3045268952846527, acc: 0.8965517282485962)
[2024-11-08 00:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:05][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 0.3329136073589325, acc: 0.8888888955116272)
[2024-11-08 00:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:06][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 0.3500960171222687, acc: 0.9230769276618958)
[2024-11-08 00:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:06][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.15759775042533875, acc: 0.9047619104385376)
[2024-11-08 00:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:06][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 0.3548508584499359, acc: 0.8947368264198303)
[2024-11-08 00:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:07][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 0.24820064008235931, acc: 0.9473684430122375)
[2024-11-08 00:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:07][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 0.38017311692237854, acc: 0.7727272510528564)
[2024-11-08 00:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:07][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.2773497402667999, acc: 0.9047619104385376)
[2024-11-08 00:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:08][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 0.24333547055721283, acc: 0.8965517282485962)
[2024-11-08 00:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:08][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 0.2054600566625595, acc: 0.9259259104728699)
[2024-11-08 00:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:08][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 0.22085507214069366, acc: 0.9047619104385376)
[2024-11-08 00:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:09][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 0.19556622207164764, acc: 0.9523809552192688)
[2024-11-08 00:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:09][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 0.22210021317005157, acc: 0.9444444179534912)
[2024-11-08 00:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:09][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 0.39696112275123596, acc: 0.8181818127632141)
[2024-11-08 00:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:10][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 0.25276944041252136, acc: 0.9047619104385376)
[2024-11-08 00:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:10][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 0.39347705245018005, acc: 0.8965517282485962)
[2024-11-08 00:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:40][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4215, device='cuda:0') eval_epoch_loss=tensor(0.3517, device='cuda:0') eval_epoch_acc=tensor(0.8744, device='cuda:0')
[2024-11-08 00:32:40][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:32:40][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:32:40][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_284_loss_0.35174399614334106/model.pt
[2024-11-08 00:32:40][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:32:40][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8744377493858337
[2024-11-08 00:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:40][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 0.2598096430301666, acc: 0.8999999761581421)
[2024-11-08 00:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:41][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 0.34293875098228455, acc: 0.8965517282485962)
[2024-11-08 00:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:41][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 0.2959032654762268, acc: 0.9047619104385376)
[2024-11-08 00:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:41][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 0.5726125836372375, acc: 0.8421052694320679)
[2024-11-08 00:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:42][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 0.5081331133842468, acc: 0.7894737124443054)
[2024-11-08 00:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:42][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 0.36760807037353516, acc: 0.9090909361839294)
[2024-11-08 00:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:42][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 0.248375803232193, acc: 0.8636363744735718)
[2024-11-08 00:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:42][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.21717584133148193, acc: 0.9200000166893005)
[2024-11-08 00:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:43][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 0.2657942473888397, acc: 0.8666666746139526)
[2024-11-08 00:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:43][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 0.3566054701805115, acc: 0.8095238208770752)
[2024-11-08 00:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:44][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 0.2613396644592285, acc: 0.8999999761581421)
[2024-11-08 00:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:44][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 0.4440586566925049, acc: 0.8421052694320679)
[2024-11-08 00:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:44][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 0.3617888391017914, acc: 0.8636363744735718)
[2024-11-08 00:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:45][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 0.40797755122184753, acc: 0.8571428656578064)
[2024-11-08 00:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:45][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 0.3620675504207611, acc: 0.875)
[2024-11-08 00:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:45][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 0.38921940326690674, acc: 0.9354838728904724)
[2024-11-08 00:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:46][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 0.42983531951904297, acc: 0.8387096524238586)
[2024-11-08 00:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:46][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 0.5240322947502136, acc: 0.807692289352417)
[2024-11-08 00:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:46][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 0.31650346517562866, acc: 0.8095238208770752)
[2024-11-08 00:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:47][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 0.2675701677799225, acc: 1.0)
[2024-11-08 00:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:47][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 0.341400682926178, acc: 0.8421052694320679)
[2024-11-08 00:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:47][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 0.2813611626625061, acc: 0.9130434989929199)
[2024-11-08 00:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:48][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.29321232438087463, acc: 0.8636363744735718)
[2024-11-08 00:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:48][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.1359257847070694, acc: 0.9230769276618958)
[2024-11-08 00:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:49][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 0.1856389343738556, acc: 0.9047619104385376)
[2024-11-08 00:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:49][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 0.4674640893936157, acc: 0.7894737124443054)
[2024-11-08 00:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:49][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 0.288302481174469, acc: 0.8421052694320679)
[2024-11-08 00:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:50][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 0.38282841444015503, acc: 0.8636363744735718)
[2024-11-08 00:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:50][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 0.441547691822052, acc: 0.8181818127632141)
[2024-11-08 00:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:50][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.20091505348682404, acc: 0.9285714030265808)
[2024-11-08 00:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:50][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 0.29980790615081787, acc: 0.8888888955116272)
[2024-11-08 00:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:51][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.2860972285270691, acc: 0.8857142925262451)
[2024-11-08 00:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:51][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 0.19364799559116364, acc: 0.9615384340286255)
[2024-11-08 00:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:51][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 0.1159646064043045, acc: 0.9523809552192688)
[2024-11-08 00:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:52][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 0.48989206552505493, acc: 0.699999988079071)
[2024-11-08 00:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:52][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 0.17646099627017975, acc: 0.949999988079071)
[2024-11-08 00:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:52][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 0.4597075879573822, acc: 0.8571428656578064)
[2024-11-08 00:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:53][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.15549692511558533, acc: 0.9545454382896423)
[2024-11-08 00:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:53][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 0.34965354204177856, acc: 0.8260869383811951)
[2024-11-08 00:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:53][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 0.36234214901924133, acc: 0.8571428656578064)
[2024-11-08 00:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:54][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 0.4411070942878723, acc: 0.8421052694320679)
[2024-11-08 00:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:54][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 0.502690315246582, acc: 0.7727272510528564)
[2024-11-08 00:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:54][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 0.3760477602481842, acc: 0.8421052694320679)
[2024-11-08 00:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:54][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 0.3354271352291107, acc: 0.875)
[2024-11-08 00:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:55][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.4721613824367523, acc: 0.8275862336158752)
[2024-11-08 00:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:55][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 0.6039867997169495, acc: 0.8148148059844971)
[2024-11-08 00:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:55][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.31859615445137024, acc: 0.9047619104385376)
[2024-11-08 00:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:56][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 0.2779831886291504, acc: 0.8947368264198303)
[2024-11-08 00:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:56][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 0.2515733540058136, acc: 0.8636363744735718)
[2024-11-08 00:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:57][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 0.5777638554573059, acc: 0.8421052694320679)
[2024-11-08 00:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:57][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 0.49258938431739807, acc: 0.7727272510528564)
[2024-11-08 00:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:57][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 0.5600559711456299, acc: 0.807692289352417)
[2024-11-08 00:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:58][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 0.4685990810394287, acc: 0.7916666865348816)
[2024-11-08 00:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:58][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 0.4797370433807373, acc: 0.761904776096344)
[2024-11-08 00:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:58][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 0.22163474559783936, acc: 0.8695651888847351)
[2024-11-08 00:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:59][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 0.24746352434158325, acc: 0.9545454382896423)
[2024-11-08 00:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:59][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.31373122334480286, acc: 0.9333333373069763)
[2024-11-08 00:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:32:59][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 0.30017781257629395, acc: 0.8787878751754761)
[2024-11-08 00:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:00][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 0.33343976736068726, acc: 0.949999988079071)
[2024-11-08 00:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:00][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 0.40079182386398315, acc: 0.8500000238418579)
[2024-11-08 00:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:00][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 0.4341961145401001, acc: 0.7894737124443054)
[2024-11-08 00:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:01][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 0.3223598301410675, acc: 0.9090909361839294)
[2024-11-08 00:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:01][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 0.6424592733383179, acc: 0.75)
[2024-11-08 00:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:01][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.1862054467201233, acc: 0.9583333134651184)
[2024-11-08 00:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:02][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 0.32362812757492065, acc: 0.8695651888847351)
[2024-11-08 00:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:02][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 0.2343887835741043, acc: 0.8500000238418579)
[2024-11-08 00:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:02][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 0.3747985363006592, acc: 0.8421052694320679)
[2024-11-08 00:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:03][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 0.33035874366760254, acc: 0.8181818127632141)
[2024-11-08 00:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:03][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 0.3916068971157074, acc: 0.8999999761581421)
[2024-11-08 00:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:03][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.21050189435482025, acc: 0.8799999952316284)
[2024-11-08 00:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:04][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 0.3683527708053589, acc: 0.8799999952316284)
[2024-11-08 00:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:04][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 0.3771871030330658, acc: 0.8095238208770752)
[2024-11-08 00:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:05][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 0.38013818860054016, acc: 0.800000011920929)
[2024-11-08 00:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:05][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 0.30906689167022705, acc: 0.8571428656578064)
[2024-11-08 00:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:05][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 0.23990823328495026, acc: 0.9090909361839294)
[2024-11-08 00:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:05][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.14607129991054535, acc: 0.9642857313156128)
[2024-11-08 00:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:06][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.5056775212287903, acc: 0.8275862336158752)
[2024-11-08 00:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:06][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 0.35158076882362366, acc: 0.9142857193946838)
[2024-11-08 00:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:06][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 0.2692500948905945, acc: 0.9090909361839294)
[2024-11-08 00:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:07][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 0.3044285178184509, acc: 0.8421052694320679)
[2024-11-08 00:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:07][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 0.14439243078231812, acc: 0.9523809552192688)
[2024-11-08 00:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:07][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 0.26468905806541443, acc: 0.8947368264198303)
[2024-11-08 00:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:08][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.12182840704917908, acc: 0.9545454382896423)
[2024-11-08 00:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:08][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.26890167593955994, acc: 0.8888888955116272)
[2024-11-08 00:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:08][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.20472244918346405, acc: 0.9375)
[2024-11-08 00:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:09][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 0.28509706258773804, acc: 0.8095238208770752)
[2024-11-08 00:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:09][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 0.3755621314048767, acc: 0.8571428656578064)
[2024-11-08 00:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:10][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 0.270969957113266, acc: 0.7894737124443054)
[2024-11-08 00:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:10][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 0.2272273451089859, acc: 0.9090909361839294)
[2024-11-08 00:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:11][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 0.18007051944732666, acc: 0.9473684430122375)
[2024-11-08 00:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:11][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.423890084028244, acc: 0.8636363744735718)
[2024-11-08 00:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:11][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.19894450902938843, acc: 0.931034505367279)
[2024-11-08 00:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:12][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.3668766915798187, acc: 0.9354838728904724)
[2024-11-08 00:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:12][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 0.3593679964542389, acc: 0.761904776096344)
[2024-11-08 00:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:12][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 0.30488982796669006, acc: 0.8571428656578064)
[2024-11-08 00:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:13][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 0.23156768083572388, acc: 0.8947368264198303)
[2024-11-08 00:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:13][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 0.27248066663742065, acc: 0.8181818127632141)
[2024-11-08 00:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:14][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 0.5194916725158691, acc: 0.8947368264198303)
[2024-11-08 00:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:15][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 0.37675219774246216, acc: 0.8636363744735718)
[2024-11-08 00:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:15][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.3041495382785797, acc: 0.9285714030265808)
[2024-11-08 00:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:15][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.1432604044675827, acc: 0.9666666388511658)
[2024-11-08 00:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:16][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.243671715259552, acc: 0.9142857193946838)
[2024-11-08 00:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:16][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 0.29450860619544983, acc: 0.8999999761581421)
[2024-11-08 00:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:16][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.34549298882484436, acc: 0.7894737124443054)
[2024-11-08 00:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:17][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.15088918805122375, acc: 0.949999988079071)
[2024-11-08 00:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:17][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.5225618481636047, acc: 0.8500000238418579)
[2024-11-08 00:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:17][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.1430359035730362, acc: 1.0)
[2024-11-08 00:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:18][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 0.14336749911308289, acc: 0.9047619104385376)
[2024-11-08 00:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:18][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 0.35358089208602905, acc: 0.8947368264198303)
[2024-11-08 00:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:18][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 0.38073235750198364, acc: 0.7894737124443054)
[2024-11-08 00:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:19][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 0.3234592378139496, acc: 0.8636363744735718)
[2024-11-08 00:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:19][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 0.24319349229335785, acc: 0.9047619104385376)
[2024-11-08 00:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:19][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 0.3331780731678009, acc: 0.8620689511299133)
[2024-11-08 00:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:20][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.4118456542491913, acc: 0.9333333373069763)
[2024-11-08 00:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:20][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 0.3315625786781311, acc: 0.9047619104385376)
[2024-11-08 00:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:20][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 0.3526042401790619, acc: 0.8333333134651184)
[2024-11-08 00:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:21][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 0.19534355401992798, acc: 0.9090909361839294)
[2024-11-08 00:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:21][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 0.19132597744464874, acc: 0.9047619104385376)
[2024-11-08 00:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:21][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 0.2090819627046585, acc: 0.875)
[2024-11-08 00:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:22][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 0.2997027039527893, acc: 0.90625)
[2024-11-08 00:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:22][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.22779574990272522, acc: 0.9142857193946838)
[2024-11-08 00:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:22][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 0.2837955057621002, acc: 0.8846153616905212)
[2024-11-08 00:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:23][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.30705466866493225, acc: 0.8571428656578064)
[2024-11-08 00:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:23][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.47700440883636475, acc: 0.7894737124443054)
[2024-11-08 00:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:23][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.21118894219398499, acc: 0.949999988079071)
[2024-11-08 00:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:24][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.265453964471817, acc: 0.8571428656578064)
[2024-11-08 00:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:24][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 0.1330101490020752, acc: 1.0)
[2024-11-08 00:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:24][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.5319603681564331, acc: 0.8571428656578064)
[2024-11-08 00:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:24][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.21910646557807922, acc: 0.9259259104728699)
[2024-11-08 00:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:25][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.2063913643360138, acc: 0.9428571462631226)
[2024-11-08 00:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:25][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 0.16994193196296692, acc: 0.9615384340286255)
[2024-11-08 00:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:25][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 0.16798201203346252, acc: 0.9047619104385376)
[2024-11-08 00:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:26][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 0.3675890862941742, acc: 0.8421052694320679)
[2024-11-08 00:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:26][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 0.30282819271087646, acc: 0.8947368264198303)
[2024-11-08 00:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:26][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 0.5923612117767334, acc: 0.8636363744735718)
[2024-11-08 00:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:26][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 0.3801235556602478, acc: 0.8571428656578064)
[2024-11-08 00:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:27][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.09685068577528, acc: 1.0)
[2024-11-08 00:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:27][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 0.43492379784584045, acc: 0.9032257795333862)
[2024-11-08 00:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:27][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 0.522354781627655, acc: 0.8387096524238586)
[2024-11-08 00:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:28][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 0.3856344223022461, acc: 0.807692289352417)
[2024-11-08 00:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:28][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.15280187129974365, acc: 0.9523809552192688)
[2024-11-08 00:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:28][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 0.33813732862472534, acc: 0.8421052694320679)
[2024-11-08 00:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:29][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.29259514808654785, acc: 0.8421052694320679)
[2024-11-08 00:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:58][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3121, device='cuda:0') eval_epoch_loss=tensor(0.2717, device='cuda:0') eval_epoch_acc=tensor(0.8990, device='cuda:0')
[2024-11-08 00:33:58][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:33:58][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:33:59][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_427_loss_0.2716529667377472/model.pt
[2024-11-08 00:33:59][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:33:59][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.2716529667377472
[2024-11-08 00:33:59][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8989887833595276
[2024-11-08 00:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:59][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.2199704647064209, acc: 0.9545454382896423)
[2024-11-08 00:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:33:59][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.30093273520469666, acc: 0.9047619104385376)
[2024-11-08 00:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:00][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 0.1306465119123459, acc: 1.0)
[2024-11-08 00:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:00][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.14638394117355347, acc: 0.9677419066429138)
[2024-11-08 00:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:00][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.14389851689338684, acc: 0.9354838728904724)
[2024-11-08 00:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:00][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.10788796842098236, acc: 1.0)
[2024-11-08 00:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:01][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 0.19419457018375397, acc: 0.9523809552192688)
[2024-11-08 00:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:01][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.21130217611789703, acc: 1.0)
[2024-11-08 00:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:01][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.2775900959968567, acc: 0.8947368264198303)
[2024-11-08 00:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:02][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 0.1622343510389328, acc: 0.9545454382896423)
[2024-11-08 00:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:02][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 0.1902768462896347, acc: 0.9047619104385376)
[2024-11-08 00:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:02][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.11678154766559601, acc: 0.9545454382896423)
[2024-11-08 00:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:03][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 0.16775961220264435, acc: 0.9642857313156128)
[2024-11-08 00:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:03][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 0.15135346353054047, acc: 0.9523809552192688)
[2024-11-08 00:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:04][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 0.4268615245819092, acc: 0.7894737124443054)
[2024-11-08 00:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:04][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 0.2626286745071411, acc: 0.8947368264198303)
[2024-11-08 00:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:05][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 0.3334956765174866, acc: 0.8181818127632141)
[2024-11-08 00:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:05][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 0.375282883644104, acc: 0.8571428656578064)
[2024-11-08 00:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:05][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 0.10415247082710266, acc: 0.9583333134651184)
[2024-11-08 00:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:06][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 0.44894519448280334, acc: 0.9032257795333862)
[2024-11-08 00:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:06][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 0.12237855046987534, acc: 0.9354838728904724)
[2024-11-08 00:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:06][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.27571970224380493, acc: 0.9032257795333862)
[2024-11-08 00:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:07][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 0.400664746761322, acc: 0.800000011920929)
[2024-11-08 00:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:07][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 0.11243549734354019, acc: 0.8999999761581421)
[2024-11-08 00:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:07][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 0.24125802516937256, acc: 0.8947368264198303)
[2024-11-08 00:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:08][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 0.23423632979393005, acc: 0.9545454382896423)
[2024-11-08 00:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:08][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 0.2076752632856369, acc: 0.8999999761581421)
[2024-11-08 00:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:08][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 0.4749106466770172, acc: 0.9090909361839294)
[2024-11-08 00:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:08][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 0.340858519077301, acc: 0.8709677457809448)
[2024-11-08 00:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:09][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 0.19554652273654938, acc: 0.949999988079071)
[2024-11-08 00:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:09][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 0.4651493430137634, acc: 0.8500000238418579)
[2024-11-08 00:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:09][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 0.44225743412971497, acc: 0.7894737124443054)
[2024-11-08 00:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:10][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 0.2351202815771103, acc: 0.8636363744735718)
[2024-11-08 00:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:10][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 0.40725961327552795, acc: 0.8999999761581421)
[2024-11-08 00:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:10][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 0.10967624187469482, acc: 0.9545454382896423)
[2024-11-08 00:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:11][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 0.5712244510650635, acc: 0.939393937587738)
[2024-11-08 00:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:11][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.46927887201309204, acc: 0.8888888955116272)
[2024-11-08 00:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:11][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 0.2975740432739258, acc: 0.8787878751754761)
[2024-11-08 00:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:12][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 0.7032520771026611, acc: 0.800000011920929)
[2024-11-08 00:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:12][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 0.2194332629442215, acc: 0.949999988079071)
[2024-11-08 00:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:12][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 0.392981618642807, acc: 0.7894737124443054)
[2024-11-08 00:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:12][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 0.23101806640625, acc: 0.949999988079071)
[2024-11-08 00:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:13][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 0.35036367177963257, acc: 0.9047619104385376)
[2024-11-08 00:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:13][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.13518370687961578, acc: 0.9166666865348816)
[2024-11-08 00:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:13][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 0.49161750078201294, acc: 0.8333333134651184)
[2024-11-08 00:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:14][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 0.23602409660816193, acc: 0.9166666865348816)
[2024-11-08 00:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:14][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 0.31767040491104126, acc: 0.8571428656578064)
[2024-11-08 00:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:14][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 0.2770087718963623, acc: 0.8947368264198303)
[2024-11-08 00:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:15][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 0.3455560803413391, acc: 0.800000011920929)
[2024-11-08 00:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:15][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 0.576993465423584, acc: 0.761904776096344)
[2024-11-08 00:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:15][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 0.4257792532444, acc: 0.8636363744735718)
[2024-11-08 00:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:16][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 0.29796501994132996, acc: 0.9285714030265808)
[2024-11-08 00:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:16][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 0.5137064456939697, acc: 0.774193525314331)
[2024-11-08 00:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:16][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.32432806491851807, acc: 0.9032257795333862)
[2024-11-08 00:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:17][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 0.3389366567134857, acc: 0.8571428656578064)
[2024-11-08 00:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:17][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.6125599145889282, acc: 0.8421052694320679)
[2024-11-08 00:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:17][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 0.4878278076648712, acc: 0.7727272510528564)
[2024-11-08 00:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:18][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.281658411026001, acc: 0.9090909361839294)
[2024-11-08 00:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:18][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.12524941563606262, acc: 0.9642857313156128)
[2024-11-08 00:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:18][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 0.34730085730552673, acc: 0.9259259104728699)
[2024-11-08 00:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:19][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 0.43697845935821533, acc: 0.9142857193946838)
[2024-11-08 00:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:19][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.5664821267127991, acc: 0.8461538553237915)
[2024-11-08 00:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:19][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 0.1402086466550827, acc: 0.9047619104385376)
[2024-11-08 00:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:20][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 0.5243551135063171, acc: 0.7894737124443054)
[2024-11-08 00:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:20][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 0.4695686995983124, acc: 0.7894737124443054)
[2024-11-08 00:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:20][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 0.2952020466327667, acc: 0.8636363744735718)
[2024-11-08 00:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:21][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 0.535573422908783, acc: 0.8095238208770752)
[2024-11-08 00:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:21][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.2697877585887909, acc: 0.875)
[2024-11-08 00:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:21][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 0.2590181231498718, acc: 0.8928571343421936)
[2024-11-08 00:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:22][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 0.36948150396347046, acc: 0.8500000238418579)
[2024-11-08 00:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:22][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 0.24829065799713135, acc: 0.8999999761581421)
[2024-11-08 00:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:22][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 0.37552541494369507, acc: 0.8947368264198303)
[2024-11-08 00:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:23][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 0.18395094573497772, acc: 0.9090909361839294)
[2024-11-08 00:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:23][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 0.45179957151412964, acc: 0.800000011920929)
[2024-11-08 00:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:23][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.17485111951828003, acc: 0.9583333134651184)
[2024-11-08 00:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:24][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.35704144835472107, acc: 0.9032257795333862)
[2024-11-08 00:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:24][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.32761919498443604, acc: 0.9032257795333862)
[2024-11-08 00:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:24][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 0.26438668370246887, acc: 0.8695651888847351)
[2024-11-08 00:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:24][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 0.27898725867271423, acc: 0.9047619104385376)
[2024-11-08 00:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:25][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 0.4334256052970886, acc: 0.7894737124443054)
[2024-11-08 00:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:25][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 0.0875055342912674, acc: 1.0)
[2024-11-08 00:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:26][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 0.46872884035110474, acc: 0.800000011920929)
[2024-11-08 00:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:26][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.6820883750915527, acc: 0.8181818127632141)
[2024-11-08 00:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:26][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.23278692364692688, acc: 0.939393937587738)
[2024-11-08 00:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:27][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.48037973046302795, acc: 0.800000011920929)
[2024-11-08 00:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:29][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 0.28393977880477905, acc: 0.8571428656578064)
[2024-11-08 00:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:30][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 0.2239946573972702, acc: 0.8571428656578064)
[2024-11-08 00:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:30][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 0.379793256521225, acc: 0.7894737124443054)
[2024-11-08 00:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:30][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.3610767722129822, acc: 0.8999999761581421)
[2024-11-08 00:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:31][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 0.1988544464111328, acc: 0.9523809552192688)
[2024-11-08 00:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:31][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.12978923320770264, acc: 1.0)
[2024-11-08 00:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:32][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 0.2942187786102295, acc: 0.9285714030265808)
[2024-11-08 00:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:32][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 0.3496173620223999, acc: 0.8518518805503845)
[2024-11-08 00:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:32][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 0.26379522681236267, acc: 0.9142857193946838)
[2024-11-08 00:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:33][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 0.10450765490531921, acc: 0.95652174949646)
[2024-11-08 00:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:33][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 0.2469511479139328, acc: 0.8571428656578064)
[2024-11-08 00:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:34][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 0.7324578762054443, acc: 0.6842105388641357)
[2024-11-08 00:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:34][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 0.3051989674568176, acc: 0.8181818127632141)
[2024-11-08 00:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:35][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.5011237859725952, acc: 0.8500000238418579)
[2024-11-08 00:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:35][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.24709540605545044, acc: 0.8636363744735718)
[2024-11-08 00:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:35][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 0.20620891451835632, acc: 0.939393937587738)
[2024-11-08 00:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:36][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 0.33254241943359375, acc: 0.8571428656578064)
[2024-11-08 00:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:36][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 0.20810677111148834, acc: 0.9047619104385376)
[2024-11-08 00:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:36][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 0.3217456340789795, acc: 0.8421052694320679)
[2024-11-08 00:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:36][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 0.20455484092235565, acc: 0.949999988079071)
[2024-11-08 00:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:37][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 0.29069334268569946, acc: 0.8571428656578064)
[2024-11-08 00:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:37][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 0.20547567307949066, acc: 0.9090909361839294)
[2024-11-08 00:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:37][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 0.1595776230096817, acc: 0.9642857313156128)
[2024-11-08 00:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:38][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.1716049313545227, acc: 0.9259259104728699)
[2024-11-08 00:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:38][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.41626426577568054, acc: 0.800000011920929)
[2024-11-08 00:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:38][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 0.2730880379676819, acc: 0.8095238208770752)
[2024-11-08 00:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:39][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 0.35796526074409485, acc: 0.8947368264198303)
[2024-11-08 00:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:39][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.17122910916805267, acc: 0.8999999761581421)
[2024-11-08 00:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:39][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 0.32012030482292175, acc: 0.8571428656578064)
[2024-11-08 00:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:40][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.06848548352718353, acc: 1.0)
[2024-11-08 00:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:40][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.06276515871286392, acc: 1.0)
[2024-11-08 00:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:40][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.24483633041381836, acc: 0.8888888955116272)
[2024-11-08 00:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:41][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 0.4067498445510864, acc: 0.8484848737716675)
[2024-11-08 00:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:41][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.17357297241687775, acc: 0.8999999761581421)
[2024-11-08 00:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:41][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.09640941768884659, acc: 0.949999988079071)
[2024-11-08 00:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:42][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.5582361221313477, acc: 0.8095238208770752)
[2024-11-08 00:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:42][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.17017026245594025, acc: 0.9523809552192688)
[2024-11-08 00:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:42][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.06877299398183823, acc: 0.9545454382896423)
[2024-11-08 00:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:43][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.08843151479959488, acc: 1.0)
[2024-11-08 00:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:43][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.23632808029651642, acc: 0.9032257795333862)
[2024-11-08 00:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:43][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.37341535091400146, acc: 0.8888888955116272)
[2024-11-08 00:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:44][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 0.3774271309375763, acc: 0.8571428656578064)
[2024-11-08 00:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:44][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 0.28623467683792114, acc: 0.8947368264198303)
[2024-11-08 00:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:44][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 0.4148949682712555, acc: 0.800000011920929)
[2024-11-08 00:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:44][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 0.828209400177002, acc: 0.8571428656578064)
[2024-11-08 00:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:45][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 0.4297366440296173, acc: 0.8181818127632141)
[2024-11-08 00:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:45][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.2649426758289337, acc: 0.8928571343421936)
[2024-11-08 00:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:45][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 0.44169214367866516, acc: 0.8888888955116272)
[2024-11-08 00:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:46][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.24104921519756317, acc: 0.939393937587738)
[2024-11-08 00:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:46][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 0.4968279302120209, acc: 0.8571428656578064)
[2024-11-08 00:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:46][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 0.41830217838287354, acc: 0.8571428656578064)
[2024-11-08 00:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:47][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 0.5798376202583313, acc: 0.7222222089767456)
[2024-11-08 00:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:47][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 0.2840268909931183, acc: 0.9090909361839294)
[2024-11-08 00:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:47][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.6497041583061218, acc: 0.75)
[2024-11-08 00:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:48][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 0.15476584434509277, acc: 1.0)
[2024-11-08 00:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:48][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.15448592603206635, acc: 0.9259259104728699)
[2024-11-08 00:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:48][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.13582843542099, acc: 0.9677419066429138)
[2024-11-08 00:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:49][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 0.44417816400527954, acc: 0.8500000238418579)
[2024-11-08 00:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:17][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3314, device='cuda:0') eval_epoch_loss=tensor(0.2862, device='cuda:0') eval_epoch_acc=tensor(0.8878, device='cuda:0')
[2024-11-08 00:35:17][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:35:17][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:35:18][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_570_loss_0.2862189710140228/model.pt
[2024-11-08 00:35:18][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:18][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.1469758301973343, acc: 0.949999988079071)
[2024-11-08 00:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:18][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 0.29775819182395935, acc: 0.7894737124443054)
[2024-11-08 00:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:19][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 0.20660892128944397, acc: 0.9545454382896423)
[2024-11-08 00:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:19][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 0.378462016582489, acc: 0.8500000238418579)
[2024-11-08 00:35:19][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.3945, train_epoch_loss=0.3325, epoch time 328.9415146522224s
[2024-11-08 00:35:19][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 10 GB
[2024-11-08 00:35:19][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 00:35:19][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 10 GB
[2024-11-08 00:35:19][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 00:35:19][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 00:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:20][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 0.2688353955745697, acc: 0.9285714030265808)
[2024-11-08 00:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:21][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 0.17099878191947937, acc: 1.0)
[2024-11-08 00:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:21][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 0.3011375665664673, acc: 0.8857142925262451)
[2024-11-08 00:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:21][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 0.27141937613487244, acc: 0.8095238208770752)
[2024-11-08 00:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:22][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 0.2222086638212204, acc: 0.9047619104385376)
[2024-11-08 00:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:22][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.31410059332847595, acc: 0.8947368264198303)
[2024-11-08 00:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:22][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 0.16235628724098206, acc: 0.9545454382896423)
[2024-11-08 00:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:23][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.20648956298828125, acc: 0.9523809552192688)
[2024-11-08 00:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:23][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.14590086042881012, acc: 0.9166666865348816)
[2024-11-08 00:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:23][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.2252487987279892, acc: 0.9032257795333862)
[2024-11-08 00:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:24][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.32871416211128235, acc: 0.9354838728904724)
[2024-11-08 00:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:24][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 0.19602064788341522, acc: 0.9599999785423279)
[2024-11-08 00:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:24][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.10276790708303452, acc: 0.949999988079071)
[2024-11-08 00:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:25][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.3417126536369324, acc: 0.8947368264198303)
[2024-11-08 00:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:25][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 0.28554555773735046, acc: 0.95652174949646)
[2024-11-08 00:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:25][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 0.26822081208229065, acc: 0.9545454382896423)
[2024-11-08 00:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:26][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.07086746394634247, acc: 0.9642857313156128)
[2024-11-08 00:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:26][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.25487348437309265, acc: 0.8928571343421936)
[2024-11-08 00:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:26][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 0.20156432688236237, acc: 0.9545454382896423)
[2024-11-08 00:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:27][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.41015490889549255, acc: 0.8947368264198303)
[2024-11-08 00:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:27][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.3791210949420929, acc: 0.7894737124443054)
[2024-11-08 00:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:27][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 0.3533737063407898, acc: 0.8636363744735718)
[2024-11-08 00:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:28][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 0.3250488340854645, acc: 0.8571428656578064)
[2024-11-08 00:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:28][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.062378376722335815, acc: 1.0)
[2024-11-08 00:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:28][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.32718589901924133, acc: 0.8999999761581421)
[2024-11-08 00:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:29][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 0.45370692014694214, acc: 0.761904776096344)
[2024-11-08 00:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:29][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 0.26836663484573364, acc: 0.8571428656578064)
[2024-11-08 00:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:30][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 0.2867482900619507, acc: 0.8947368264198303)
[2024-11-08 00:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:30][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 0.16630993783473969, acc: 0.9090909361839294)
[2024-11-08 00:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:31][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 0.45903581380844116, acc: 0.800000011920929)
[2024-11-08 00:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:31][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 0.186055988073349, acc: 0.9090909361839294)
[2024-11-08 00:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:31][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 0.2696656584739685, acc: 0.931034505367279)
[2024-11-08 00:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:32][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.4134427011013031, acc: 0.8787878751754761)
[2024-11-08 00:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:32][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.21535465121269226, acc: 0.9130434989929199)
[2024-11-08 00:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:32][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 0.16998998820781708, acc: 0.9523809552192688)
[2024-11-08 00:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:32][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.39150896668434143, acc: 0.7894737124443054)
[2024-11-08 00:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:33][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 0.22160179913043976, acc: 0.8999999761581421)
[2024-11-08 00:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:33][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 0.16419126093387604, acc: 0.9523809552192688)
[2024-11-08 00:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:33][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.06079297885298729, acc: 1.0)
[2024-11-08 00:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:34][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.2532474398612976, acc: 0.8928571343421936)
[2024-11-08 00:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:34][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 0.3262752294540405, acc: 0.8857142925262451)
[2024-11-08 00:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:34][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 0.16878639161586761, acc: 0.949999988079071)
[2024-11-08 00:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:35][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 0.29830268025398254, acc: 0.8500000238418579)
[2024-11-08 00:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:35][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 0.4436472952365875, acc: 0.8947368264198303)
[2024-11-08 00:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:36][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 0.31699758768081665, acc: 0.8181818127632141)
[2024-11-08 00:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:36][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 0.3091222047805786, acc: 0.9047619104385376)
[2024-11-08 00:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:36][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.1933431774377823, acc: 0.9166666865348816)
[2024-11-08 00:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:37][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.17725862562656403, acc: 0.9677419066429138)
[2024-11-08 00:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:37][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.2059945911169052, acc: 0.9428571462631226)
[2024-11-08 00:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:37][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.07115741074085236, acc: 1.0)
[2024-11-08 00:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:38][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 0.24152106046676636, acc: 0.9047619104385376)
[2024-11-08 00:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:38][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 0.3002397119998932, acc: 0.8947368264198303)
[2024-11-08 00:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:38][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 0.28541049361228943, acc: 0.8999999761581421)
[2024-11-08 00:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:39][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 0.339466392993927, acc: 0.9047619104385376)
[2024-11-08 00:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:39][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 0.3484821617603302, acc: 0.9090909361839294)
[2024-11-08 00:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:39][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.02945738099515438, acc: 1.0)
[2024-11-08 00:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:42][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 0.30617383122444153, acc: 0.8571428656578064)
[2024-11-08 00:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:43][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 0.1518489569425583, acc: 0.9523809552192688)
[2024-11-08 00:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:44][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 0.36575520038604736, acc: 0.8947368264198303)
[2024-11-08 00:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:44][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 0.29404589533805847, acc: 0.8181818127632141)
[2024-11-08 00:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:45][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 0.29805535078048706, acc: 0.8947368264198303)
[2024-11-08 00:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:45][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 0.11801832914352417, acc: 0.9523809552192688)
[2024-11-08 00:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:45][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.1852625161409378, acc: 0.8888888955116272)
[2024-11-08 00:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:46][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.15746824443340302, acc: 0.8928571343421936)
[2024-11-08 00:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:46][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.38968250155448914, acc: 0.8095238208770752)
[2024-11-08 00:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:47][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.3599258363246918, acc: 0.8999999761581421)
[2024-11-08 00:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:47][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 0.22315435111522675, acc: 0.9473684430122375)
[2024-11-08 00:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:47][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 0.34276583790779114, acc: 0.8500000238418579)
[2024-11-08 00:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:48][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.009773890487849712, acc: 1.0)
[2024-11-08 00:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:48][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 0.09622571617364883, acc: 0.9696969985961914)
[2024-11-08 00:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:48][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 0.2297937124967575, acc: 0.9259259104728699)
[2024-11-08 00:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:49][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 0.27983301877975464, acc: 0.8999999761581421)
[2024-11-08 00:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:49][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 0.12952859699726105, acc: 0.9047619104385376)
[2024-11-08 00:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:49][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 0.41689106822013855, acc: 0.8421052694320679)
[2024-11-08 00:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:50][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 0.22401635348796844, acc: 0.949999988079071)
[2024-11-08 00:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:50][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 0.3116143047809601, acc: 0.9047619104385376)
[2024-11-08 00:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:50][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 0.16831399500370026, acc: 0.9090909361839294)
[2024-11-08 00:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:51][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.12562605738639832, acc: 0.9642857313156128)
[2024-11-08 00:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:51][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.08603917807340622, acc: 1.0)
[2024-11-08 00:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:51][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.1483653485774994, acc: 0.9428571462631226)
[2024-11-08 00:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:52][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.18443447351455688, acc: 0.9230769276618958)
[2024-11-08 00:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:52][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 0.1037830039858818, acc: 0.9523809552192688)
[2024-11-08 00:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:52][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 0.37594133615493774, acc: 0.7894737124443054)
[2024-11-08 00:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:53][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.22630250453948975, acc: 0.9473684430122375)
[2024-11-08 00:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:53][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.06198783218860626, acc: 1.0)
[2024-11-08 00:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:53][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 0.34454643726348877, acc: 0.9047619104385376)
[2024-11-08 00:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:54][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.039609890431165695, acc: 1.0)
[2024-11-08 00:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:54][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 0.2715208828449249, acc: 0.8846153616905212)
[2024-11-08 00:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:54][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 0.11574597656726837, acc: 0.9523809552192688)
[2024-11-08 00:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:55][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 0.3883856236934662, acc: 0.7894737124443054)
[2024-11-08 00:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:56][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 0.4718762934207916, acc: 0.7894737124443054)
[2024-11-08 00:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:57][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 0.38204288482666016, acc: 0.9090909361839294)
[2024-11-08 00:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:58][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 0.49708324670791626, acc: 0.9047619104385376)
[2024-11-08 00:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:58][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 0.7588002681732178, acc: 0.7692307829856873)
[2024-11-08 00:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:59][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 0.23554524779319763, acc: 0.9285714030265808)
[2024-11-08 00:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:59][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 0.30495113134384155, acc: 0.8571428656578064)
[2024-11-08 00:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:35:59][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 0.3997478485107422, acc: 0.7894737124443054)
[2024-11-08 00:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:00][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 0.2720213532447815, acc: 0.8999999761581421)
[2024-11-08 00:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:00][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 0.6073165535926819, acc: 0.8095238208770752)
[2024-11-08 00:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:01][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 0.4926615059375763, acc: 0.7727272510528564)
[2024-11-08 00:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:01][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.14087308943271637, acc: 0.9523809552192688)
[2024-11-08 00:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:01][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.3138957917690277, acc: 0.8799999952316284)
[2024-11-08 00:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:02][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.41919317841529846, acc: 0.8571428656578064)
[2024-11-08 00:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:02][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.23861929774284363, acc: 0.8947368264198303)
[2024-11-08 00:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:02][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.17686624825000763, acc: 0.9545454382896423)
[2024-11-08 00:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:03][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.3762094974517822, acc: 0.7894737124443054)
[2024-11-08 00:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:03][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.2871135175228119, acc: 0.8636363744735718)
[2024-11-08 00:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:03][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.17468594014644623, acc: 0.9642857313156128)
[2024-11-08 00:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:04][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.31531694531440735, acc: 0.8333333134651184)
[2024-11-08 00:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:04][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.3826214075088501, acc: 0.8571428656578064)
[2024-11-08 00:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:04][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.2093079835176468, acc: 0.9523809552192688)
[2024-11-08 00:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:05][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 0.31295689940452576, acc: 0.8421052694320679)
[2024-11-08 00:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:05][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 0.14914436638355255, acc: 0.9090909361839294)
[2024-11-08 00:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:05][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 0.2068655639886856, acc: 0.9473684430122375)
[2024-11-08 00:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:06][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.19592593610286713, acc: 0.9545454382896423)
[2024-11-08 00:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:06][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.180076465010643, acc: 0.9615384340286255)
[2024-11-08 00:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:06][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 0.2734946608543396, acc: 0.9200000166893005)
[2024-11-08 00:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:07][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 0.33776748180389404, acc: 0.8999999761581421)
[2024-11-08 00:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:07][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 0.2126486897468567, acc: 0.9473684430122375)
[2024-11-08 00:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:08][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 0.2700187861919403, acc: 0.8636363744735718)
[2024-11-08 00:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:08][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.2160307765007019, acc: 0.9473684430122375)
[2024-11-08 00:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:09][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.43166491389274597, acc: 0.8636363744735718)
[2024-11-08 00:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:09][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.14789126813411713, acc: 0.9642857313156128)
[2024-11-08 00:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:09][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.4572100341320038, acc: 0.8846153616905212)
[2024-11-08 00:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:09][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 0.29238206148147583, acc: 0.949999988079071)
[2024-11-08 00:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:10][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 0.2917226254940033, acc: 0.8500000238418579)
[2024-11-08 00:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:10][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 0.24450033903121948, acc: 0.8947368264198303)
[2024-11-08 00:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:10][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 0.2649179697036743, acc: 0.8636363744735718)
[2024-11-08 00:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:11][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 0.48962825536727905, acc: 0.800000011920929)
[2024-11-08 00:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:11][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 0.10002394020557404, acc: 0.9545454382896423)
[2024-11-08 00:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:12][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.14233550429344177, acc: 0.9696969985961914)
[2024-11-08 00:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:12][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.16422110795974731, acc: 0.9629629850387573)
[2024-11-08 00:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:12][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 0.20295846462249756, acc: 0.9696969985961914)
[2024-11-08 00:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:13][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 0.3749659061431885, acc: 0.8999999761581421)
[2024-11-08 00:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:13][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 0.20050902664661407, acc: 0.8999999761581421)
[2024-11-08 00:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:13][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.5823666453361511, acc: 0.7368420958518982)
[2024-11-08 00:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:14][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 0.24935020506381989, acc: 0.9090909361839294)
[2024-11-08 00:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:14][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 0.4753190577030182, acc: 0.800000011920929)
[2024-11-08 00:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:14][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 0.17012804746627808, acc: 0.9090909361839294)
[2024-11-08 00:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:43][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2829, device='cuda:0') eval_epoch_loss=tensor(0.2492, device='cuda:0') eval_epoch_acc=tensor(0.9071, device='cuda:0')
[2024-11-08 00:36:43][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:36:43][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:36:44][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_3_step_139_loss_0.2491619884967804/model.pt
[2024-11-08 00:36:44][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:36:44][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.2491619884967804
[2024-11-08 00:36:44][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.907089352607727
[2024-11-08 00:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:44][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 0.21318279206752777, acc: 0.939393937587738)
[2024-11-08 00:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:44][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 0.17606109380722046, acc: 0.9259259104728699)
[2024-11-08 00:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:45][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 0.28302162885665894, acc: 0.8787878751754761)
[2024-11-08 00:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:45][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 0.3701443374156952, acc: 0.8500000238418579)
[2024-11-08 00:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:45][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 0.4090675413608551, acc: 0.8500000238418579)
[2024-11-08 00:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:46][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 0.2616453170776367, acc: 0.8421052694320679)
[2024-11-08 00:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:46][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 0.18680769205093384, acc: 0.9545454382896423)
[2024-11-08 00:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:47][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 0.37505316734313965, acc: 0.8999999761581421)
[2024-11-08 00:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:47][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 0.18187262117862701, acc: 0.9090909361839294)
[2024-11-08 00:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:47][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 0.16306690871715546, acc: 0.9090909361839294)
[2024-11-08 00:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:48][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 0.27796676754951477, acc: 0.9629629850387573)
[2024-11-08 00:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:48][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 0.27559858560562134, acc: 0.9032257795333862)
[2024-11-08 00:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:48][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 0.29898568987846375, acc: 0.8999999761581421)
[2024-11-08 00:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:48][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 0.11701111495494843, acc: 1.0)
[2024-11-08 00:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:49][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 0.2985421121120453, acc: 0.8421052694320679)
[2024-11-08 00:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:49][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 0.0881042331457138, acc: 1.0)
[2024-11-08 00:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:49][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.23917467892169952, acc: 0.949999988079071)
[2024-11-08 00:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:50][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.18477864563465118, acc: 0.9545454382896423)
[2024-11-08 00:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:50][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 0.2167876958847046, acc: 0.939393937587738)
[2024-11-08 00:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:52][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 0.20885653793811798, acc: 0.949999988079071)
[2024-11-08 00:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:52][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 0.26760175824165344, acc: 0.8500000238418579)
[2024-11-08 00:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:52][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 0.3615780472755432, acc: 0.8947368264198303)
[2024-11-08 00:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:53][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 0.34078747034072876, acc: 0.9090909361839294)
[2024-11-08 00:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:53][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 0.6190868616104126, acc: 0.800000011920929)
[2024-11-08 00:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:54][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.20129163563251495, acc: 0.9200000166893005)
[2024-11-08 00:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:54][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.17460358142852783, acc: 0.9259259104728699)
[2024-11-08 00:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:54][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 0.18820016086101532, acc: 0.9166666865348816)
[2024-11-08 00:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:55][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.19244039058685303, acc: 0.9130434989929199)
[2024-11-08 00:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:55][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.18445155024528503, acc: 0.9047619104385376)
[2024-11-08 00:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:55][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 0.2186579406261444, acc: 0.9473684430122375)
[2024-11-08 00:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:56][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 0.17156578600406647, acc: 0.9090909361839294)
[2024-11-08 00:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:57][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 0.1760597974061966, acc: 0.8999999761581421)
[2024-11-08 00:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:57][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.062007710337638855, acc: 1.0)
[2024-11-08 00:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:57][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.2066529244184494, acc: 0.939393937587738)
[2024-11-08 00:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:58][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.30524739623069763, acc: 0.8999999761581421)
[2024-11-08 00:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:58][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 0.3455503284931183, acc: 0.8571428656578064)
[2024-11-08 00:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:58][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 0.3469057083129883, acc: 0.8571428656578064)
[2024-11-08 00:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:59][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.2051451951265335, acc: 0.9473684430122375)
[2024-11-08 00:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:36:59][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 0.3202267587184906, acc: 0.8636363744735718)
[2024-11-08 00:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:00][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 0.33088862895965576, acc: 0.8947368264198303)
[2024-11-08 00:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:00][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.23832806944847107, acc: 0.9545454382896423)
[2024-11-08 00:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:01][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.05553550273180008, acc: 1.0)
[2024-11-08 00:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:01][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.12110913544893265, acc: 0.9666666388511658)
[2024-11-08 00:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:01][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.17219756543636322, acc: 0.9142857193946838)
[2024-11-08 00:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:02][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 0.14313767850399017, acc: 0.95652174949646)
[2024-11-08 00:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:02][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 0.20553553104400635, acc: 0.9047619104385376)
[2024-11-08 00:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:02][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 0.2124319076538086, acc: 0.9473684430122375)
[2024-11-08 00:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:03][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 0.26763907074928284, acc: 0.8500000238418579)
[2024-11-08 00:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:03][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 0.15927326679229736, acc: 0.9047619104385376)
[2024-11-08 00:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:04][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 0.09488697350025177, acc: 1.0)
[2024-11-08 00:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:04][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.472075492143631, acc: 0.8799999952316284)
[2024-11-08 00:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:04][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 0.4037427008152008, acc: 0.800000011920929)
[2024-11-08 00:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:05][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 0.74141925573349, acc: 0.8095238208770752)
[2024-11-08 00:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:06][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 0.36960363388061523, acc: 0.8421052694320679)
[2024-11-08 00:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:07][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 0.2077719122171402, acc: 0.8636363744735718)
[2024-11-08 00:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:08][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 0.3985110819339752, acc: 0.8571428656578064)
[2024-11-08 00:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:09][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 0.1587435007095337, acc: 0.9166666865348816)
[2024-11-08 00:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:09][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.10360363125801086, acc: 0.9677419066429138)
[2024-11-08 00:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:09][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 0.27600300312042236, acc: 0.8999999761581421)
[2024-11-08 00:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:10][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 0.08344395458698273, acc: 1.0)
[2024-11-08 00:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:10][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 0.2452685534954071, acc: 0.8999999761581421)
[2024-11-08 00:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:11][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 0.29086071252822876, acc: 0.8947368264198303)
[2024-11-08 00:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:11][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 0.2658849060535431, acc: 0.9090909361839294)
[2024-11-08 00:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:11][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 0.4240117073059082, acc: 0.8500000238418579)
[2024-11-08 00:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:11][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 0.24137406051158905, acc: 0.8695651888847351)
[2024-11-08 00:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:12][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.27059128880500793, acc: 0.8928571343421936)
[2024-11-08 00:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:12][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 0.29804959893226624, acc: 0.8571428656578064)
[2024-11-08 00:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:13][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 0.207222580909729, acc: 0.9473684430122375)
[2024-11-08 00:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:13][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 0.40827345848083496, acc: 0.7894737124443054)
[2024-11-08 00:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:13][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.41862431168556213, acc: 0.8636363744735718)
[2024-11-08 00:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:14][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.2634330987930298, acc: 0.8571428656578064)
[2024-11-08 00:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:14][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 0.13684900104999542, acc: 0.9166666865348816)
[2024-11-08 00:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:14][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.2384006530046463, acc: 0.9354838728904724)
[2024-11-08 00:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:15][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.221063494682312, acc: 0.9354838728904724)
[2024-11-08 00:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:15][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.391815721988678, acc: 0.8461538553237915)
[2024-11-08 00:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:16][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.26022329926490784, acc: 0.9047619104385376)
[2024-11-08 00:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:16][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.2561052739620209, acc: 0.8947368264198303)
[2024-11-08 00:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:16][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.20814573764801025, acc: 0.8947368264198303)
[2024-11-08 00:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:17][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.14523322880268097, acc: 0.9545454382896423)
[2024-11-08 00:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:17][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.18929380178451538, acc: 0.9523809552192688)
[2024-11-08 00:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:17][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.17025114595890045, acc: 0.9583333134651184)
[2024-11-08 00:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:18][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.06972636282444, acc: 0.9677419066429138)
[2024-11-08 00:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:18][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.18669600784778595, acc: 0.9354838728904724)
[2024-11-08 00:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:18][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 0.13335663080215454, acc: 0.9615384340286255)
[2024-11-08 00:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:19][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.11058491468429565, acc: 0.9523809552192688)
[2024-11-08 00:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:20][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.5122254490852356, acc: 0.8421052694320679)
[2024-11-08 00:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:20][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 0.255051851272583, acc: 0.8421052694320679)
[2024-11-08 00:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:20][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 0.29852721095085144, acc: 0.9090909361839294)
[2024-11-08 00:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:21][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.4002234637737274, acc: 0.9047619104385376)
[2024-11-08 00:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:21][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.2822628915309906, acc: 0.9166666865348816)
[2024-11-08 00:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:21][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 0.4232584536075592, acc: 0.782608687877655)
[2024-11-08 00:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:22][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 0.2723301351070404, acc: 0.9047619104385376)
[2024-11-08 00:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:22][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 0.33439168334007263, acc: 0.7894737124443054)
[2024-11-08 00:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:22][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 0.26290270686149597, acc: 0.8421052694320679)
[2024-11-08 00:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:23][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 0.4018956124782562, acc: 0.8181818127632141)
[2024-11-08 00:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:23][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 0.32276421785354614, acc: 0.8636363744735718)
[2024-11-08 00:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:24][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.19186556339263916, acc: 0.9285714030265808)
[2024-11-08 00:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:24][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 0.29176753759384155, acc: 0.8888888955116272)
[2024-11-08 00:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:24][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 0.2714300751686096, acc: 0.9090909361839294)
[2024-11-08 00:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:25][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 0.22830583155155182, acc: 0.9090909361839294)
[2024-11-08 00:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:25][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.34427428245544434, acc: 0.9047619104385376)
[2024-11-08 00:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:25][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 0.2462455779314041, acc: 0.8947368264198303)
[2024-11-08 00:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:25][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 0.20349858701229095, acc: 0.9090909361839294)
[2024-11-08 00:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:26][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 0.42964041233062744, acc: 0.8421052694320679)
[2024-11-08 00:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:27][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 0.30704593658447266, acc: 0.9090909361839294)
[2024-11-08 00:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:27][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.06117283180356026, acc: 0.9642857313156128)
[2024-11-08 00:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:27][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.2571209669113159, acc: 0.9333333373069763)
[2024-11-08 00:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:27][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.2411884069442749, acc: 0.9428571462631226)
[2024-11-08 00:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:28][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.15057440102100372, acc: 0.95652174949646)
[2024-11-08 00:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:28][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.3563051223754883, acc: 0.8571428656578064)
[2024-11-08 00:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:29][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.3154070973396301, acc: 0.8947368264198303)
[2024-11-08 00:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:29][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.48025330901145935, acc: 0.800000011920929)
[2024-11-08 00:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:29][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.32635733485221863, acc: 0.8571428656578064)
[2024-11-08 00:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:30][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.14802373945713043, acc: 0.9090909361839294)
[2024-11-08 00:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:30][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.08722029626369476, acc: 0.9642857313156128)
[2024-11-08 00:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:30][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.17049598693847656, acc: 0.9655172228813171)
[2024-11-08 00:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:30][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.2963690459728241, acc: 0.9142857193946838)
[2024-11-08 00:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:31][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.1534632295370102, acc: 1.0)
[2024-11-08 00:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:31][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.3360247015953064, acc: 0.8095238208770752)
[2024-11-08 00:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:31][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.23071880638599396, acc: 0.8421052694320679)
[2024-11-08 00:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:32][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.19034495949745178, acc: 0.9090909361839294)
[2024-11-08 00:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:33][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.1617499142885208, acc: 0.9473684430122375)
[2024-11-08 00:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:33][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.08407619595527649, acc: 0.9545454382896423)
[2024-11-08 00:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:33][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.24023336172103882, acc: 0.9411764740943909)
[2024-11-08 00:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:34][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 0.158641517162323, acc: 1.0)
[2024-11-08 00:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:34][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 0.2273774892091751, acc: 0.8999999761581421)
[2024-11-08 00:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:35][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 0.3150273561477661, acc: 0.8947368264198303)
[2024-11-08 00:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:35][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 0.3762023448944092, acc: 0.8695651888847351)
[2024-11-08 00:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:35][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 0.2928486466407776, acc: 0.8095238208770752)
[2024-11-08 00:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:36][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 0.14554458856582642, acc: 0.9655172228813171)
[2024-11-08 00:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:36][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.2818948030471802, acc: 0.8888888955116272)
[2024-11-08 00:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:36][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.22812722623348236, acc: 0.8846153616905212)
[2024-11-08 00:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:37][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.06011810153722763, acc: 1.0)
[2024-11-08 00:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:37][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.09301344305276871, acc: 0.9473684430122375)
[2024-11-08 00:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:38][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 0.15454010665416718, acc: 0.9473684430122375)
[2024-11-08 00:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:38][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.2193763703107834, acc: 0.9090909361839294)
[2024-11-08 00:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:38][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.1895558089017868, acc: 0.9047619104385376)
[2024-11-08 00:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:38][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.12287125736474991, acc: 0.9655172228813171)
[2024-11-08 00:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:39][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.13028208911418915, acc: 0.9629629850387573)
[2024-11-08 00:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:39][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 0.25799092650413513, acc: 0.8571428656578064)
[2024-11-08 00:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:39][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.1456977128982544, acc: 0.9047619104385376)
[2024-11-08 00:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:40][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.1297181397676468, acc: 0.9444444179534912)
[2024-11-08 00:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:40][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 0.2501824200153351, acc: 0.9545454382896423)
[2024-11-08 00:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:09][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2800, device='cuda:0') eval_epoch_loss=tensor(0.2469, device='cuda:0') eval_epoch_acc=tensor(0.9152, device='cuda:0')
[2024-11-08 00:38:09][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:38:09][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:38:09][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_3_step_282_loss_0.24686865508556366/model.pt
[2024-11-08 00:38:09][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:38:09][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.24686865508556366
[2024-11-08 00:38:09][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.9151889085769653
[2024-11-08 00:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:10][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 0.09795357286930084, acc: 1.0)
[2024-11-08 00:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:10][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 0.660071074962616, acc: 0.8620689511299133)
[2024-11-08 00:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:10][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 0.4361611306667328, acc: 0.8333333134651184)
[2024-11-08 00:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:10][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.4756213128566742, acc: 0.8620689511299133)
[2024-11-08 00:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:11][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.20929311215877533, acc: 0.8571428656578064)
[2024-11-08 00:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:11][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 0.43918564915657043, acc: 0.8421052694320679)
[2024-11-08 00:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:11][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.4933861196041107, acc: 0.8421052694320679)
[2024-11-08 00:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:12][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 0.5037736296653748, acc: 0.8181818127632141)
[2024-11-08 00:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:12][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 0.4866669476032257, acc: 0.8181818127632141)
[2024-11-08 00:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:12][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.09142297506332397, acc: 0.9599999785423279)
[2024-11-08 00:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:13][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.21695171296596527, acc: 0.9333333373069763)
[2024-11-08 00:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:13][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.3135831356048584, acc: 0.9047619104385376)
[2024-11-08 00:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:14][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.29577475786209106, acc: 0.8999999761581421)
[2024-11-08 00:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:14][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 0.39183467626571655, acc: 0.7368420958518982)
[2024-11-08 00:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:14][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 0.30089953541755676, acc: 0.8636363744735718)
[2024-11-08 00:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:15][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.36049437522888184, acc: 0.9047619104385376)
[2024-11-08 00:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:15][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 0.1601598858833313, acc: 0.9583333134651184)
[2024-11-08 00:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:15][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.20288312435150146, acc: 0.9354838728904724)
[2024-11-08 00:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:15][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.32875677943229675, acc: 0.8709677457809448)
[2024-11-08 00:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:16][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.2877700924873352, acc: 0.8461538553237915)
[2024-11-08 00:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:16][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.17282883822917938, acc: 0.9047619104385376)
[2024-11-08 00:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:16][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.2102869600057602, acc: 0.9473684430122375)
[2024-11-08 00:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:17][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.17497026920318604, acc: 0.8947368264198303)
[2024-11-08 00:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:17][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.20084434747695923, acc: 0.95652174949646)
[2024-11-08 00:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:17][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.2027306705713272, acc: 0.9090909361839294)
[2024-11-08 00:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:18][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.03146330639719963, acc: 1.0)
[2024-11-08 00:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:18][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.10246474295854568, acc: 1.0)
[2024-11-08 00:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:18][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.5475726127624512, acc: 0.7894737124443054)
[2024-11-08 00:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:19][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.2680712640285492, acc: 0.8947368264198303)
[2024-11-08 00:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:19][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.33799269795417786, acc: 0.8636363744735718)
[2024-11-08 00:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:19][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.3180842101573944, acc: 0.8636363744735718)
[2024-11-08 00:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:19][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.16629068553447723, acc: 0.9285714030265808)
[2024-11-08 00:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:20][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.23717214167118073, acc: 0.8888888955116272)
[2024-11-08 00:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:20][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.2730333209037781, acc: 0.8857142925262451)
[2024-11-08 00:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:20][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.17377151548862457, acc: 0.9230769276618958)
[2024-11-08 00:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:21][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.02939719334244728, acc: 1.0)
[2024-11-08 00:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:21][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.28293901681900024, acc: 0.8500000238418579)
[2024-11-08 00:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:21][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.0903831347823143, acc: 0.949999988079071)
[2024-11-08 00:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:22][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.15970072150230408, acc: 0.9523809552192688)
[2024-11-08 00:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:22][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.022136090323328972, acc: 1.0)
[2024-11-08 00:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:22][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 0.2650981843471527, acc: 0.9130434989929199)
[2024-11-08 00:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:23][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 0.44335034489631653, acc: 0.8571428656578064)
[2024-11-08 00:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:23][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 0.5136469602584839, acc: 0.8421052694320679)
[2024-11-08 00:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:23][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 0.3448292315006256, acc: 0.8181818127632141)
[2024-11-08 00:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:24][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 0.32229939103126526, acc: 0.8947368264198303)
[2024-11-08 00:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:24][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.2293979674577713, acc: 0.875)
[2024-11-08 00:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:24][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.30122342705726624, acc: 0.8620689511299133)
[2024-11-08 00:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:24][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.3067885637283325, acc: 0.8518518805503845)
[2024-11-08 00:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:25][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.21619465947151184, acc: 0.9047619104385376)
[2024-11-08 00:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:25][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.18356263637542725, acc: 0.9473684430122375)
[2024-11-08 00:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:25][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.18186458945274353, acc: 0.9090909361839294)
[2024-11-08 00:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:26][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.6852047443389893, acc: 0.7894737124443054)
[2024-11-08 00:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:26][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.24940328299999237, acc: 0.9545454382896423)
[2024-11-08 00:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:27][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.468992680311203, acc: 0.807692289352417)
[2024-11-08 00:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:27][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 0.30760136246681213, acc: 0.875)
[2024-11-08 00:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:27][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 0.4240236282348633, acc: 0.8571428656578064)
[2024-11-08 00:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:28][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 0.30593937635421753, acc: 0.8260869383811951)
[2024-11-08 00:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:28][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 0.20730207860469818, acc: 0.9090909361839294)
[2024-11-08 00:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:28][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.24650536477565765, acc: 0.9666666388511658)
[2024-11-08 00:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:29][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 0.25691571831703186, acc: 0.8484848737716675)
[2024-11-08 00:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:29][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 0.20789703726768494, acc: 0.8999999761581421)
[2024-11-08 00:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:29][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.38128599524497986, acc: 0.800000011920929)
[2024-11-08 00:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:30][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.37200164794921875, acc: 0.8947368264198303)
[2024-11-08 00:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:30][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.1680612862110138, acc: 1.0)
[2024-11-08 00:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:30][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 0.3955622613430023, acc: 0.800000011920929)
[2024-11-08 00:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:30][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.1582234650850296, acc: 0.9583333134651184)
[2024-11-08 00:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:31][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.24169136583805084, acc: 0.95652174949646)
[2024-11-08 00:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:31][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 0.18537423014640808, acc: 0.949999988079071)
[2024-11-08 00:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:32][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 0.34314411878585815, acc: 0.8421052694320679)
[2024-11-08 00:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:32][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.2778262794017792, acc: 0.8636363744735718)
[2024-11-08 00:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:32][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 0.2788556218147278, acc: 0.8999999761581421)
[2024-11-08 00:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:33][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.23259897530078888, acc: 0.9200000166893005)
[2024-11-08 00:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:33][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.40531471371650696, acc: 0.8399999737739563)
[2024-11-08 00:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:33][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 0.34787604212760925, acc: 0.8571428656578064)
[2024-11-08 00:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:34][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 0.35823139548301697, acc: 0.800000011920929)
[2024-11-08 00:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:34][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.3207094371318817, acc: 0.8571428656578064)
[2024-11-08 00:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:34][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 0.239786759018898, acc: 0.8636363744735718)
[2024-11-08 00:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:35][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.08242972940206528, acc: 0.9642857313156128)
[2024-11-08 00:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:35][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.24368184804916382, acc: 0.8965517282485962)
[2024-11-08 00:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:35][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.15305833518505096, acc: 0.9142857193946838)
[2024-11-08 00:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:35][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.16769514977931976, acc: 0.9090909361839294)
[2024-11-08 00:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:36][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.2815679609775543, acc: 0.8947368264198303)
[2024-11-08 00:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:36][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.2517073154449463, acc: 0.9047619104385376)
[2024-11-08 00:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:36][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.168728306889534, acc: 0.9473684430122375)
[2024-11-08 00:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:37][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.0849383994936943, acc: 1.0)
[2024-11-08 00:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:37][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.1315789520740509, acc: 0.9629629850387573)
[2024-11-08 00:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:37][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.1779433637857437, acc: 0.90625)
[2024-11-08 00:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:38][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.1984560638666153, acc: 0.9047619104385376)
[2024-11-08 00:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:38][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 0.40185388922691345, acc: 0.8571428656578064)
[2024-11-08 00:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:39][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.2670437693595886, acc: 0.8947368264198303)
[2024-11-08 00:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:39][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.16687631607055664, acc: 0.9090909361839294)
[2024-11-08 00:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:40][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.15972180664539337, acc: 0.8947368264198303)
[2024-11-08 00:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:40][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.0937005952000618, acc: 1.0)
[2024-11-08 00:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:40][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.023826245218515396, acc: 1.0)
[2024-11-08 00:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:40][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.1447652280330658, acc: 0.9677419066429138)
[2024-11-08 00:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:41][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.3899531960487366, acc: 0.9047619104385376)
[2024-11-08 00:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:41][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.1622970551252365, acc: 0.9523809552192688)
[2024-11-08 00:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:42][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.14534229040145874, acc: 0.9473684430122375)
[2024-11-08 00:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:42][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.26288965344429016, acc: 0.8636363744735718)
[2024-11-08 00:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:43][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.2555685341358185, acc: 0.8947368264198303)
[2024-11-08 00:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:44][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.09747254848480225, acc: 0.9545454382896423)
[2024-11-08 00:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:44][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.02150857262313366, acc: 1.0)
[2024-11-08 00:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:44][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.034314531832933426, acc: 1.0)
[2024-11-08 00:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:45][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.024958549067378044, acc: 1.0)
[2024-11-08 00:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:45][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.1213236078619957, acc: 0.949999988079071)
[2024-11-08 00:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:45][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.11350499838590622, acc: 1.0)
[2024-11-08 00:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:46][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.02811788022518158, acc: 1.0)
[2024-11-08 00:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:46][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.20455217361450195, acc: 0.8999999761581421)
[2024-11-08 00:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:46][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.056848201900720596, acc: 1.0)
[2024-11-08 00:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:46][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 0.09935744851827621, acc: 0.9523809552192688)
[2024-11-08 00:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:47][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 0.36325588822364807, acc: 0.8421052694320679)
[2024-11-08 00:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:47][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 0.297304630279541, acc: 0.8421052694320679)
[2024-11-08 00:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:48][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 0.30497118830680847, acc: 0.8636363744735718)
[2024-11-08 00:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:48][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 0.2510392367839813, acc: 0.9047619104385376)
[2024-11-08 00:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:48][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.08849198371171951, acc: 1.0)
[2024-11-08 00:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:49][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.25081807374954224, acc: 0.9333333373069763)
[2024-11-08 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:49][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.23593451082706451, acc: 0.9523809552192688)
[2024-11-08 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:49][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.4393068552017212, acc: 0.8888888955116272)
[2024-11-08 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:50][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.19411039352416992, acc: 0.9090909361839294)
[2024-11-08 00:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:50][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.08949310332536697, acc: 1.0)
[2024-11-08 00:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:50][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.029796170070767403, acc: 1.0)
[2024-11-08 00:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:51][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.04391947016119957, acc: 1.0)
[2024-11-08 00:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:51][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.08591774851083755, acc: 0.9714285731315613)
[2024-11-08 00:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:51][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.09550762921571732, acc: 0.9615384340286255)
[2024-11-08 00:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:52][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.3050708472728729, acc: 0.9523809552192688)
[2024-11-08 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:52][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.29844507575035095, acc: 0.8421052694320679)
[2024-11-08 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:52][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.17498430609703064, acc: 0.949999988079071)
[2024-11-08 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:53][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.2506813406944275, acc: 0.9523809552192688)
[2024-11-08 00:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:53][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.02674509398639202, acc: 1.0)
[2024-11-08 00:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:53][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.2350151389837265, acc: 0.9285714030265808)
[2024-11-08 00:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:54][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.048835936933755875, acc: 1.0)
[2024-11-08 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:54][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.07992666214704514, acc: 0.9714285731315613)
[2024-11-08 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:54][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.08462551981210709, acc: 0.9615384340286255)
[2024-11-08 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:55][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 0.1725884974002838, acc: 0.9047619104385376)
[2024-11-08 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:55][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.12872739136219025, acc: 1.0)
[2024-11-08 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:55][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.0911463275551796, acc: 1.0)
[2024-11-08 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:55][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.11371135711669922, acc: 0.9545454382896423)
[2024-11-08 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:56][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.24176190793514252, acc: 0.9047619104385376)
[2024-11-08 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:56][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.05875213444232941, acc: 1.0)
[2024-11-08 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:56][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.22962509095668793, acc: 0.9032257795333862)
[2024-11-08 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:57][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.3060932159423828, acc: 0.9677419066429138)
[2024-11-08 00:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:57][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.23911410570144653, acc: 0.9230769276618958)
[2024-11-08 00:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:57][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.05135033652186394, acc: 1.0)
[2024-11-08 00:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:27][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2669, device='cuda:0') eval_epoch_loss=tensor(0.2366, device='cuda:0') eval_epoch_acc=tensor(0.9116, device='cuda:0')
[2024-11-08 00:39:27][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:39:27][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:39:27][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_3_step_425_loss_0.2365761548280716/model.pt
[2024-11-08 00:39:27][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:39:27][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.2365761548280716
[2024-11-08 00:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:27][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.10270126163959503, acc: 0.9473684430122375)
[2024-11-08 00:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:28][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.17075598239898682, acc: 0.9473684430122375)
[2024-11-08 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:28][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.1364818811416626, acc: 0.9545454382896423)
[2024-11-08 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:28][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.13049210608005524, acc: 0.9523809552192688)
[2024-11-08 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:29][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.03361886739730835, acc: 1.0)
[2024-11-08 00:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:29][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.011434805579483509, acc: 1.0)
[2024-11-08 00:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:29][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.017008624970912933, acc: 1.0)
[2024-11-08 00:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:30][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.060132842510938644, acc: 0.9615384340286255)
[2024-11-08 00:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:30][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.06067642569541931, acc: 1.0)
[2024-11-08 00:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:30][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.03342040255665779, acc: 1.0)
[2024-11-08 00:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:30][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.12650521099567413, acc: 0.9473684430122375)
[2024-11-08 00:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:31][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.06057758629322052, acc: 1.0)
[2024-11-08 00:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:31][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.08449514210224152, acc: 1.0)
[2024-11-08 00:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:31][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.0055802795104682446, acc: 1.0)
[2024-11-08 00:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:32][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 0.14329850673675537, acc: 0.9285714030265808)
[2024-11-08 00:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:32][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.038807179778814316, acc: 1.0)
[2024-11-08 00:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:33][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 0.42716753482818604, acc: 0.8421052694320679)
[2024-11-08 00:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:33][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 0.19619625806808472, acc: 0.9473684430122375)
[2024-11-08 00:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:34][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.1936999261379242, acc: 0.9545454382896423)
[2024-11-08 00:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:34][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.14291909337043762, acc: 0.9523809552192688)
[2024-11-08 00:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:35][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.04021556302905083, acc: 1.0)
[2024-11-08 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:35][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.3024948239326477, acc: 0.9677419066429138)
[2024-11-08 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:35][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.12176806479692459, acc: 0.9677419066429138)
[2024-11-08 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:36][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.20511430501937866, acc: 0.9354838728904724)
[2024-11-08 00:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:36][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.2319299727678299, acc: 0.8500000238418579)
[2024-11-08 00:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:36][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.018264947459101677, acc: 1.0)
[2024-11-08 00:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:37][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.11381664872169495, acc: 1.0)
[2024-11-08 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:37][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.0646810457110405, acc: 0.9545454382896423)
[2024-11-08 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:37][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.1325606256723404, acc: 0.949999988079071)
[2024-11-08 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:38][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.017435474321246147, acc: 1.0)
[2024-11-08 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:38][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.21220466494560242, acc: 0.9354838728904724)
[2024-11-08 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:38][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 0.18523970246315002, acc: 0.8999999761581421)
[2024-11-08 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:39][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.15436869859695435, acc: 0.8999999761581421)
[2024-11-08 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:39][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 0.17332999408245087, acc: 0.9473684430122375)
[2024-11-08 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:39][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.19274654984474182, acc: 0.8636363744735718)
[2024-11-08 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:39][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.228419229388237, acc: 0.949999988079071)
[2024-11-08 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:40][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.005852332804352045, acc: 1.0)
[2024-11-08 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:40][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.1096198707818985, acc: 0.9696969985961914)
[2024-11-08 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:40][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.12066826224327087, acc: 0.9259259104728699)
[2024-11-08 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:41][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.10088837146759033, acc: 0.9696969985961914)
[2024-11-08 00:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:41][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.044837355613708496, acc: 1.0)
[2024-11-08 00:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:41][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.10762761533260345, acc: 0.949999988079071)
[2024-11-08 00:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:42][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.22429096698760986, acc: 0.8421052694320679)
[2024-11-08 00:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:42][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 0.0735132172703743, acc: 1.0)
[2024-11-08 00:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:42][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.08533848822116852, acc: 0.9523809552192688)
[2024-11-08 00:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:43][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.016428036615252495, acc: 1.0)
[2024-11-08 00:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:43][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.1692948192358017, acc: 0.9666666388511658)
[2024-11-08 00:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:43][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 0.6678622364997864, acc: 0.7916666865348816)
[2024-11-08 00:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:44][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 0.22435006499290466, acc: 0.9047619104385376)
[2024-11-08 00:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:44][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.4311191141605377, acc: 0.8947368264198303)
[2024-11-08 00:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:45][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.21287384629249573, acc: 0.8500000238418579)
[2024-11-08 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:45][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.2080896943807602, acc: 0.9047619104385376)
[2024-11-08 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:45][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 0.4977077543735504, acc: 0.8636363744735718)
[2024-11-08 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:46][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.17110395431518555, acc: 0.9642857313156128)
[2024-11-08 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:46][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.25319817662239075, acc: 0.9032257795333862)
[2024-11-08 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:46][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.03423581272363663, acc: 1.0)
[2024-11-08 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:46][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.44052451848983765, acc: 0.8571428656578064)
[2024-11-08 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:47][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.2606324255466461, acc: 0.8947368264198303)
[2024-11-08 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:47][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 0.13477925956249237, acc: 0.9545454382896423)
[2024-11-08 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:47][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.15541788935661316, acc: 0.9545454382896423)
[2024-11-08 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:48][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.18354125320911407, acc: 0.9285714030265808)
[2024-11-08 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:48][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 0.32317906618118286, acc: 0.9259259104728699)
[2024-11-08 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:48][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.18412181735038757, acc: 0.9714285731315613)
[2024-11-08 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:49][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.5481568574905396, acc: 0.8461538553237915)
[2024-11-08 00:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:49][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 0.061906564980745316, acc: 0.9523809552192688)
[2024-11-08 00:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:49][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.45989322662353516, acc: 0.9473684430122375)
[2024-11-08 00:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:50][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.3626019358634949, acc: 0.8947368264198303)
[2024-11-08 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:50][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.20889462530612946, acc: 0.9090909361839294)
[2024-11-08 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:50][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.3389846086502075, acc: 0.9047619104385376)
[2024-11-08 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:50][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.24901829659938812, acc: 0.9166666865348816)
[2024-11-08 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:51][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 0.23248060047626495, acc: 0.9285714030265808)
[2024-11-08 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:51][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.202735036611557, acc: 0.949999988079071)
[2024-11-08 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:52][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.23784320056438446, acc: 0.8999999761581421)
[2024-11-08 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:52][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 0.45300137996673584, acc: 0.8421052694320679)
[2024-11-08 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:52][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 0.22592496871948242, acc: 0.9090909361839294)
[2024-11-08 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:53][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 0.5466927289962769, acc: 0.800000011920929)
[2024-11-08 00:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:53][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.05679259076714516, acc: 1.0)
[2024-11-08 00:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:53][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.046277496963739395, acc: 1.0)
[2024-11-08 00:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:53][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.14073556661605835, acc: 0.9354838728904724)
[2024-11-08 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:54][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.24544301629066467, acc: 0.9130434989929199)
[2024-11-08 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:54][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.09223571419715881, acc: 1.0)
[2024-11-08 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:54][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.34178152680397034, acc: 0.8421052694320679)
[2024-11-08 00:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:55][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 0.10569997876882553, acc: 0.949999988079071)
[2024-11-08 00:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:56][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.5345667600631714, acc: 0.800000011920929)
[2024-11-08 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:56][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.3713207244873047, acc: 0.9090909361839294)
[2024-11-08 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:56][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.07068639248609543, acc: 0.9696969985961914)
[2024-11-08 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:57][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.10926429182291031, acc: 0.9666666388511658)
[2024-11-08 00:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:59][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 0.24167096614837646, acc: 0.9285714030265808)
[2024-11-08 00:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:39:59][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.12077567726373672, acc: 0.9523809552192688)
[2024-11-08 00:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:00][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.38347896933555603, acc: 0.8421052694320679)
[2024-11-08 00:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:00][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.1655312478542328, acc: 1.0)
[2024-11-08 00:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:01][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.05826062709093094, acc: 1.0)
[2024-11-08 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:01][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.1531306803226471, acc: 0.9090909361839294)
[2024-11-08 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:01][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.2246057540178299, acc: 0.9285714030265808)
[2024-11-08 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:02][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.2901974022388458, acc: 0.9259259104728699)
[2024-11-08 00:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:02][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.220494344830513, acc: 0.9428571462631226)
[2024-11-08 00:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:03][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 0.08542586117982864, acc: 0.95652174949646)
[2024-11-08 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:03][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.15423065423965454, acc: 0.9523809552192688)
[2024-11-08 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:03][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.33051493763923645, acc: 0.8947368264198303)
[2024-11-08 00:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:04][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 0.2584975063800812, acc: 0.8636363744735718)
[2024-11-08 00:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:04][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.49262556433677673, acc: 0.8500000238418579)
[2024-11-08 00:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:05][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.2887862026691437, acc: 0.8636363744735718)
[2024-11-08 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:05][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.25924500823020935, acc: 0.939393937587738)
[2024-11-08 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:05][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 0.27820053696632385, acc: 0.8571428656578064)
[2024-11-08 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:05][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.13733460009098053, acc: 0.9047619104385376)
[2024-11-08 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:06][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 0.3554493188858032, acc: 0.8947368264198303)
[2024-11-08 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:06][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 0.35199248790740967, acc: 0.8500000238418579)
[2024-11-08 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:06][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 0.2713823914527893, acc: 0.9047619104385376)
[2024-11-08 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:07][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.09074569493532181, acc: 0.9545454382896423)
[2024-11-08 00:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:07][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.08012985438108444, acc: 0.9642857313156128)
[2024-11-08 00:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:07][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.12150779366493225, acc: 0.9629629850387573)
[2024-11-08 00:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:08][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.24027858674526215, acc: 0.9200000166893005)
[2024-11-08 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:08][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.11449258774518967, acc: 1.0)
[2024-11-08 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:08][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.37098637223243713, acc: 0.8947368264198303)
[2024-11-08 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:09][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.25707700848579407, acc: 0.8999999761581421)
[2024-11-08 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:09][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.2595736086368561, acc: 0.9047619104385376)
[2024-11-08 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:09][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.08186522126197815, acc: 0.9523809552192688)
[2024-11-08 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:10][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.009808367118239403, acc: 1.0)
[2024-11-08 00:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:10][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.11341500282287598, acc: 0.9259259104728699)
[2024-11-08 00:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:10][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.17098189890384674, acc: 0.9090909361839294)
[2024-11-08 00:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:11][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.055945973843336105, acc: 1.0)
[2024-11-08 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:11][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.031518470495939255, acc: 1.0)
[2024-11-08 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:11][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.13157913088798523, acc: 0.9523809552192688)
[2024-11-08 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:11][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.0939495638012886, acc: 0.9523809552192688)
[2024-11-08 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:12][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.02078164927661419, acc: 1.0)
[2024-11-08 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:12][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.020640645176172256, acc: 1.0)
[2024-11-08 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:12][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.05883434787392616, acc: 1.0)
[2024-11-08 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:13][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.1239783763885498, acc: 0.9259259104728699)
[2024-11-08 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:13][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.20229485630989075, acc: 0.9047619104385376)
[2024-11-08 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:13][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.12110072374343872, acc: 0.9473684430122375)
[2024-11-08 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:13][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.1827279031276703, acc: 0.949999988079071)
[2024-11-08 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:14][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.16596853733062744, acc: 0.9523809552192688)
[2024-11-08 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:14][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.04834417998790741, acc: 0.9545454382896423)
[2024-11-08 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:14][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.10049297660589218, acc: 0.9642857313156128)
[2024-11-08 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:15][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.16235297918319702, acc: 0.9629629850387573)
[2024-11-08 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:15][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.039933983236551285, acc: 1.0)
[2024-11-08 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:15][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.07036296278238297, acc: 1.0)
[2024-11-08 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:16][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.012724689207971096, acc: 1.0)
[2024-11-08 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:16][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.15811139345169067, acc: 0.9444444179534912)
[2024-11-08 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:16][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.1815260499715805, acc: 0.9545454382896423)
[2024-11-08 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:17][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.20964780449867249, acc: 0.949999988079071)
[2024-11-08 00:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:17][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.05479392036795616, acc: 0.9545454382896423)
[2024-11-08 00:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:17][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.12241224944591522, acc: 0.9629629850387573)
[2024-11-08 00:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:46][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3298, device='cuda:0') eval_epoch_loss=tensor(0.2851, device='cuda:0') eval_epoch_acc=tensor(0.9052, device='cuda:0')
[2024-11-08 00:40:46][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:40:46][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:40:47][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_3_step_568_loss_0.28506141901016235/model.pt
[2024-11-08 00:40:47][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:47][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.03391198813915253, acc: 1.0)
[2024-11-08 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:47][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.12231312692165375, acc: 0.949999988079071)
[2024-11-08 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:48][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.05056926608085632, acc: 1.0)
[2024-11-08 00:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:48][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.06968426704406738, acc: 1.0)
[2024-11-08 00:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:48][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.17806072533130646, acc: 0.9090909361839294)
[2024-11-08 00:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:49][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.3926190733909607, acc: 0.8500000238418579)
[2024-11-08 00:40:49][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=1.2584, train_epoch_loss=0.2299, epoch time 329.5955590158701s
[2024-11-08 00:40:49][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 00:40:49][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 00:40:49][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 00:40:49][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 00:40:49][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 00:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:50][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.08704371750354767, acc: 0.9642857313156128)
[2024-11-08 00:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:50][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.004471011459827423, acc: 1.0)
[2024-11-08 00:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:51][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.008237376809120178, acc: 1.0)
[2024-11-08 00:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:51][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.18164610862731934, acc: 0.9047619104385376)
[2024-11-08 00:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:51][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.1605682671070099, acc: 0.9523809552192688)
[2024-11-08 00:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:52][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.4038354158401489, acc: 0.8947368264198303)
[2024-11-08 00:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:52][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 0.03969163820147514, acc: 1.0)
[2024-11-08 00:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:52][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.07125058025121689, acc: 0.9523809552192688)
[2024-11-08 00:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:53][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.011597629636526108, acc: 1.0)
[2024-11-08 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:53][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.2873552441596985, acc: 0.9354838728904724)
[2024-11-08 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:53][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.0146672073751688, acc: 1.0)
[2024-11-08 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:54][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.04129349812865257, acc: 1.0)
[2024-11-08 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:54][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.06674091517925262, acc: 0.949999988079071)
[2024-11-08 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:54][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.13878951966762543, acc: 0.9473684430122375)
[2024-11-08 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:55][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.2505306601524353, acc: 0.95652174949646)
[2024-11-08 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:55][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.009287186898291111, acc: 1.0)
[2024-11-08 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:55][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.0908847525715828, acc: 0.9642857313156128)
[2024-11-08 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:55][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.10105516761541367, acc: 0.9285714030265808)
[2024-11-08 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:56][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 0.05888616666197777, acc: 1.0)
[2024-11-08 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:56][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.3653337061405182, acc: 0.8421052694320679)
[2024-11-08 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:56][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.18636111915111542, acc: 0.9473684430122375)
[2024-11-08 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:57][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.08439651131629944, acc: 1.0)
[2024-11-08 00:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:57][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.22468030452728271, acc: 0.9047619104385376)
[2024-11-08 00:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:57][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.012313904240727425, acc: 1.0)
[2024-11-08 00:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:58][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.3261272609233856, acc: 0.8999999761581421)
[2024-11-08 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:58][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.31618258357048035, acc: 0.8571428656578064)
[2024-11-08 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:58][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.3055691123008728, acc: 0.9047619104385376)
[2024-11-08 00:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:40:59][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 0.22796645760536194, acc: 0.8947368264198303)
[2024-11-08 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:00][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.0611569844186306, acc: 1.0)
[2024-11-08 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:00][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.3051931858062744, acc: 0.8999999761581421)
[2024-11-08 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:01][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.28751352429389954, acc: 0.9090909361839294)
[2024-11-08 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:01][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.17464220523834229, acc: 0.931034505367279)
[2024-11-08 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:01][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.41799795627593994, acc: 0.9090909361839294)
[2024-11-08 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:02][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.1894303560256958, acc: 0.9130434989929199)
[2024-11-08 00:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:02][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.07257720082998276, acc: 1.0)
[2024-11-08 00:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:02][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.23838409781455994, acc: 0.8947368264198303)
[2024-11-08 00:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:03][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.1385660618543625, acc: 0.949999988079071)
[2024-11-08 00:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:03][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.13591332733631134, acc: 0.9047619104385376)
[2024-11-08 00:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:03][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.014548190869390965, acc: 1.0)
[2024-11-08 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:04][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.13972963392734528, acc: 0.9642857313156128)
[2024-11-08 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:04][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.30001145601272583, acc: 0.8857142925262451)
[2024-11-08 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:04][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.25942549109458923, acc: 0.949999988079071)
[2024-11-08 00:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:05][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.22822901606559753, acc: 0.8999999761581421)
[2024-11-08 00:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:05][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.4797956049442291, acc: 0.8421052694320679)
[2024-11-08 00:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:06][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.2537865936756134, acc: 0.9090909361839294)
[2024-11-08 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:06][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.2392185777425766, acc: 0.9047619104385376)
[2024-11-08 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:06][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.10995129495859146, acc: 0.9583333134651184)
[2024-11-08 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:07][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.12237861007452011, acc: 0.9354838728904724)
[2024-11-08 00:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:07][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.05998022109270096, acc: 1.0)
[2024-11-08 00:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:07][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.07369493693113327, acc: 0.9599999785423279)
[2024-11-08 00:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:07][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.29503491520881653, acc: 0.9047619104385376)
[2024-11-08 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:08][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.21188056468963623, acc: 0.8947368264198303)
[2024-11-08 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:08][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 0.22021785378456116, acc: 0.8999999761581421)
[2024-11-08 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:08][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 0.311873197555542, acc: 0.8571428656578064)
[2024-11-08 00:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:09][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.282816618680954, acc: 0.9090909361839294)
[2024-11-08 00:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:09][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.022598225623369217, acc: 1.0)
[2024-11-08 00:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:12][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 0.303243488073349, acc: 0.8571428656578064)
[2024-11-08 00:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:13][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 0.17549288272857666, acc: 0.9523809552192688)
[2024-11-08 00:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:13][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 0.26163172721862793, acc: 0.8947368264198303)
[2024-11-08 00:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:14][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.2461191713809967, acc: 0.8636363744735718)
[2024-11-08 00:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:14][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.273075670003891, acc: 0.8947368264198303)
[2024-11-08 00:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:15][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.1414605677127838, acc: 0.9523809552192688)
[2024-11-08 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:15][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.09128174185752869, acc: 0.9722222089767456)
[2024-11-08 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:15][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.055396098643541336, acc: 1.0)
[2024-11-08 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:16][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.3152480721473694, acc: 0.9047619104385376)
[2024-11-08 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:16][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.19379211962223053, acc: 0.949999988079071)
[2024-11-08 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:16][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.25130191445350647, acc: 0.8421052694320679)
[2024-11-08 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:17][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.2404450923204422, acc: 0.8999999761581421)
[2024-11-08 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:17][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.047871172428131104, acc: 1.0)
[2024-11-08 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:17][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.04047759622335434, acc: 1.0)
[2024-11-08 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:18][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.16686828434467316, acc: 0.9629629850387573)
[2024-11-08 00:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:18][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 0.23452909290790558, acc: 0.9333333373069763)
[2024-11-08 00:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:18][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.10908263176679611, acc: 0.9523809552192688)
[2024-11-08 00:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:19][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 0.42695677280426025, acc: 0.8421052694320679)
[2024-11-08 00:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:19][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 0.21577942371368408, acc: 0.8999999761581421)
[2024-11-08 00:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:19][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 0.19024968147277832, acc: 0.9523809552192688)
[2024-11-08 00:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:20][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 0.0593067966401577, acc: 1.0)
[2024-11-08 00:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:20][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.06890025734901428, acc: 0.9642857313156128)
[2024-11-08 00:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:20][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.026563886553049088, acc: 1.0)
[2024-11-08 00:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:20][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.1020994707942009, acc: 0.9714285731315613)
[2024-11-08 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:21][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.02631751447916031, acc: 1.0)
[2024-11-08 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:21][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.047797441482543945, acc: 0.9523809552192688)
[2024-11-08 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:21][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.24478328227996826, acc: 0.8947368264198303)
[2024-11-08 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:22][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.14191141724586487, acc: 0.9473684430122375)
[2024-11-08 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:22][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.07134630531072617, acc: 0.9545454382896423)
[2024-11-08 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:22][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.09071759134531021, acc: 0.9523809552192688)
[2024-11-08 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:23][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.02893899567425251, acc: 1.0)
[2024-11-08 00:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:23][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 0.5311508774757385, acc: 0.8846153616905212)
[2024-11-08 00:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:24][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 0.09623904526233673, acc: 0.9047619104385376)
[2024-11-08 00:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:25][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 0.36003947257995605, acc: 0.8421052694320679)
[2024-11-08 00:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:25][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 0.30310019850730896, acc: 0.8421052694320679)
[2024-11-08 00:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:26][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 0.09332144260406494, acc: 1.0)
[2024-11-08 00:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:27][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.0962606817483902, acc: 0.9523809552192688)
[2024-11-08 00:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:28][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 0.397382527589798, acc: 0.807692289352417)
[2024-11-08 00:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:28][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 0.240693598985672, acc: 0.8214285969734192)
[2024-11-08 00:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:28][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 0.24501454830169678, acc: 0.8571428656578064)
[2024-11-08 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:29][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 0.37041541934013367, acc: 0.7894737124443054)
[2024-11-08 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:29][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 0.21709728240966797, acc: 0.8999999761581421)
[2024-11-08 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:29][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 0.6636922359466553, acc: 0.8095238208770752)
[2024-11-08 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:30][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 0.312880277633667, acc: 0.8636363744735718)
[2024-11-08 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:30][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.185675248503685, acc: 0.9523809552192688)
[2024-11-08 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:30][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.025893740355968475, acc: 1.0)
[2024-11-08 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:31][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.20417173206806183, acc: 0.9047619104385376)
[2024-11-08 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:31][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.06676016002893448, acc: 1.0)
[2024-11-08 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:31][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.09994272887706757, acc: 0.9545454382896423)
[2024-11-08 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:32][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.12838199734687805, acc: 0.9473684430122375)
[2024-11-08 00:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:32][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.3847215473651886, acc: 0.8636363744735718)
[2024-11-08 00:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:32][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.038218818604946136, acc: 1.0)
[2024-11-08 00:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:33][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.06636975705623627, acc: 1.0)
[2024-11-08 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:33][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.2652962803840637, acc: 0.9047619104385376)
[2024-11-08 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:33][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.08392240107059479, acc: 0.9523809552192688)
[2024-11-08 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:34][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.3035622537136078, acc: 0.8947368264198303)
[2024-11-08 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:34][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 0.14110960066318512, acc: 0.9090909361839294)
[2024-11-08 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:34][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.04761364683508873, acc: 1.0)
[2024-11-08 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:35][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.20386233925819397, acc: 0.9090909361839294)
[2024-11-08 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:35][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.09614010900259018, acc: 0.9615384340286255)
[2024-11-08 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:35][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.30337950587272644, acc: 0.9200000166893005)
[2024-11-08 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:36][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.16667777299880981, acc: 0.949999988079071)
[2024-11-08 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:36][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.11920402199029922, acc: 0.9473684430122375)
[2024-11-08 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:37][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 0.1281081587076187, acc: 0.9545454382896423)
[2024-11-08 00:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:37][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.15864971280097961, acc: 0.9473684430122375)
[2024-11-08 00:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:37][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.29730120301246643, acc: 0.8636363744735718)
[2024-11-08 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:38][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.0497422032058239, acc: 1.0)
[2024-11-08 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:38][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.5867675542831421, acc: 0.8461538553237915)
[2024-11-08 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:39][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 0.14565959572792053, acc: 0.949999988079071)
[2024-11-08 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:39][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 0.38468047976493835, acc: 0.8500000238418579)
[2024-11-08 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:39][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 0.20420223474502563, acc: 0.8947368264198303)
[2024-11-08 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:40][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 0.38719597458839417, acc: 0.8181818127632141)
[2024-11-08 00:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:40][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 0.46939438581466675, acc: 0.800000011920929)
[2024-11-08 00:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:40][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 0.14321781694889069, acc: 0.9545454382896423)
[2024-11-08 00:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:41][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.19225932657718658, acc: 0.939393937587738)
[2024-11-08 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:41][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.10979543626308441, acc: 0.9629629850387573)
[2024-11-08 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:41][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.21130238473415375, acc: 0.939393937587738)
[2024-11-08 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:41][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.35807591676712036, acc: 0.8500000238418579)
[2024-11-08 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:42][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.104121133685112, acc: 0.949999988079071)
[2024-11-08 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:42][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.5175161957740784, acc: 0.7894737124443054)
[2024-11-08 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:42][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.08549762517213821, acc: 0.9090909361839294)
[2024-11-08 00:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:11][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3078, device='cuda:0') eval_epoch_loss=tensor(0.2683, device='cuda:0') eval_epoch_acc=tensor(0.9025, device='cuda:0')
[2024-11-08 00:42:11][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:42:11][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:42:12][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_4_step_137_loss_0.2683221697807312/model.pt
[2024-11-08 00:42:12][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:12][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.6255882978439331, acc: 0.800000011920929)
[2024-11-08 00:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:13][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.109352707862854, acc: 1.0)
[2024-11-08 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:13][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.21352967619895935, acc: 0.939393937587738)
[2024-11-08 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:13][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.2031884342432022, acc: 0.9259259104728699)
[2024-11-08 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:13][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.26189836859703064, acc: 0.9090909361839294)
[2024-11-08 00:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:14][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.5696220397949219, acc: 0.800000011920929)
[2024-11-08 00:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:14][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 0.3376246392726898, acc: 0.8500000238418579)
[2024-11-08 00:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:15][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.18563738465309143, acc: 0.9473684430122375)
[2024-11-08 00:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:15][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.15411309897899628, acc: 0.9545454382896423)
[2024-11-08 00:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:15][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 0.48843303322792053, acc: 0.8999999761581421)
[2024-11-08 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:16][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.18203826248645782, acc: 0.9545454382896423)
[2024-11-08 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:16][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.18125835061073303, acc: 0.939393937587738)
[2024-11-08 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:16][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.174889475107193, acc: 0.9259259104728699)
[2024-11-08 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:17][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.20738078653812408, acc: 0.9032257795333862)
[2024-11-08 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:17][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.2228844165802002, acc: 0.8999999761581421)
[2024-11-08 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:17][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.09949936717748642, acc: 1.0)
[2024-11-08 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:18][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 0.24659979343414307, acc: 0.8947368264198303)
[2024-11-08 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:18][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.03826472908258438, acc: 1.0)
[2024-11-08 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:18][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.1356470286846161, acc: 0.949999988079071)
[2024-11-08 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:19][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.16546720266342163, acc: 0.9545454382896423)
[2024-11-08 00:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:19][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 0.16660074889659882, acc: 0.939393937587738)
[2024-11-08 00:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:21][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 0.37370938062667847, acc: 0.8500000238418579)
[2024-11-08 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:21][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 0.14665615558624268, acc: 0.949999988079071)
[2024-11-08 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:21][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 0.3959987163543701, acc: 0.7894737124443054)
[2024-11-08 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:22][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 0.36878693103790283, acc: 0.9090909361839294)
[2024-11-08 00:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:22][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 0.6436882019042969, acc: 0.8500000238418579)
[2024-11-08 00:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:23][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.22317111492156982, acc: 0.9200000166893005)
[2024-11-08 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:23][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.13954861462116241, acc: 0.9629629850387573)
[2024-11-08 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:23][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 0.23147861659526825, acc: 0.9166666865348816)
[2024-11-08 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:24][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.18384192883968353, acc: 0.9130434989929199)
[2024-11-08 00:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:24][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.20049069821834564, acc: 0.9047619104385376)
[2024-11-08 00:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:24][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.12240413576364517, acc: 1.0)
[2024-11-08 00:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:25][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 0.07243196666240692, acc: 1.0)
[2024-11-08 00:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:26][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 0.19292442500591278, acc: 0.8999999761581421)
[2024-11-08 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:26][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.07648953795433044, acc: 0.9545454382896423)
[2024-11-08 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:26][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.17457914352416992, acc: 0.9696969985961914)
[2024-11-08 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:27][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.12132151424884796, acc: 0.9333333373069763)
[2024-11-08 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:27][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 0.12665116786956787, acc: 1.0)
[2024-11-08 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:27][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.27090388536453247, acc: 0.9047619104385376)
[2024-11-08 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:28][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.05877189338207245, acc: 1.0)
[2024-11-08 00:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:28][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 0.2693018615245819, acc: 0.8636363744735718)
[2024-11-08 00:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:29][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 0.22205078601837158, acc: 0.8947368264198303)
[2024-11-08 00:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:29][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.14582563936710358, acc: 0.9545454382896423)
[2024-11-08 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:30][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.006632742937654257, acc: 1.0)
[2024-11-08 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:30][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.009971299208700657, acc: 1.0)
[2024-11-08 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:30][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.10040326416492462, acc: 0.9714285731315613)
[2024-11-08 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:31][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.06004542484879494, acc: 1.0)
[2024-11-08 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:31][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.10158048570156097, acc: 1.0)
[2024-11-08 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:31][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 0.062432169914245605, acc: 1.0)
[2024-11-08 00:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:32][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.22305706143379211, acc: 0.8999999761581421)
[2024-11-08 00:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:32][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 0.061287540942430496, acc: 1.0)
[2024-11-08 00:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:33][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.017972219735383987, acc: 1.0)
[2024-11-08 00:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:33][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.1796499639749527, acc: 0.9200000166893005)
[2024-11-08 00:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:34][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 0.3877721130847931, acc: 0.8500000238418579)
[2024-11-08 00:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:34][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 0.5481769442558289, acc: 0.761904776096344)
[2024-11-08 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:35][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 0.27384841442108154, acc: 0.8947368264198303)
[2024-11-08 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:36][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.24147948622703552, acc: 0.9090909361839294)
[2024-11-08 00:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:37][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 0.24372971057891846, acc: 0.9523809552192688)
[2024-11-08 00:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:38][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.1046784520149231, acc: 0.9166666865348816)
[2024-11-08 00:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:38][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.07667835056781769, acc: 0.9677419066429138)
[2024-11-08 00:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:39][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.06696276366710663, acc: 1.0)
[2024-11-08 00:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:39][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.034304216504096985, acc: 1.0)
[2024-11-08 00:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:39][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 0.0690683051943779, acc: 1.0)
[2024-11-08 00:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:40][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.14794926345348358, acc: 0.9473684430122375)
[2024-11-08 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:40][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 0.17349299788475037, acc: 0.9090909361839294)
[2024-11-08 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:40][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.15944832563400269, acc: 0.8999999761581421)
[2024-11-08 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:41][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.2448577731847763, acc: 0.9130434989929199)
[2024-11-08 00:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:41][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.2041037231683731, acc: 0.9285714030265808)
[2024-11-08 00:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:41][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.08225785195827484, acc: 1.0)
[2024-11-08 00:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:42][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.27045106887817383, acc: 0.8947368264198303)
[2024-11-08 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:42][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.2390614002943039, acc: 0.8947368264198303)
[2024-11-08 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:42][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.16581934690475464, acc: 0.9545454382896423)
[2024-11-08 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:42][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.19480164349079132, acc: 0.9523809552192688)
[2024-11-08 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:43][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.06960419565439224, acc: 1.0)
[2024-11-08 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:43][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.15803150832653046, acc: 0.9677419066429138)
[2024-11-08 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:43][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.13389581441879272, acc: 0.9354838728904724)
[2024-11-08 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:44][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.4581059217453003, acc: 0.8461538553237915)
[2024-11-08 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:44][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.17141881585121155, acc: 0.9523809552192688)
[2024-11-08 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:45][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.12095610797405243, acc: 0.9473684430122375)
[2024-11-08 00:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:45][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.05691838264465332, acc: 1.0)
[2024-11-08 00:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:46][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.06320905685424805, acc: 0.9545454382896423)
[2024-11-08 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:46][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.08253959566354752, acc: 1.0)
[2024-11-08 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:46][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.11619458347558975, acc: 0.9583333134651184)
[2024-11-08 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:47][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.061372675001621246, acc: 0.9677419066429138)
[2024-11-08 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:47][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.023413553833961487, acc: 1.0)
[2024-11-08 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:47][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.042809996753931046, acc: 1.0)
[2024-11-08 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:48][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.09207671880722046, acc: 0.9523809552192688)
[2024-11-08 00:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:48][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.3258976936340332, acc: 0.8947368264198303)
[2024-11-08 00:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:49][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.2885788679122925, acc: 0.8947368264198303)
[2024-11-08 00:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:49][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.08057565242052078, acc: 0.9545454382896423)
[2024-11-08 00:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:49][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.29377734661102295, acc: 0.9047619104385376)
[2024-11-08 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:50][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.18467199802398682, acc: 0.9583333134651184)
[2024-11-08 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:50][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.3281862437725067, acc: 0.9130434989929199)
[2024-11-08 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:50][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 0.287610799074173, acc: 0.8571428656578064)
[2024-11-08 00:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:51][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 0.2879924178123474, acc: 0.8947368264198303)
[2024-11-08 00:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:51][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 0.261115163564682, acc: 0.8947368264198303)
[2024-11-08 00:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:52][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 0.48471733927726746, acc: 0.8181818127632141)
[2024-11-08 00:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:52][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 0.31408825516700745, acc: 0.8636363744735718)
[2024-11-08 00:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:52][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.14165353775024414, acc: 0.9642857313156128)
[2024-11-08 00:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:53][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.15839038789272308, acc: 0.9259259104728699)
[2024-11-08 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:53][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.33623215556144714, acc: 0.8787878751754761)
[2024-11-08 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:53][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.20850996673107147, acc: 0.9090909361839294)
[2024-11-08 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:54][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.2247587889432907, acc: 0.9047619104385376)
[2024-11-08 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:54][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.29754123091697693, acc: 0.8947368264198303)
[2024-11-08 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:54][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.30753347277641296, acc: 0.8636363744735718)
[2024-11-08 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:55][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 0.3733459711074829, acc: 0.7894737124443054)
[2024-11-08 00:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:55][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 0.20131172239780426, acc: 0.9545454382896423)
[2024-11-08 00:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:56][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.03545336052775383, acc: 0.9642857313156128)
[2024-11-08 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:56][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.18972459435462952, acc: 0.9333333373069763)
[2024-11-08 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:56][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.1200348436832428, acc: 0.9714285731315613)
[2024-11-08 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:57][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.08275668323040009, acc: 1.0)
[2024-11-08 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:57][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.3223206102848053, acc: 0.9047619104385376)
[2024-11-08 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:57][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.5163549184799194, acc: 0.8421052694320679)
[2024-11-08 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:58][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.38553473353385925, acc: 0.75)
[2024-11-08 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:58][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.24786117672920227, acc: 0.9047619104385376)
[2024-11-08 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:58][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.10512632876634598, acc: 0.9545454382896423)
[2024-11-08 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:59][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.02723855711519718, acc: 1.0)
[2024-11-08 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:59][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.11107764393091202, acc: 0.9655172228813171)
[2024-11-08 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:42:59][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.0766105055809021, acc: 0.9428571462631226)
[2024-11-08 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:00][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.21948771178722382, acc: 0.9047619104385376)
[2024-11-08 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:00][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.37468719482421875, acc: 0.9047619104385376)
[2024-11-08 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:00][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.13882003724575043, acc: 1.0)
[2024-11-08 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:01][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.13630007207393646, acc: 1.0)
[2024-11-08 00:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:01][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.08635721355676651, acc: 1.0)
[2024-11-08 00:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:02][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.04229849949479103, acc: 1.0)
[2024-11-08 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:02][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.31791600584983826, acc: 0.8823529481887817)
[2024-11-08 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:02][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 0.14024178683757782, acc: 0.9545454382896423)
[2024-11-08 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:03][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.17387273907661438, acc: 0.8999999761581421)
[2024-11-08 00:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:03][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 0.3051217496395111, acc: 0.8947368264198303)
[2024-11-08 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:04][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 0.2376350313425064, acc: 0.9130434989929199)
[2024-11-08 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:04][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.13405334949493408, acc: 0.9523809552192688)
[2024-11-08 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:05][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.12995392084121704, acc: 0.931034505367279)
[2024-11-08 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:05][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.09289363026618958, acc: 0.9629629850387573)
[2024-11-08 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:05][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.14125750958919525, acc: 0.9230769276618958)
[2024-11-08 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:06][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.1171593889594078, acc: 0.9523809552192688)
[2024-11-08 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:06][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.12074481695890427, acc: 0.8947368264198303)
[2024-11-08 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:06][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.1357540786266327, acc: 0.9473684430122375)
[2024-11-08 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:07][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.16680601239204407, acc: 0.9090909361839294)
[2024-11-08 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:07][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.06344541907310486, acc: 1.0)
[2024-11-08 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:07][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.30269718170166016, acc: 0.931034505367279)
[2024-11-08 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:07][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.025238700211048126, acc: 1.0)
[2024-11-08 00:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:08][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.10529481619596481, acc: 0.9523809552192688)
[2024-11-08 00:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:08][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.02511952817440033, acc: 1.0)
[2024-11-08 00:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:38][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2625, device='cuda:0') eval_epoch_loss=tensor(0.2331, device='cuda:0') eval_epoch_acc=tensor(0.9174, device='cuda:0')
[2024-11-08 00:43:38][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:43:38][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:43:39][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_4_step_280_loss_0.23310443758964539/model.pt
[2024-11-08 00:43:39][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:43:39][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 4 is 0.23310443758964539
[2024-11-08 00:43:39][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.9174153208732605
[2024-11-08 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:39][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.11677256971597672, acc: 1.0)
[2024-11-08 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:39][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 0.1825851947069168, acc: 0.9545454382896423)
[2024-11-08 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:40][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 0.04282055422663689, acc: 1.0)
[2024-11-08 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:40][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.33458754420280457, acc: 0.8620689511299133)
[2024-11-08 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:40][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.3485020101070404, acc: 0.9333333373069763)
[2024-11-08 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:40][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.26091185212135315, acc: 0.8620689511299133)
[2024-11-08 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:41][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.157648965716362, acc: 0.9047619104385376)
[2024-11-08 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:41][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.38892972469329834, acc: 0.8947368264198303)
[2024-11-08 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:41][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.33830904960632324, acc: 0.8947368264198303)
[2024-11-08 00:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:42][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.39662182331085205, acc: 0.8181818127632141)
[2024-11-08 00:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:42][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.23471559584140778, acc: 0.9090909361839294)
[2024-11-08 00:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:42][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.04393896460533142, acc: 1.0)
[2024-11-08 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:43][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.15156452357769012, acc: 0.9333333373069763)
[2024-11-08 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:43][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.15569664537906647, acc: 0.9523809552192688)
[2024-11-08 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:44][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.20478832721710205, acc: 0.949999988079071)
[2024-11-08 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:44][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.4369892179965973, acc: 0.8421052694320679)
[2024-11-08 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:44][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.2791454493999481, acc: 0.9090909361839294)
[2024-11-08 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:45][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.1815246343612671, acc: 0.9523809552192688)
[2024-11-08 00:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:45][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.07994920760393143, acc: 0.9583333134651184)
[2024-11-08 00:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:45][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.0843089297413826, acc: 0.9677419066429138)
[2024-11-08 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:46][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.1156712993979454, acc: 0.9354838728904724)
[2024-11-08 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:46][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.09477531164884567, acc: 0.9615384340286255)
[2024-11-08 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:47][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.20860333740711212, acc: 0.9523809552192688)
[2024-11-08 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:47][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.14320217072963715, acc: 0.9473684430122375)
[2024-11-08 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:47][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.3327828347682953, acc: 0.8947368264198303)
[2024-11-08 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:48][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.2182965874671936, acc: 0.9130434989929199)
[2024-11-08 00:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:48][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.04869629070162773, acc: 1.0)
[2024-11-08 00:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:48][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.01135784201323986, acc: 1.0)
[2024-11-08 00:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:49][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.028817469254136086, acc: 1.0)
[2024-11-08 00:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:49][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.31594744324684143, acc: 0.8421052694320679)
[2024-11-08 00:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:49][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.1694944202899933, acc: 0.9473684430122375)
[2024-11-08 00:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:50][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.24139323830604553, acc: 0.9090909361839294)
[2024-11-08 00:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:50][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.2712676227092743, acc: 0.8636363744735718)
[2024-11-08 00:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:50][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.02690143510699272, acc: 1.0)
[2024-11-08 00:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:51][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.14802445471286774, acc: 0.9629629850387573)
[2024-11-08 00:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:51][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.1943957358598709, acc: 0.9142857193946838)
[2024-11-08 00:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:51][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.25490978360176086, acc: 0.8846153616905212)
[2024-11-08 00:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:52][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.05695849284529686, acc: 0.9523809552192688)
[2024-11-08 00:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:52][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.22803056240081787, acc: 0.8999999761581421)
[2024-11-08 00:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:52][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.035044051706790924, acc: 1.0)
[2024-11-08 00:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:53][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.07312940806150436, acc: 0.9523809552192688)
[2024-11-08 00:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:53][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.02344183810055256, acc: 1.0)
[2024-11-08 00:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:53][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.36268559098243713, acc: 0.8260869383811951)
[2024-11-08 00:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:54][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 0.3562043011188507, acc: 0.9047619104385376)
[2024-11-08 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:54][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.34854236245155334, acc: 0.8947368264198303)
[2024-11-08 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:54][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 0.25431928038597107, acc: 0.8636363744735718)
[2024-11-08 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:54][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 0.33212050795555115, acc: 0.8947368264198303)
[2024-11-08 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:55][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.26085737347602844, acc: 0.9166666865348816)
[2024-11-08 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:55][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.36185222864151, acc: 0.8965517282485962)
[2024-11-08 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:55][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.1992093175649643, acc: 0.9259259104728699)
[2024-11-08 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:56][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.1842561960220337, acc: 0.9047619104385376)
[2024-11-08 00:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:56][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.08375733345746994, acc: 1.0)
[2024-11-08 00:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:56][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.11278732120990753, acc: 0.9090909361839294)
[2024-11-08 00:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:57][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.6235638856887817, acc: 0.8947368264198303)
[2024-11-08 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:57][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.11007413268089294, acc: 0.9545454382896423)
[2024-11-08 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:57][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.28448978066444397, acc: 0.9615384340286255)
[2024-11-08 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:58][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 0.3568120300769806, acc: 0.8333333134651184)
[2024-11-08 00:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:58][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 0.4886287450790405, acc: 0.9047619104385376)
[2024-11-08 00:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:58][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 0.17498847842216492, acc: 0.9130434989929199)
[2024-11-08 00:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:59][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 0.13761110603809357, acc: 1.0)
[2024-11-08 00:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:59][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.1144852563738823, acc: 0.9666666388511658)
[2024-11-08 00:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:43:59][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.20127196609973907, acc: 0.9090909361839294)
[2024-11-08 00:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:00][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.13326701521873474, acc: 0.949999988079071)
[2024-11-08 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:00][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.29801997542381287, acc: 0.8999999761581421)
[2024-11-08 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:00][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.312455415725708, acc: 0.8421052694320679)
[2024-11-08 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:01][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.15615205466747284, acc: 0.9090909361839294)
[2024-11-08 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:01][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.14977046847343445, acc: 1.0)
[2024-11-08 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:01][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.016620326787233353, acc: 1.0)
[2024-11-08 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:01][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.10194150358438492, acc: 0.95652174949646)
[2024-11-08 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:02][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.20425984263420105, acc: 0.8999999761581421)
[2024-11-08 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:02][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.26238131523132324, acc: 0.8947368264198303)
[2024-11-08 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:02][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.15406407415866852, acc: 0.9090909361839294)
[2024-11-08 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:03][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 0.26585525274276733, acc: 0.8999999761581421)
[2024-11-08 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:03][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.1696637123823166, acc: 0.9599999785423279)
[2024-11-08 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:04][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.18531213700771332, acc: 0.8799999952316284)
[2024-11-08 00:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:04][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 0.26320087909698486, acc: 0.9523809552192688)
[2024-11-08 00:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:04][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.3473377823829651, acc: 0.8500000238418579)
[2024-11-08 00:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:05][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.2624654471874237, acc: 0.9047619104385376)
[2024-11-08 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:05][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.1988980621099472, acc: 0.9090909361839294)
[2024-11-08 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:05][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.015148456208407879, acc: 1.0)
[2024-11-08 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:06][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.13114148378372192, acc: 0.9655172228813171)
[2024-11-08 00:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:06][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.06481105089187622, acc: 0.9714285731315613)
[2024-11-08 00:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:06][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.06132850423455238, acc: 0.9545454382896423)
[2024-11-08 00:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:07][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.25245416164398193, acc: 0.9473684430122375)
[2024-11-08 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:07][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.22881996631622314, acc: 0.9523809552192688)
[2024-11-08 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:07][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.026586808264255524, acc: 1.0)
[2024-11-08 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:08][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.044392261654138565, acc: 1.0)
[2024-11-08 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:08][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.11793769150972366, acc: 0.9629629850387573)
[2024-11-08 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:08][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.1261131763458252, acc: 0.96875)
[2024-11-08 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:08][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.07672211527824402, acc: 1.0)
[2024-11-08 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:09][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.3080984652042389, acc: 0.9047619104385376)
[2024-11-08 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:10][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.33997926115989685, acc: 0.7894737124443054)
[2024-11-08 00:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:10][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.08151831477880478, acc: 0.9545454382896423)
[2024-11-08 00:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:11][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.09284041821956635, acc: 1.0)
[2024-11-08 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:11][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.09399432688951492, acc: 1.0)
[2024-11-08 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:11][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.024200929328799248, acc: 1.0)
[2024-11-08 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:12][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.02535306289792061, acc: 1.0)
[2024-11-08 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:12][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.24615973234176636, acc: 0.9523809552192688)
[2024-11-08 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:12][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.1277482956647873, acc: 0.9047619104385376)
[2024-11-08 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:13][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.046245742589235306, acc: 1.0)
[2024-11-08 00:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:13][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.2862621545791626, acc: 0.9090909361839294)
[2024-11-08 00:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:14][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.08803129196166992, acc: 0.9473684430122375)
[2024-11-08 00:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:15][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.1575193554162979, acc: 0.9545454382896423)
[2024-11-08 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:15][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.010412479750812054, acc: 1.0)
[2024-11-08 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:15][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.027034210041165352, acc: 1.0)
[2024-11-08 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:16][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.07769615948200226, acc: 0.9714285731315613)
[2024-11-08 00:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:16][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.015848424285650253, acc: 1.0)
[2024-11-08 00:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:16][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.009251181967556477, acc: 1.0)
[2024-11-08 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:17][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.02088335156440735, acc: 1.0)
[2024-11-08 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:17][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.07179267704486847, acc: 0.949999988079071)
[2024-11-08 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:17][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.01813133805990219, acc: 1.0)
[2024-11-08 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:18][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 0.28672194480895996, acc: 0.9047619104385376)
[2024-11-08 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:18][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 0.25692805647850037, acc: 0.8947368264198303)
[2024-11-08 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:19][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 0.1624368280172348, acc: 0.9473684430122375)
[2024-11-08 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:19][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 0.24559427797794342, acc: 0.9090909361839294)
[2024-11-08 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:19][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.22335851192474365, acc: 0.9047619104385376)
[2024-11-08 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:20][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.031132949516177177, acc: 1.0)
[2024-11-08 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:20][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.3519669771194458, acc: 0.9666666388511658)
[2024-11-08 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:20][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.14360809326171875, acc: 0.9523809552192688)
[2024-11-08 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:21][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.28610455989837646, acc: 0.8888888955116272)
[2024-11-08 00:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:21][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.12248890846967697, acc: 0.9545454382896423)
[2024-11-08 00:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:22][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.0172265712171793, acc: 1.0)
[2024-11-08 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:22][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.020245468243956566, acc: 1.0)
[2024-11-08 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:22][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.012036154046654701, acc: 1.0)
[2024-11-08 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:22][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.11578807979822159, acc: 0.9714285731315613)
[2024-11-08 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:23][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.012034333311021328, acc: 1.0)
[2024-11-08 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:23][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.19428732991218567, acc: 0.9047619104385376)
[2024-11-08 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:23][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.3001652657985687, acc: 0.9473684430122375)
[2024-11-08 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:24][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.06570462137460709, acc: 1.0)
[2024-11-08 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:24][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.10997282713651657, acc: 0.9523809552192688)
[2024-11-08 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:24][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.02195911295711994, acc: 1.0)
[2024-11-08 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:25][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.06479663401842117, acc: 0.9642857313156128)
[2024-11-08 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:25][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.038895368576049805, acc: 1.0)
[2024-11-08 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:25][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.04526069015264511, acc: 0.9714285731315613)
[2024-11-08 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:25][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.017939649522304535, acc: 1.0)
[2024-11-08 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:26][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.02172655053436756, acc: 1.0)
[2024-11-08 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:26][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.11451312154531479, acc: 0.9473684430122375)
[2024-11-08 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:26][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.22227400541305542, acc: 0.9473684430122375)
[2024-11-08 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:27][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.04327889904379845, acc: 1.0)
[2024-11-08 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:27][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.18173547089099884, acc: 0.9047619104385376)
[2024-11-08 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:27][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.018730731680989265, acc: 1.0)
[2024-11-08 00:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:28][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.06444744020700455, acc: 0.9677419066429138)
[2024-11-08 00:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:28][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.05272724851965904, acc: 0.9677419066429138)
[2024-11-08 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:57][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2645, device='cuda:0') eval_epoch_loss=tensor(0.2347, device='cuda:0') eval_epoch_acc=tensor(0.9241, device='cuda:0')
[2024-11-08 00:44:57][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:44:57][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:44:57][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_4_step_423_loss_0.23468713462352753/model.pt
[2024-11-08 00:44:57][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:44:57][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.9241483211517334
[2024-11-08 00:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:58][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.07374586910009384, acc: 1.0)
[2024-11-08 00:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:58][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.036058101803064346, acc: 1.0)
[2024-11-08 00:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:58][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.07001695781946182, acc: 0.9473684430122375)
[2024-11-08 00:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:59][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.022802801802754402, acc: 1.0)
[2024-11-08 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:59][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.02654174529016018, acc: 1.0)
[2024-11-08 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:44:59][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.03868953511118889, acc: 1.0)
[2024-11-08 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:00][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.004827824886888266, acc: 1.0)
[2024-11-08 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:00][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.007946023717522621, acc: 1.0)
[2024-11-08 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:00][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.006612213794142008, acc: 1.0)
[2024-11-08 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:01][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.01129329763352871, acc: 1.0)
[2024-11-08 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:01][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.015059571713209152, acc: 1.0)
[2024-11-08 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:01][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.06973887234926224, acc: 0.9473684430122375)
[2024-11-08 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:02][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.016852444037795067, acc: 1.0)
[2024-11-08 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:02][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.01951327733695507, acc: 1.0)
[2024-11-08 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:02][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.09267558157444, acc: 0.9523809552192688)
[2024-11-08 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:02][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.018686991184949875, acc: 1.0)
[2024-11-08 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:03][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.17241522669792175, acc: 0.9285714030265808)
[2024-11-08 00:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:03][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.04449595883488655, acc: 1.0)
[2024-11-08 00:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:04][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.35783201456069946, acc: 0.8421052694320679)
[2024-11-08 00:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:04][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.21407155692577362, acc: 0.8947368264198303)
[2024-11-08 00:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:05][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.11494198441505432, acc: 0.9090909361839294)
[2024-11-08 00:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:05][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.036183957010507584, acc: 1.0)
[2024-11-08 00:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:05][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.2932610511779785, acc: 0.9583333134651184)
[2024-11-08 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:06][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.202993243932724, acc: 0.9677419066429138)
[2024-11-08 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:06][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.14712965488433838, acc: 0.9354838728904724)
[2024-11-08 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:06][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.032091718167066574, acc: 1.0)
[2024-11-08 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:07][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.003792405594140291, acc: 1.0)
[2024-11-08 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:07][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.0019030753755941987, acc: 1.0)
[2024-11-08 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:07][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.11802814155817032, acc: 0.9473684430122375)
[2024-11-08 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:08][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.01790737174451351, acc: 1.0)
[2024-11-08 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:08][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.3678176701068878, acc: 0.8500000238418579)
[2024-11-08 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:08][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.0397108756005764, acc: 1.0)
[2024-11-08 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:09][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.18465672433376312, acc: 0.9677419066429138)
[2024-11-08 00:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:09][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.056999146938323975, acc: 1.0)
[2024-11-08 00:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:09][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.05470936372876167, acc: 1.0)
[2024-11-08 00:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:09][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.1488637626171112, acc: 0.9473684430122375)
[2024-11-08 00:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:10][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.4537709355354309, acc: 0.8636363744735718)
[2024-11-08 00:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:10][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.07275374233722687, acc: 1.0)
[2024-11-08 00:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:10][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.011663209646940231, acc: 1.0)
[2024-11-08 00:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:11][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.0944172590970993, acc: 0.9696969985961914)
[2024-11-08 00:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:11][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.039956070482730865, acc: 1.0)
[2024-11-08 00:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:11][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.03529274836182594, acc: 1.0)
[2024-11-08 00:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:12][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.046579960733652115, acc: 1.0)
[2024-11-08 00:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:12][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.020928582176566124, acc: 1.0)
[2024-11-08 00:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:12][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.2490106076002121, acc: 0.8947368264198303)
[2024-11-08 00:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:13][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.14503146708011627, acc: 0.949999988079071)
[2024-11-08 00:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:13][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.046982843428850174, acc: 1.0)
[2024-11-08 00:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:13][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.030975649133324623, acc: 1.0)
[2024-11-08 00:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:13][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.012631992809474468, acc: 1.0)
[2024-11-08 00:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:14][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.21397359669208527, acc: 0.875)
[2024-11-08 00:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:14][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.1201406866312027, acc: 0.9523809552192688)
[2024-11-08 00:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:15][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.10004673898220062, acc: 1.0)
[2024-11-08 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:15][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.12263508141040802, acc: 0.949999988079071)
[2024-11-08 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:15][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.17899978160858154, acc: 0.9523809552192688)
[2024-11-08 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:16][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.24089032411575317, acc: 0.9090909361839294)
[2024-11-08 00:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:16][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.06206074357032776, acc: 0.9642857313156128)
[2024-11-08 00:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:16][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.17588256299495697, acc: 0.9677419066429138)
[2024-11-08 00:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:17][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.02025296911597252, acc: 1.0)
[2024-11-08 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:17][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.23680292069911957, acc: 0.9047619104385376)
[2024-11-08 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:17][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.32307326793670654, acc: 0.8947368264198303)
[2024-11-08 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:18][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.49944794178009033, acc: 0.9090909361839294)
[2024-11-08 00:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:18][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.1786063313484192, acc: 0.9090909361839294)
[2024-11-08 00:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:18][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.036742955446243286, acc: 1.0)
[2024-11-08 00:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:19][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.12471378594636917, acc: 0.9259259104728699)
[2024-11-08 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:19][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.3073233664035797, acc: 0.9142857193946838)
[2024-11-08 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:19][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.5324177145957947, acc: 0.8461538553237915)
[2024-11-08 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:20][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.015046315267682076, acc: 1.0)
[2024-11-08 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:20][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.14407320320606232, acc: 0.9473684430122375)
[2024-11-08 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:20][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.18361131846904755, acc: 0.8947368264198303)
[2024-11-08 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:20][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.06302370876073837, acc: 1.0)
[2024-11-08 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:21][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.23531527817249298, acc: 0.9047619104385376)
[2024-11-08 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:21][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.33440420031547546, acc: 0.9166666865348816)
[2024-11-08 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:21][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.3053923547267914, acc: 0.9642857313156128)
[2024-11-08 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:22][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.1411748081445694, acc: 0.8999999761581421)
[2024-11-08 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:22][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.1999751478433609, acc: 0.8999999761581421)
[2024-11-08 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:22][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.3338170349597931, acc: 0.8421052694320679)
[2024-11-08 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:23][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 0.20812372863292694, acc: 0.9090909361839294)
[2024-11-08 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:23][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 0.3036597669124603, acc: 0.8999999761581421)
[2024-11-08 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:23][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.021799683570861816, acc: 1.0)
[2024-11-08 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:24][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.015686502680182457, acc: 1.0)
[2024-11-08 00:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:24][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.10329928994178772, acc: 0.9677419066429138)
[2024-11-08 00:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:24][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.4630366563796997, acc: 0.8260869383811951)
[2024-11-08 00:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:25][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.12766721844673157, acc: 0.9523809552192688)
[2024-11-08 00:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:25][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.34203454852104187, acc: 0.8421052694320679)
[2024-11-08 00:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:25][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 0.07978948205709457, acc: 1.0)
[2024-11-08 00:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:26][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.42909979820251465, acc: 0.800000011920929)
[2024-11-08 00:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:26][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.09486612677574158, acc: 1.0)
[2024-11-08 00:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:27][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.18076498806476593, acc: 0.939393937587738)
[2024-11-08 00:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:27][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.15165911614894867, acc: 0.9333333373069763)
[2024-11-08 00:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:29][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 0.32429543137550354, acc: 0.8928571343421936)
[2024-11-08 00:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:30][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.1279643326997757, acc: 0.9523809552192688)
[2024-11-08 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:30][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.22895529866218567, acc: 0.9473684430122375)
[2024-11-08 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:30][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.1893504559993744, acc: 0.949999988079071)
[2024-11-08 00:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:31][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.09525112807750702, acc: 0.9523809552192688)
[2024-11-08 00:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:31][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.0329241044819355, acc: 1.0)
[2024-11-08 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:32][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.2289433628320694, acc: 0.9285714030265808)
[2024-11-08 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:32][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.08582593500614166, acc: 0.9629629850387573)
[2024-11-08 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:32][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.10319061577320099, acc: 0.9714285731315613)
[2024-11-08 00:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:33][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.1098368912935257, acc: 0.95652174949646)
[2024-11-08 00:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:34][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.10250963270664215, acc: 1.0)
[2024-11-08 00:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:34][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.4267747402191162, acc: 0.8421052694320679)
[2024-11-08 00:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:35][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.20596317946910858, acc: 0.9090909361839294)
[2024-11-08 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:35][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.1021667867898941, acc: 0.949999988079071)
[2024-11-08 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:35][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.20263294875621796, acc: 0.8636363744735718)
[2024-11-08 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:36][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.11170775443315506, acc: 0.939393937587738)
[2024-11-08 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:36][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 0.36237189173698425, acc: 0.9285714030265808)
[2024-11-08 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:36][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.07149773836135864, acc: 1.0)
[2024-11-08 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:36][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 0.45757079124450684, acc: 0.8421052694320679)
[2024-11-08 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:37][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.21177196502685547, acc: 0.949999988079071)
[2024-11-08 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:37][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 0.18528898060321808, acc: 0.9047619104385376)
[2024-11-08 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:37][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.13982199132442474, acc: 0.9545454382896423)
[2024-11-08 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:38][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.15434685349464417, acc: 0.9642857313156128)
[2024-11-08 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:38][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.07879915088415146, acc: 0.9629629850387573)
[2024-11-08 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:38][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.14529916644096375, acc: 0.9200000166893005)
[2024-11-08 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:39][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.1560351848602295, acc: 0.9047619104385376)
[2024-11-08 00:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:39][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.30849167704582214, acc: 0.8421052694320679)
[2024-11-08 00:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:39][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.20945589244365692, acc: 0.8999999761581421)
[2024-11-08 00:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:40][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.5310987234115601, acc: 0.8571428656578064)
[2024-11-08 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:40][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.09148291498422623, acc: 0.9523809552192688)
[2024-11-08 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:40][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.13984842598438263, acc: 0.9696969985961914)
[2024-11-08 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:41][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.015283787623047829, acc: 1.0)
[2024-11-08 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:41][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.0312429741024971, acc: 1.0)
[2024-11-08 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:41][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.03774462640285492, acc: 1.0)
[2024-11-08 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:41][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.04904499277472496, acc: 1.0)
[2024-11-08 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:42][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.13083350658416748, acc: 0.9523809552192688)
[2024-11-08 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:42][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.03778840973973274, acc: 1.0)
[2024-11-08 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:42][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.0026100664399564266, acc: 1.0)
[2024-11-08 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:43][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.09227251261472702, acc: 0.9655172228813171)
[2024-11-08 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:43][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.05005626752972603, acc: 1.0)
[2024-11-08 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:43][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.04909332096576691, acc: 1.0)
[2024-11-08 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:44][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.13104800879955292, acc: 0.9523809552192688)
[2024-11-08 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:44][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.055602505803108215, acc: 1.0)
[2024-11-08 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:44][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.2073415070772171, acc: 0.8999999761581421)
[2024-11-08 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:44][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.10266490280628204, acc: 0.9523809552192688)
[2024-11-08 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:45][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.006234152242541313, acc: 1.0)
[2024-11-08 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:45][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.09111829847097397, acc: 0.9642857313156128)
[2024-11-08 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:45][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.14778201282024384, acc: 0.9629629850387573)
[2024-11-08 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:46][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.08595510572195053, acc: 0.939393937587738)
[2024-11-08 00:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:46][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.3211260437965393, acc: 0.9523809552192688)
[2024-11-08 00:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:46][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.0058143590576946735, acc: 1.0)
[2024-11-08 00:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:47][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.061097707599401474, acc: 1.0)
[2024-11-08 00:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:47][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.16265326738357544, acc: 0.9545454382896423)
[2024-11-08 00:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:47][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.7116407155990601, acc: 0.800000011920929)
[2024-11-08 00:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:16][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3245, device='cuda:0') eval_epoch_loss=tensor(0.2811, device='cuda:0') eval_epoch_acc=tensor(0.9234, device='cuda:0')
[2024-11-08 00:46:16][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:46:16][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:46:16][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_4_step_566_loss_0.28106167912483215/model.pt
[2024-11-08 00:46:16][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:17][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.0023649318609386683, acc: 1.0)
[2024-11-08 00:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:17][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.05732148513197899, acc: 1.0)
[2024-11-08 00:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:17][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.04602060094475746, acc: 1.0)
[2024-11-08 00:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:18][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.043949101120233536, acc: 1.0)
[2024-11-08 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:18][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.11484773457050323, acc: 0.949999988079071)
[2024-11-08 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:18][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.19405066967010498, acc: 0.8947368264198303)
[2024-11-08 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:19][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.045799270272254944, acc: 1.0)
[2024-11-08 00:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:19][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.2350439578294754, acc: 0.949999988079071)
[2024-11-08 00:46:19][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.1877, train_epoch_loss=0.1720, epoch time 330.31807543709874s
[2024-11-08 00:46:19][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 00:46:19][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 00:46:19][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 00:46:19][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 00:46:19][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 00:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:20][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.35053780674934387, acc: 0.9285714030265808)
[2024-11-08 00:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:21][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.09841780364513397, acc: 0.9629629850387573)
[2024-11-08 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:21][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.4328599274158478, acc: 0.8285714387893677)
[2024-11-08 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:21][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.13444049656391144, acc: 1.0)
[2024-11-08 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:22][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.27280423045158386, acc: 0.9047619104385376)
[2024-11-08 00:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:22][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.42596250772476196, acc: 0.8421052694320679)
[2024-11-08 00:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:22][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.14252573251724243, acc: 0.9545454382896423)
[2024-11-08 00:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:23][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.3550134599208832, acc: 0.9047619104385376)
[2024-11-08 00:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:23][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 1.0261696577072144, acc: 0.75)
[2024-11-08 00:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:23][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.9443917274475098, acc: 0.774193525314331)
[2024-11-08 00:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:24][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.3335704803466797, acc: 0.8709677457809448)
[2024-11-08 00:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:24][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.5257558226585388, acc: 0.8399999737739563)
[2024-11-08 00:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:24][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.3260350227355957, acc: 0.800000011920929)
[2024-11-08 00:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:24][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.6878131031990051, acc: 0.7368420958518982)
[2024-11-08 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:25][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.6062704920768738, acc: 0.8695651888847351)
[2024-11-08 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:25][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.19539475440979004, acc: 0.9545454382896423)
[2024-11-08 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:25][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.17592814564704895, acc: 0.9642857313156128)
[2024-11-08 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:26][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.4201506972312927, acc: 0.8928571343421936)
[2024-11-08 00:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:26][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.46558868885040283, acc: 0.8181818127632141)
[2024-11-08 00:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:26][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.5829675197601318, acc: 0.8947368264198303)
[2024-11-08 00:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:27][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.29564785957336426, acc: 0.8947368264198303)
[2024-11-08 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:27][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.437877357006073, acc: 0.8636363744735718)
[2024-11-08 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:27][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.6413208246231079, acc: 0.9047619104385376)
[2024-11-08 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:28][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.4784531891345978, acc: 0.9047619104385376)
[2024-11-08 00:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:28][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.34631308913230896, acc: 0.8666666746139526)
[2024-11-08 00:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:28][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.4795791506767273, acc: 0.8095238208770752)
[2024-11-08 00:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:29][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.45747992396354675, acc: 0.8095238208770752)
[2024-11-08 00:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:30][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 0.1898721158504486, acc: 0.9473684430122375)
[2024-11-08 00:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:30][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.08429746329784393, acc: 1.0)
[2024-11-08 00:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:30][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.3673256039619446, acc: 0.8500000238418579)
[2024-11-08 00:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:31][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.15886427462100983, acc: 0.9545454382896423)
[2024-11-08 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:31][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.2875894606113434, acc: 0.931034505367279)
[2024-11-08 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:31][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.6157723069190979, acc: 0.8181818127632141)
[2024-11-08 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:32][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.33997735381126404, acc: 0.9130434989929199)
[2024-11-08 00:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:32][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.07996022701263428, acc: 1.0)
[2024-11-08 00:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:32][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.28345274925231934, acc: 0.8947368264198303)
[2024-11-08 00:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:32][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.16319674253463745, acc: 0.949999988079071)
[2024-11-08 00:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:33][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.06905543059110641, acc: 1.0)
[2024-11-08 00:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:33][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.014649507589638233, acc: 1.0)
[2024-11-08 00:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:33][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.28275924921035767, acc: 0.8571428656578064)
[2024-11-08 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:34][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.27386945486068726, acc: 0.9142857193946838)
[2024-11-08 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:34][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.15508514642715454, acc: 0.949999988079071)
[2024-11-08 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:35][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.3360755443572998, acc: 0.8999999761581421)
[2024-11-08 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:35][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.513599693775177, acc: 0.7894737124443054)
[2024-11-08 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:35][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.20792029798030853, acc: 0.9090909361839294)
[2024-11-08 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:36][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.3043482005596161, acc: 0.9047619104385376)
[2024-11-08 00:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:36][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.06506374478340149, acc: 1.0)
[2024-11-08 00:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:36][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.11857708543539047, acc: 0.9354838728904724)
[2024-11-08 00:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:37][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.03372363746166229, acc: 1.0)
[2024-11-08 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:37][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.07521018385887146, acc: 0.9599999785423279)
[2024-11-08 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:37][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.13008545339107513, acc: 0.9523809552192688)
[2024-11-08 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:38][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.24215786159038544, acc: 0.8947368264198303)
[2024-11-08 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:38][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.1505669504404068, acc: 0.949999988079071)
[2024-11-08 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:38][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 0.3305976688861847, acc: 0.9047619104385376)
[2024-11-08 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:39][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.17406202852725983, acc: 0.9545454382896423)
[2024-11-08 00:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:39][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.015308404341340065, acc: 1.0)
[2024-11-08 00:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:42][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 0.2381376028060913, acc: 0.8571428656578064)
[2024-11-08 00:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:43][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 0.16214844584465027, acc: 0.9523809552192688)
[2024-11-08 00:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:43][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.1705855131149292, acc: 0.9473684430122375)
[2024-11-08 00:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:44][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.1299838125705719, acc: 0.9545454382896423)
[2024-11-08 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:44][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.19073908030986786, acc: 0.9473684430122375)
[2024-11-08 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:45][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.1410239040851593, acc: 0.9523809552192688)
[2024-11-08 00:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:45][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.07446049898862839, acc: 0.9444444179534912)
[2024-11-08 00:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:45][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.05898665264248848, acc: 0.9642857313156128)
[2024-11-08 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:46][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.26186317205429077, acc: 0.8571428656578064)
[2024-11-08 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:46][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.2771744132041931, acc: 0.949999988079071)
[2024-11-08 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:47][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.2940956950187683, acc: 0.8421052694320679)
[2024-11-08 00:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:47][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.27146115899086, acc: 0.8999999761581421)
[2024-11-08 00:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:47][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.02174864523112774, acc: 1.0)
[2024-11-08 00:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:48][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.020930210128426552, acc: 1.0)
[2024-11-08 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:48][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.09614639729261398, acc: 0.9629629850387573)
[2024-11-08 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:48][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 0.10889114439487457, acc: 0.9666666388511658)
[2024-11-08 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:49][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.16537287831306458, acc: 0.9523809552192688)
[2024-11-08 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:49][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 0.5084567666053772, acc: 0.7894737124443054)
[2024-11-08 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:49][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.12369374930858612, acc: 1.0)
[2024-11-08 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:50][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 0.28927063941955566, acc: 0.9047619104385376)
[2024-11-08 00:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:50][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 0.06557314842939377, acc: 1.0)
[2024-11-08 00:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:50][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.03353318199515343, acc: 1.0)
[2024-11-08 00:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:51][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.05496445670723915, acc: 0.9629629850387573)
[2024-11-08 00:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:51][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.10726531594991684, acc: 0.9714285731315613)
[2024-11-08 00:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:51][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.044239502400159836, acc: 1.0)
[2024-11-08 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:52][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.03281354159116745, acc: 1.0)
[2024-11-08 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:52][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.20767265558242798, acc: 0.8947368264198303)
[2024-11-08 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:52][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.05322514846920967, acc: 1.0)
[2024-11-08 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:53][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.21034906804561615, acc: 0.9545454382896423)
[2024-11-08 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:53][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.12408708035945892, acc: 0.9523809552192688)
[2024-11-08 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:53][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.03176215663552284, acc: 1.0)
[2024-11-08 00:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:54][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.48768150806427, acc: 0.807692289352417)
[2024-11-08 00:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:54][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.07811959832906723, acc: 0.9523809552192688)
[2024-11-08 00:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:55][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.2191704511642456, acc: 0.9473684430122375)
[2024-11-08 00:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:56][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 0.16256259381771088, acc: 0.8947368264198303)
[2024-11-08 00:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:57][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.1887778341770172, acc: 0.9090909361839294)
[2024-11-08 00:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:57][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.13370755314826965, acc: 0.9523809552192688)
[2024-11-08 00:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:58][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 0.4410734474658966, acc: 0.8461538553237915)
[2024-11-08 00:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:59][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.15180149674415588, acc: 0.9285714030265808)
[2024-11-08 00:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:59][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.2375650703907013, acc: 0.9047619104385376)
[2024-11-08 00:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:46:59][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 0.3388074040412903, acc: 0.9473684430122375)
[2024-11-08 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:00][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 0.2477310597896576, acc: 0.8999999761581421)
[2024-11-08 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:00][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 0.45040467381477356, acc: 0.8095238208770752)
[2024-11-08 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:01][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.24439053237438202, acc: 0.9090909361839294)
[2024-11-08 00:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:01][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.21401508152484894, acc: 0.9523809552192688)
[2024-11-08 00:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:01][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.12310764938592911, acc: 0.9200000166893005)
[2024-11-08 00:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:02][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.04920978099107742, acc: 0.9523809552192688)
[2024-11-08 00:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:02][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.03171563148498535, acc: 1.0)
[2024-11-08 00:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:03][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.03646751865744591, acc: 1.0)
[2024-11-08 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:03][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.16049665212631226, acc: 0.8947368264198303)
[2024-11-08 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:03][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.07777217030525208, acc: 0.9545454382896423)
[2024-11-08 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:04][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.022561090067029, acc: 1.0)
[2024-11-08 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:04][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.0632312148809433, acc: 1.0)
[2024-11-08 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:04][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.3994286358356476, acc: 0.8571428656578064)
[2024-11-08 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:05][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.06221044808626175, acc: 1.0)
[2024-11-08 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:05][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.2776467800140381, acc: 0.8947368264198303)
[2024-11-08 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:05][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.0225929394364357, acc: 1.0)
[2024-11-08 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:06][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.028266653418540955, acc: 1.0)
[2024-11-08 00:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:06][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.023199575021862984, acc: 1.0)
[2024-11-08 00:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:06][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.01810925453901291, acc: 1.0)
[2024-11-08 00:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:07][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.16349822282791138, acc: 0.9200000166893005)
[2024-11-08 00:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:07][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.20012478530406952, acc: 0.949999988079071)
[2024-11-08 00:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:07][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.04640105366706848, acc: 1.0)
[2024-11-08 00:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:08][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.05443790182471275, acc: 0.9545454382896423)
[2024-11-08 00:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:09][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.18715932965278625, acc: 0.8947368264198303)
[2024-11-08 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:09][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.08283089846372604, acc: 0.9545454382896423)
[2024-11-08 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:09][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.03177977353334427, acc: 1.0)
[2024-11-08 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:10][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.608816921710968, acc: 0.8846153616905212)
[2024-11-08 00:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:10][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 0.0946439728140831, acc: 0.949999988079071)
[2024-11-08 00:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:10][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 0.2948750853538513, acc: 0.8999999761581421)
[2024-11-08 00:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:10][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 0.2430063784122467, acc: 0.9473684430122375)
[2024-11-08 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:11][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 0.3013981580734253, acc: 0.8636363744735718)
[2024-11-08 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:11][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.613061249256134, acc: 0.8500000238418579)
[2024-11-08 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:12][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.008628299459815025, acc: 1.0)
[2024-11-08 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:12][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.16436120867729187, acc: 0.9696969985961914)
[2024-11-08 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:12][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.11951733380556107, acc: 0.9259259104728699)
[2024-11-08 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:12][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.18691615760326385, acc: 0.9090909361839294)
[2024-11-08 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:13][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.13924817740917206, acc: 0.949999988079071)
[2024-11-08 00:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:13][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.10973254591226578, acc: 0.949999988079071)
[2024-11-08 00:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:42][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3514, device='cuda:0') eval_epoch_loss=tensor(0.3012, device='cuda:0') eval_epoch_acc=tensor(0.9100, device='cuda:0')
[2024-11-08 00:47:42][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:47:42][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:47:42][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_5_step_135_loss_0.3011612594127655/model.pt
[2024-11-08 00:47:42][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:42][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.4601607024669647, acc: 0.8421052694320679)
[2024-11-08 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:43][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.009392927400767803, acc: 1.0)
[2024-11-08 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:43][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.38006535172462463, acc: 0.8999999761581421)
[2024-11-08 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:43][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.4051665961742401, acc: 0.9090909361839294)
[2024-11-08 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:44][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.19042328000068665, acc: 0.939393937587738)
[2024-11-08 00:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:44][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.12548460066318512, acc: 0.9259259104728699)
[2024-11-08 00:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:44][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.07447494566440582, acc: 0.9696969985961914)
[2024-11-08 00:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:45][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.5251890420913696, acc: 0.8500000238418579)
[2024-11-08 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:45][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.16825313866138458, acc: 0.8999999761581421)
[2024-11-08 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:45][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.1508600264787674, acc: 1.0)
[2024-11-08 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:46][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.11074060946702957, acc: 0.9545454382896423)
[2024-11-08 00:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:46][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 0.2797005772590637, acc: 0.8999999761581421)
[2024-11-08 00:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:46][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.14939001202583313, acc: 0.9545454382896423)
[2024-11-08 00:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:47][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.14753440022468567, acc: 0.939393937587738)
[2024-11-08 00:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:47][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.12618808448314667, acc: 0.9629629850387573)
[2024-11-08 00:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:47][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.04855967313051224, acc: 1.0)
[2024-11-08 00:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:48][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.23976698517799377, acc: 0.8999999761581421)
[2024-11-08 00:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:48][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.05788996070623398, acc: 1.0)
[2024-11-08 00:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:48][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.08792326599359512, acc: 1.0)
[2024-11-08 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:49][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.022445695474743843, acc: 1.0)
[2024-11-08 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:49][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.22417935729026794, acc: 0.8999999761581421)
[2024-11-08 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:49][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.1510564684867859, acc: 0.9545454382896423)
[2024-11-08 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:50][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 0.29194891452789307, acc: 0.939393937587738)
[2024-11-08 00:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:51][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.40614286065101624, acc: 0.75)
[2024-11-08 00:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:52][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 0.21695645153522491, acc: 0.8500000238418579)
[2024-11-08 00:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:52][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 0.29388928413391113, acc: 0.8421052694320679)
[2024-11-08 00:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:53][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 0.22372885048389435, acc: 0.9090909361839294)
[2024-11-08 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:53][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 0.3264862895011902, acc: 0.8999999761581421)
[2024-11-08 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:53][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.21355484426021576, acc: 0.8799999952316284)
[2024-11-08 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:54][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.1396452784538269, acc: 0.9629629850387573)
[2024-11-08 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:54][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.258426308631897, acc: 0.9444444179534912)
[2024-11-08 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:54][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.2444467395544052, acc: 0.95652174949646)
[2024-11-08 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:55][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.14844530820846558, acc: 0.9523809552192688)
[2024-11-08 00:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:55][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 0.08957496285438538, acc: 0.9473684430122375)
[2024-11-08 00:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:55][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 0.099608413875103, acc: 0.9545454382896423)
[2024-11-08 00:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:56][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 0.35267752408981323, acc: 0.8999999761581421)
[2024-11-08 00:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:57][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.05802355706691742, acc: 1.0)
[2024-11-08 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:57][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.11339648067951202, acc: 0.9696969985961914)
[2024-11-08 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:57][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.32285577058792114, acc: 0.8999999761581421)
[2024-11-08 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:58][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 0.16534140706062317, acc: 0.9047619104385376)
[2024-11-08 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:58][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.24982482194900513, acc: 0.9047619104385376)
[2024-11-08 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:58][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.2840026915073395, acc: 0.8947368264198303)
[2024-11-08 00:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:47:59][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 0.3040439784526825, acc: 0.8636363744735718)
[2024-11-08 00:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:00][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 0.18979769945144653, acc: 0.9473684430122375)
[2024-11-08 00:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:00][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.08387921005487442, acc: 0.9545454382896423)
[2024-11-08 00:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:01][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.004620978143066168, acc: 1.0)
[2024-11-08 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:01][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.0149995107203722, acc: 1.0)
[2024-11-08 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:01][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.11055745929479599, acc: 0.9428571462631226)
[2024-11-08 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:01][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.0980607122182846, acc: 0.95652174949646)
[2024-11-08 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:02][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.1501811295747757, acc: 0.9523809552192688)
[2024-11-08 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:02][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.10230115801095963, acc: 0.9473684430122375)
[2024-11-08 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:03][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.15558411180973053, acc: 0.8999999761581421)
[2024-11-08 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:03][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 0.1634853482246399, acc: 0.9523809552192688)
[2024-11-08 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:03][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.06554637104272842, acc: 1.0)
[2024-11-08 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:04][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.2955211400985718, acc: 0.8799999952316284)
[2024-11-08 00:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:04][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.1594477742910385, acc: 0.949999988079071)
[2024-11-08 00:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:05][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 0.2846637964248657, acc: 0.9047619104385376)
[2024-11-08 00:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:06][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 0.2735334038734436, acc: 0.8421052694320679)
[2024-11-08 00:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:07][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.13914476335048676, acc: 0.9545454382896423)
[2024-11-08 00:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:08][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.22802451252937317, acc: 0.9047619104385376)
[2024-11-08 00:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:08][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.11052330583333969, acc: 0.9583333134651184)
[2024-11-08 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:09][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.018491465598344803, acc: 1.0)
[2024-11-08 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:09][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.1582207977771759, acc: 0.9333333373069763)
[2024-11-08 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:09][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.03428230434656143, acc: 1.0)
[2024-11-08 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:10][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.01991996355354786, acc: 1.0)
[2024-11-08 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:10][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.10468695312738419, acc: 1.0)
[2024-11-08 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:10][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.13408173620700836, acc: 0.9545454382896423)
[2024-11-08 00:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:11][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.21999922394752502, acc: 0.8999999761581421)
[2024-11-08 00:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:11][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.05141936242580414, acc: 1.0)
[2024-11-08 00:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:11][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.041495390236377716, acc: 1.0)
[2024-11-08 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:12][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.1231299564242363, acc: 0.9523809552192688)
[2024-11-08 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:12][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.1948612630367279, acc: 0.8947368264198303)
[2024-11-08 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:12][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.09654834866523743, acc: 1.0)
[2024-11-08 00:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:13][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.09235739707946777, acc: 0.9545454382896423)
[2024-11-08 00:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:13][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.03533875569701195, acc: 1.0)
[2024-11-08 00:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:14][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.15947963297367096, acc: 0.9166666865348816)
[2024-11-08 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:14][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.04725925624370575, acc: 0.9677419066429138)
[2024-11-08 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:14][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.1071152463555336, acc: 0.9354838728904724)
[2024-11-08 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:15][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.22606633603572845, acc: 0.9615384340286255)
[2024-11-08 00:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:15][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.20177295804023743, acc: 0.9523809552192688)
[2024-11-08 00:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:16][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.005769912153482437, acc: 1.0)
[2024-11-08 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:16][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.03313515707850456, acc: 1.0)
[2024-11-08 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:16][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.12450607120990753, acc: 0.9090909361839294)
[2024-11-08 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:17][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.020037641748785973, acc: 1.0)
[2024-11-08 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:17][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.01775023341178894, acc: 1.0)
[2024-11-08 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:17][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.020261626690626144, acc: 1.0)
[2024-11-08 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:18][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.0025720393750816584, acc: 1.0)
[2024-11-08 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:18][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.022158579900860786, acc: 1.0)
[2024-11-08 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:19][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.009442699141800404, acc: 1.0)
[2024-11-08 00:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:19][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.21237865090370178, acc: 0.8947368264198303)
[2024-11-08 00:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:20][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.03953374922275543, acc: 1.0)
[2024-11-08 00:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:20][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.029195530340075493, acc: 1.0)
[2024-11-08 00:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:21][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.16507425904273987, acc: 0.9523809552192688)
[2024-11-08 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:21][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.456120103597641, acc: 0.9166666865348816)
[2024-11-08 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:21][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.2863481044769287, acc: 0.8695651888847351)
[2024-11-08 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:22][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 0.0701121911406517, acc: 0.9523809552192688)
[2024-11-08 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:22][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 0.2350548654794693, acc: 0.8947368264198303)
[2024-11-08 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:22][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 0.19468176364898682, acc: 0.8947368264198303)
[2024-11-08 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:23][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 0.23025473952293396, acc: 0.8636363744735718)
[2024-11-08 00:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:23][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 0.37075597047805786, acc: 0.8181818127632141)
[2024-11-08 00:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:24][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.15835116803646088, acc: 0.9285714030265808)
[2024-11-08 00:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:24][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.26707300543785095, acc: 0.8888888955116272)
[2024-11-08 00:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:24][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.1947534680366516, acc: 0.939393937587738)
[2024-11-08 00:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:25][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.1732122004032135, acc: 0.9545454382896423)
[2024-11-08 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:25][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.22602467238903046, acc: 0.9047619104385376)
[2024-11-08 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:25][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.23373959958553314, acc: 0.8947368264198303)
[2024-11-08 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:25][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.15334047377109528, acc: 0.9545454382896423)
[2024-11-08 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:26][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 0.12236817181110382, acc: 0.9473684430122375)
[2024-11-08 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:27][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.2043737918138504, acc: 0.9090909361839294)
[2024-11-08 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:27][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.0043554906733334064, acc: 1.0)
[2024-11-08 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:27][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.2262490689754486, acc: 0.9333333373069763)
[2024-11-08 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:27][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.1570024937391281, acc: 0.9714285731315613)
[2024-11-08 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:28][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.06907942146062851, acc: 1.0)
[2024-11-08 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:28][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.26583942770957947, acc: 0.9047619104385376)
[2024-11-08 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:28][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.3510252833366394, acc: 0.8947368264198303)
[2024-11-08 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:29][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.43591880798339844, acc: 0.8500000238418579)
[2024-11-08 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:29][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.30325958132743835, acc: 0.9047619104385376)
[2024-11-08 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:29][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.04856960475444794, acc: 1.0)
[2024-11-08 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:30][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.041260235011577606, acc: 1.0)
[2024-11-08 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:30][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.06669929623603821, acc: 0.9655172228813171)
[2024-11-08 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:30][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.34241798520088196, acc: 0.9142857193946838)
[2024-11-08 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:31][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.08084140717983246, acc: 0.9523809552192688)
[2024-11-08 00:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:31][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.29417216777801514, acc: 0.8571428656578064)
[2024-11-08 00:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:31][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.09692656993865967, acc: 0.9473684430122375)
[2024-11-08 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:32][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.17388711869716644, acc: 0.9090909361839294)
[2024-11-08 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:32][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.05197494104504585, acc: 1.0)
[2024-11-08 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:33][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.11317669600248337, acc: 1.0)
[2024-11-08 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:33][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.27223241329193115, acc: 0.9411764740943909)
[2024-11-08 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:33][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.17817115783691406, acc: 0.9545454382896423)
[2024-11-08 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:34][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.14411231875419617, acc: 0.8999999761581421)
[2024-11-08 00:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:34][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 0.2876376211643219, acc: 0.8947368264198303)
[2024-11-08 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:35][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.224112406373024, acc: 0.9130434989929199)
[2024-11-08 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:35][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.20306839048862457, acc: 0.9523809552192688)
[2024-11-08 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:36][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.0730886310338974, acc: 1.0)
[2024-11-08 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:36][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.20348747074604034, acc: 0.9259259104728699)
[2024-11-08 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:36][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.09882113337516785, acc: 0.9615384340286255)
[2024-11-08 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:37][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.08490640670061111, acc: 0.9523809552192688)
[2024-11-08 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:37][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.025524593889713287, acc: 1.0)
[2024-11-08 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:37][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.2084818333387375, acc: 0.9473684430122375)
[2024-11-08 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:37][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.06716131418943405, acc: 1.0)
[2024-11-08 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:38][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.028004124760627747, acc: 1.0)
[2024-11-08 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:38][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.03634397312998772, acc: 1.0)
[2024-11-08 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:39][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.06325127184391022, acc: 0.9629629850387573)
[2024-11-08 00:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:07][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2327, device='cuda:0') eval_epoch_loss=tensor(0.2092, device='cuda:0') eval_epoch_acc=tensor(0.9309, device='cuda:0')
[2024-11-08 00:49:07][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:49:07][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:49:08][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_5_step_278_loss_0.2092050015926361/model.pt
[2024-11-08 00:49:08][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:49:08][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 5 is 0.2092050015926361
[2024-11-08 00:49:08][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.9308938384056091
[2024-11-08 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:08][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.05755343288183212, acc: 1.0)
[2024-11-08 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:08][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.017926111817359924, acc: 1.0)
[2024-11-08 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:08][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.03979241102933884, acc: 1.0)
[2024-11-08 00:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:09][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.1680040955543518, acc: 0.9545454382896423)
[2024-11-08 00:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:09][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.06213436275720596, acc: 1.0)
[2024-11-08 00:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:10][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.21568092703819275, acc: 0.931034505367279)
[2024-11-08 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:10][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.06252826005220413, acc: 1.0)
[2024-11-08 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:10][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.18169590830802917, acc: 0.931034505367279)
[2024-11-08 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:11][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.09635951370000839, acc: 0.9523809552192688)
[2024-11-08 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:11][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.21121905744075775, acc: 0.9473684430122375)
[2024-11-08 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:11][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.2868206799030304, acc: 0.8947368264198303)
[2024-11-08 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:12][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.28734898567199707, acc: 0.8636363744735718)
[2024-11-08 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:12][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.14070042967796326, acc: 0.9545454382896423)
[2024-11-08 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:12][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.0019582125823944807, acc: 1.0)
[2024-11-08 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:12][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.2228870987892151, acc: 0.9333333373069763)
[2024-11-08 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:13][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.13305391371250153, acc: 0.9523809552192688)
[2024-11-08 00:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:13][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.044191405177116394, acc: 1.0)
[2024-11-08 00:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:14][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.23365819454193115, acc: 0.9473684430122375)
[2024-11-08 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:14][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.1682266891002655, acc: 0.9090909361839294)
[2024-11-08 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:14][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.23846085369586945, acc: 0.9047619104385376)
[2024-11-08 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:15][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.13376273214817047, acc: 0.9166666865348816)
[2024-11-08 00:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:15][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.046653129160404205, acc: 0.9677419066429138)
[2024-11-08 00:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:15][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.07480110973119736, acc: 0.9677419066429138)
[2024-11-08 00:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:15][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.05127628892660141, acc: 0.9615384340286255)
[2024-11-08 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:16][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.22802318632602692, acc: 0.9523809552192688)
[2024-11-08 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:16][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.050431665033102036, acc: 1.0)
[2024-11-08 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:16][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.0448712594807148, acc: 1.0)
[2024-11-08 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:17][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.11349392682313919, acc: 0.95652174949646)
[2024-11-08 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:17][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.03317239135503769, acc: 1.0)
[2024-11-08 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:17][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.006097850855439901, acc: 1.0)
[2024-11-08 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:18][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.1770203858613968, acc: 0.9523809552192688)
[2024-11-08 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:18][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.35009872913360596, acc: 0.8421052694320679)
[2024-11-08 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:19][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.2646220326423645, acc: 0.9473684430122375)
[2024-11-08 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:19][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.2251415252685547, acc: 0.9545454382896423)
[2024-11-08 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:19][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.17267820239067078, acc: 0.9545454382896423)
[2024-11-08 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:20][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.140467569231987, acc: 0.9642857313156128)
[2024-11-08 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:20][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.13123685121536255, acc: 0.9259259104728699)
[2024-11-08 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:20][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.10974662005901337, acc: 0.9428571462631226)
[2024-11-08 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:20][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.19700942933559418, acc: 0.9230769276618958)
[2024-11-08 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:21][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.0022614188492298126, acc: 1.0)
[2024-11-08 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:21][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.14316199719905853, acc: 0.949999988079071)
[2024-11-08 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:21][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.022918488830327988, acc: 1.0)
[2024-11-08 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:22][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.04603924602270126, acc: 1.0)
[2024-11-08 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:22][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.006535529624670744, acc: 1.0)
[2024-11-08 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:22][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.18666860461235046, acc: 0.9130434989929199)
[2024-11-08 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:23][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.6983283162117004, acc: 0.8571428656578064)
[2024-11-08 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:23][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.5419975519180298, acc: 0.8421052694320679)
[2024-11-08 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:23][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 0.36319878697395325, acc: 0.8181818127632141)
[2024-11-08 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:24][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.27225229144096375, acc: 0.8947368264198303)
[2024-11-08 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:24][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.1612638235092163, acc: 0.9166666865348816)
[2024-11-08 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:24][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.2511570453643799, acc: 0.931034505367279)
[2024-11-08 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:24][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.2180081158876419, acc: 0.9629629850387573)
[2024-11-08 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:25][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.10817881673574448, acc: 0.9523809552192688)
[2024-11-08 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:25][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.044622063636779785, acc: 1.0)
[2024-11-08 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:26][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.176577627658844, acc: 0.9090909361839294)
[2024-11-08 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:26][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.3488449156284332, acc: 0.9473684430122375)
[2024-11-08 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:26][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.16718792915344238, acc: 0.9545454382896423)
[2024-11-08 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:26][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.20514310896396637, acc: 0.9230769276618958)
[2024-11-08 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:27][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.3053876459598541, acc: 0.9166666865348816)
[2024-11-08 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:27][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 0.34676623344421387, acc: 0.8095238208770752)
[2024-11-08 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:28][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 0.16714124381542206, acc: 0.9130434989929199)
[2024-11-08 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:28][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 0.14096589386463165, acc: 0.9545454382896423)
[2024-11-08 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:28][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.12866786122322083, acc: 0.9666666388511658)
[2024-11-08 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:28][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.035561900585889816, acc: 1.0)
[2024-11-08 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:29][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.06842658668756485, acc: 0.949999988079071)
[2024-11-08 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:29][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.25972962379455566, acc: 0.8999999761581421)
[2024-11-08 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:29][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.07497517764568329, acc: 1.0)
[2024-11-08 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:30][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.05497730150818825, acc: 0.9545454382896423)
[2024-11-08 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:30][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.31061017513275146, acc: 0.800000011920929)
[2024-11-08 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:30][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.011076942086219788, acc: 1.0)
[2024-11-08 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:31][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.14502520859241486, acc: 0.9130434989929199)
[2024-11-08 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:31][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.33754390478134155, acc: 0.949999988079071)
[2024-11-08 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:31][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.20548570156097412, acc: 0.8421052694320679)
[2024-11-08 00:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:32][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.15303519368171692, acc: 0.9545454382896423)
[2024-11-08 00:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:32][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.36458295583724976, acc: 0.8500000238418579)
[2024-11-08 00:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:32][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.058477651327848434, acc: 1.0)
[2024-11-08 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:33][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.2840532064437866, acc: 0.9599999785423279)
[2024-11-08 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:33][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.2600002884864807, acc: 0.9047619104385376)
[2024-11-08 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:34][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.31257426738739014, acc: 0.8500000238418579)
[2024-11-08 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:34][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.22494681179523468, acc: 0.9047619104385376)
[2024-11-08 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:34][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.11313089728355408, acc: 0.9545454382896423)
[2024-11-08 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:35][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.0033828136511147022, acc: 1.0)
[2024-11-08 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:35][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.11239908635616302, acc: 0.9655172228813171)
[2024-11-08 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:35][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.10467284172773361, acc: 0.9428571462631226)
[2024-11-08 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:36][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.1429719179868698, acc: 0.9545454382896423)
[2024-11-08 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:36][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.1540629267692566, acc: 0.9473684430122375)
[2024-11-08 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:36][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.1890200823545456, acc: 0.9047619104385376)
[2024-11-08 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:36][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.09554702788591385, acc: 0.9473684430122375)
[2024-11-08 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:37][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.046191275119781494, acc: 1.0)
[2024-11-08 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:37][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.05565479397773743, acc: 0.9629629850387573)
[2024-11-08 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:37][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.16747677326202393, acc: 0.90625)
[2024-11-08 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:38][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.06285031884908676, acc: 1.0)
[2024-11-08 00:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:38][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.26517418026924133, acc: 0.9047619104385376)
[2024-11-08 00:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:39][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.26559916138648987, acc: 0.8947368264198303)
[2024-11-08 00:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:39][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.07430488616228104, acc: 1.0)
[2024-11-08 00:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:40][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.1431293487548828, acc: 0.9473684430122375)
[2024-11-08 00:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:40][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.05019544064998627, acc: 1.0)
[2024-11-08 00:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:40][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.018385635688900948, acc: 1.0)
[2024-11-08 00:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:41][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.03800247237086296, acc: 0.9677419066429138)
[2024-11-08 00:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:41][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.2156546711921692, acc: 0.9047619104385376)
[2024-11-08 00:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:41][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.058728933334350586, acc: 1.0)
[2024-11-08 00:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:42][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.03332700580358505, acc: 1.0)
[2024-11-08 00:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:42][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.12259075045585632, acc: 0.9090909361839294)
[2024-11-08 00:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:43][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.05567493662238121, acc: 1.0)
[2024-11-08 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:44][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.013722067698836327, acc: 1.0)
[2024-11-08 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:44][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.010396731086075306, acc: 1.0)
[2024-11-08 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:45][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.010938433930277824, acc: 1.0)
[2024-11-08 00:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:45][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.00607473636046052, acc: 1.0)
[2024-11-08 00:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:45][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.03015763685107231, acc: 1.0)
[2024-11-08 00:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:46][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.00955159030854702, acc: 1.0)
[2024-11-08 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:46][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.007882487028837204, acc: 1.0)
[2024-11-08 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:46][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.03704192861914635, acc: 1.0)
[2024-11-08 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:46][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.39994165301322937, acc: 0.9545454382896423)
[2024-11-08 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:47][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.08479409664869308, acc: 0.9523809552192688)
[2024-11-08 00:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:47][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.17531992495059967, acc: 0.9473684430122375)
[2024-11-08 00:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:48][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.03423367813229561, acc: 1.0)
[2024-11-08 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:48][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.2454373985528946, acc: 0.8636363744735718)
[2024-11-08 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:48][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.2730487287044525, acc: 0.8571428656578064)
[2024-11-08 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:49][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.024172283709049225, acc: 1.0)
[2024-11-08 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:49][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.16438721120357513, acc: 0.9333333373069763)
[2024-11-08 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:49][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.023572925478219986, acc: 1.0)
[2024-11-08 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:50][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.05265698954463005, acc: 1.0)
[2024-11-08 00:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:50][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.13271090388298035, acc: 0.9090909361839294)
[2024-11-08 00:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:51][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.006667799782007933, acc: 1.0)
[2024-11-08 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:51][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.006395666394382715, acc: 1.0)
[2024-11-08 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:51][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.009605941362679005, acc: 1.0)
[2024-11-08 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:52][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.07048671692609787, acc: 1.0)
[2024-11-08 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:52][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.009761855006217957, acc: 1.0)
[2024-11-08 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:52][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.030410582199692726, acc: 1.0)
[2024-11-08 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:53][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.16777272522449493, acc: 0.9473684430122375)
[2024-11-08 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:53][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.13977620005607605, acc: 0.949999988079071)
[2024-11-08 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:53][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.03201443701982498, acc: 1.0)
[2024-11-08 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:53][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.0012354825157672167, acc: 1.0)
[2024-11-08 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:54][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.19017019867897034, acc: 0.9285714030265808)
[2024-11-08 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:54][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.010224666446447372, acc: 1.0)
[2024-11-08 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:54][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.003979192115366459, acc: 1.0)
[2024-11-08 00:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:55][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.04211889207363129, acc: 0.9615384340286255)
[2024-11-08 00:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:55][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.1250476837158203, acc: 0.9523809552192688)
[2024-11-08 00:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:55][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.10869482904672623, acc: 0.9473684430122375)
[2024-11-08 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:56][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.06535647809505463, acc: 0.9473684430122375)
[2024-11-08 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:56][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.032626569271087646, acc: 1.0)
[2024-11-08 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:57][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.03329942747950554, acc: 1.0)
[2024-11-08 00:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:57][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.029232777655124664, acc: 1.0)
[2024-11-08 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:26][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3471, device='cuda:0') eval_epoch_loss=tensor(0.2980, device='cuda:0') eval_epoch_acc=tensor(0.9191, device='cuda:0')
[2024-11-08 00:50:26][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:50:26][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:50:27][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_5_step_421_loss_0.29797184467315674/model.pt
[2024-11-08 00:50:27][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:27][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.058566365391016006, acc: 0.9677419066429138)
[2024-11-08 00:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:27][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.31263941526412964, acc: 0.9354838728904724)
[2024-11-08 00:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:28][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.1209593340754509, acc: 0.9230769276618958)
[2024-11-08 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:28][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.008013923652470112, acc: 1.0)
[2024-11-08 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:28][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.12212400883436203, acc: 0.8947368264198303)
[2024-11-08 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:29][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 0.16611994802951813, acc: 0.9473684430122375)
[2024-11-08 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:29][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.019983617588877678, acc: 1.0)
[2024-11-08 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:29][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.010575338266789913, acc: 1.0)
[2024-11-08 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:29][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.08999865502119064, acc: 0.9583333134651184)
[2024-11-08 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:30][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.014518504031002522, acc: 1.0)
[2024-11-08 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:30][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.011271876282989979, acc: 1.0)
[2024-11-08 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:30][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.005005410872399807, acc: 1.0)
[2024-11-08 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:31][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.011623172089457512, acc: 1.0)
[2024-11-08 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:31][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.0677156001329422, acc: 0.9473684430122375)
[2024-11-08 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:31][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.063691645860672, acc: 0.9473684430122375)
[2024-11-08 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:32][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.06206542253494263, acc: 0.9545454382896423)
[2024-11-08 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:32][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.031289733946323395, acc: 1.0)
[2024-11-08 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:32][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.006322458386421204, acc: 1.0)
[2024-11-08 00:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:33][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.10040237754583359, acc: 0.9642857313156128)
[2024-11-08 00:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:33][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.0825137346982956, acc: 0.9523809552192688)
[2024-11-08 00:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:34][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.2705467641353607, acc: 0.8947368264198303)
[2024-11-08 00:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:34][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.1052466556429863, acc: 0.9473684430122375)
[2024-11-08 00:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:35][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.11706871539354324, acc: 0.9090909361839294)
[2024-11-08 00:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:35][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.026820171624422073, acc: 1.0)
[2024-11-08 00:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:36][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.00893339142203331, acc: 1.0)
[2024-11-08 00:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:36][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.15847919881343842, acc: 0.9677419066429138)
[2024-11-08 00:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:36][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.06190166994929314, acc: 0.9677419066429138)
[2024-11-08 00:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:37][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.0040824152529239655, acc: 1.0)
[2024-11-08 00:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:37][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.009614134207367897, acc: 1.0)
[2024-11-08 00:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:37][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.007324013859033585, acc: 1.0)
[2024-11-08 00:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:38][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.07674972712993622, acc: 0.9473684430122375)
[2024-11-08 00:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:38][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.005087495781481266, acc: 1.0)
[2024-11-08 00:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:38][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.102106474339962, acc: 0.949999988079071)
[2024-11-08 00:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:39][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.009050152264535427, acc: 1.0)
[2024-11-08 00:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:39][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.028633911162614822, acc: 0.9677419066429138)
[2024-11-08 00:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:39][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.1105470284819603, acc: 0.8999999761581421)
[2024-11-08 00:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:40][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.009469754993915558, acc: 1.0)
[2024-11-08 00:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:40][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.01068530697375536, acc: 1.0)
[2024-11-08 00:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:40][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.12027933448553085, acc: 0.9545454382896423)
[2024-11-08 00:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:41][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.037153758108615875, acc: 1.0)
[2024-11-08 00:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:41][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.00452033756300807, acc: 1.0)
[2024-11-08 00:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:41][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.042140088975429535, acc: 0.9696969985961914)
[2024-11-08 00:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:41][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.11203236132860184, acc: 0.9629629850387573)
[2024-11-08 00:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:42][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.03210197389125824, acc: 0.9696969985961914)
[2024-11-08 00:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:42][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.0010835870634764433, acc: 1.0)
[2024-11-08 00:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:42][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.003815834643319249, acc: 1.0)
[2024-11-08 00:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:43][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.02790464460849762, acc: 1.0)
[2024-11-08 00:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:43][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.005064993165433407, acc: 1.0)
[2024-11-08 00:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:43][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.010802509263157845, acc: 1.0)
[2024-11-08 00:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:44][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.0034393491223454475, acc: 1.0)
[2024-11-08 00:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:44][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.02335134893655777, acc: 1.0)
[2024-11-08 00:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:44][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.3866102397441864, acc: 0.875)
[2024-11-08 00:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:45][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.28352850675582886, acc: 0.9047619104385376)
[2024-11-08 00:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:45][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.01172563899308443, acc: 1.0)
[2024-11-08 00:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:45][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.0555906668305397, acc: 1.0)
[2024-11-08 00:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:46][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.017428524792194366, acc: 1.0)
[2024-11-08 00:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:46][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.47525903582572937, acc: 0.9090909361839294)
[2024-11-08 00:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:46][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.09588529914617538, acc: 0.9285714030265808)
[2024-11-08 00:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:47][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.19713358581066132, acc: 0.9354838728904724)
[2024-11-08 00:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:47][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.026296157389879227, acc: 1.0)
[2024-11-08 00:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:47][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.16677501797676086, acc: 0.9047619104385376)
[2024-11-08 00:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:48][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.16170769929885864, acc: 0.9473684430122375)
[2024-11-08 00:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:48][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.36920812726020813, acc: 0.9090909361839294)
[2024-11-08 00:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:48][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.10773206502199173, acc: 0.9545454382896423)
[2024-11-08 00:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:49][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.06849471479654312, acc: 0.9642857313156128)
[2024-11-08 00:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:49][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.2147749960422516, acc: 0.8888888955116272)
[2024-11-08 00:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:49][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.17972725629806519, acc: 0.9142857193946838)
[2024-11-08 00:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:50][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.38305002450942993, acc: 0.8461538553237915)
[2024-11-08 00:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:50][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.06709517538547516, acc: 0.9523809552192688)
[2024-11-08 00:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:50][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.16418500244617462, acc: 0.9473684430122375)
[2024-11-08 00:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:51][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.08265941590070724, acc: 0.9473684430122375)
[2024-11-08 00:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:51][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.30280014872550964, acc: 0.9545454382896423)
[2024-11-08 00:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:51][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.1709122359752655, acc: 0.9523809552192688)
[2024-11-08 00:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:52][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.15279817581176758, acc: 0.9166666865348816)
[2024-11-08 00:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:52][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.20232617855072021, acc: 0.9285714030265808)
[2024-11-08 00:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:52][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.07041891664266586, acc: 1.0)
[2024-11-08 00:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:53][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.28245747089385986, acc: 0.8500000238418579)
[2024-11-08 00:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:53][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.3656170964241028, acc: 0.8947368264198303)
[2024-11-08 00:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:53][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 0.10616850107908249, acc: 1.0)
[2024-11-08 00:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:54][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.4987434446811676, acc: 0.8999999761581421)
[2024-11-08 00:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:54][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.0652991309762001, acc: 1.0)
[2024-11-08 00:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:54][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.06866534799337387, acc: 0.9677419066429138)
[2024-11-08 00:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:54][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.07988731563091278, acc: 1.0)
[2024-11-08 00:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:55][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.045701976865530014, acc: 1.0)
[2024-11-08 00:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:55][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.11144044250249863, acc: 0.9047619104385376)
[2024-11-08 00:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:55][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.32815074920654297, acc: 0.8947368264198303)
[2024-11-08 00:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:56][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 0.035797253251075745, acc: 1.0)
[2024-11-08 00:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:56][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.4341304302215576, acc: 0.8500000238418579)
[2024-11-08 00:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:57][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.2081846296787262, acc: 0.9545454382896423)
[2024-11-08 00:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:57][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.13484176993370056, acc: 0.939393937587738)
[2024-11-08 00:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:57][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.06326087564229965, acc: 1.0)
[2024-11-08 00:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:50:59][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 0.20711293816566467, acc: 0.9285714030265808)
[2024-11-08 00:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:00][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.12118447571992874, acc: 0.9523809552192688)
[2024-11-08 00:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:00][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.34990212321281433, acc: 0.8947368264198303)
[2024-11-08 00:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:01][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.05055820196866989, acc: 1.0)
[2024-11-08 00:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:01][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.1124715656042099, acc: 0.9523809552192688)
[2024-11-08 00:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:02][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.005531460512429476, acc: 1.0)
[2024-11-08 00:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:02][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.05620265379548073, acc: 1.0)
[2024-11-08 00:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:02][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.2420821487903595, acc: 0.9259259104728699)
[2024-11-08 00:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:03][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.12608392536640167, acc: 0.9714285731315613)
[2024-11-08 00:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:03][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.06438218802213669, acc: 0.95652174949646)
[2024-11-08 00:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:04][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.11260777711868286, acc: 0.9523809552192688)
[2024-11-08 00:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:04][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.29889655113220215, acc: 0.8947368264198303)
[2024-11-08 00:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:05][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.19777606427669525, acc: 0.9090909361839294)
[2024-11-08 00:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:05][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.1024981141090393, acc: 0.8999999761581421)
[2024-11-08 00:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:05][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.09577751159667969, acc: 1.0)
[2024-11-08 00:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:06][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.17638608813285828, acc: 0.939393937587738)
[2024-11-08 00:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:06][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 0.32178112864494324, acc: 0.8928571343421936)
[2024-11-08 00:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:06][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.04234016686677933, acc: 1.0)
[2024-11-08 00:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:07][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.2616961896419525, acc: 0.8947368264198303)
[2024-11-08 00:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:07][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.1668722927570343, acc: 0.8999999761581421)
[2024-11-08 00:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:07][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.19746199250221252, acc: 0.9523809552192688)
[2024-11-08 00:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:07][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.04414410516619682, acc: 1.0)
[2024-11-08 00:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:08][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.04644737020134926, acc: 1.0)
[2024-11-08 00:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:08][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.025093471631407738, acc: 1.0)
[2024-11-08 00:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:08][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.10930214077234268, acc: 0.9599999785423279)
[2024-11-08 00:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:09][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.20925673842430115, acc: 0.9047619104385376)
[2024-11-08 00:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:09][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.3786807358264923, acc: 0.8421052694320679)
[2024-11-08 00:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:10][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.09435350447893143, acc: 1.0)
[2024-11-08 00:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:10][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.25708797574043274, acc: 0.9047619104385376)
[2024-11-08 00:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:10][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.06640737503767014, acc: 1.0)
[2024-11-08 00:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:10][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.005610088352113962, acc: 1.0)
[2024-11-08 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:11][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.009133386425673962, acc: 1.0)
[2024-11-08 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:11][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.10733630508184433, acc: 0.9696969985961914)
[2024-11-08 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:11][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.007257679011672735, acc: 1.0)
[2024-11-08 00:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:12][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.020758623257279396, acc: 1.0)
[2024-11-08 00:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:12][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.06423932313919067, acc: 1.0)
[2024-11-08 00:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:13][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.045171186327934265, acc: 1.0)
[2024-11-08 00:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:13][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.007848544977605343, acc: 1.0)
[2024-11-08 00:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:13][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.010744300670921803, acc: 1.0)
[2024-11-08 00:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:13][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.024493025615811348, acc: 1.0)
[2024-11-08 00:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:14][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.08035872876644135, acc: 0.9629629850387573)
[2024-11-08 00:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:14][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.12537823617458344, acc: 0.9523809552192688)
[2024-11-08 00:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:14][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.14554016292095184, acc: 0.9473684430122375)
[2024-11-08 00:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:15][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.09173591434955597, acc: 1.0)
[2024-11-08 00:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:15][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.031458865851163864, acc: 1.0)
[2024-11-08 00:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:15][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.0013062796788290143, acc: 1.0)
[2024-11-08 00:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:16][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.01614769548177719, acc: 1.0)
[2024-11-08 00:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:16][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.17852963507175446, acc: 0.9629629850387573)
[2024-11-08 00:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:16][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.017907684668898582, acc: 1.0)
[2024-11-08 00:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:17][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.013900363817811012, acc: 1.0)
[2024-11-08 00:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:17][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.0035848726984113455, acc: 1.0)
[2024-11-08 00:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:17][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.057145554572343826, acc: 1.0)
[2024-11-08 00:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:47][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3394, device='cuda:0') eval_epoch_loss=tensor(0.2922, device='cuda:0') eval_epoch_acc=tensor(0.9237, device='cuda:0')
[2024-11-08 00:51:47][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:51:47][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:51:48][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_5_step_564_loss_0.2922380268573761/model.pt
[2024-11-08 00:51:48][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:48][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.027784524485468864, acc: 1.0)
[2024-11-08 00:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:48][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.21576900780200958, acc: 0.949999988079071)
[2024-11-08 00:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:49][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.03563898429274559, acc: 1.0)
[2024-11-08 00:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:49][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.02546682395040989, acc: 1.0)
[2024-11-08 00:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:49][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.011210616677999496, acc: 1.0)
[2024-11-08 00:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:50][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.010056163184344769, acc: 1.0)
[2024-11-08 00:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:50][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.000504810712300241, acc: 1.0)
[2024-11-08 00:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:50][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.25345078110694885, acc: 0.8947368264198303)
[2024-11-08 00:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:51][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.006613889243453741, acc: 1.0)
[2024-11-08 00:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:51][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.24269047379493713, acc: 0.949999988079071)
[2024-11-08 00:51:52][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.1694, train_epoch_loss=0.1565, epoch time 332.1696890350431s
[2024-11-08 00:51:52][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 00:51:52][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 00:51:52][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 00:51:52][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 00:51:52][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 00:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:53][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.02938893809914589, acc: 1.0)
[2024-11-08 00:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:53][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.0025279114488512278, acc: 1.0)
[2024-11-08 00:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:53][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.00890988577157259, acc: 1.0)
[2024-11-08 00:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:54][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.031593725085258484, acc: 1.0)
[2024-11-08 00:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:54][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.020356085151433945, acc: 1.0)
[2024-11-08 00:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:54][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.1154342070221901, acc: 0.9473684430122375)
[2024-11-08 00:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:55][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.001914496999233961, acc: 1.0)
[2024-11-08 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:55][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.009823570027947426, acc: 1.0)
[2024-11-08 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:55][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.0011966241290792823, acc: 1.0)
[2024-11-08 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:56][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.26602810621261597, acc: 0.9354838728904724)
[2024-11-08 00:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:56][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.02005734108388424, acc: 1.0)
[2024-11-08 00:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:56][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.11412069946527481, acc: 0.9599999785423279)
[2024-11-08 00:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:57][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.026991283521056175, acc: 1.0)
[2024-11-08 00:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:57][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.04785143584012985, acc: 1.0)
[2024-11-08 00:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:57][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.24531254172325134, acc: 0.95652174949646)
[2024-11-08 00:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:57][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.0025074940640479326, acc: 1.0)
[2024-11-08 00:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:58][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.12343358248472214, acc: 0.9642857313156128)
[2024-11-08 00:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:58][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.028454216197133064, acc: 1.0)
[2024-11-08 00:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:58][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.33727866411209106, acc: 0.9545454382896423)
[2024-11-08 00:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:59][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.2093372344970703, acc: 0.8947368264198303)
[2024-11-08 00:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:59][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.18184500932693481, acc: 0.9473684430122375)
[2024-11-08 00:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:51:59][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.1982240229845047, acc: 0.9090909361839294)
[2024-11-08 00:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:00][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.10437329858541489, acc: 0.9523809552192688)
[2024-11-08 00:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:00][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.0005731877172365785, acc: 1.0)
[2024-11-08 00:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:00][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.020082714036107063, acc: 1.0)
[2024-11-08 00:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:01][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.08973211795091629, acc: 0.9523809552192688)
[2024-11-08 00:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:01][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.38398438692092896, acc: 0.8571428656578064)
[2024-11-08 00:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:02][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 0.2440631240606308, acc: 0.9473684430122375)
[2024-11-08 00:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:02][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.09849905967712402, acc: 0.9545454382896423)
[2024-11-08 00:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:03][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.17569662630558014, acc: 0.8999999761581421)
[2024-11-08 00:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:03][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.017368854954838753, acc: 1.0)
[2024-11-08 00:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:03][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.02886277250945568, acc: 1.0)
[2024-11-08 00:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:04][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.169650599360466, acc: 0.939393937587738)
[2024-11-08 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:04][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.04368400201201439, acc: 1.0)
[2024-11-08 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:04][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.10146892815828323, acc: 0.9047619104385376)
[2024-11-08 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:04][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.07655370235443115, acc: 1.0)
[2024-11-08 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:05][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.018882017582654953, acc: 1.0)
[2024-11-08 00:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:05][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.03026128187775612, acc: 1.0)
[2024-11-08 00:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:05][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.0015699189389124513, acc: 1.0)
[2024-11-08 00:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:06][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.24981984496116638, acc: 0.9285714030265808)
[2024-11-08 00:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:06][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.4280377924442291, acc: 0.8571428656578064)
[2024-11-08 00:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:06][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.04946780949831009, acc: 0.949999988079071)
[2024-11-08 00:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:07][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.11414603888988495, acc: 0.8999999761581421)
[2024-11-08 00:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:07][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.4677780568599701, acc: 0.8421052694320679)
[2024-11-08 00:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:08][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.22843755781650543, acc: 0.9545454382896423)
[2024-11-08 00:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:08][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.12488655745983124, acc: 0.9523809552192688)
[2024-11-08 00:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:08][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.08953443169593811, acc: 0.9583333134651184)
[2024-11-08 00:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:09][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.047430992126464844, acc: 0.9677419066429138)
[2024-11-08 00:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:09][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.016088763251900673, acc: 1.0)
[2024-11-08 00:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:09][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.07296083867549896, acc: 0.9599999785423279)
[2024-11-08 00:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:10][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.09942837804555893, acc: 0.9523809552192688)
[2024-11-08 00:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:10][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.3370947241783142, acc: 0.8421052694320679)
[2024-11-08 00:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:10][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.17115619778633118, acc: 0.949999988079071)
[2024-11-08 00:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:11][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 0.3698286712169647, acc: 0.9047619104385376)
[2024-11-08 00:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:11][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.248149573802948, acc: 0.9090909361839294)
[2024-11-08 00:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:11][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.0033640828914940357, acc: 1.0)
[2024-11-08 00:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:14][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 0.1143781766295433, acc: 0.9523809552192688)
[2024-11-08 00:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:15][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 0.15886718034744263, acc: 0.9047619104385376)
[2024-11-08 00:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:16][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.018297722563147545, acc: 1.0)
[2024-11-08 00:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:16][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.10961785912513733, acc: 0.9545454382896423)
[2024-11-08 00:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:17][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.413764089345932, acc: 0.8947368264198303)
[2024-11-08 00:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:17][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.07234480232000351, acc: 0.9523809552192688)
[2024-11-08 00:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:17][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.06327906250953674, acc: 0.9722222089767456)
[2024-11-08 00:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:18][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.38027021288871765, acc: 0.8571428656578064)
[2024-11-08 00:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:18][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.20300474762916565, acc: 0.9047619104385376)
[2024-11-08 00:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:18][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.190419539809227, acc: 0.949999988079071)
[2024-11-08 00:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:19][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.2363717406988144, acc: 0.9473684430122375)
[2024-11-08 00:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:19][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.2102327048778534, acc: 0.8500000238418579)
[2024-11-08 00:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:19][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.017264407128095627, acc: 1.0)
[2024-11-08 00:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:19][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.08627074956893921, acc: 0.9696969985961914)
[2024-11-08 00:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:20][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.14787612855434418, acc: 0.9629629850387573)
[2024-11-08 00:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:20][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.1950405389070511, acc: 0.9333333373069763)
[2024-11-08 00:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:20][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.02910950593650341, acc: 1.0)
[2024-11-08 00:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:21][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 0.19761061668395996, acc: 0.9473684430122375)
[2024-11-08 00:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:21][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.05788451433181763, acc: 1.0)
[2024-11-08 00:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:21][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.11477247625589371, acc: 0.9523809552192688)
[2024-11-08 00:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:22][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 0.20406363904476166, acc: 0.8636363744735718)
[2024-11-08 00:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:22][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.027187252417206764, acc: 1.0)
[2024-11-08 00:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:22][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.06481003016233444, acc: 0.9629629850387573)
[2024-11-08 00:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:23][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.10860836505889893, acc: 0.9428571462631226)
[2024-11-08 00:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:23][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.05469885841012001, acc: 0.9615384340286255)
[2024-11-08 00:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:23][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.06838739663362503, acc: 0.9523809552192688)
[2024-11-08 00:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:24][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.169187530875206, acc: 0.9473684430122375)
[2024-11-08 00:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:24][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.045072294771671295, acc: 1.0)
[2024-11-08 00:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:24][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.011313795112073421, acc: 1.0)
[2024-11-08 00:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:25][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.193014457821846, acc: 0.9523809552192688)
[2024-11-08 00:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:25][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.03203984722495079, acc: 1.0)
[2024-11-08 00:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:25][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.2828558683395386, acc: 0.8846153616905212)
[2024-11-08 00:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:26][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.027211172506213188, acc: 1.0)
[2024-11-08 00:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:27][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.3169100284576416, acc: 0.8421052694320679)
[2024-11-08 00:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:28][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 0.26783397793769836, acc: 0.8947368264198303)
[2024-11-08 00:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:28][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.140486940741539, acc: 0.9545454382896423)
[2024-11-08 00:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:29][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.0947074368596077, acc: 1.0)
[2024-11-08 00:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:30][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 0.3916546106338501, acc: 0.9230769276618958)
[2024-11-08 00:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:30][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.13723252713680267, acc: 0.9285714030265808)
[2024-11-08 00:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:31][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.22355106472969055, acc: 0.9047619104385376)
[2024-11-08 00:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:31][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.28346261382102966, acc: 0.8947368264198303)
[2024-11-08 00:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:31][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.1309662163257599, acc: 0.949999988079071)
[2024-11-08 00:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:32][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 0.36185911297798157, acc: 0.8095238208770752)
[2024-11-08 00:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:32][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.40212565660476685, acc: 0.8636363744735718)
[2024-11-08 00:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:32][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.06848879903554916, acc: 0.9523809552192688)
[2024-11-08 00:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:33][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.27030041813850403, acc: 0.9200000166893005)
[2024-11-08 00:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:33][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.021191101521253586, acc: 1.0)
[2024-11-08 00:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:34][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.004396042320877314, acc: 1.0)
[2024-11-08 00:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:34][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.10292568057775497, acc: 0.9090909361839294)
[2024-11-08 00:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:34][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.13715240359306335, acc: 0.9473684430122375)
[2024-11-08 00:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:35][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.33949029445648193, acc: 0.9545454382896423)
[2024-11-08 00:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:35][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.015100156888365746, acc: 1.0)
[2024-11-08 00:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:35][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.03776237741112709, acc: 1.0)
[2024-11-08 00:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:36][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.02844090946018696, acc: 1.0)
[2024-11-08 00:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:36][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.011288215406239033, acc: 1.0)
[2024-11-08 00:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:36][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.1098446473479271, acc: 0.9473684430122375)
[2024-11-08 00:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:37][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.012260385788977146, acc: 1.0)
[2024-11-08 00:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:37][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.004460279364138842, acc: 1.0)
[2024-11-08 00:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:37][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.04091384634375572, acc: 0.9545454382896423)
[2024-11-08 00:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:38][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.08182214200496674, acc: 0.9615384340286255)
[2024-11-08 00:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:38][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.30180081725120544, acc: 0.9200000166893005)
[2024-11-08 00:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:38][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.07311607152223587, acc: 0.949999988079071)
[2024-11-08 00:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:39][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.02008485235273838, acc: 1.0)
[2024-11-08 00:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:39][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.024879448115825653, acc: 1.0)
[2024-11-08 00:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:40][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.19227294623851776, acc: 0.9473684430122375)
[2024-11-08 00:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:40][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.016782784834504128, acc: 1.0)
[2024-11-08 00:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:40][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.01864698901772499, acc: 1.0)
[2024-11-08 00:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:41][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.27482011914253235, acc: 0.8846153616905212)
[2024-11-08 00:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:41][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.01590624637901783, acc: 1.0)
[2024-11-08 00:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:41][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 0.2623923718929291, acc: 0.8999999761581421)
[2024-11-08 00:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:42][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.1122845858335495, acc: 0.9473684430122375)
[2024-11-08 00:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:42][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.21434345841407776, acc: 0.9090909361839294)
[2024-11-08 00:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:42][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.3285726010799408, acc: 0.8500000238418579)
[2024-11-08 00:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:43][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.004748864099383354, acc: 1.0)
[2024-11-08 00:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:43][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.08360546082258224, acc: 0.9696969985961914)
[2024-11-08 00:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:43][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.028773346915841103, acc: 1.0)
[2024-11-08 00:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:44][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.16492460668087006, acc: 0.939393937587738)
[2024-11-08 00:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:14][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3261, device='cuda:0') eval_epoch_loss=tensor(0.2822, device='cuda:0') eval_epoch_acc=tensor(0.9208, device='cuda:0')
[2024-11-08 00:53:14][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:53:14][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:53:14][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_6_step_133_loss_0.28221410512924194/model.pt
[2024-11-08 00:53:14][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:15][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.026018163189291954, acc: 1.0)
[2024-11-08 00:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:15][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.00953847449272871, acc: 1.0)
[2024-11-08 00:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:15][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.2134108543395996, acc: 0.9473684430122375)
[2024-11-08 00:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:16][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.029966704547405243, acc: 1.0)
[2024-11-08 00:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:16][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.4443184435367584, acc: 0.8999999761581421)
[2024-11-08 00:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:16][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.0348028689622879, acc: 1.0)
[2024-11-08 00:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:17][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.052739277482032776, acc: 0.9696969985961914)
[2024-11-08 00:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:17][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.27218812704086304, acc: 0.8888888955116272)
[2024-11-08 00:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:17][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.07058995962142944, acc: 0.9696969985961914)
[2024-11-08 00:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:18][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.41184002161026, acc: 0.800000011920929)
[2024-11-08 00:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:18][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.316226065158844, acc: 0.949999988079071)
[2024-11-08 00:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:18][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.22804099321365356, acc: 0.9473684430122375)
[2024-11-08 00:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:19][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.03610735759139061, acc: 1.0)
[2024-11-08 00:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:19][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.17404410243034363, acc: 0.949999988079071)
[2024-11-08 00:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:20][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.06574079394340515, acc: 1.0)
[2024-11-08 00:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:20][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.11666375398635864, acc: 0.9696969985961914)
[2024-11-08 00:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:20][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.04568100348114967, acc: 0.9629629850387573)
[2024-11-08 00:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:21][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.03496833145618439, acc: 1.0)
[2024-11-08 00:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:21][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.19396989047527313, acc: 0.949999988079071)
[2024-11-08 00:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:21][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.0016915580490604043, acc: 1.0)
[2024-11-08 00:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:22][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.13564853370189667, acc: 0.9473684430122375)
[2024-11-08 00:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:22][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.00891843531280756, acc: 1.0)
[2024-11-08 00:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:22][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.03357508406043053, acc: 1.0)
[2024-11-08 00:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:23][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.08004825562238693, acc: 1.0)
[2024-11-08 00:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:23][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.31660088896751404, acc: 0.8787878751754761)
[2024-11-08 00:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:25][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.5491520166397095, acc: 0.8500000238418579)
[2024-11-08 00:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:25][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.13551335036754608, acc: 0.949999988079071)
[2024-11-08 00:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:26][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 0.21333910524845123, acc: 0.8947368264198303)
[2024-11-08 00:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:26][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 0.19637244939804077, acc: 0.9090909361839294)
[2024-11-08 00:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:27][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 0.5353884696960449, acc: 0.8500000238418579)
[2024-11-08 00:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:27][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.25012245774269104, acc: 0.9200000166893005)
[2024-11-08 00:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:28][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.04161551967263222, acc: 1.0)
[2024-11-08 00:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:28][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.17890869081020355, acc: 0.9444444179534912)
[2024-11-08 00:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:28][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.04224415868520737, acc: 1.0)
[2024-11-08 00:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:28][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.062021296471357346, acc: 1.0)
[2024-11-08 00:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:29][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.03799047693610191, acc: 1.0)
[2024-11-08 00:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:29][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.016912288963794708, acc: 1.0)
[2024-11-08 00:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:30][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.14593717455863953, acc: 0.949999988079071)
[2024-11-08 00:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:30][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.03789336979389191, acc: 1.0)
[2024-11-08 00:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:31][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.012165747582912445, acc: 1.0)
[2024-11-08 00:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:31][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.18813221156597137, acc: 0.8999999761581421)
[2024-11-08 00:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:32][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 0.21878167986869812, acc: 0.9047619104385376)
[2024-11-08 00:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:32][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.22576838731765747, acc: 0.9523809552192688)
[2024-11-08 00:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:32][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.02744843065738678, acc: 1.0)
[2024-11-08 00:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:33][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 0.15531297028064728, acc: 0.9090909361839294)
[2024-11-08 00:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:34][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.14021889865398407, acc: 0.8947368264198303)
[2024-11-08 00:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:34][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.06922301650047302, acc: 0.9545454382896423)
[2024-11-08 00:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:34][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.008213160559535027, acc: 1.0)
[2024-11-08 00:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:35][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.00221795286051929, acc: 1.0)
[2024-11-08 00:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:35][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.030404353514313698, acc: 1.0)
[2024-11-08 00:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:35][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.016313908621668816, acc: 1.0)
[2024-11-08 00:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:36][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.026732459664344788, acc: 1.0)
[2024-11-08 00:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:36][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.030682645738124847, acc: 1.0)
[2024-11-08 00:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:37][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.045962147414684296, acc: 1.0)
[2024-11-08 00:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:37][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.04299815744161606, acc: 1.0)
[2024-11-08 00:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:37][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.00736284302547574, acc: 1.0)
[2024-11-08 00:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:38][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.18578965961933136, acc: 0.9200000166893005)
[2024-11-08 00:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:38][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.27131956815719604, acc: 0.8999999761581421)
[2024-11-08 00:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:39][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 0.3221389353275299, acc: 0.8095238208770752)
[2024-11-08 00:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:40][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.4086027443408966, acc: 0.8421052694320679)
[2024-11-08 00:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:41][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.15050317347049713, acc: 0.9545454382896423)
[2024-11-08 00:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:42][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.1818513572216034, acc: 0.9047619104385376)
[2024-11-08 00:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:43][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.027925601229071617, acc: 1.0)
[2024-11-08 00:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:43][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.03194722160696983, acc: 0.9677419066429138)
[2024-11-08 00:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:43][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.044466037303209305, acc: 1.0)
[2024-11-08 00:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:44][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.05055652931332588, acc: 1.0)
[2024-11-08 00:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:44][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.04647977650165558, acc: 1.0)
[2024-11-08 00:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:44][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.012910967692732811, acc: 1.0)
[2024-11-08 00:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:45][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.16243548691272736, acc: 0.9545454382896423)
[2024-11-08 00:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:45][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.00542785320430994, acc: 1.0)
[2024-11-08 00:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:45][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.09577447175979614, acc: 0.95652174949646)
[2024-11-08 00:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:46][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.1674138456583023, acc: 0.9285714030265808)
[2024-11-08 00:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:46][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.015799472108483315, acc: 1.0)
[2024-11-08 00:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:46][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.030066216364502907, acc: 1.0)
[2024-11-08 00:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:47][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.05928264930844307, acc: 1.0)
[2024-11-08 00:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:47][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.017033766955137253, acc: 1.0)
[2024-11-08 00:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:47][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.008032710291445255, acc: 1.0)
[2024-11-08 00:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:48][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.02598811686038971, acc: 1.0)
[2024-11-08 00:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:48][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.1849491149187088, acc: 0.9354838728904724)
[2024-11-08 00:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:48][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.06794704496860504, acc: 0.9677419066429138)
[2024-11-08 00:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:48][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.25599434971809387, acc: 0.9615384340286255)
[2024-11-08 00:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:49][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.031030694022774696, acc: 1.0)
[2024-11-08 00:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:49][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.04682712256908417, acc: 1.0)
[2024-11-08 00:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:50][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.02067304030060768, acc: 1.0)
[2024-11-08 00:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:50][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.0009127874509431422, acc: 1.0)
[2024-11-08 00:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:51][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.06304885447025299, acc: 1.0)
[2024-11-08 00:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:51][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.0032807188108563423, acc: 1.0)
[2024-11-08 00:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:51][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.056494150310754776, acc: 0.9677419066429138)
[2024-11-08 00:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:52][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.0026660345029085875, acc: 1.0)
[2024-11-08 00:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:52][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.024732965975999832, acc: 1.0)
[2024-11-08 00:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:53][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.0081026591360569, acc: 1.0)
[2024-11-08 00:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:53][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.10276637226343155, acc: 0.9473684430122375)
[2024-11-08 00:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:53][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.020683644339442253, acc: 1.0)
[2024-11-08 00:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:54][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.013477688655257225, acc: 1.0)
[2024-11-08 00:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:54][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.07453770935535431, acc: 0.9523809552192688)
[2024-11-08 00:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:55][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.47450923919677734, acc: 0.875)
[2024-11-08 00:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:55][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.3023513853549957, acc: 0.8695651888847351)
[2024-11-08 00:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:55][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 0.18627746403217316, acc: 0.9523809552192688)
[2024-11-08 00:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:56][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 0.2368135154247284, acc: 0.8421052694320679)
[2024-11-08 00:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:56][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 0.10343862324953079, acc: 1.0)
[2024-11-08 00:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:56][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 0.23324021697044373, acc: 0.9090909361839294)
[2024-11-08 00:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:57][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 0.3916877806186676, acc: 0.8636363744735718)
[2024-11-08 00:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:57][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.012361775152385235, acc: 1.0)
[2024-11-08 00:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:58][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.06817925721406937, acc: 0.9629629850387573)
[2024-11-08 00:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:58][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.17483371496200562, acc: 0.9696969985961914)
[2024-11-08 00:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:58][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.2229725420475006, acc: 0.9545454382896423)
[2024-11-08 00:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:59][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.15720972418785095, acc: 0.9523809552192688)
[2024-11-08 00:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:59][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.35364049673080444, acc: 0.9473684430122375)
[2024-11-08 00:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:53:59][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.13042894005775452, acc: 0.9090909361839294)
[2024-11-08 00:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:00][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.04267309978604317, acc: 1.0)
[2024-11-08 00:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:00][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.12937702238559723, acc: 0.9090909361839294)
[2024-11-08 00:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:01][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.004005020949989557, acc: 1.0)
[2024-11-08 00:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:01][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.14518804848194122, acc: 0.9333333373069763)
[2024-11-08 00:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:01][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.16195662319660187, acc: 0.9428571462631226)
[2024-11-08 00:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:01][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.06170608103275299, acc: 1.0)
[2024-11-08 00:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:02][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.25395911931991577, acc: 0.9047619104385376)
[2024-11-08 00:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:02][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.161152184009552, acc: 0.9473684430122375)
[2024-11-08 00:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:02][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.24604901671409607, acc: 0.949999988079071)
[2024-11-08 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:03][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.5051099061965942, acc: 0.9047619104385376)
[2024-11-08 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:03][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.006781608331948519, acc: 1.0)
[2024-11-08 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:03][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.07749380171298981, acc: 0.9642857313156128)
[2024-11-08 00:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:04][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.005466182716190815, acc: 1.0)
[2024-11-08 00:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:04][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.011808606795966625, acc: 1.0)
[2024-11-08 00:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:04][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.06541436165571213, acc: 1.0)
[2024-11-08 00:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:05][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.30765804648399353, acc: 0.9047619104385376)
[2024-11-08 00:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:05][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.084368035197258, acc: 0.9473684430122375)
[2024-11-08 00:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:05][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.10217177122831345, acc: 0.9545454382896423)
[2024-11-08 00:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:06][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.12128052115440369, acc: 1.0)
[2024-11-08 00:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:06][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.08540502935647964, acc: 0.9545454382896423)
[2024-11-08 00:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:07][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.15343746542930603, acc: 0.9117646813392639)
[2024-11-08 00:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:07][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.037416208535432816, acc: 1.0)
[2024-11-08 00:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:07][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.03747984394431114, acc: 1.0)
[2024-11-08 00:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:08][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.2734220325946808, acc: 0.9473684430122375)
[2024-11-08 00:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:08][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.29518750309944153, acc: 0.9130434989929199)
[2024-11-08 00:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:09][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.27512118220329285, acc: 0.8095238208770752)
[2024-11-08 00:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:09][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.05737689509987831, acc: 1.0)
[2024-11-08 00:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:09][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.1865643560886383, acc: 0.9629629850387573)
[2024-11-08 00:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:10][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.11472095549106598, acc: 0.9615384340286255)
[2024-11-08 00:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:10][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.019708458334207535, acc: 1.0)
[2024-11-08 00:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:10][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.014459001831710339, acc: 1.0)
[2024-11-08 00:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:11][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.05489411950111389, acc: 1.0)
[2024-11-08 00:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:11][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.03705562651157379, acc: 1.0)
[2024-11-08 00:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:11][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.018240204080939293, acc: 1.0)
[2024-11-08 00:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:42][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2789, device='cuda:0') eval_epoch_loss=tensor(0.2460, device='cuda:0') eval_epoch_acc=tensor(0.9241, device='cuda:0')
[2024-11-08 00:54:42][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:54:42][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:54:42][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_6_step_276_loss_0.24603509902954102/model.pt
[2024-11-08 00:54:42][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:43][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.12171874940395355, acc: 0.9655172228813171)
[2024-11-08 00:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:43][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.025294968858361244, acc: 1.0)
[2024-11-08 00:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:43][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.020067205652594566, acc: 1.0)
[2024-11-08 00:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:44][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.035459235310554504, acc: 1.0)
[2024-11-08 00:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:44][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.07534794509410858, acc: 1.0)
[2024-11-08 00:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:44][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.05785262584686279, acc: 0.9545454382896423)
[2024-11-08 00:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:45][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.03310983628034592, acc: 1.0)
[2024-11-08 00:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:45][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.2816603183746338, acc: 0.931034505367279)
[2024-11-08 00:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:45][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.23342333734035492, acc: 0.8666666746139526)
[2024-11-08 00:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:46][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.2254030555486679, acc: 0.8620689511299133)
[2024-11-08 00:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:46][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.10020355880260468, acc: 0.9523809552192688)
[2024-11-08 00:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:46][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.2931908667087555, acc: 0.9473684430122375)
[2024-11-08 00:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:47][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.058388207107782364, acc: 0.9473684430122375)
[2024-11-08 00:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:47][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.30358466506004333, acc: 0.8636363744735718)
[2024-11-08 00:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:47][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.16804157197475433, acc: 0.9545454382896423)
[2024-11-08 00:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:48][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.007956120185554028, acc: 1.0)
[2024-11-08 00:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:48][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.05548615753650665, acc: 0.9666666388511658)
[2024-11-08 00:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:48][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.028835821896791458, acc: 1.0)
[2024-11-08 00:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:49][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.06431539356708527, acc: 0.949999988079071)
[2024-11-08 00:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:49][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.22483399510383606, acc: 0.9473684430122375)
[2024-11-08 00:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:50][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.31375497579574585, acc: 0.9090909361839294)
[2024-11-08 00:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:50][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.2741053104400635, acc: 0.9523809552192688)
[2024-11-08 00:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:50][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.0653560683131218, acc: 0.9583333134651184)
[2024-11-08 00:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:51][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.022321734577417374, acc: 1.0)
[2024-11-08 00:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:51][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.07061754912137985, acc: 0.9677419066429138)
[2024-11-08 00:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:51][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.014855519868433475, acc: 1.0)
[2024-11-08 00:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:52][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.2329953908920288, acc: 0.9523809552192688)
[2024-11-08 00:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:52][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.013391458429396152, acc: 1.0)
[2024-11-08 00:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:52][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.03933082893490791, acc: 1.0)
[2024-11-08 00:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:52][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.275218665599823, acc: 0.9130434989929199)
[2024-11-08 00:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:53][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.1519974023103714, acc: 0.9545454382896423)
[2024-11-08 00:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:53][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.008732423186302185, acc: 1.0)
[2024-11-08 00:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:53][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.08741801232099533, acc: 0.9523809552192688)
[2024-11-08 00:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:54][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.060312263667583466, acc: 1.0)
[2024-11-08 00:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:54][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.19622410833835602, acc: 0.9473684430122375)
[2024-11-08 00:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:54][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.026300327852368355, acc: 1.0)
[2024-11-08 00:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:55][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.054551687091588974, acc: 1.0)
[2024-11-08 00:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:55][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.018565675243735313, acc: 1.0)
[2024-11-08 00:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:55][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.039771731942892075, acc: 1.0)
[2024-11-08 00:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:56][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.08055730164051056, acc: 0.9714285731315613)
[2024-11-08 00:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:56][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.23147417604923248, acc: 0.9230769276618958)
[2024-11-08 00:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:56][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.014455754309892654, acc: 1.0)
[2024-11-08 00:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:57][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.015924740582704544, acc: 1.0)
[2024-11-08 00:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:57][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.014855588786303997, acc: 1.0)
[2024-11-08 00:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:57][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.048029575496912, acc: 1.0)
[2024-11-08 00:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:57][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.005824792664498091, acc: 1.0)
[2024-11-08 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:58][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.37035274505615234, acc: 0.8260869383811951)
[2024-11-08 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:58][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.34701839089393616, acc: 0.9047619104385376)
[2024-11-08 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:58][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.3451078534126282, acc: 0.8421052694320679)
[2024-11-08 00:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:59][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.18243326246738434, acc: 0.9090909361839294)
[2024-11-08 00:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:59][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.3229191303253174, acc: 0.8947368264198303)
[2024-11-08 00:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:54:59][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.46288469433784485, acc: 0.875)
[2024-11-08 00:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:00][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.1217874139547348, acc: 0.9655172228813171)
[2024-11-08 00:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:00][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.21272432804107666, acc: 0.9259259104728699)
[2024-11-08 00:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:00][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.1324412077665329, acc: 0.9047619104385376)
[2024-11-08 00:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:01][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.01327433716505766, acc: 1.0)
[2024-11-08 00:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:01][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.10184505581855774, acc: 1.0)
[2024-11-08 00:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:01][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.032438311725854874, acc: 1.0)
[2024-11-08 00:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:02][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.14891555905342102, acc: 0.9545454382896423)
[2024-11-08 00:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:02][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.07223866134881973, acc: 0.9615384340286255)
[2024-11-08 00:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:02][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.17850899696350098, acc: 0.9583333134651184)
[2024-11-08 00:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:03][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.20755569636821747, acc: 0.9047619104385376)
[2024-11-08 00:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:03][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.18055777251720428, acc: 0.8695651888847351)
[2024-11-08 00:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:03][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.17365318536758423, acc: 0.9545454382896423)
[2024-11-08 00:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:04][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.10659042745828629, acc: 0.9333333373069763)
[2024-11-08 00:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:04][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.030814560130238533, acc: 1.0)
[2024-11-08 00:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:04][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.03620187193155289, acc: 1.0)
[2024-11-08 00:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:05][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.1331121325492859, acc: 0.949999988079071)
[2024-11-08 00:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:05][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.019978903234004974, acc: 1.0)
[2024-11-08 00:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:05][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.1659500002861023, acc: 0.9545454382896423)
[2024-11-08 00:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:06][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.26118117570877075, acc: 0.8500000238418579)
[2024-11-08 00:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:06][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.054159875959157944, acc: 0.9583333134651184)
[2024-11-08 00:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:06][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.16520379483699799, acc: 0.95652174949646)
[2024-11-08 00:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:07][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.02080785110592842, acc: 1.0)
[2024-11-08 00:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:07][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.37332719564437866, acc: 0.8947368264198303)
[2024-11-08 00:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:07][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.12180092185735703, acc: 0.9090909361839294)
[2024-11-08 00:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:08][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.11025053262710571, acc: 1.0)
[2024-11-08 00:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:08][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.08585121482610703, acc: 0.9599999785423279)
[2024-11-08 00:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:09][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.12628360092639923, acc: 0.9599999785423279)
[2024-11-08 00:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:09][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.13725368678569794, acc: 1.0)
[2024-11-08 00:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:09][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.21566852927207947, acc: 0.8999999761581421)
[2024-11-08 00:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:10][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.2454604208469391, acc: 0.9047619104385376)
[2024-11-08 00:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:10][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.06077129766345024, acc: 1.0)
[2024-11-08 00:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:11][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.09958308190107346, acc: 0.9642857313156128)
[2024-11-08 00:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:11][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.03664228692650795, acc: 1.0)
[2024-11-08 00:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:11][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.0468023419380188, acc: 1.0)
[2024-11-08 00:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:12][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.04744459316134453, acc: 1.0)
[2024-11-08 00:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:12][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.01619965024292469, acc: 1.0)
[2024-11-08 00:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:12][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.1479668766260147, acc: 0.9523809552192688)
[2024-11-08 00:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:12][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.02893359400331974, acc: 1.0)
[2024-11-08 00:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:13][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.04313952848315239, acc: 1.0)
[2024-11-08 00:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:13][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.009369083680212498, acc: 1.0)
[2024-11-08 00:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:13][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.08733488619327545, acc: 0.9375)
[2024-11-08 00:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:14][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.05019301921129227, acc: 0.9523809552192688)
[2024-11-08 00:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:14][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.22601363062858582, acc: 0.9523809552192688)
[2024-11-08 00:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:15][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.22344925999641418, acc: 0.8947368264198303)
[2024-11-08 00:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:15][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.14655178785324097, acc: 0.9545454382896423)
[2024-11-08 00:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:16][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.01034472044557333, acc: 1.0)
[2024-11-08 00:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:16][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.01468602940440178, acc: 1.0)
[2024-11-08 00:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:16][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 0.015350007452070713, acc: 1.0)
[2024-11-08 00:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:17][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.1718207150697708, acc: 0.9677419066429138)
[2024-11-08 00:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:17][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.2710515260696411, acc: 0.9047619104385376)
[2024-11-08 00:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:17][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.05873558670282364, acc: 0.9523809552192688)
[2024-11-08 00:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:18][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.12892307341098785, acc: 0.9473684430122375)
[2024-11-08 00:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:18][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.04074894264340401, acc: 1.0)
[2024-11-08 00:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:19][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.1348770707845688, acc: 0.9473684430122375)
[2024-11-08 00:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:20][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.035345032811164856, acc: 1.0)
[2024-11-08 00:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:20][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.22204765677452087, acc: 0.9642857313156128)
[2024-11-08 00:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:20][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.03328492492437363, acc: 1.0)
[2024-11-08 00:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:21][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.13964691758155823, acc: 0.9428571462631226)
[2024-11-08 00:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:21][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.009946702048182487, acc: 1.0)
[2024-11-08 00:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:21][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.003867081133648753, acc: 1.0)
[2024-11-08 00:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:21][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.014481708407402039, acc: 1.0)
[2024-11-08 00:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:22][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.10419325530529022, acc: 0.949999988079071)
[2024-11-08 00:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:22][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.2210036814212799, acc: 0.9545454382896423)
[2024-11-08 00:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:22][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.06504932790994644, acc: 0.9523809552192688)
[2024-11-08 00:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:23][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.15312957763671875, acc: 0.9473684430122375)
[2024-11-08 00:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:23][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.0654444545507431, acc: 0.9473684430122375)
[2024-11-08 00:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:23][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.12878860533237457, acc: 0.9090909361839294)
[2024-11-08 00:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:24][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.11109031736850739, acc: 0.9523809552192688)
[2024-11-08 00:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:24][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.17467321455478668, acc: 0.9655172228813171)
[2024-11-08 00:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:24][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.2664118707180023, acc: 0.9333333373069763)
[2024-11-08 00:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:25][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.0730598196387291, acc: 0.9523809552192688)
[2024-11-08 00:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:25][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.0946943536400795, acc: 0.9444444179534912)
[2024-11-08 00:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:26][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.08882975578308105, acc: 0.9545454382896423)
[2024-11-08 00:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:26][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.09064621478319168, acc: 0.9523809552192688)
[2024-11-08 00:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:26][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.0082361726090312, acc: 1.0)
[2024-11-08 00:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:27][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.09162820875644684, acc: 0.96875)
[2024-11-08 00:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:27][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.08097628504037857, acc: 0.9714285731315613)
[2024-11-08 00:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:27][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.02425822801887989, acc: 1.0)
[2024-11-08 00:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:27][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.005966941360384226, acc: 1.0)
[2024-11-08 00:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:28][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.16276918351650238, acc: 0.9473684430122375)
[2024-11-08 00:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:28][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.014581440016627312, acc: 1.0)
[2024-11-08 00:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:28][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.019403692334890366, acc: 1.0)
[2024-11-08 00:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:29][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.004258896689862013, acc: 1.0)
[2024-11-08 00:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:29][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.0810450091958046, acc: 0.9642857313156128)
[2024-11-08 00:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:29][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.012706566601991653, acc: 1.0)
[2024-11-08 00:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:30][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.015757329761981964, acc: 1.0)
[2024-11-08 00:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:30][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.02362925000488758, acc: 1.0)
[2024-11-08 00:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:30][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.005146545358002186, acc: 1.0)
[2024-11-08 00:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:30][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.020910872146487236, acc: 1.0)
[2024-11-08 00:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:31][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.026458868756890297, acc: 1.0)
[2024-11-08 00:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:31][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.03189034387469292, acc: 1.0)
[2024-11-08 00:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:01][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2860, device='cuda:0') eval_epoch_loss=tensor(0.2515, device='cuda:0') eval_epoch_acc=tensor(0.9321, device='cuda:0')
[2024-11-08 00:56:01][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:56:01][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:56:01][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_6_step_419_loss_0.2515455484390259/model.pt
[2024-11-08 00:56:01][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:56:01][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 6 is 0.9320707321166992
[2024-11-08 00:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:02][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.08410574495792389, acc: 0.9523809552192688)
[2024-11-08 00:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:02][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.014693029224872589, acc: 1.0)
[2024-11-08 00:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:02][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.040949463844299316, acc: 0.9677419066429138)
[2024-11-08 00:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:03][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.028703005984425545, acc: 1.0)
[2024-11-08 00:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:03][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.05892423912882805, acc: 0.9615384340286255)
[2024-11-08 00:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:03][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.010076301172375679, acc: 1.0)
[2024-11-08 00:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:03][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.045608557760715485, acc: 1.0)
[2024-11-08 00:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:04][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.015738967806100845, acc: 1.0)
[2024-11-08 00:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:04][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.016288913786411285, acc: 1.0)
[2024-11-08 00:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:04][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.0017213865648955107, acc: 1.0)
[2024-11-08 00:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:05][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.03742264583706856, acc: 1.0)
[2024-11-08 00:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:05][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.0003409699129406363, acc: 1.0)
[2024-11-08 00:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:05][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.0031429727096110582, acc: 1.0)
[2024-11-08 00:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:06][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.0018087095813825727, acc: 1.0)
[2024-11-08 00:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:06][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.001566569204442203, acc: 1.0)
[2024-11-08 00:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:07][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.012030107900500298, acc: 1.0)
[2024-11-08 00:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:07][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.0018712949240580201, acc: 1.0)
[2024-11-08 00:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:07][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.006722325459122658, acc: 1.0)
[2024-11-08 00:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:08][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.05086837336421013, acc: 0.9523809552192688)
[2024-11-08 00:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:08][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.07808113843202591, acc: 0.9545454382896423)
[2024-11-08 00:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:08][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.2293088436126709, acc: 0.9285714030265808)
[2024-11-08 00:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:09][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.022883957251906395, acc: 1.0)
[2024-11-08 00:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:09][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.2011290341615677, acc: 0.9473684430122375)
[2024-11-08 00:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:10][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.2339460402727127, acc: 0.8421052694320679)
[2024-11-08 00:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:10][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.03492039814591408, acc: 1.0)
[2024-11-08 00:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:11][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.26670774817466736, acc: 0.9523809552192688)
[2024-11-08 00:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:11][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.004624973516911268, acc: 1.0)
[2024-11-08 00:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:12][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.13433493673801422, acc: 0.9677419066429138)
[2024-11-08 00:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:12][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.06771289557218552, acc: 0.9677419066429138)
[2024-11-08 00:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:12][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.00643764715641737, acc: 1.0)
[2024-11-08 00:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:12][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.04295143485069275, acc: 1.0)
[2024-11-08 00:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:13][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.03327912837266922, acc: 1.0)
[2024-11-08 00:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:13][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.12739495933055878, acc: 0.9473684430122375)
[2024-11-08 00:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:13][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.12383251637220383, acc: 0.9090909361839294)
[2024-11-08 00:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:14][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.16628149151802063, acc: 0.8999999761581421)
[2024-11-08 00:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:14][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.21833796799182892, acc: 0.9545454382896423)
[2024-11-08 00:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:14][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.07181666046380997, acc: 0.9354838728904724)
[2024-11-08 00:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:15][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.054113179445266724, acc: 1.0)
[2024-11-08 00:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:15][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.04902157932519913, acc: 1.0)
[2024-11-08 00:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:15][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.13787154853343964, acc: 0.9473684430122375)
[2024-11-08 00:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:16][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.20663899183273315, acc: 0.9545454382896423)
[2024-11-08 00:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:16][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.10407711565494537, acc: 0.949999988079071)
[2024-11-08 00:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:16][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.06714095920324326, acc: 0.9545454382896423)
[2024-11-08 00:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:17][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.01358833909034729, acc: 1.0)
[2024-11-08 00:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:17][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.07856282591819763, acc: 0.9629629850387573)
[2024-11-08 00:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:17][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.09315266460180283, acc: 0.9696969985961914)
[2024-11-08 00:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:18][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.03496561571955681, acc: 1.0)
[2024-11-08 00:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:18][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.12531286478042603, acc: 0.949999988079071)
[2024-11-08 00:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:18][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.10894898325204849, acc: 1.0)
[2024-11-08 00:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:18][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.039632417261600494, acc: 1.0)
[2024-11-08 00:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:19][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.06748821586370468, acc: 0.9523809552192688)
[2024-11-08 00:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:19][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.02500973828136921, acc: 1.0)
[2024-11-08 00:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:19][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.02777034044265747, acc: 1.0)
[2024-11-08 00:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:20][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.13619905710220337, acc: 0.9583333134651184)
[2024-11-08 00:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:20][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.08684252202510834, acc: 0.9523809552192688)
[2024-11-08 00:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:20][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.07857627421617508, acc: 0.9473684430122375)
[2024-11-08 00:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:21][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.05590357258915901, acc: 1.0)
[2024-11-08 00:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:21][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.19705292582511902, acc: 0.9523809552192688)
[2024-11-08 00:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:21][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.3750886619091034, acc: 0.9090909361839294)
[2024-11-08 00:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:22][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.08710943162441254, acc: 0.9642857313156128)
[2024-11-08 00:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:22][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.04560551419854164, acc: 1.0)
[2024-11-08 00:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:22][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.033740922808647156, acc: 1.0)
[2024-11-08 00:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:23][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.039573222398757935, acc: 1.0)
[2024-11-08 00:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:23][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.27265259623527527, acc: 0.8421052694320679)
[2024-11-08 00:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:24][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.046155642718076706, acc: 1.0)
[2024-11-08 00:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:24][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.04599662870168686, acc: 1.0)
[2024-11-08 00:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:24][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.03734179586172104, acc: 1.0)
[2024-11-08 00:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:25][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.033553630113601685, acc: 1.0)
[2024-11-08 00:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:25][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.03644672408699989, acc: 1.0)
[2024-11-08 00:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:25][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.2267618477344513, acc: 0.9230769276618958)
[2024-11-08 00:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:26][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.17092813551425934, acc: 0.9047619104385376)
[2024-11-08 00:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:26][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.05262131243944168, acc: 0.9473684430122375)
[2024-11-08 00:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:26][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.03016771748661995, acc: 1.0)
[2024-11-08 00:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:26][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.03393051400780678, acc: 1.0)
[2024-11-08 00:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:27][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.11653833091259003, acc: 0.9523809552192688)
[2024-11-08 00:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:27][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.05844796076416969, acc: 1.0)
[2024-11-08 00:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:28][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.17157724499702454, acc: 0.9285714030265808)
[2024-11-08 00:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:28][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.03783491253852844, acc: 1.0)
[2024-11-08 00:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:28][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.1268477886915207, acc: 0.949999988079071)
[2024-11-08 00:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:29][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.21193541586399078, acc: 0.9473684430122375)
[2024-11-08 00:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:29][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.35345229506492615, acc: 0.9090909361839294)
[2024-11-08 00:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:29][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.10957743227481842, acc: 0.949999988079071)
[2024-11-08 00:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:29][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.014108166098594666, acc: 1.0)
[2024-11-08 00:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:30][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.002085332293063402, acc: 1.0)
[2024-11-08 00:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:30][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.05906471237540245, acc: 1.0)
[2024-11-08 00:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:30][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.008790718391537666, acc: 1.0)
[2024-11-08 00:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:31][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.09751445800065994, acc: 0.9523809552192688)
[2024-11-08 00:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:31][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.47987595200538635, acc: 0.8421052694320679)
[2024-11-08 00:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:32][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 0.016602084040641785, acc: 1.0)
[2024-11-08 00:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:32][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.3573222756385803, acc: 0.8999999761581421)
[2024-11-08 00:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:32][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.051242899149656296, acc: 1.0)
[2024-11-08 00:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:33][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.028192393481731415, acc: 0.9696969985961914)
[2024-11-08 00:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:33][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.031076280400156975, acc: 1.0)
[2024-11-08 00:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:35][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.07305755466222763, acc: 0.9642857313156128)
[2024-11-08 00:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:36][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.2803630530834198, acc: 0.9047619104385376)
[2024-11-08 00:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:36][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.20285089313983917, acc: 0.9473684430122375)
[2024-11-08 00:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:37][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.03893943876028061, acc: 1.0)
[2024-11-08 00:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:37][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.03814864158630371, acc: 1.0)
[2024-11-08 00:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:38][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.028216155245900154, acc: 1.0)
[2024-11-08 00:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:38][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.16398513317108154, acc: 0.9285714030265808)
[2024-11-08 00:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:38][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.018359269946813583, acc: 1.0)
[2024-11-08 00:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:39][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.024801332503557205, acc: 1.0)
[2024-11-08 00:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:40][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.035336412489414215, acc: 1.0)
[2024-11-08 00:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:40][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.0629950612783432, acc: 1.0)
[2024-11-08 00:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:40][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.05710464343428612, acc: 1.0)
[2024-11-08 00:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:41][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.1887509822845459, acc: 0.9545454382896423)
[2024-11-08 00:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:41][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.20278668403625488, acc: 0.8999999761581421)
[2024-11-08 00:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:41][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.20356394350528717, acc: 0.9090909361839294)
[2024-11-08 00:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:42][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.11840372532606125, acc: 0.9696969985961914)
[2024-11-08 00:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:42][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.047077056020498276, acc: 1.0)
[2024-11-08 00:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:42][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.010925800539553165, acc: 1.0)
[2024-11-08 00:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:43][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.2270565778017044, acc: 0.8947368264198303)
[2024-11-08 00:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:43][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.1954643428325653, acc: 0.949999988079071)
[2024-11-08 00:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:43][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.15992304682731628, acc: 0.9523809552192688)
[2024-11-08 00:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:44][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.013279261998832226, acc: 1.0)
[2024-11-08 00:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:44][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.11598356068134308, acc: 0.9642857313156128)
[2024-11-08 00:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:44][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.040699902921915054, acc: 1.0)
[2024-11-08 00:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:45][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.18393586575984955, acc: 0.9200000166893005)
[2024-11-08 00:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:45][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.16905882954597473, acc: 0.9523809552192688)
[2024-11-08 00:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:45][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.09219993650913239, acc: 1.0)
[2024-11-08 00:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:46][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.050479162484407425, acc: 1.0)
[2024-11-08 00:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:46][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.1838647574186325, acc: 0.9523809552192688)
[2024-11-08 00:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:46][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.004316489677876234, acc: 1.0)
[2024-11-08 00:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:47][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.004699351731687784, acc: 1.0)
[2024-11-08 00:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:47][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.0017318801255896688, acc: 1.0)
[2024-11-08 00:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:47][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.022342130541801453, acc: 1.0)
[2024-11-08 00:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:48][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.003226562635973096, acc: 1.0)
[2024-11-08 00:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:48][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.017745893448591232, acc: 1.0)
[2024-11-08 00:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:48][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.025713086128234863, acc: 1.0)
[2024-11-08 00:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:48][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.004349032882601023, acc: 1.0)
[2024-11-08 00:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:49][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.003180239349603653, acc: 1.0)
[2024-11-08 00:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:49][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.008236265741288662, acc: 1.0)
[2024-11-08 00:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:49][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.031081195920705795, acc: 0.9677419066429138)
[2024-11-08 00:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:50][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.17499825358390808, acc: 0.9629629850387573)
[2024-11-08 00:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:50][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.030482837930321693, acc: 1.0)
[2024-11-08 00:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:50][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.046608008444309235, acc: 0.9473684430122375)
[2024-11-08 00:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:51][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.076346255838871, acc: 0.949999988079071)
[2024-11-08 00:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:51][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.0010105186374858022, acc: 1.0)
[2024-11-08 00:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:51][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.00401152390986681, acc: 1.0)
[2024-11-08 00:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:51][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.010984694585204124, acc: 1.0)
[2024-11-08 00:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:52][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.06854551285505295, acc: 0.9629629850387573)
[2024-11-08 00:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:52][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.004698384553194046, acc: 1.0)
[2024-11-08 00:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:52][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.0689745545387268, acc: 0.9523809552192688)
[2024-11-08 00:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:22][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3894, device='cuda:0') eval_epoch_loss=tensor(0.3288, device='cuda:0') eval_epoch_acc=tensor(0.9225, device='cuda:0')
[2024-11-08 00:57:22][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:57:22][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:57:23][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_6_step_562_loss_0.32884272933006287/model.pt
[2024-11-08 00:57:23][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:23][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.010641122236847878, acc: 1.0)
[2024-11-08 00:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:24][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.0521768257021904, acc: 0.9444444179534912)
[2024-11-08 00:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:24][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.0646171122789383, acc: 0.9545454382896423)
[2024-11-08 00:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:24][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.1971578299999237, acc: 0.949999988079071)
[2024-11-08 00:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:25][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.12536577880382538, acc: 0.9545454382896423)
[2024-11-08 00:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:25][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.004831713158637285, acc: 1.0)
[2024-11-08 00:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:25][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.0018771353643387556, acc: 1.0)
[2024-11-08 00:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:26][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.010262279771268368, acc: 1.0)
[2024-11-08 00:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:26][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.004909423179924488, acc: 1.0)
[2024-11-08 00:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:26][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.11101541668176651, acc: 0.9473684430122375)
[2024-11-08 00:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:26][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.17023253440856934, acc: 0.9545454382896423)
[2024-11-08 00:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:27][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.06589619815349579, acc: 1.0)
[2024-11-08 00:57:27][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.1165, train_epoch_loss=0.1102, epoch time 335.73124817945063s
[2024-11-08 00:57:27][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 00:57:27][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 00:57:27][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 00:57:27][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 00:57:27][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 00:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:28][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.005050050560384989, acc: 1.0)
[2024-11-08 00:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:29][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.0031859984155744314, acc: 1.0)
[2024-11-08 00:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:29][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.01197416614741087, acc: 1.0)
[2024-11-08 00:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:29][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.00838298350572586, acc: 1.0)
[2024-11-08 00:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:30][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.02912185899913311, acc: 1.0)
[2024-11-08 00:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:30][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.10134575515985489, acc: 0.9473684430122375)
[2024-11-08 00:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:30][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.009980984032154083, acc: 1.0)
[2024-11-08 00:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:31][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.6557823419570923, acc: 0.9047619104385376)
[2024-11-08 00:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:31][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.007112027611583471, acc: 1.0)
[2024-11-08 00:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:31][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.28829288482666016, acc: 0.9354838728904724)
[2024-11-08 00:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:32][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.004693145398050547, acc: 1.0)
[2024-11-08 00:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:32][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.007362775970250368, acc: 1.0)
[2024-11-08 00:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:32][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.012921737506985664, acc: 1.0)
[2024-11-08 00:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:33][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.07491233199834824, acc: 0.9473684430122375)
[2024-11-08 00:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:33][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.17032529413700104, acc: 0.95652174949646)
[2024-11-08 00:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:33][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.005670796148478985, acc: 1.0)
[2024-11-08 00:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:34][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.13544303178787231, acc: 0.9642857313156128)
[2024-11-08 00:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:34][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.04403740540146828, acc: 1.0)
[2024-11-08 00:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:34][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.06101640686392784, acc: 0.9545454382896423)
[2024-11-08 00:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:34][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.008834866806864738, acc: 1.0)
[2024-11-08 00:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:35][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.06751476228237152, acc: 0.9473684430122375)
[2024-11-08 00:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:35][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.2013377696275711, acc: 0.9545454382896423)
[2024-11-08 00:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:35][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.06830836087465286, acc: 1.0)
[2024-11-08 00:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:36][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.0035437732003629208, acc: 1.0)
[2024-11-08 00:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:36][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.004397308453917503, acc: 1.0)
[2024-11-08 00:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:36][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.05786774680018425, acc: 1.0)
[2024-11-08 00:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:37][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.20172354578971863, acc: 0.8571428656578064)
[2024-11-08 00:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:38][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.10117275267839432, acc: 0.9473684430122375)
[2024-11-08 00:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:38][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.062225356698036194, acc: 0.9545454382896423)
[2024-11-08 00:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:38][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.16682229936122894, acc: 0.8999999761581421)
[2024-11-08 00:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:39][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.07734350860118866, acc: 0.9545454382896423)
[2024-11-08 00:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:39][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.007006635423749685, acc: 1.0)
[2024-11-08 00:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:39][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.0694030225276947, acc: 0.9696969985961914)
[2024-11-08 00:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:40][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.06473001092672348, acc: 0.95652174949646)
[2024-11-08 00:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:40][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.004614682402461767, acc: 1.0)
[2024-11-08 00:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:40][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.033980146050453186, acc: 1.0)
[2024-11-08 00:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:41][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.1473040133714676, acc: 0.949999988079071)
[2024-11-08 00:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:41][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.013361983001232147, acc: 1.0)
[2024-11-08 00:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:41][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.002048912923783064, acc: 1.0)
[2024-11-08 00:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:41][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.056244201958179474, acc: 1.0)
[2024-11-08 00:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:42][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.23123304545879364, acc: 0.9142857193946838)
[2024-11-08 00:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:42][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.09626436233520508, acc: 0.949999988079071)
[2024-11-08 00:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:43][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.3645765781402588, acc: 0.8999999761581421)
[2024-11-08 00:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:43][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.13769659399986267, acc: 0.9473684430122375)
[2024-11-08 00:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:43][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.13608378171920776, acc: 0.9545454382896423)
[2024-11-08 00:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:44][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.029814008623361588, acc: 1.0)
[2024-11-08 00:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:44][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.045797135680913925, acc: 1.0)
[2024-11-08 00:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:44][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.029141761362552643, acc: 1.0)
[2024-11-08 00:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:45][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.04821103811264038, acc: 0.9714285731315613)
[2024-11-08 00:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:45][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.005564265884459019, acc: 1.0)
[2024-11-08 00:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:45][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.07290304452180862, acc: 0.9523809552192688)
[2024-11-08 00:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:46][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.07207614928483963, acc: 1.0)
[2024-11-08 00:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:46][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.022618871182203293, acc: 1.0)
[2024-11-08 00:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:46][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 0.12119884788990021, acc: 0.9523809552192688)
[2024-11-08 00:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:47][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.2773570418357849, acc: 0.9090909361839294)
[2024-11-08 00:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:47][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.060243021696805954, acc: 0.9642857313156128)
[2024-11-08 00:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:49][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.24626827239990234, acc: 0.9047619104385376)
[2024-11-08 00:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:51][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 0.11501628905534744, acc: 0.9523809552192688)
[2024-11-08 00:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:51][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.011600269936025143, acc: 1.0)
[2024-11-08 00:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:52][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.06440725922584534, acc: 0.9545454382896423)
[2024-11-08 00:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:52][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.11511969566345215, acc: 0.9473684430122375)
[2024-11-08 00:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:53][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.08329030126333237, acc: 0.9523809552192688)
[2024-11-08 00:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:53][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.0064444225281476974, acc: 1.0)
[2024-11-08 00:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:53][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.17208874225616455, acc: 0.9642857313156128)
[2024-11-08 00:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:54][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.24817779660224915, acc: 0.9047619104385376)
[2024-11-08 00:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:54][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.036838602274656296, acc: 1.0)
[2024-11-08 00:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:54][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.16885757446289062, acc: 0.9473684430122375)
[2024-11-08 00:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:55][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.030459588393568993, acc: 1.0)
[2024-11-08 00:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:55][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.003428083611652255, acc: 1.0)
[2024-11-08 00:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:55][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.0012631500139832497, acc: 1.0)
[2024-11-08 00:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:56][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.033522456884384155, acc: 1.0)
[2024-11-08 00:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:56][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.04950980097055435, acc: 0.9666666388511658)
[2024-11-08 00:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:56][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.009655222296714783, acc: 1.0)
[2024-11-08 00:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:57][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 0.23829936981201172, acc: 0.9473684430122375)
[2024-11-08 00:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:57][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.60356205701828, acc: 0.949999988079071)
[2024-11-08 00:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:57][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.1410616636276245, acc: 0.9523809552192688)
[2024-11-08 00:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:58][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 0.05912195146083832, acc: 0.9545454382896423)
[2024-11-08 00:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:58][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.07977210730314255, acc: 0.9642857313156128)
[2024-11-08 00:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:58][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.013303319923579693, acc: 1.0)
[2024-11-08 00:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:59][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.029813598841428757, acc: 1.0)
[2024-11-08 00:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:59][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.008530172519385815, acc: 1.0)
[2024-11-08 00:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:57:59][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.07092425972223282, acc: 0.9523809552192688)
[2024-11-08 00:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:00][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.06821135431528091, acc: 0.9473684430122375)
[2024-11-08 00:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:00][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.018582535907626152, acc: 1.0)
[2024-11-08 00:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:00][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.13898801803588867, acc: 0.9545454382896423)
[2024-11-08 00:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:01][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.130791574716568, acc: 0.9523809552192688)
[2024-11-08 00:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:01][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.03823401406407356, acc: 1.0)
[2024-11-08 00:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:01][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.40171119570732117, acc: 0.8461538553237915)
[2024-11-08 00:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:02][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.03410390764474869, acc: 1.0)
[2024-11-08 00:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:03][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.12744396924972534, acc: 0.9473684430122375)
[2024-11-08 00:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:04][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.0621119849383831, acc: 1.0)
[2024-11-08 00:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:04][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.06719279289245605, acc: 0.9545454382896423)
[2024-11-08 00:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:05][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.10103002935647964, acc: 0.9523809552192688)
[2024-11-08 00:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:06][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.37886422872543335, acc: 0.8461538553237915)
[2024-11-08 00:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:06][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.3574027717113495, acc: 0.8928571343421936)
[2024-11-08 00:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:07][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.16683749854564667, acc: 0.9047619104385376)
[2024-11-08 00:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:07][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.20652280747890472, acc: 0.9473684430122375)
[2024-11-08 00:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:07][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.11778470128774643, acc: 0.949999988079071)
[2024-11-08 00:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:08][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.16145607829093933, acc: 0.9047619104385376)
[2024-11-08 00:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:08][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.1323358565568924, acc: 0.9090909361839294)
[2024-11-08 00:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:09][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.20415037870407104, acc: 0.9523809552192688)
[2024-11-08 00:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:09][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.021689048036932945, acc: 1.0)
[2024-11-08 00:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:09][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.011711550876498222, acc: 1.0)
[2024-11-08 00:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:10][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.0066941771656274796, acc: 1.0)
[2024-11-08 00:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:10][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.08201397955417633, acc: 0.9545454382896423)
[2024-11-08 00:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:10][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.18682913482189178, acc: 0.9473684430122375)
[2024-11-08 00:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:10][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.1201803907752037, acc: 0.9545454382896423)
[2024-11-08 00:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:11][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.0014921770198270679, acc: 1.0)
[2024-11-08 00:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:11][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.023241344839334488, acc: 1.0)
[2024-11-08 00:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:11][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.0968041867017746, acc: 0.9523809552192688)
[2024-11-08 00:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:12][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.10134874284267426, acc: 0.9523809552192688)
[2024-11-08 00:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:12][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.1599189043045044, acc: 0.8947368264198303)
[2024-11-08 00:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:12][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.052550628781318665, acc: 1.0)
[2024-11-08 00:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:13][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.030364764854311943, acc: 1.0)
[2024-11-08 00:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:13][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.006914675701409578, acc: 1.0)
[2024-11-08 00:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:14][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.01812032051384449, acc: 1.0)
[2024-11-08 00:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:14][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.047990214079618454, acc: 1.0)
[2024-11-08 00:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:14][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.058695606887340546, acc: 1.0)
[2024-11-08 00:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:14][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.034119635820388794, acc: 1.0)
[2024-11-08 00:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:15][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.10594412684440613, acc: 0.9545454382896423)
[2024-11-08 00:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:16][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.09978117793798447, acc: 0.9473684430122375)
[2024-11-08 00:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:16][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.14096499979496002, acc: 0.9545454382896423)
[2024-11-08 00:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:16][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.003465755609795451, acc: 1.0)
[2024-11-08 00:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:17][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.20308245718479156, acc: 0.9230769276618958)
[2024-11-08 00:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:17][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.06367284059524536, acc: 0.949999988079071)
[2024-11-08 00:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:17][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.17597641050815582, acc: 0.8999999761581421)
[2024-11-08 00:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:18][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.25683021545410156, acc: 0.8947368264198303)
[2024-11-08 00:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:18][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.3022530674934387, acc: 0.8636363744735718)
[2024-11-08 00:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:18][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.14598265290260315, acc: 0.949999988079071)
[2024-11-08 00:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:19][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.001316805835813284, acc: 1.0)
[2024-11-08 00:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:19][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.19625124335289001, acc: 0.939393937587738)
[2024-11-08 00:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:48][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3321, device='cuda:0') eval_epoch_loss=tensor(0.2868, device='cuda:0') eval_epoch_acc=tensor(0.9324, device='cuda:0')
[2024-11-08 00:58:48][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 00:58:48][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 00:58:49][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_7_step_131_loss_0.28676384687423706/model.pt
[2024-11-08 00:58:49][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 00:58:49][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 7 is 0.9323854446411133
[2024-11-08 00:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:49][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.08516699820756912, acc: 0.9629629850387573)
[2024-11-08 00:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:49][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.12377288192510605, acc: 0.9696969985961914)
[2024-11-08 00:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:50][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.008158029988408089, acc: 1.0)
[2024-11-08 00:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:50][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.013861211948096752, acc: 1.0)
[2024-11-08 00:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:50][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.16726042330265045, acc: 0.9473684430122375)
[2024-11-08 00:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:51][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.09457406401634216, acc: 0.9545454382896423)
[2024-11-08 00:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:51][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.40001511573791504, acc: 0.8999999761581421)
[2024-11-08 00:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:51][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.059093177318573, acc: 0.9545454382896423)
[2024-11-08 00:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:52][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.05434878543019295, acc: 0.9696969985961914)
[2024-11-08 00:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:52][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.004468449391424656, acc: 1.0)
[2024-11-08 00:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:52][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.07360750436782837, acc: 0.9696969985961914)
[2024-11-08 00:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:53][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.2770841717720032, acc: 0.949999988079071)
[2024-11-08 00:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:53][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.09025199711322784, acc: 0.949999988079071)
[2024-11-08 00:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:53][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.025027457624673843, acc: 1.0)
[2024-11-08 00:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:54][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.011545170098543167, acc: 1.0)
[2024-11-08 00:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:54][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.0749601498246193, acc: 1.0)
[2024-11-08 00:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:55][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.16655690968036652, acc: 0.9090909361839294)
[2024-11-08 00:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:55][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.07950961589813232, acc: 0.9696969985961914)
[2024-11-08 00:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:55][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.023922638967633247, acc: 1.0)
[2024-11-08 00:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:56][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.06449064612388611, acc: 0.9677419066429138)
[2024-11-08 00:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:56][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.2220662534236908, acc: 0.8999999761581421)
[2024-11-08 00:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:56][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.03405009210109711, acc: 1.0)
[2024-11-08 00:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:57][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.188208669424057, acc: 0.9473684430122375)
[2024-11-08 00:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:57][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.0013216735096648335, acc: 1.0)
[2024-11-08 00:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:57][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.023651519790291786, acc: 1.0)
[2024-11-08 00:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:58][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.3012208640575409, acc: 0.8636363744735718)
[2024-11-08 00:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:58][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.3055068254470825, acc: 0.8787878751754761)
[2024-11-08 00:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:58:59][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.35028135776519775, acc: 0.8500000238418579)
[2024-11-08 00:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:00][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.12145121395587921, acc: 0.949999988079071)
[2024-11-08 00:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:00][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.17847119271755219, acc: 0.9473684430122375)
[2024-11-08 00:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:01][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.1124652549624443, acc: 1.0)
[2024-11-08 00:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:01][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.43966421484947205, acc: 0.8500000238418579)
[2024-11-08 00:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:02][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.20253972709178925, acc: 0.9200000166893005)
[2024-11-08 00:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:02][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.05355650931596756, acc: 1.0)
[2024-11-08 00:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:02][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.14294423162937164, acc: 0.9722222089767456)
[2024-11-08 00:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:02][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.01156347431242466, acc: 1.0)
[2024-11-08 00:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:03][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.1380065679550171, acc: 0.9047619104385376)
[2024-11-08 00:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:03][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.009595191106200218, acc: 1.0)
[2024-11-08 00:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:03][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.02093355357646942, acc: 1.0)
[2024-11-08 00:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:04][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.06147874519228935, acc: 0.949999988079071)
[2024-11-08 00:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:05][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.03144904598593712, acc: 1.0)
[2024-11-08 00:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:05][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.008269603364169598, acc: 1.0)
[2024-11-08 00:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:05][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.13832873106002808, acc: 0.9333333373069763)
[2024-11-08 00:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:06][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.16928760707378387, acc: 0.9523809552192688)
[2024-11-08 00:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:06][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.18491065502166748, acc: 0.9523809552192688)
[2024-11-08 00:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:07][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.015056992881000042, acc: 1.0)
[2024-11-08 00:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:07][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.2686421573162079, acc: 0.8636363744735718)
[2024-11-08 00:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:08][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.020067812874913216, acc: 1.0)
[2024-11-08 00:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:08][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.10935864597558975, acc: 0.9545454382896423)
[2024-11-08 00:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:09][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.002354307333007455, acc: 1.0)
[2024-11-08 00:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:09][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.002362519735470414, acc: 1.0)
[2024-11-08 00:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:09][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.18753476440906525, acc: 0.9428571462631226)
[2024-11-08 00:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:10][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.013078607618808746, acc: 1.0)
[2024-11-08 00:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:10][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.011638473719358444, acc: 1.0)
[2024-11-08 00:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:10][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.07844000309705734, acc: 0.9473684430122375)
[2024-11-08 00:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:11][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.06284977495670319, acc: 0.949999988079071)
[2024-11-08 00:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:11][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.01574144884943962, acc: 1.0)
[2024-11-08 00:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:12][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.0063393013551831245, acc: 1.0)
[2024-11-08 00:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:12][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.3131358027458191, acc: 0.8799999952316284)
[2024-11-08 00:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:13][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.2725299298763275, acc: 0.8500000238418579)
[2024-11-08 00:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:13][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.38486313819885254, acc: 0.9047619104385376)
[2024-11-08 00:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:14][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.2269429713487625, acc: 0.8947368264198303)
[2024-11-08 00:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:15][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.240984246134758, acc: 0.9090909361839294)
[2024-11-08 00:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:16][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.21959009766578674, acc: 0.9523809552192688)
[2024-11-08 00:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:17][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.039502453058958054, acc: 1.0)
[2024-11-08 00:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:17][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.003959164023399353, acc: 1.0)
[2024-11-08 00:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:18][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.342284232378006, acc: 0.8999999761581421)
[2024-11-08 00:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:18][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.014847828075289726, acc: 1.0)
[2024-11-08 00:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:18][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.04219549894332886, acc: 1.0)
[2024-11-08 00:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:19][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.10008404403924942, acc: 0.9473684430122375)
[2024-11-08 00:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:19][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.16067856550216675, acc: 0.9545454382896423)
[2024-11-08 00:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:19][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.16622282564640045, acc: 0.949999988079071)
[2024-11-08 00:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:20][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.049549199640750885, acc: 1.0)
[2024-11-08 00:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:20][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.008323146030306816, acc: 1.0)
[2024-11-08 00:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:20][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.02094339020550251, acc: 1.0)
[2024-11-08 00:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:21][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.03649267554283142, acc: 1.0)
[2024-11-08 00:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:21][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.06076124310493469, acc: 0.9473684430122375)
[2024-11-08 00:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:21][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.07211413234472275, acc: 0.9545454382896423)
[2024-11-08 00:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:22][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.016122952103614807, acc: 1.0)
[2024-11-08 00:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:22][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.031204843893647194, acc: 1.0)
[2024-11-08 00:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:22][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.19854038953781128, acc: 0.9677419066429138)
[2024-11-08 00:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:23][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.04885305464267731, acc: 0.9677419066429138)
[2024-11-08 00:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:23][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.11928711831569672, acc: 0.9615384340286255)
[2024-11-08 00:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:24][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.021150115877389908, acc: 1.0)
[2024-11-08 00:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:24][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.0258213821798563, acc: 1.0)
[2024-11-08 00:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:24][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.017495587468147278, acc: 1.0)
[2024-11-08 00:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:25][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.0023278486914932728, acc: 1.0)
[2024-11-08 00:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:25][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.02722698450088501, acc: 1.0)
[2024-11-08 00:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:25][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.07575216889381409, acc: 0.9583333134651184)
[2024-11-08 00:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:26][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.011908773332834244, acc: 1.0)
[2024-11-08 00:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:26][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.0021300376392900944, acc: 1.0)
[2024-11-08 00:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:26][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.014742187224328518, acc: 1.0)
[2024-11-08 00:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:27][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.004561905283480883, acc: 1.0)
[2024-11-08 00:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:28][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.052163977175951004, acc: 1.0)
[2024-11-08 00:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:28][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.0032409292180091143, acc: 1.0)
[2024-11-08 00:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:28][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.015894444659352303, acc: 1.0)
[2024-11-08 00:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:29][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.1227266937494278, acc: 0.9523809552192688)
[2024-11-08 00:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:29][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.1563749611377716, acc: 0.9583333134651184)
[2024-11-08 00:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:29][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.21412372589111328, acc: 0.9130434989929199)
[2024-11-08 00:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:30][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 0.13056066632270813, acc: 0.9523809552192688)
[2024-11-08 00:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:30][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.039412323385477066, acc: 1.0)
[2024-11-08 00:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:30][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 0.15807251632213593, acc: 0.9473684430122375)
[2024-11-08 00:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:31][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 0.1235964298248291, acc: 0.9545454382896423)
[2024-11-08 00:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:31][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.36823606491088867, acc: 0.8181818127632141)
[2024-11-08 00:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:32][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.17037877440452576, acc: 0.9642857313156128)
[2024-11-08 00:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:32][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.09015637636184692, acc: 0.9629629850387573)
[2024-11-08 00:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:32][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.32599738240242004, acc: 0.9090909361839294)
[2024-11-08 00:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:32][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.08258995413780212, acc: 0.9090909361839294)
[2024-11-08 00:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:33][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.06674560904502869, acc: 1.0)
[2024-11-08 00:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:33][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.0772293359041214, acc: 1.0)
[2024-11-08 00:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:33][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.006209431681782007, acc: 1.0)
[2024-11-08 00:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:34][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.06154291331768036, acc: 1.0)
[2024-11-08 00:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:34][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.11053664982318878, acc: 0.9545454382896423)
[2024-11-08 00:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:35][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.0020136695820838213, acc: 1.0)
[2024-11-08 00:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:35][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.24506719410419464, acc: 0.9666666388511658)
[2024-11-08 00:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:35][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.11385856568813324, acc: 0.9428571462631226)
[2024-11-08 00:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:36][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.22372861206531525, acc: 0.95652174949646)
[2024-11-08 00:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:36][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.09958859533071518, acc: 0.9523809552192688)
[2024-11-08 00:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:36][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.07552947849035263, acc: 0.9473684430122375)
[2024-11-08 00:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:37][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.051335327327251434, acc: 1.0)
[2024-11-08 00:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:37][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.16910907626152039, acc: 0.9047619104385376)
[2024-11-08 00:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:37][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.007104289252310991, acc: 1.0)
[2024-11-08 00:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:37][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.08611704409122467, acc: 0.9642857313156128)
[2024-11-08 00:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:38][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.003091165330260992, acc: 1.0)
[2024-11-08 00:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:38][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.001718376763164997, acc: 1.0)
[2024-11-08 00:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:38][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.0015599291073158383, acc: 1.0)
[2024-11-08 00:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:39][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.0074864840134978294, acc: 1.0)
[2024-11-08 00:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:39][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.1327400654554367, acc: 0.9473684430122375)
[2024-11-08 00:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:40][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.009603074751794338, acc: 1.0)
[2024-11-08 00:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:40][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.002522119553759694, acc: 1.0)
[2024-11-08 00:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:40][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.1920350044965744, acc: 0.9545454382896423)
[2024-11-08 00:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:41][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.08430006355047226, acc: 0.9411764740943909)
[2024-11-08 00:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:41][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.06320080161094666, acc: 0.9545454382896423)
[2024-11-08 00:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:41][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.08358246088027954, acc: 0.949999988079071)
[2024-11-08 00:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:42][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.2786479890346527, acc: 0.9473684430122375)
[2024-11-08 00:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:42][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.23038272559642792, acc: 0.95652174949646)
[2024-11-08 00:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:43][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.04026380181312561, acc: 1.0)
[2024-11-08 00:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:43][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.05616109073162079, acc: 1.0)
[2024-11-08 00:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:44][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.11195042729377747, acc: 0.9629629850387573)
[2024-11-08 00:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:44][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.12789520621299744, acc: 0.9230769276618958)
[2024-11-08 00:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:44][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.06274136900901794, acc: 0.9523809552192688)
[2024-11-08 00:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:44][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.02122197113931179, acc: 1.0)
[2024-11-08 00:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:45][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.07335000485181808, acc: 0.9473684430122375)
[2024-11-08 00:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:14][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3639, device='cuda:0') eval_epoch_loss=tensor(0.3104, device='cuda:0') eval_epoch_acc=tensor(0.9193, device='cuda:0')
[2024-11-08 01:00:14][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:00:14][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:00:15][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_7_step_274_loss_0.3103821575641632/model.pt
[2024-11-08 01:00:15][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:15][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.12006206810474396, acc: 0.9545454382896423)
[2024-11-08 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:15][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.014295583590865135, acc: 1.0)
[2024-11-08 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:16][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.02717510424554348, acc: 1.0)
[2024-11-08 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:16][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.06151517853140831, acc: 1.0)
[2024-11-08 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:16][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.007003555539995432, acc: 1.0)
[2024-11-08 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:17][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.015040640719234943, acc: 1.0)
[2024-11-08 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:17][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.0338161438703537, acc: 1.0)
[2024-11-08 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:17][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.02906748838722706, acc: 1.0)
[2024-11-08 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:18][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.07799109071493149, acc: 0.9523809552192688)
[2024-11-08 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:18][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.44186919927597046, acc: 0.8620689511299133)
[2024-11-08 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:18][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.17118023335933685, acc: 0.8999999761581421)
[2024-11-08 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:18][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.06084483116865158, acc: 0.9655172228813171)
[2024-11-08 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:19][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.13269805908203125, acc: 0.9047619104385376)
[2024-11-08 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:19][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.4479687213897705, acc: 0.8421052694320679)
[2024-11-08 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:19][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.034149348735809326, acc: 1.0)
[2024-11-08 01:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:20][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.06995154917240143, acc: 1.0)
[2024-11-08 01:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:20][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.03490237891674042, acc: 1.0)
[2024-11-08 01:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:20][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.005561275873333216, acc: 1.0)
[2024-11-08 01:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:21][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.07638565450906754, acc: 0.9666666388511658)
[2024-11-08 01:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:21][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.17879383265972137, acc: 0.9523809552192688)
[2024-11-08 01:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:21][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.03245409578084946, acc: 1.0)
[2024-11-08 01:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:22][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.14604413509368896, acc: 0.9473684430122375)
[2024-11-08 01:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:22][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.19248098134994507, acc: 0.9090909361839294)
[2024-11-08 01:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:23][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.25863319635391235, acc: 0.9523809552192688)
[2024-11-08 01:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:23][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.04404110834002495, acc: 1.0)
[2024-11-08 01:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:23][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.00679053645581007, acc: 1.0)
[2024-11-08 01:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:23][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.03251821547746658, acc: 0.9677419066429138)
[2024-11-08 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:24][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.00957957562059164, acc: 1.0)
[2024-11-08 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:24][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.21631434559822083, acc: 0.9523809552192688)
[2024-11-08 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:24][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.011836635880172253, acc: 1.0)
[2024-11-08 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:25][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.10356034338474274, acc: 0.8947368264198303)
[2024-11-08 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:25][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.12316838651895523, acc: 0.95652174949646)
[2024-11-08 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:25][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.06465775519609451, acc: 0.9545454382896423)
[2024-11-08 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:26][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.06138467416167259, acc: 0.9615384340286255)
[2024-11-08 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:26][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.14538976550102234, acc: 0.9047619104385376)
[2024-11-08 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:27][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.017869550734758377, acc: 1.0)
[2024-11-08 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:27][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.4773294925689697, acc: 0.9473684430122375)
[2024-11-08 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:27][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.2803823947906494, acc: 0.9090909361839294)
[2024-11-08 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:27][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.1889657974243164, acc: 0.9090909361839294)
[2024-11-08 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:28][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.013134795241057873, acc: 1.0)
[2024-11-08 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:28][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.1172252669930458, acc: 0.9629629850387573)
[2024-11-08 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:28][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.06316474825143814, acc: 1.0)
[2024-11-08 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:29][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.1328447461128235, acc: 0.9615384340286255)
[2024-11-08 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:29][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.0027627137023955584, acc: 1.0)
[2024-11-08 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:30][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.074053093791008, acc: 0.949999988079071)
[2024-11-08 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:30][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.003760160878300667, acc: 1.0)
[2024-11-08 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:30][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.03195151686668396, acc: 1.0)
[2024-11-08 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:30][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.0025536215398460627, acc: 1.0)
[2024-11-08 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:31][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.379925400018692, acc: 0.8695651888847351)
[2024-11-08 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:31][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.41164955496788025, acc: 0.8571428656578064)
[2024-11-08 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:32][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.3288438320159912, acc: 0.8421052694320679)
[2024-11-08 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:32][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.3932786285877228, acc: 0.8636363744735718)
[2024-11-08 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:32][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.3468064069747925, acc: 0.8947368264198303)
[2024-11-08 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:33][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.1563471555709839, acc: 0.9166666865348816)
[2024-11-08 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:33][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.10236581414937973, acc: 0.931034505367279)
[2024-11-08 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:33][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.0693318098783493, acc: 0.9629629850387573)
[2024-11-08 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:34][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.2532498836517334, acc: 0.9523809552192688)
[2024-11-08 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:34][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.18486423790454865, acc: 0.9473684430122375)
[2024-11-08 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:34][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.05253889784216881, acc: 1.0)
[2024-11-08 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:35][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.08439037948846817, acc: 0.9473684430122375)
[2024-11-08 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:35][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.03553621843457222, acc: 1.0)
[2024-11-08 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:35][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.17921055853366852, acc: 0.8846153616905212)
[2024-11-08 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:36][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.2755683660507202, acc: 0.9166666865348816)
[2024-11-08 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:36][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.11661309003829956, acc: 0.9523809552192688)
[2024-11-08 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:36][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.21188530325889587, acc: 0.95652174949646)
[2024-11-08 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:37][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.11043810844421387, acc: 0.9090909361839294)
[2024-11-08 01:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:37][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.004956253804266453, acc: 1.0)
[2024-11-08 01:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:37][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.0029326037038117647, acc: 1.0)
[2024-11-08 01:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:38][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.009541725739836693, acc: 1.0)
[2024-11-08 01:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:38][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.21753481030464172, acc: 0.949999988079071)
[2024-11-08 01:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:38][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.009977300651371479, acc: 1.0)
[2024-11-08 01:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:38][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.09266722202301025, acc: 0.9545454382896423)
[2024-11-08 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:39][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.06672421842813492, acc: 1.0)
[2024-11-08 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:39][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.03802004083991051, acc: 0.9583333134651184)
[2024-11-08 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:39][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.042100343853235245, acc: 1.0)
[2024-11-08 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:40][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.03872016817331314, acc: 1.0)
[2024-11-08 01:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:40][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.09539787471294403, acc: 0.9473684430122375)
[2024-11-08 01:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:40][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.05147423967719078, acc: 0.9545454382896423)
[2024-11-08 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:41][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.11040128767490387, acc: 1.0)
[2024-11-08 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:41][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.14077137410640717, acc: 0.9200000166893005)
[2024-11-08 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:41][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.28036701679229736, acc: 0.8799999952316284)
[2024-11-08 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:42][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.15598861873149872, acc: 0.9523809552192688)
[2024-11-08 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:42][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.09481470286846161, acc: 0.949999988079071)
[2024-11-08 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:43][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.057280559092760086, acc: 0.9523809552192688)
[2024-11-08 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:43][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.07981770485639572, acc: 1.0)
[2024-11-08 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:43][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.011392099782824516, acc: 1.0)
[2024-11-08 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:44][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.013279411010444164, acc: 1.0)
[2024-11-08 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:44][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.04594695195555687, acc: 0.9714285731315613)
[2024-11-08 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:44][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.247179314494133, acc: 0.9545454382896423)
[2024-11-08 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:44][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.07847168296575546, acc: 0.9473684430122375)
[2024-11-08 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:45][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.06355053186416626, acc: 0.9523809552192688)
[2024-11-08 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:45][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.03459427133202553, acc: 1.0)
[2024-11-08 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:45][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.015844637528061867, acc: 1.0)
[2024-11-08 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:46][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.12327297776937485, acc: 0.9629629850387573)
[2024-11-08 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:46][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.07327727973461151, acc: 0.96875)
[2024-11-08 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:46][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.00929515715688467, acc: 1.0)
[2024-11-08 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:47][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.0412236824631691, acc: 1.0)
[2024-11-08 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:48][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.03460758551955223, acc: 1.0)
[2024-11-08 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:48][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.2049809992313385, acc: 0.9090909361839294)
[2024-11-08 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:48][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.010967426002025604, acc: 1.0)
[2024-11-08 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:49][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.4316769540309906, acc: 0.8636363744735718)
[2024-11-08 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:49][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.010077856481075287, acc: 1.0)
[2024-11-08 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:49][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.0012899867724627256, acc: 1.0)
[2024-11-08 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:49][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.06613773107528687, acc: 0.9523809552192688)
[2024-11-08 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:50][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.17212697863578796, acc: 0.9523809552192688)
[2024-11-08 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:50][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.07194731384515762, acc: 0.9473684430122375)
[2024-11-08 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:51][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.29714223742485046, acc: 0.9090909361839294)
[2024-11-08 01:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:52][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.042089276015758514, acc: 1.0)
[2024-11-08 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:52][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.08443699777126312, acc: 0.9545454382896423)
[2024-11-08 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:52][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.02123703435063362, acc: 1.0)
[2024-11-08 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:53][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.004615520592778921, acc: 1.0)
[2024-11-08 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:53][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.006076489575207233, acc: 1.0)
[2024-11-08 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:53][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.021046845242381096, acc: 1.0)
[2024-11-08 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:54][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.0045220088213682175, acc: 1.0)
[2024-11-08 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:54][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.0015493066748604178, acc: 1.0)
[2024-11-08 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:54][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.00819153431802988, acc: 1.0)
[2024-11-08 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:55][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.053936898708343506, acc: 1.0)
[2024-11-08 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:55][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.03014599159359932, acc: 1.0)
[2024-11-08 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:55][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.06451650708913803, acc: 1.0)
[2024-11-08 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:56][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.1890821009874344, acc: 0.8947368264198303)
[2024-11-08 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:56][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.4192669689655304, acc: 0.8636363744735718)
[2024-11-08 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:56][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.042358558624982834, acc: 1.0)
[2024-11-08 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:57][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.0838252380490303, acc: 0.9655172228813171)
[2024-11-08 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:57][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.152899831533432, acc: 0.9333333373069763)
[2024-11-08 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:57][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.02719096839427948, acc: 1.0)
[2024-11-08 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:58][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.009440184570848942, acc: 1.0)
[2024-11-08 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:58][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.09377992153167725, acc: 0.9545454382896423)
[2024-11-08 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:58][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.05736524611711502, acc: 0.9523809552192688)
[2024-11-08 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:59][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.009261985309422016, acc: 1.0)
[2024-11-08 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:59][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.013971587643027306, acc: 1.0)
[2024-11-08 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:00:59][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.0984673947095871, acc: 0.9714285731315613)
[2024-11-08 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:00][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.0034709072206169367, acc: 1.0)
[2024-11-08 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:00][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.12039601057767868, acc: 0.9047619104385376)
[2024-11-08 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:00][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.04260138049721718, acc: 1.0)
[2024-11-08 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:00][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.008630245923995972, acc: 1.0)
[2024-11-08 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:01][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.18042853474617004, acc: 0.9523809552192688)
[2024-11-08 01:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:01][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.02171013318002224, acc: 1.0)
[2024-11-08 01:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:01][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.08520976454019547, acc: 0.9642857313156128)
[2024-11-08 01:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:02][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.2616436183452606, acc: 0.9629629850387573)
[2024-11-08 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:02][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.010072801262140274, acc: 1.0)
[2024-11-08 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:02][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.0503184013068676, acc: 0.9615384340286255)
[2024-11-08 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:03][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.07533634454011917, acc: 0.9523809552192688)
[2024-11-08 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:03][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.023883890360593796, acc: 1.0)
[2024-11-08 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:35][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3929, device='cuda:0') eval_epoch_loss=tensor(0.3314, device='cuda:0') eval_epoch_acc=tensor(0.9231, device='cuda:0')
[2024-11-08 01:01:35][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:01:35][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:01:35][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_7_step_417_loss_0.33138778805732727/model.pt
[2024-11-08 01:01:35][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:36][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.041683539748191833, acc: 1.0)
[2024-11-08 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:36][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.06568974256515503, acc: 0.9545454382896423)
[2024-11-08 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:36][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.053380660712718964, acc: 1.0)
[2024-11-08 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:37][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.02638145536184311, acc: 1.0)
[2024-11-08 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:37][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.1154671162366867, acc: 0.9677419066429138)
[2024-11-08 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:37][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.8908564448356628, acc: 0.9354838728904724)
[2024-11-08 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:38][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.032505832612514496, acc: 1.0)
[2024-11-08 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:38][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.010286654345691204, acc: 1.0)
[2024-11-08 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:38][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.004082916304469109, acc: 1.0)
[2024-11-08 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:39][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.1180657371878624, acc: 0.9473684430122375)
[2024-11-08 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:39][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.05396242067217827, acc: 0.9545454382896423)
[2024-11-08 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:39][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.008925209753215313, acc: 1.0)
[2024-11-08 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:40][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.006069168448448181, acc: 1.0)
[2024-11-08 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:40][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.030642801895737648, acc: 1.0)
[2024-11-08 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:40][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.0426948145031929, acc: 0.9677419066429138)
[2024-11-08 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:41][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.03718128427863121, acc: 1.0)
[2024-11-08 01:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:41][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.015665754675865173, acc: 1.0)
[2024-11-08 01:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:41][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.033027101308107376, acc: 1.0)
[2024-11-08 01:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:42][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.013412035070359707, acc: 1.0)
[2024-11-08 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:42][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.003912113606929779, acc: 1.0)
[2024-11-08 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:42][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.005523239262402058, acc: 1.0)
[2024-11-08 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:43][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.0033505859319120646, acc: 1.0)
[2024-11-08 01:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:43][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.027097037062048912, acc: 1.0)
[2024-11-08 01:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:43][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.0017621550941839814, acc: 1.0)
[2024-11-08 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:44][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.09964340180158615, acc: 0.9473684430122375)
[2024-11-08 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:44][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.05464525148272514, acc: 1.0)
[2024-11-08 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:45][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.04172108322381973, acc: 1.0)
[2024-11-08 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:45][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.09602880477905273, acc: 0.9523809552192688)
[2024-11-08 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:46][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.0030264481902122498, acc: 1.0)
[2024-11-08 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:46][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.44148117303848267, acc: 0.9032257795333862)
[2024-11-08 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:46][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.02005746215581894, acc: 1.0)
[2024-11-08 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:47][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.00809505209326744, acc: 1.0)
[2024-11-08 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:47][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.0024715489707887173, acc: 1.0)
[2024-11-08 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:47][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.0015395849477499723, acc: 1.0)
[2024-11-08 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:48][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.004941192921251059, acc: 1.0)
[2024-11-08 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:48][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.005494606215506792, acc: 1.0)
[2024-11-08 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:48][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.00087288289796561, acc: 1.0)
[2024-11-08 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:49][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.07439100742340088, acc: 0.9545454382896423)
[2024-11-08 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:49][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.011588877066969872, acc: 1.0)
[2024-11-08 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:49][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.05606982856988907, acc: 1.0)
[2024-11-08 01:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:50][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.06619014590978622, acc: 0.949999988079071)
[2024-11-08 01:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:50][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.0023455112241208553, acc: 1.0)
[2024-11-08 01:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:50][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.12038767337799072, acc: 0.9090909361839294)
[2024-11-08 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:51][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.028769981116056442, acc: 1.0)
[2024-11-08 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:51][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.0029813579749315977, acc: 1.0)
[2024-11-08 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:51][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.010301756672561169, acc: 1.0)
[2024-11-08 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:52][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.04817112907767296, acc: 0.9629629850387573)
[2024-11-08 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:52][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.0024404304567724466, acc: 1.0)
[2024-11-08 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:52][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.0014878397341817617, acc: 1.0)
[2024-11-08 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:53][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.0012626789975911379, acc: 1.0)
[2024-11-08 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:53][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.4290935695171356, acc: 0.9473684430122375)
[2024-11-08 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:53][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.0007577389478683472, acc: 1.0)
[2024-11-08 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:54][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.00423382967710495, acc: 1.0)
[2024-11-08 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:54][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.004080594051629305, acc: 1.0)
[2024-11-08 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:54][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.01046125590801239, acc: 1.0)
[2024-11-08 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:55][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.07704857736825943, acc: 0.9583333134651184)
[2024-11-08 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:55][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.027742234990000725, acc: 1.0)
[2024-11-08 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:55][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.014196891337633133, acc: 1.0)
[2024-11-08 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:56][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.02050813101232052, acc: 1.0)
[2024-11-08 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:56][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.0022222939878702164, acc: 1.0)
[2024-11-08 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:56][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.42587795853614807, acc: 0.8636363744735718)
[2024-11-08 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:57][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.008898711763322353, acc: 1.0)
[2024-11-08 01:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:57][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.08422331511974335, acc: 0.9677419066429138)
[2024-11-08 01:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:57][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.05723142996430397, acc: 0.9677419066429138)
[2024-11-08 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:58][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.4134741723537445, acc: 0.9047619104385376)
[2024-11-08 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:58][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.18455879390239716, acc: 0.8947368264198303)
[2024-11-08 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:59][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.2430189549922943, acc: 0.9545454382896423)
[2024-11-08 01:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:59][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.008222117088735104, acc: 1.0)
[2024-11-08 01:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:01:59][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.011938582174479961, acc: 1.0)
[2024-11-08 01:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:00][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.0722622275352478, acc: 0.9629629850387573)
[2024-11-08 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:00][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.2250571995973587, acc: 0.9428571462631226)
[2024-11-08 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:00][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.13934443891048431, acc: 0.9615384340286255)
[2024-11-08 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:01][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.008086382411420345, acc: 1.0)
[2024-11-08 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:01][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.007963255979120731, acc: 1.0)
[2024-11-08 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:01][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.015227166004478931, acc: 1.0)
[2024-11-08 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:02][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.010444595478475094, acc: 1.0)
[2024-11-08 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:02][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.022274872288107872, acc: 1.0)
[2024-11-08 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:02][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.30874529480934143, acc: 0.875)
[2024-11-08 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:03][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.14829078316688538, acc: 0.9285714030265808)
[2024-11-08 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:03][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.07328682392835617, acc: 1.0)
[2024-11-08 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:03][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.21468834578990936, acc: 0.949999988079071)
[2024-11-08 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:04][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.21295519173145294, acc: 0.9473684430122375)
[2024-11-08 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:04][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.14583583176136017, acc: 0.9090909361839294)
[2024-11-08 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:04][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.04317821189761162, acc: 1.0)
[2024-11-08 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:05][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.04767891764640808, acc: 1.0)
[2024-11-08 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:05][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.051889028400182724, acc: 0.9677419066429138)
[2024-11-08 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:05][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.13264769315719604, acc: 0.9354838728904724)
[2024-11-08 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:06][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.06418151408433914, acc: 0.95652174949646)
[2024-11-08 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:06][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.02048269845545292, acc: 1.0)
[2024-11-08 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:06][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.37287449836730957, acc: 0.8947368264198303)
[2024-11-08 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:07][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.049693040549755096, acc: 0.949999988079071)
[2024-11-08 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:07][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.45264536142349243, acc: 0.800000011920929)
[2024-11-08 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:08][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.021575547754764557, acc: 1.0)
[2024-11-08 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:08][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.14041085541248322, acc: 0.939393937587738)
[2024-11-08 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:08][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.013495417311787605, acc: 1.0)
[2024-11-08 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:10][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.15146397054195404, acc: 0.9285714030265808)
[2024-11-08 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:11][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.002038419246673584, acc: 1.0)
[2024-11-08 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:11][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.17583109438419342, acc: 0.9473684430122375)
[2024-11-08 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:12][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.03094344399869442, acc: 1.0)
[2024-11-08 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:12][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.12055553495883942, acc: 0.9523809552192688)
[2024-11-08 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:13][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.0076733920723199844, acc: 1.0)
[2024-11-08 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:13][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.0173474308103323, acc: 1.0)
[2024-11-08 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:13][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.03177475929260254, acc: 1.0)
[2024-11-08 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:14][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.043497804552316666, acc: 1.0)
[2024-11-08 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:14][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.25790688395500183, acc: 0.95652174949646)
[2024-11-08 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:15][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.11761574447154999, acc: 0.9523809552192688)
[2024-11-08 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:15][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.1841202676296234, acc: 0.9473684430122375)
[2024-11-08 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:16][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.3075147867202759, acc: 0.9090909361839294)
[2024-11-08 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:16][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.010005509480834007, acc: 1.0)
[2024-11-08 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:16][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.19216561317443848, acc: 0.9545454382896423)
[2024-11-08 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:17][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.06413540989160538, acc: 0.9696969985961914)
[2024-11-08 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:17][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.34471771121025085, acc: 0.8928571343421936)
[2024-11-08 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:17][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.03514474630355835, acc: 1.0)
[2024-11-08 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:18][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.3120270371437073, acc: 0.9473684430122375)
[2024-11-08 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:18][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.38497838377952576, acc: 0.8999999761581421)
[2024-11-08 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:18][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.28905004262924194, acc: 0.8571428656578064)
[2024-11-08 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:19][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.07913733273744583, acc: 1.0)
[2024-11-08 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:19][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.12197905778884888, acc: 0.9642857313156128)
[2024-11-08 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:19][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.014745501801371574, acc: 1.0)
[2024-11-08 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:20][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.1570013463497162, acc: 0.9599999785423279)
[2024-11-08 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:20][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.05945651978254318, acc: 0.9523809552192688)
[2024-11-08 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:20][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.15925441682338715, acc: 0.8947368264198303)
[2024-11-08 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:21][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.09009714424610138, acc: 1.0)
[2024-11-08 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:21][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.21284684538841248, acc: 0.9047619104385376)
[2024-11-08 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:21][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.08675843477249146, acc: 1.0)
[2024-11-08 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:22][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.002664158819243312, acc: 1.0)
[2024-11-08 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:22][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.006507150363177061, acc: 1.0)
[2024-11-08 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:22][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.01186445727944374, acc: 1.0)
[2024-11-08 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:22][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.0354241207242012, acc: 1.0)
[2024-11-08 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:23][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.0013316438999027014, acc: 1.0)
[2024-11-08 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:23][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.07382910698652267, acc: 1.0)
[2024-11-08 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:23][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.03672173619270325, acc: 1.0)
[2024-11-08 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:24][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 0.007588088046759367, acc: 1.0)
[2024-11-08 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:24][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.00789080373942852, acc: 1.0)
[2024-11-08 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:24][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.004259554669260979, acc: 1.0)
[2024-11-08 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:24][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.01447973120957613, acc: 1.0)
[2024-11-08 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:25][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.05689161643385887, acc: 0.9523809552192688)
[2024-11-08 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:25][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.0023602053988724947, acc: 1.0)
[2024-11-08 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:25][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.015876390039920807, acc: 1.0)
[2024-11-08 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:26][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.03898021578788757, acc: 1.0)
[2024-11-08 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:26][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.022820882499217987, acc: 1.0)
[2024-11-08 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:26][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.025761423632502556, acc: 1.0)
[2024-11-08 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:27][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.023871634155511856, acc: 1.0)
[2024-11-08 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:57][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3527, device='cuda:0') eval_epoch_loss=tensor(0.3021, device='cuda:0') eval_epoch_acc=tensor(0.9268, device='cuda:0')
[2024-11-08 01:02:57][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:02:57][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:02:58][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_7_step_560_loss_0.3021124005317688/model.pt
[2024-11-08 01:02:58][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:58][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.014521579258143902, acc: 1.0)
[2024-11-08 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:58][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.017997631803154945, acc: 1.0)
[2024-11-08 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:59][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.0007374674314633012, acc: 1.0)
[2024-11-08 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:59][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.016584046185016632, acc: 1.0)
[2024-11-08 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:02:59][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.009195848368108273, acc: 1.0)
[2024-11-08 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:00][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.014992596581578255, acc: 1.0)
[2024-11-08 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:00][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.0035324199125170708, acc: 1.0)
[2024-11-08 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:00][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.023429004475474358, acc: 1.0)
[2024-11-08 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:01][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.004035413730889559, acc: 1.0)
[2024-11-08 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:01][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.0017129967454820871, acc: 1.0)
[2024-11-08 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:01][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.0003260248340666294, acc: 1.0)
[2024-11-08 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:02][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.001544433063827455, acc: 1.0)
[2024-11-08 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:02][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.003313004970550537, acc: 1.0)
[2024-11-08 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:02][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.042595259845256805, acc: 1.0)
[2024-11-08 01:03:03][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.1003, train_epoch_loss=0.0955, epoch time 335.46641532331705s
[2024-11-08 01:03:03][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 01:03:03][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 01:03:03][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 01:03:03][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 01:03:03][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:04][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.017351895570755005, acc: 1.0)
[2024-11-08 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:04][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.0006384635926224291, acc: 1.0)
[2024-11-08 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:05][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.005270680412650108, acc: 1.0)
[2024-11-08 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:05][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.05546411871910095, acc: 1.0)
[2024-11-08 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:05][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.0338326059281826, acc: 1.0)
[2024-11-08 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:06][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.06130026653409004, acc: 0.9473684430122375)
[2024-11-08 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:06][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.0007081001531332731, acc: 1.0)
[2024-11-08 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:06][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.013331733644008636, acc: 1.0)
[2024-11-08 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:07][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.0018024923047050834, acc: 1.0)
[2024-11-08 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:07][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.029948657378554344, acc: 1.0)
[2024-11-08 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:07][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.0044213635846972466, acc: 1.0)
[2024-11-08 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:07][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.01693768799304962, acc: 1.0)
[2024-11-08 01:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:08][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.008934633806347847, acc: 1.0)
[2024-11-08 01:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:08][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.004791902843862772, acc: 1.0)
[2024-11-08 01:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:08][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.061572104692459106, acc: 0.95652174949646)
[2024-11-08 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:09][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.0017043702537193894, acc: 1.0)
[2024-11-08 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:09][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.0025196108035743237, acc: 1.0)
[2024-11-08 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:09][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.00601668655872345, acc: 1.0)
[2024-11-08 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:10][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.020174728706479073, acc: 1.0)
[2024-11-08 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:10][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.11609790474176407, acc: 0.9473684430122375)
[2024-11-08 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:11][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.06351632624864578, acc: 0.9473684430122375)
[2024-11-08 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:11][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.007872695103287697, acc: 1.0)
[2024-11-08 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:11][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.003980474080890417, acc: 1.0)
[2024-11-08 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:12][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.0019725707825273275, acc: 1.0)
[2024-11-08 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:12][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.016946934163570404, acc: 1.0)
[2024-11-08 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:12][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.031228525564074516, acc: 1.0)
[2024-11-08 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:13][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.03227364644408226, acc: 1.0)
[2024-11-08 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:14][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.06821140646934509, acc: 0.9473684430122375)
[2024-11-08 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:14][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.0016260728007182479, acc: 1.0)
[2024-11-08 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:14][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.4297617971897125, acc: 0.8999999761581421)
[2024-11-08 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:15][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.1733035445213318, acc: 0.9545454382896423)
[2024-11-08 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:15][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.05849094316363335, acc: 1.0)
[2024-11-08 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:15][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.13685262203216553, acc: 0.9696969985961914)
[2024-11-08 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:16][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.002652909839525819, acc: 1.0)
[2024-11-08 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:16][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.0031816905830055475, acc: 1.0)
[2024-11-08 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:16][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.014627011492848396, acc: 1.0)
[2024-11-08 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:17][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.01977059617638588, acc: 1.0)
[2024-11-08 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:17][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.058410223573446274, acc: 0.9523809552192688)
[2024-11-08 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:17][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.005712851416319609, acc: 1.0)
[2024-11-08 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:18][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.14798976480960846, acc: 0.9285714030265808)
[2024-11-08 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:18][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.23980453610420227, acc: 0.9428571462631226)
[2024-11-08 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:18][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.005796428304165602, acc: 1.0)
[2024-11-08 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:19][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.24099449813365936, acc: 0.949999988079071)
[2024-11-08 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:19][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.1681293249130249, acc: 0.8947368264198303)
[2024-11-08 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:20][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.18764136731624603, acc: 0.9545454382896423)
[2024-11-08 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:20][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.052122849971055984, acc: 0.9523809552192688)
[2024-11-08 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:20][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.1636536568403244, acc: 0.9583333134651184)
[2024-11-08 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:21][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.004674085881561041, acc: 1.0)
[2024-11-08 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:21][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.014812028035521507, acc: 1.0)
[2024-11-08 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:21][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.00946094375103712, acc: 1.0)
[2024-11-08 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:22][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.11409961432218552, acc: 0.9523809552192688)
[2024-11-08 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:22][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.13283859193325043, acc: 0.9473684430122375)
[2024-11-08 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:22][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.018521035090088844, acc: 1.0)
[2024-11-08 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:23][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 0.11802078783512115, acc: 0.9523809552192688)
[2024-11-08 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:23][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.1400182843208313, acc: 0.9545454382896423)
[2024-11-08 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:23][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.004964285995811224, acc: 1.0)
[2024-11-08 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:26][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.0282188281416893, acc: 1.0)
[2024-11-08 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:27][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 0.08305701613426208, acc: 0.9523809552192688)
[2024-11-08 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:27][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.22862276434898376, acc: 0.8421052694320679)
[2024-11-08 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:28][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.025959085673093796, acc: 1.0)
[2024-11-08 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:28][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.12805068492889404, acc: 0.8947368264198303)
[2024-11-08 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:29][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.04151493310928345, acc: 1.0)
[2024-11-08 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:29][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.017125464975833893, acc: 1.0)
[2024-11-08 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:30][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.018043795600533485, acc: 1.0)
[2024-11-08 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:30][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.028295353055000305, acc: 1.0)
[2024-11-08 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:30][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.05476732179522514, acc: 0.949999988079071)
[2024-11-08 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:31][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.12912091612815857, acc: 0.9473684430122375)
[2024-11-08 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:31][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.2240455150604248, acc: 0.8999999761581421)
[2024-11-08 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:31][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.0027865665033459663, acc: 1.0)
[2024-11-08 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:32][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.005956348031759262, acc: 1.0)
[2024-11-08 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:32][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.007717349100857973, acc: 1.0)
[2024-11-08 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:32][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.06199398264288902, acc: 1.0)
[2024-11-08 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:33][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.05049179121851921, acc: 1.0)
[2024-11-08 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:33][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 0.10589379817247391, acc: 0.9473684430122375)
[2024-11-08 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:33][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.03235652297735214, acc: 1.0)
[2024-11-08 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:34][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.04304908588528633, acc: 1.0)
[2024-11-08 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:34][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 0.01488783024251461, acc: 1.0)
[2024-11-08 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:34][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.006163907703012228, acc: 1.0)
[2024-11-08 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:35][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.0031070695258677006, acc: 1.0)
[2024-11-08 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:35][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.09479250758886337, acc: 0.9714285731315613)
[2024-11-08 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:35][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.0022723807487636805, acc: 1.0)
[2024-11-08 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:35][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.0008111841743811965, acc: 1.0)
[2024-11-08 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:36][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.02962871827185154, acc: 1.0)
[2024-11-08 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:36][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.20969821512699127, acc: 0.9473684430122375)
[2024-11-08 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:36][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.0008446332067251205, acc: 1.0)
[2024-11-08 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:37][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.0016978291096165776, acc: 1.0)
[2024-11-08 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:37][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.13745205104351044, acc: 0.9583333134651184)
[2024-11-08 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:37][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.1410953551530838, acc: 0.9615384340286255)
[2024-11-08 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:38][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.008871542289853096, acc: 1.0)
[2024-11-08 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:39][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.38219591975212097, acc: 0.8947368264198303)
[2024-11-08 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:40][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.05414138361811638, acc: 1.0)
[2024-11-08 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:40][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.04364737868309021, acc: 1.0)
[2024-11-08 01:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:41][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.12774893641471863, acc: 0.9047619104385376)
[2024-11-08 01:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:42][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.12718762457370758, acc: 0.9230769276618958)
[2024-11-08 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:42][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.012189905159175396, acc: 1.0)
[2024-11-08 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:43][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.09323213249444962, acc: 1.0)
[2024-11-08 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:43][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.027386078611016273, acc: 1.0)
[2024-11-08 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:43][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.04755410924553871, acc: 0.949999988079071)
[2024-11-08 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:44][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.22984343767166138, acc: 0.9047619104385376)
[2024-11-08 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:44][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.07751621305942535, acc: 0.9545454382896423)
[2024-11-08 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:44][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.041661929339170456, acc: 1.0)
[2024-11-08 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:45][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.08595510572195053, acc: 0.9599999785423279)
[2024-11-08 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:45][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.018772484734654427, acc: 1.0)
[2024-11-08 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:45][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.0026661113370209932, acc: 1.0)
[2024-11-08 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:45][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.0065355319529771805, acc: 1.0)
[2024-11-08 01:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:46][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.0021719480864703655, acc: 1.0)
[2024-11-08 01:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:46][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.11105535179376602, acc: 0.9545454382896423)
[2024-11-08 01:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:46][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.07697969675064087, acc: 0.9642857313156128)
[2024-11-08 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:47][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.005204516928642988, acc: 1.0)
[2024-11-08 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:47][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.1576720029115677, acc: 0.9523809552192688)
[2024-11-08 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:47][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.0005866737337782979, acc: 1.0)
[2024-11-08 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:48][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.11823311448097229, acc: 0.9473684430122375)
[2024-11-08 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:48][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.0014789248816668987, acc: 1.0)
[2024-11-08 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:49][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.02855864353477955, acc: 1.0)
[2024-11-08 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:49][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.007128625642508268, acc: 1.0)
[2024-11-08 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:49][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.0016853931592777371, acc: 1.0)
[2024-11-08 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:49][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.004612200893461704, acc: 1.0)
[2024-11-08 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:50][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.0410957857966423, acc: 1.0)
[2024-11-08 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:50][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.031832657754421234, acc: 1.0)
[2024-11-08 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:51][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.15989594161510468, acc: 0.9545454382896423)
[2024-11-08 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:51][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.06892921030521393, acc: 0.9473684430122375)
[2024-11-08 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:52][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.00995542574673891, acc: 1.0)
[2024-11-08 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:52][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.003261461853981018, acc: 1.0)
[2024-11-08 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:52][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.17049624025821686, acc: 0.9230769276618958)
[2024-11-08 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:53][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.15289464592933655, acc: 0.8999999761581421)
[2024-11-08 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:53][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.14353488385677338, acc: 0.949999988079071)
[2024-11-08 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:53][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.06575307250022888, acc: 1.0)
[2024-11-08 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:54][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.11971587687730789, acc: 0.9545454382896423)
[2024-11-08 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:54][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.049855463206768036, acc: 1.0)
[2024-11-08 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:24][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2683, device='cuda:0') eval_epoch_loss=tensor(0.2377, device='cuda:0') eval_epoch_acc=tensor(0.9391, device='cuda:0')
[2024-11-08 01:04:24][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:04:24][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:04:25][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_8_step_129_loss_0.23769702017307281/model.pt
[2024-11-08 01:04:25][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:04:25][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.9390533566474915
[2024-11-08 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:25][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.00299223349429667, acc: 1.0)
[2024-11-08 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:26][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.10408768802881241, acc: 0.9696969985961914)
[2024-11-08 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:26][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.18907268345355988, acc: 0.9259259104728699)
[2024-11-08 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:26][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.3490639328956604, acc: 0.939393937587738)
[2024-11-08 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:27][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.041092585772275925, acc: 0.949999988079071)
[2024-11-08 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:27][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.001996331149712205, acc: 1.0)
[2024-11-08 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:27][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.08275729417800903, acc: 0.9473684430122375)
[2024-11-08 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:28][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.031017450615763664, acc: 1.0)
[2024-11-08 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:28][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.21629372239112854, acc: 0.949999988079071)
[2024-11-08 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:28][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.015838248655200005, acc: 1.0)
[2024-11-08 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:28][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.015884986147284508, acc: 1.0)
[2024-11-08 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:29][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.03621278330683708, acc: 1.0)
[2024-11-08 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:29][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.004219739697873592, acc: 1.0)
[2024-11-08 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:29][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.15457917749881744, acc: 0.8999999761581421)
[2024-11-08 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:30][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.29912641644477844, acc: 0.8999999761581421)
[2024-11-08 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:30][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.042281683534383774, acc: 1.0)
[2024-11-08 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:31][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.013836103491485119, acc: 1.0)
[2024-11-08 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:31][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.0988951250910759, acc: 1.0)
[2024-11-08 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:31][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.00620600301772356, acc: 1.0)
[2024-11-08 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:32][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.3713899850845337, acc: 0.939393937587738)
[2024-11-08 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:32][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.19553238153457642, acc: 0.9629629850387573)
[2024-11-08 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:32][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.009807603433728218, acc: 1.0)
[2024-11-08 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:33][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.01383634191006422, acc: 1.0)
[2024-11-08 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:33][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.009584092535078526, acc: 1.0)
[2024-11-08 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:33][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.054251525551080704, acc: 1.0)
[2024-11-08 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:34][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.30770131945610046, acc: 0.9545454382896423)
[2024-11-08 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:34][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.0650838315486908, acc: 1.0)
[2024-11-08 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:34][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.44684648513793945, acc: 0.9090909361839294)
[2024-11-08 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:35][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.4315160810947418, acc: 0.9090909361839294)
[2024-11-08 01:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:36][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.2744964361190796, acc: 0.8500000238418579)
[2024-11-08 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:36][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.16935105621814728, acc: 0.8999999761581421)
[2024-11-08 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:37][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.16129212081432343, acc: 0.8947368264198303)
[2024-11-08 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:37][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.16797147691249847, acc: 0.9090909361839294)
[2024-11-08 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:38][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.23690569400787354, acc: 0.8999999761581421)
[2024-11-08 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:38][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.20818953216075897, acc: 0.9200000166893005)
[2024-11-08 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:39][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.0861206129193306, acc: 0.9629629850387573)
[2024-11-08 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:39][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.17609849572181702, acc: 0.9722222089767456)
[2024-11-08 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:39][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.027267878875136375, acc: 1.0)
[2024-11-08 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:39][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.12677828967571259, acc: 0.9047619104385376)
[2024-11-08 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:40][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.020717795938253403, acc: 1.0)
[2024-11-08 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:40][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.013111799024045467, acc: 1.0)
[2024-11-08 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:41][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.09965342283248901, acc: 0.949999988079071)
[2024-11-08 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:42][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.02677219919860363, acc: 1.0)
[2024-11-08 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:42][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.006068647839128971, acc: 1.0)
[2024-11-08 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:42][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.09708108007907867, acc: 0.9666666388511658)
[2024-11-08 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:43][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.22363029420375824, acc: 0.8571428656578064)
[2024-11-08 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:43][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.12902233004570007, acc: 0.9523809552192688)
[2024-11-08 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:43][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.016909942030906677, acc: 1.0)
[2024-11-08 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:44][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.042843952775001526, acc: 1.0)
[2024-11-08 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:45][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.0724986344575882, acc: 0.9473684430122375)
[2024-11-08 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:45][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.044972848147153854, acc: 1.0)
[2024-11-08 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:45][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.002970056375488639, acc: 1.0)
[2024-11-08 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:46][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.008388428017497063, acc: 1.0)
[2024-11-08 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:46][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.04017871618270874, acc: 0.9714285731315613)
[2024-11-08 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:46][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.008865099400281906, acc: 1.0)
[2024-11-08 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:47][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.04971248283982277, acc: 1.0)
[2024-11-08 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:47][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.0020089077297598124, acc: 1.0)
[2024-11-08 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:48][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.5330265164375305, acc: 0.8999999761581421)
[2024-11-08 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:48][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.19584104418754578, acc: 0.9523809552192688)
[2024-11-08 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:48][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.06485646218061447, acc: 0.9545454382896423)
[2024-11-08 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:49][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.07131529599428177, acc: 0.9599999785423279)
[2024-11-08 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:49][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.024611376225948334, acc: 1.0)
[2024-11-08 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:50][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.12484146654605865, acc: 0.9047619104385376)
[2024-11-08 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:51][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.12174420803785324, acc: 0.9473684430122375)
[2024-11-08 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:52][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.03850214183330536, acc: 1.0)
[2024-11-08 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:53][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.13515609502792358, acc: 0.9047619104385376)
[2024-11-08 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:54][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.13042926788330078, acc: 0.9583333134651184)
[2024-11-08 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:54][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.2261539101600647, acc: 0.9032257795333862)
[2024-11-08 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:54][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.010237756185233593, acc: 1.0)
[2024-11-08 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:55][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.030369168147444725, acc: 1.0)
[2024-11-08 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:55][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.009954744949936867, acc: 1.0)
[2024-11-08 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:55][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.014956467784941196, acc: 1.0)
[2024-11-08 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:55][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.07158859819173813, acc: 0.9545454382896423)
[2024-11-08 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:56][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.008912482298910618, acc: 1.0)
[2024-11-08 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:56][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.02874237298965454, acc: 1.0)
[2024-11-08 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:56][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.07493919134140015, acc: 0.9642857313156128)
[2024-11-08 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:57][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.09253837168216705, acc: 0.9523809552192688)
[2024-11-08 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:57][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.1350896805524826, acc: 0.9473684430122375)
[2024-11-08 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:57][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.05808323249220848, acc: 0.9473684430122375)
[2024-11-08 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:58][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.11751819401979446, acc: 0.9545454382896423)
[2024-11-08 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:58][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.01256709173321724, acc: 1.0)
[2024-11-08 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:58][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.01088195014744997, acc: 1.0)
[2024-11-08 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:59][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.029063604772090912, acc: 1.0)
[2024-11-08 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:59][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.0371064767241478, acc: 1.0)
[2024-11-08 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:04:59][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.28269025683403015, acc: 0.9615384340286255)
[2024-11-08 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:00][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.0043832771480083466, acc: 1.0)
[2024-11-08 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:00][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.02505738101899624, acc: 1.0)
[2024-11-08 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:01][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.07569818943738937, acc: 0.9473684430122375)
[2024-11-08 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:01][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.024718226864933968, acc: 1.0)
[2024-11-08 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:02][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.02224758453667164, acc: 1.0)
[2024-11-08 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:02][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.0038201473653316498, acc: 1.0)
[2024-11-08 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:02][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.03756105527281761, acc: 1.0)
[2024-11-08 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:03][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.007004153449088335, acc: 1.0)
[2024-11-08 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:03][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.07743639498949051, acc: 0.9615384340286255)
[2024-11-08 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:04][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.029850943014025688, acc: 1.0)
[2024-11-08 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:04][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.03558695316314697, acc: 1.0)
[2024-11-08 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:05][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.01065878290683031, acc: 1.0)
[2024-11-08 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:05][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.015836086124181747, acc: 1.0)
[2024-11-08 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:05][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.05253692343831062, acc: 1.0)
[2024-11-08 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:06][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.2908875644207001, acc: 0.9166666865348816)
[2024-11-08 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:06][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.2409457564353943, acc: 0.8695651888847351)
[2024-11-08 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:06][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.09202805906534195, acc: 0.9523809552192688)
[2024-11-08 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:07][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.4023335576057434, acc: 0.8947368264198303)
[2024-11-08 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:07][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.0839858204126358, acc: 0.9473684430122375)
[2024-11-08 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:07][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 0.07677542418241501, acc: 1.0)
[2024-11-08 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:08][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.4096059799194336, acc: 0.8636363744735718)
[2024-11-08 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:08][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.06378380954265594, acc: 0.9642857313156128)
[2024-11-08 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:09][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.10586428642272949, acc: 0.9259259104728699)
[2024-11-08 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:09][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.17242011427879333, acc: 0.939393937587738)
[2024-11-08 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:09][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.06435678154230118, acc: 1.0)
[2024-11-08 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:09][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.34307152032852173, acc: 0.8571428656578064)
[2024-11-08 01:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:10][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.07231079041957855, acc: 1.0)
[2024-11-08 01:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:10][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.051603976637125015, acc: 1.0)
[2024-11-08 01:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:11][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.22843408584594727, acc: 0.8947368264198303)
[2024-11-08 01:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:11][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.12568344175815582, acc: 0.9090909361839294)
[2024-11-08 01:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:12][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 0.003455991391092539, acc: 1.0)
[2024-11-08 01:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:12][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.1952628791332245, acc: 0.9333333373069763)
[2024-11-08 01:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:12][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.018651239573955536, acc: 1.0)
[2024-11-08 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:13][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.007002916187047958, acc: 1.0)
[2024-11-08 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:13][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.08271954208612442, acc: 0.9523809552192688)
[2024-11-08 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:13][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.02158459648489952, acc: 1.0)
[2024-11-08 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:14][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.11996781826019287, acc: 0.949999988079071)
[2024-11-08 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:14][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.5482048988342285, acc: 0.9047619104385376)
[2024-11-08 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:14][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.051133859902620316, acc: 0.9545454382896423)
[2024-11-08 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:15][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.032822370529174805, acc: 1.0)
[2024-11-08 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:15][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 0.0478004515171051, acc: 0.9655172228813171)
[2024-11-08 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:15][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.007555306889116764, acc: 1.0)
[2024-11-08 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:16][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.002326174173504114, acc: 1.0)
[2024-11-08 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:16][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.06436055153608322, acc: 0.9523809552192688)
[2024-11-08 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:16][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.012865289114415646, acc: 1.0)
[2024-11-08 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:17][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.025954170152544975, acc: 1.0)
[2024-11-08 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:17][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.053973663598299026, acc: 0.9473684430122375)
[2024-11-08 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:18][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.09435069561004639, acc: 0.9545454382896423)
[2024-11-08 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:18][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.05854681879281998, acc: 1.0)
[2024-11-08 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:18][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.003043167991563678, acc: 1.0)
[2024-11-08 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:19][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.13276192545890808, acc: 0.949999988079071)
[2024-11-08 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:19][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.094872385263443, acc: 0.9473684430122375)
[2024-11-08 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:20][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.11382006853818893, acc: 0.95652174949646)
[2024-11-08 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:20][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.07206720113754272, acc: 0.9523809552192688)
[2024-11-08 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:21][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.15415309369564056, acc: 0.9655172228813171)
[2024-11-08 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:21][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.2366286665201187, acc: 0.9259259104728699)
[2024-11-08 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:21][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.04041203111410141, acc: 1.0)
[2024-11-08 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:21][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.006380405277013779, acc: 1.0)
[2024-11-08 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:50][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2944, device='cuda:0') eval_epoch_loss=tensor(0.2581, device='cuda:0') eval_epoch_acc=tensor(0.9272, device='cuda:0')
[2024-11-08 01:05:50][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:05:50][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:05:51][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_8_step_272_loss_0.25805896520614624/model.pt
[2024-11-08 01:05:51][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:51][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.027374088764190674, acc: 1.0)
[2024-11-08 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:52][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.016575763002038002, acc: 1.0)
[2024-11-08 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:52][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.0031784551683813334, acc: 1.0)
[2024-11-08 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:52][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.010244573466479778, acc: 1.0)
[2024-11-08 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:53][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.041359174996614456, acc: 1.0)
[2024-11-08 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:53][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.04216698929667473, acc: 1.0)
[2024-11-08 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:53][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.13103488087654114, acc: 0.9523809552192688)
[2024-11-08 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:53][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.010572484694421291, acc: 1.0)
[2024-11-08 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:54][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.14294442534446716, acc: 0.9444444179534912)
[2024-11-08 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:54][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.020930297672748566, acc: 1.0)
[2024-11-08 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:54][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.011679121293127537, acc: 1.0)
[2024-11-08 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:55][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.06243497505784035, acc: 0.9655172228813171)
[2024-11-08 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:55][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.061497773975133896, acc: 0.9666666388511658)
[2024-11-08 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:55][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.1606312096118927, acc: 0.8965517282485962)
[2024-11-08 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:56][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.02912263572216034, acc: 1.0)
[2024-11-08 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:56][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.0746849924325943, acc: 1.0)
[2024-11-08 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:56][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.04882039502263069, acc: 1.0)
[2024-11-08 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:57][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.3767068088054657, acc: 0.9545454382896423)
[2024-11-08 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:57][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.025911303237080574, acc: 1.0)
[2024-11-08 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:57][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.006086042616516352, acc: 1.0)
[2024-11-08 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:58][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.07109405100345612, acc: 0.9666666388511658)
[2024-11-08 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:58][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.041816942393779755, acc: 1.0)
[2024-11-08 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:58][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.06979448348283768, acc: 1.0)
[2024-11-08 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:59][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.01103085558861494, acc: 1.0)
[2024-11-08 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:05:59][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.1706731915473938, acc: 0.9090909361839294)
[2024-11-08 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:00][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.27355360984802246, acc: 0.9047619104385376)
[2024-11-08 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:00][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.07799822837114334, acc: 0.9583333134651184)
[2024-11-08 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:00][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.01167635153979063, acc: 1.0)
[2024-11-08 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:00][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.005960832349956036, acc: 1.0)
[2024-11-08 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:01][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.10714174062013626, acc: 0.9615384340286255)
[2024-11-08 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:01][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.10316821187734604, acc: 0.9523809552192688)
[2024-11-08 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:01][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.002589007606729865, acc: 1.0)
[2024-11-08 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:02][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.016603758558630943, acc: 1.0)
[2024-11-08 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:02][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.02555462159216404, acc: 1.0)
[2024-11-08 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:02][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.015579093247652054, acc: 1.0)
[2024-11-08 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:03][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.0015063423197716475, acc: 1.0)
[2024-11-08 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:03][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.06932033598423004, acc: 0.9523809552192688)
[2024-11-08 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:03][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.30590149760246277, acc: 0.8947368264198303)
[2024-11-08 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:03][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.03348155319690704, acc: 1.0)
[2024-11-08 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:04][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.010004901327192783, acc: 1.0)
[2024-11-08 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:04][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.01874629780650139, acc: 1.0)
[2024-11-08 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:04][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.0027368254959583282, acc: 1.0)
[2024-11-08 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:05][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.08479446172714233, acc: 0.9629629850387573)
[2024-11-08 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:05][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.017733927816152573, acc: 1.0)
[2024-11-08 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:05][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.11538746953010559, acc: 0.9230769276618958)
[2024-11-08 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:06][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.0070756846107542515, acc: 1.0)
[2024-11-08 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:06][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.026319552212953568, acc: 1.0)
[2024-11-08 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:06][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.00814669020473957, acc: 1.0)
[2024-11-08 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:07][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.039004068821668625, acc: 1.0)
[2024-11-08 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:07][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.0014641922898590565, acc: 1.0)
[2024-11-08 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:07][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.08632562309503555, acc: 1.0)
[2024-11-08 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:08][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.07053852826356888, acc: 1.0)
[2024-11-08 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:08][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.38626033067703247, acc: 0.8947368264198303)
[2024-11-08 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:08][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.22305011749267578, acc: 0.9545454382896423)
[2024-11-08 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:08][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.28220292925834656, acc: 0.8421052694320679)
[2024-11-08 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:09][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.13915479183197021, acc: 0.9166666865348816)
[2024-11-08 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:09][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.1876762956380844, acc: 0.931034505367279)
[2024-11-08 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:09][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.23069658875465393, acc: 0.9259259104728699)
[2024-11-08 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:10][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.009604162536561489, acc: 1.0)
[2024-11-08 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:10][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.0007381661562249064, acc: 1.0)
[2024-11-08 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:10][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.05752532556653023, acc: 0.9545454382896423)
[2024-11-08 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:11][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.3343523144721985, acc: 0.8947368264198303)
[2024-11-08 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:11][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.04550659656524658, acc: 1.0)
[2024-11-08 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:11][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.20092175900936127, acc: 0.9230769276618958)
[2024-11-08 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:12][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.33482351899147034, acc: 0.875)
[2024-11-08 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:12][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.29151737689971924, acc: 0.9047619104385376)
[2024-11-08 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:12][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.10353201627731323, acc: 0.95652174949646)
[2024-11-08 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:13][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.09465263038873672, acc: 0.9545454382896423)
[2024-11-08 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:13][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.0032959519885480404, acc: 1.0)
[2024-11-08 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:13][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.11467435956001282, acc: 0.9696969985961914)
[2024-11-08 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:14][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.010291706770658493, acc: 1.0)
[2024-11-08 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:14][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.16392266750335693, acc: 0.949999988079071)
[2024-11-08 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:14][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.004547875374555588, acc: 1.0)
[2024-11-08 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:14][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.061003390699625015, acc: 0.9545454382896423)
[2024-11-08 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:15][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.04272456094622612, acc: 1.0)
[2024-11-08 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:15][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.06064369156956673, acc: 0.9583333134651184)
[2024-11-08 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:15][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.004614097531884909, acc: 1.0)
[2024-11-08 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:16][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.19390307366847992, acc: 0.949999988079071)
[2024-11-08 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:16][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.03955616056919098, acc: 1.0)
[2024-11-08 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:16][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.033569078892469406, acc: 1.0)
[2024-11-08 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:17][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.03286118805408478, acc: 1.0)
[2024-11-08 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:17][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.025111116468906403, acc: 1.0)
[2024-11-08 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:17][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.1332741528749466, acc: 0.9599999785423279)
[2024-11-08 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:18][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.11489241570234299, acc: 1.0)
[2024-11-08 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:18][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.2558794915676117, acc: 0.8999999761581421)
[2024-11-08 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:19][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.014660464599728584, acc: 1.0)
[2024-11-08 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:19][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.020430654287338257, acc: 1.0)
[2024-11-08 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:19][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.003424775553867221, acc: 1.0)
[2024-11-08 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:20][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.08095623552799225, acc: 0.9655172228813171)
[2024-11-08 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:20][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.020810868591070175, acc: 1.0)
[2024-11-08 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:20][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.05070032179355621, acc: 1.0)
[2024-11-08 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:21][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.047336433082818985, acc: 0.9473684430122375)
[2024-11-08 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:21][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.39133772253990173, acc: 0.9047619104385376)
[2024-11-08 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:21][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.032427120953798294, acc: 1.0)
[2024-11-08 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:21][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.002880927175283432, acc: 1.0)
[2024-11-08 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:22][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.12466217577457428, acc: 0.9629629850387573)
[2024-11-08 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:22][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.039583612233400345, acc: 1.0)
[2024-11-08 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:22][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.06648190319538116, acc: 0.9523809552192688)
[2024-11-08 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:23][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.21469971537590027, acc: 0.9523809552192688)
[2024-11-08 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:24][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.006820492912083864, acc: 1.0)
[2024-11-08 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:24][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.0018659386551007628, acc: 1.0)
[2024-11-08 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:24][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.02459091693162918, acc: 1.0)
[2024-11-08 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:25][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.03434586152434349, acc: 1.0)
[2024-11-08 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:25][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 0.0018164103385061026, acc: 1.0)
[2024-11-08 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:25][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.007597866468131542, acc: 1.0)
[2024-11-08 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:26][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.059696048498153687, acc: 1.0)
[2024-11-08 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:26][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.0016219030367210507, acc: 1.0)
[2024-11-08 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:27][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.0011331629939377308, acc: 1.0)
[2024-11-08 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:27][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.022242199629545212, acc: 1.0)
[2024-11-08 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:28][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.037851300090551376, acc: 1.0)
[2024-11-08 01:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:29][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.04115802422165871, acc: 0.9545454382896423)
[2024-11-08 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:29][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.004416866693645716, acc: 1.0)
[2024-11-08 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:29][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.003877131035551429, acc: 1.0)
[2024-11-08 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:30][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.005680495407432318, acc: 1.0)
[2024-11-08 01:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:30][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.012112635187804699, acc: 1.0)
[2024-11-08 01:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:30][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.0004692928050644696, acc: 1.0)
[2024-11-08 01:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:30][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.003607668448239565, acc: 1.0)
[2024-11-08 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:31][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 0.0099716791883111, acc: 1.0)
[2024-11-08 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:31][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.23289112746715546, acc: 0.8636363744735718)
[2024-11-08 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:31][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.0039425985887646675, acc: 1.0)
[2024-11-08 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:32][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.06111295148730278, acc: 0.9473684430122375)
[2024-11-08 01:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:32][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.04948466643691063, acc: 1.0)
[2024-11-08 01:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:33][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.014465123414993286, acc: 1.0)
[2024-11-08 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:33][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.06918536126613617, acc: 0.9523809552192688)
[2024-11-08 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:33][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.0074544609524309635, acc: 1.0)
[2024-11-08 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:34][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.264407753944397, acc: 0.9666666388511658)
[2024-11-08 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:34][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.03471916913986206, acc: 1.0)
[2024-11-08 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:34][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.023491911590099335, acc: 1.0)
[2024-11-08 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:35][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.11491694301366806, acc: 0.9090909361839294)
[2024-11-08 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:35][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.011524810455739498, acc: 1.0)
[2024-11-08 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:35][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.005455105099827051, acc: 1.0)
[2024-11-08 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:36][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.013796884566545486, acc: 1.0)
[2024-11-08 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:36][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.0898754671216011, acc: 0.9714285731315613)
[2024-11-08 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:36][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.01957075484097004, acc: 1.0)
[2024-11-08 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:37][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.01372609380632639, acc: 1.0)
[2024-11-08 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:37][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.08002712577581406, acc: 0.9473684430122375)
[2024-11-08 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:37][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.007718520704656839, acc: 1.0)
[2024-11-08 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:38][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.001915175118483603, acc: 1.0)
[2024-11-08 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:38][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.0006981530459597707, acc: 1.0)
[2024-11-08 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:38][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.33671775460243225, acc: 0.9285714030265808)
[2024-11-08 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:39][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.0411934070289135, acc: 0.9629629850387573)
[2024-11-08 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:39][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.0014262032927945256, acc: 1.0)
[2024-11-08 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:39][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.007333564572036266, acc: 1.0)
[2024-11-08 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:09][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3276, device='cuda:0') eval_epoch_loss=tensor(0.2834, device='cuda:0') eval_epoch_acc=tensor(0.9332, device='cuda:0')
[2024-11-08 01:07:09][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:07:09][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:07:09][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_8_step_415_loss_0.28338563442230225/model.pt
[2024-11-08 01:07:09][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:09][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.08347683399915695, acc: 0.9523809552192688)
[2024-11-08 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:10][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.005582610610872507, acc: 1.0)
[2024-11-08 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:10][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.0004584690323099494, acc: 1.0)
[2024-11-08 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:10][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.02039344422519207, acc: 1.0)
[2024-11-08 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:11][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.04402633383870125, acc: 0.9523809552192688)
[2024-11-08 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:11][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.016856824979186058, acc: 1.0)
[2024-11-08 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:11][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.057020142674446106, acc: 0.9677419066429138)
[2024-11-08 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:11][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.2948756814002991, acc: 0.9677419066429138)
[2024-11-08 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:12][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.08626970648765564, acc: 0.9615384340286255)
[2024-11-08 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:12][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.003156182589009404, acc: 1.0)
[2024-11-08 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:12][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.249397873878479, acc: 0.9473684430122375)
[2024-11-08 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:13][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.00451098196208477, acc: 1.0)
[2024-11-08 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:13][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.007768884766846895, acc: 1.0)
[2024-11-08 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:13][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.010161764919757843, acc: 1.0)
[2024-11-08 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:14][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.0645822063088417, acc: 0.9583333134651184)
[2024-11-08 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:14][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.013824204914271832, acc: 1.0)
[2024-11-08 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:14][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.0031952636782079935, acc: 1.0)
[2024-11-08 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:14][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.01155894435942173, acc: 1.0)
[2024-11-08 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:15][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.010408421978354454, acc: 1.0)
[2024-11-08 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:15][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.13689465820789337, acc: 0.9473684430122375)
[2024-11-08 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:15][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.3207186758518219, acc: 0.9473684430122375)
[2024-11-08 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:16][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.0026821529027074575, acc: 1.0)
[2024-11-08 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:16][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.020033666864037514, acc: 1.0)
[2024-11-08 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:16][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.04094951972365379, acc: 0.9545454382896423)
[2024-11-08 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:17][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.0224132239818573, acc: 1.0)
[2024-11-08 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:17][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.0026040999218821526, acc: 1.0)
[2024-11-08 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:18][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.15669748187065125, acc: 0.9473684430122375)
[2024-11-08 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:18][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.010796023532748222, acc: 1.0)
[2024-11-08 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:19][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.026473313570022583, acc: 1.0)
[2024-11-08 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:19][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.008854511193931103, acc: 1.0)
[2024-11-08 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:19][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.002623409265652299, acc: 1.0)
[2024-11-08 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:20][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.03932900354266167, acc: 0.9677419066429138)
[2024-11-08 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:20][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.028215421363711357, acc: 1.0)
[2024-11-08 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:20][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.07472129911184311, acc: 0.9677419066429138)
[2024-11-08 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:21][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.0025618320796638727, acc: 1.0)
[2024-11-08 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:21][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.012231143191456795, acc: 1.0)
[2024-11-08 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:21][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.019824834540486336, acc: 1.0)
[2024-11-08 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:22][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.00117947265971452, acc: 1.0)
[2024-11-08 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:22][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.14402779936790466, acc: 0.949999988079071)
[2024-11-08 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:22][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.05350277200341225, acc: 0.9545454382896423)
[2024-11-08 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:23][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.00412413477897644, acc: 1.0)
[2024-11-08 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:23][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.02700909599661827, acc: 1.0)
[2024-11-08 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:23][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.11378437280654907, acc: 0.949999988079071)
[2024-11-08 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:24][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.013344771228730679, acc: 1.0)
[2024-11-08 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:24][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.11923537403345108, acc: 0.9545454382896423)
[2024-11-08 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:24][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.044328268617391586, acc: 0.949999988079071)
[2024-11-08 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:25][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.0020633339881896973, acc: 1.0)
[2024-11-08 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:25][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.007864931598305702, acc: 1.0)
[2024-11-08 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:25][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.016630180180072784, acc: 1.0)
[2024-11-08 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:26][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.00199520424939692, acc: 1.0)
[2024-11-08 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:26][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.0010641206754371524, acc: 1.0)
[2024-11-08 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:26][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.006841978989541531, acc: 1.0)
[2024-11-08 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:27][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.01619892753660679, acc: 1.0)
[2024-11-08 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:27][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.0013054204173386097, acc: 1.0)
[2024-11-08 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:27][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.0014764285879209638, acc: 1.0)
[2024-11-08 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:27][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.002932819537818432, acc: 1.0)
[2024-11-08 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:28][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.027178393676877022, acc: 1.0)
[2024-11-08 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:28][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.048361558467149734, acc: 0.9583333134651184)
[2024-11-08 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:28][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.005335514899343252, acc: 1.0)
[2024-11-08 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:29][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.008725643157958984, acc: 1.0)
[2024-11-08 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:29][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.016395239159464836, acc: 1.0)
[2024-11-08 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:29][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.0023385430686175823, acc: 1.0)
[2024-11-08 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:30][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.02944972552359104, acc: 1.0)
[2024-11-08 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:30][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.0033292188309133053, acc: 1.0)
[2024-11-08 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:30][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.003591334680095315, acc: 1.0)
[2024-11-08 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:31][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.04411265626549721, acc: 0.9677419066429138)
[2024-11-08 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:31][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.06374753266572952, acc: 0.9523809552192688)
[2024-11-08 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:31][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.06901808828115463, acc: 0.9473684430122375)
[2024-11-08 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:32][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.0659388080239296, acc: 1.0)
[2024-11-08 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:32][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.04752613604068756, acc: 0.9545454382896423)
[2024-11-08 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:32][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.03563190624117851, acc: 0.9642857313156128)
[2024-11-08 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:33][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.017299383878707886, acc: 1.0)
[2024-11-08 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:33][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.0304954145103693, acc: 0.9714285731315613)
[2024-11-08 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:33][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.07690806686878204, acc: 0.9615384340286255)
[2024-11-08 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:34][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.006156448274850845, acc: 1.0)
[2024-11-08 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:34][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.07341692596673965, acc: 0.9473684430122375)
[2024-11-08 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:34][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.016813719645142555, acc: 1.0)
[2024-11-08 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:35][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.006323599722236395, acc: 1.0)
[2024-11-08 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:35][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.14055214822292328, acc: 0.9523809552192688)
[2024-11-08 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:35][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.13828742504119873, acc: 0.9583333134651184)
[2024-11-08 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:35][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.15565216541290283, acc: 0.9285714030265808)
[2024-11-08 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:36][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.022740252315998077, acc: 1.0)
[2024-11-08 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:36][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.0014804222155362368, acc: 1.0)
[2024-11-08 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:37][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.05581183359026909, acc: 1.0)
[2024-11-08 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:37][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.09779846668243408, acc: 0.9545454382896423)
[2024-11-08 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:37][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.25535938143730164, acc: 0.8999999761581421)
[2024-11-08 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:37][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.0032554809004068375, acc: 1.0)
[2024-11-08 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:38][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.0022504206281155348, acc: 1.0)
[2024-11-08 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:38][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.09895577281713486, acc: 0.9677419066429138)
[2024-11-08 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:38][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.012580099515616894, acc: 1.0)
[2024-11-08 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:39][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.09083551168441772, acc: 0.9047619104385376)
[2024-11-08 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:39][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.6196439862251282, acc: 0.8421052694320679)
[2024-11-08 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:39][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.03851801156997681, acc: 0.949999988079071)
[2024-11-08 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:40][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.3962911367416382, acc: 0.949999988079071)
[2024-11-08 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:40][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.2788114845752716, acc: 0.9090909361839294)
[2024-11-08 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:40][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.011836468242108822, acc: 1.0)
[2024-11-08 01:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:41][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.0018934175604954362, acc: 1.0)
[2024-11-08 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:43][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.049069903790950775, acc: 0.9642857313156128)
[2024-11-08 01:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:44][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.002755619352683425, acc: 1.0)
[2024-11-08 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:44][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.26105237007141113, acc: 0.9473684430122375)
[2024-11-08 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:44][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.046060800552368164, acc: 0.949999988079071)
[2024-11-08 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:45][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.01826716586947441, acc: 1.0)
[2024-11-08 01:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:45][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.05063537508249283, acc: 1.0)
[2024-11-08 01:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:46][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.19536615908145905, acc: 0.9642857313156128)
[2024-11-08 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:46][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.04358118027448654, acc: 1.0)
[2024-11-08 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:46][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.017160702496767044, acc: 1.0)
[2024-11-08 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:47][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.027510806918144226, acc: 1.0)
[2024-11-08 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:47][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.015187517739832401, acc: 1.0)
[2024-11-08 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:48][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.04247548058629036, acc: 1.0)
[2024-11-08 01:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:48][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.11582329124212265, acc: 0.9090909361839294)
[2024-11-08 01:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:49][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.017789088189601898, acc: 1.0)
[2024-11-08 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:49][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.19871124625205994, acc: 0.9545454382896423)
[2024-11-08 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:49][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.18057020008563995, acc: 0.939393937587738)
[2024-11-08 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:50][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.08379370719194412, acc: 0.9642857313156128)
[2024-11-08 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:50][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.012586303055286407, acc: 1.0)
[2024-11-08 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:50][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.24736130237579346, acc: 0.8947368264198303)
[2024-11-08 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:50][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.12869778275489807, acc: 0.949999988079071)
[2024-11-08 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:51][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.19409310817718506, acc: 0.9047619104385376)
[2024-11-08 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:51][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.005797360558062792, acc: 1.0)
[2024-11-08 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:51][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.13016287982463837, acc: 0.9642857313156128)
[2024-11-08 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:52][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.0021161374170333147, acc: 1.0)
[2024-11-08 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:52][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.10126826912164688, acc: 0.9599999785423279)
[2024-11-08 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:52][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.08273619413375854, acc: 0.9523809552192688)
[2024-11-08 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:53][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.04351947084069252, acc: 1.0)
[2024-11-08 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:53][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.025607768446207047, acc: 1.0)
[2024-11-08 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:53][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.2442554086446762, acc: 0.8571428656578064)
[2024-11-08 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:54][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.004375871270895004, acc: 1.0)
[2024-11-08 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:54][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.001245609950274229, acc: 1.0)
[2024-11-08 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:54][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.17809227108955383, acc: 0.9629629850387573)
[2024-11-08 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:55][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.0904826819896698, acc: 0.9696969985961914)
[2024-11-08 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:55][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.006554844323545694, acc: 1.0)
[2024-11-08 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:55][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.012324647977948189, acc: 1.0)
[2024-11-08 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:56][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.0125496881082654, acc: 1.0)
[2024-11-08 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:56][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.004128811880946159, acc: 1.0)
[2024-11-08 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:56][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.009389948099851608, acc: 1.0)
[2024-11-08 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:57][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.0042878007516264915, acc: 1.0)
[2024-11-08 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:57][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.04969913512468338, acc: 0.9677419066429138)
[2024-11-08 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:57][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.000994766247458756, acc: 1.0)
[2024-11-08 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:58][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.050095632672309875, acc: 0.9523809552192688)
[2024-11-08 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:58][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.10998788475990295, acc: 0.8947368264198303)
[2024-11-08 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:58][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.05384267121553421, acc: 1.0)
[2024-11-08 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:59][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.002184499055147171, acc: 1.0)
[2024-11-08 01:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:07:59][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.002644638530910015, acc: 1.0)
[2024-11-08 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:28][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3238, device='cuda:0') eval_epoch_loss=tensor(0.2805, device='cuda:0') eval_epoch_acc=tensor(0.9302, device='cuda:0')
[2024-11-08 01:08:28][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:08:28][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:08:29][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_8_step_558_loss_0.28052428364753723/model.pt
[2024-11-08 01:08:29][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:29][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.024503594264388084, acc: 1.0)
[2024-11-08 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:29][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.06511983275413513, acc: 0.9629629850387573)
[2024-11-08 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:30][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.006760860327631235, acc: 1.0)
[2024-11-08 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:30][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.027307573705911636, acc: 1.0)
[2024-11-08 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:30][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.005838511046022177, acc: 1.0)
[2024-11-08 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:31][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.057927388697862625, acc: 0.9444444179534912)
[2024-11-08 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:31][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.017069444060325623, acc: 1.0)
[2024-11-08 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:31][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.09502947330474854, acc: 1.0)
[2024-11-08 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:32][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.0016148124122992158, acc: 1.0)
[2024-11-08 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:32][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.0016111863078549504, acc: 1.0)
[2024-11-08 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:32][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.009825228713452816, acc: 1.0)
[2024-11-08 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:33][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.0004645824374165386, acc: 1.0)
[2024-11-08 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:33][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.0009747048025019467, acc: 1.0)
[2024-11-08 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:33][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.0383111871778965, acc: 1.0)
[2024-11-08 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:33][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.0113917151466012, acc: 1.0)
[2024-11-08 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:34][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.08555607497692108, acc: 0.949999988079071)
[2024-11-08 01:08:34][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.0763, train_epoch_loss=0.0735, epoch time 331.43053141608834s
[2024-11-08 01:08:34][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 01:08:34][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 01:08:34][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 01:08:34][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 01:08:34][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:35][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.014956002123653889, acc: 1.0)
[2024-11-08 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:35][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.004164343699812889, acc: 1.0)
[2024-11-08 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:36][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.02606170065701008, acc: 0.9714285731315613)
[2024-11-08 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:36][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.008998626843094826, acc: 1.0)
[2024-11-08 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:37][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.05161149799823761, acc: 0.9523809552192688)
[2024-11-08 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:37][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.06580493599176407, acc: 0.9473684430122375)
[2024-11-08 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:37][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.001836653333157301, acc: 1.0)
[2024-11-08 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:38][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.02777084894478321, acc: 1.0)
[2024-11-08 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:38][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.0014731022529304028, acc: 1.0)
[2024-11-08 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:38][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.027647316455841064, acc: 1.0)
[2024-11-08 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:38][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.002431437373161316, acc: 1.0)
[2024-11-08 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:39][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.05194702744483948, acc: 0.9599999785423279)
[2024-11-08 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:39][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.014626210555434227, acc: 1.0)
[2024-11-08 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:39][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.0030229631811380386, acc: 1.0)
[2024-11-08 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:40][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.003096590284258127, acc: 1.0)
[2024-11-08 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:40][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.0009483235189691186, acc: 1.0)
[2024-11-08 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:40][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.003898895112797618, acc: 1.0)
[2024-11-08 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:41][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.018247291445732117, acc: 1.0)
[2024-11-08 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:41][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.0801345631480217, acc: 0.9545454382896423)
[2024-11-08 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:41][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.023101074621081352, acc: 1.0)
[2024-11-08 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:42][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.016185324639081955, acc: 1.0)
[2024-11-08 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:42][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.03293852135539055, acc: 1.0)
[2024-11-08 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:42][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.018443556502461433, acc: 1.0)
[2024-11-08 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:43][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.000590163457673043, acc: 1.0)
[2024-11-08 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:43][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.0009525142377242446, acc: 1.0)
[2024-11-08 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:43][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.1624886840581894, acc: 0.9523809552192688)
[2024-11-08 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:44][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.12264394760131836, acc: 0.9523809552192688)
[2024-11-08 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:45][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.08633914589881897, acc: 0.9473684430122375)
[2024-11-08 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:45][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.000575594196561724, acc: 1.0)
[2024-11-08 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:45][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.011899380944669247, acc: 1.0)
[2024-11-08 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:46][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.0004828156961593777, acc: 1.0)
[2024-11-08 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:46][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.00099672912620008, acc: 1.0)
[2024-11-08 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:46][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.0005191219388507307, acc: 1.0)
[2024-11-08 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:47][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.00048784841783344746, acc: 1.0)
[2024-11-08 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:47][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.03334107995033264, acc: 1.0)
[2024-11-08 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:47][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.02011866308748722, acc: 1.0)
[2024-11-08 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:48][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.0051898714154958725, acc: 1.0)
[2024-11-08 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:48][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.026177767664194107, acc: 1.0)
[2024-11-08 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:48][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.0005813061143271625, acc: 1.0)
[2024-11-08 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:49][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.02868019975721836, acc: 1.0)
[2024-11-08 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:49][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.20298439264297485, acc: 0.9428571462631226)
[2024-11-08 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:49][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.0004730855580419302, acc: 1.0)
[2024-11-08 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:50][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.017066150903701782, acc: 1.0)
[2024-11-08 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:50][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.059088826179504395, acc: 0.9473684430122375)
[2024-11-08 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:50][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.29736918210983276, acc: 0.9090909361839294)
[2024-11-08 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:51][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.006357650738209486, acc: 1.0)
[2024-11-08 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:51][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.026735255494713783, acc: 1.0)
[2024-11-08 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:51][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.0016988302813842893, acc: 1.0)
[2024-11-08 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:52][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.001260487362742424, acc: 1.0)
[2024-11-08 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:52][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.000967981293797493, acc: 1.0)
[2024-11-08 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:52][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.004772627260535955, acc: 1.0)
[2024-11-08 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:53][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.1185017004609108, acc: 0.9473684430122375)
[2024-11-08 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:53][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.08110098540782928, acc: 0.949999988079071)
[2024-11-08 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:53][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.014647637493908405, acc: 1.0)
[2024-11-08 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:54][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.16210831701755524, acc: 0.9545454382896423)
[2024-11-08 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:54][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.03450847789645195, acc: 0.9642857313156128)
[2024-11-08 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:57][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.27785563468933105, acc: 0.9523809552192688)
[2024-11-08 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:58][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 0.3073784410953522, acc: 0.9523809552192688)
[2024-11-08 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:58][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.00233651464805007, acc: 1.0)
[2024-11-08 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:59][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.08650919795036316, acc: 0.9545454382896423)
[2024-11-08 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:08:59][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.023147236555814743, acc: 1.0)
[2024-11-08 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:00][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.024546748027205467, acc: 1.0)
[2024-11-08 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:00][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.057932063937187195, acc: 0.9722222089767456)
[2024-11-08 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:01][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.029945218935608864, acc: 1.0)
[2024-11-08 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:01][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.0035368422977626324, acc: 1.0)
[2024-11-08 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:01][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.0035644129384309053, acc: 1.0)
[2024-11-08 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:01][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.025360770523548126, acc: 1.0)
[2024-11-08 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:02][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.08637890964746475, acc: 0.949999988079071)
[2024-11-08 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:02][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.02974218688905239, acc: 1.0)
[2024-11-08 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:02][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.01327038835734129, acc: 1.0)
[2024-11-08 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:03][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.03892819583415985, acc: 1.0)
[2024-11-08 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:03][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.046905383467674255, acc: 0.9666666388511658)
[2024-11-08 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:03][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.022493595257401466, acc: 1.0)
[2024-11-08 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:04][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.25102463364601135, acc: 0.9473684430122375)
[2024-11-08 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:04][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.20430850982666016, acc: 0.949999988079071)
[2024-11-08 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:04][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.08844998478889465, acc: 0.9523809552192688)
[2024-11-08 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:05][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 0.004201782867312431, acc: 1.0)
[2024-11-08 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:05][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.0008590416400693357, acc: 1.0)
[2024-11-08 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:05][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.0011937508825212717, acc: 1.0)
[2024-11-08 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:06][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.012082880362868309, acc: 1.0)
[2024-11-08 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:06][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.0006025762995705009, acc: 1.0)
[2024-11-08 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:06][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.008865909650921822, acc: 1.0)
[2024-11-08 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:06][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.01178833469748497, acc: 1.0)
[2024-11-08 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:07][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.012115441262722015, acc: 1.0)
[2024-11-08 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:07][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.0009443240123800933, acc: 1.0)
[2024-11-08 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:07][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.007791307754814625, acc: 1.0)
[2024-11-08 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:08][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.0009653789456933737, acc: 1.0)
[2024-11-08 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:08][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.25315138697624207, acc: 0.9230769276618958)
[2024-11-08 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:09][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.4129638075828552, acc: 0.9047619104385376)
[2024-11-08 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:10][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.1400674283504486, acc: 0.9473684430122375)
[2024-11-08 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:10][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.3607208728790283, acc: 0.8947368264198303)
[2024-11-08 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:11][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.023650651797652245, acc: 1.0)
[2024-11-08 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:12][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.0360010489821434, acc: 1.0)
[2024-11-08 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:13][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.028095344081521034, acc: 1.0)
[2024-11-08 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:13][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.1111697107553482, acc: 0.9642857313156128)
[2024-11-08 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:13][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.07697802037000656, acc: 0.9523809552192688)
[2024-11-08 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:14][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.05334525927901268, acc: 1.0)
[2024-11-08 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:14][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.4670391082763672, acc: 0.8500000238418579)
[2024-11-08 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:15][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.05416332557797432, acc: 1.0)
[2024-11-08 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:15][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.2634253203868866, acc: 0.8636363744735718)
[2024-11-08 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:15][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.1452796757221222, acc: 0.9523809552192688)
[2024-11-08 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:16][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.036094680428504944, acc: 1.0)
[2024-11-08 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:16][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.03561728447675705, acc: 1.0)
[2024-11-08 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:16][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.0018604847136884928, acc: 1.0)
[2024-11-08 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:17][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.007408295292407274, acc: 1.0)
[2024-11-08 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:17][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.05373794957995415, acc: 0.9473684430122375)
[2024-11-08 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:17][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.18289422988891602, acc: 0.9545454382896423)
[2024-11-08 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:18][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.0063108778558671474, acc: 1.0)
[2024-11-08 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:18][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.002980821533128619, acc: 1.0)
[2024-11-08 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:18][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.01930961199104786, acc: 1.0)
[2024-11-08 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:19][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.1741396188735962, acc: 0.9523809552192688)
[2024-11-08 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:19][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.03106253407895565, acc: 1.0)
[2024-11-08 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:20][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.015732523053884506, acc: 1.0)
[2024-11-08 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:20][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.051363956183195114, acc: 0.9473684430122375)
[2024-11-08 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:20][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.0013303868472576141, acc: 1.0)
[2024-11-08 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:21][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.02464349940419197, acc: 1.0)
[2024-11-08 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:21][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.09266933798789978, acc: 0.9599999785423279)
[2024-11-08 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:21][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.06068996712565422, acc: 0.949999988079071)
[2024-11-08 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:22][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.027418851852416992, acc: 1.0)
[2024-11-08 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:22][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.02743060514330864, acc: 1.0)
[2024-11-08 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:23][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.14732010662555695, acc: 0.9473684430122375)
[2024-11-08 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:23][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.05366051197052002, acc: 0.9545454382896423)
[2024-11-08 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:23][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.039204370230436325, acc: 0.9642857313156128)
[2024-11-08 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:24][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.11510782688856125, acc: 0.9615384340286255)
[2024-11-08 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:24][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.12605325877666473, acc: 0.8999999761581421)
[2024-11-08 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:24][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.09235607832670212, acc: 0.949999988079071)
[2024-11-08 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:25][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.007617777679115534, acc: 1.0)
[2024-11-08 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:55][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3314, device='cuda:0') eval_epoch_loss=tensor(0.2863, device='cuda:0') eval_epoch_acc=tensor(0.9314, device='cuda:0')
[2024-11-08 01:09:55][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:09:55][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:09:55][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_9_step_127_loss_0.28626295924186707/model.pt
[2024-11-08 01:09:55][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:56][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.1349959820508957, acc: 0.9545454382896423)
[2024-11-08 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:56][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.18015147745609283, acc: 0.949999988079071)
[2024-11-08 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:57][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.0012682019732892513, acc: 1.0)
[2024-11-08 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:57][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.07921173423528671, acc: 0.9696969985961914)
[2024-11-08 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:57][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.06761091202497482, acc: 1.0)
[2024-11-08 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:57][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.05906025320291519, acc: 0.9696969985961914)
[2024-11-08 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:58][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.002390623791143298, acc: 1.0)
[2024-11-08 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:58][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.012203310616314411, acc: 1.0)
[2024-11-08 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:58][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.20013165473937988, acc: 0.8947368264198303)
[2024-11-08 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:59][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.061180293560028076, acc: 0.9545454382896423)
[2024-11-08 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:59][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.10945577919483185, acc: 0.949999988079071)
[2024-11-08 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:59][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.051091909408569336, acc: 0.9545454382896423)
[2024-11-08 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:09:59][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.034040164202451706, acc: 0.9696969985961914)
[2024-11-08 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:00][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.005771860014647245, acc: 1.0)
[2024-11-08 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:00][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.01902744174003601, acc: 1.0)
[2024-11-08 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:00][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.0438578799366951, acc: 1.0)
[2024-11-08 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:01][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.015254408121109009, acc: 1.0)
[2024-11-08 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:01][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.020902985706925392, acc: 1.0)
[2024-11-08 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:02][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.024781210348010063, acc: 1.0)
[2024-11-08 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:02][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.0820113942027092, acc: 0.949999988079071)
[2024-11-08 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:02][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.05069525167346001, acc: 0.9545454382896423)
[2024-11-08 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:03][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.007087636739015579, acc: 1.0)
[2024-11-08 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:03][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.0015854629455134273, acc: 1.0)
[2024-11-08 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:03][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.555496096611023, acc: 0.9354838728904724)
[2024-11-08 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:03][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.08868078887462616, acc: 0.949999988079071)
[2024-11-08 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:04][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.00534868473187089, acc: 1.0)
[2024-11-08 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:04][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.034393470734357834, acc: 1.0)
[2024-11-08 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:04][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.009468919597566128, acc: 1.0)
[2024-11-08 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:05][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.031367845833301544, acc: 1.0)
[2024-11-08 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:05][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.09344428032636642, acc: 1.0)
[2024-11-08 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:05][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.3480784296989441, acc: 0.9090909361839294)
[2024-11-08 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:07][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.2712944746017456, acc: 0.8500000238418579)
[2024-11-08 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:07][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 0.12727634608745575, acc: 0.949999988079071)
[2024-11-08 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:08][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.22839227318763733, acc: 0.8947368264198303)
[2024-11-08 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:08][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.06639793515205383, acc: 1.0)
[2024-11-08 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:09][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.4051736891269684, acc: 0.8999999761581421)
[2024-11-08 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:09][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.174268901348114, acc: 0.9599999785423279)
[2024-11-08 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:09][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.012688892893493176, acc: 1.0)
[2024-11-08 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:10][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.17356130480766296, acc: 0.9444444179534912)
[2024-11-08 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:10][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.007057629060000181, acc: 1.0)
[2024-11-08 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:10][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.09135831892490387, acc: 0.9523809552192688)
[2024-11-08 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:11][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.04535342752933502, acc: 0.9473684430122375)
[2024-11-08 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:11][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.0035548978485167027, acc: 1.0)
[2024-11-08 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:12][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.3447186350822449, acc: 0.949999988079071)
[2024-11-08 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:12][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.20806746184825897, acc: 0.9545454382896423)
[2024-11-08 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:13][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.003159906482324004, acc: 1.0)
[2024-11-08 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:13][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.05770478770136833, acc: 0.9666666388511658)
[2024-11-08 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:13][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.2296445667743683, acc: 0.9047619104385376)
[2024-11-08 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:14][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.44900479912757874, acc: 0.8571428656578064)
[2024-11-08 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:14][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.031877048313617706, acc: 1.0)
[2024-11-08 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:15][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.08118444681167603, acc: 0.9545454382896423)
[2024-11-08 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:16][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.10168583691120148, acc: 0.9473684430122375)
[2024-11-08 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:16][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.007059074938297272, acc: 1.0)
[2024-11-08 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:16][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.0048536513932049274, acc: 1.0)
[2024-11-08 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:16][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.015056028962135315, acc: 1.0)
[2024-11-08 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:17][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.0367702841758728, acc: 1.0)
[2024-11-08 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:17][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.013271205127239227, acc: 1.0)
[2024-11-08 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:17][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.01225504744797945, acc: 1.0)
[2024-11-08 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:18][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.021131521090865135, acc: 1.0)
[2024-11-08 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:18][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.1833166778087616, acc: 0.949999988079071)
[2024-11-08 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:19][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.3317881226539612, acc: 0.9047619104385376)
[2024-11-08 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:19][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.02426740527153015, acc: 1.0)
[2024-11-08 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:19][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.19269897043704987, acc: 0.9200000166893005)
[2024-11-08 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:20][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.1760261356830597, acc: 0.8999999761581421)
[2024-11-08 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:21][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.16685934364795685, acc: 0.9523809552192688)
[2024-11-08 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:22][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.33959609270095825, acc: 0.8421052694320679)
[2024-11-08 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:22][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.0174571480602026, acc: 1.0)
[2024-11-08 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:23][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.0561082661151886, acc: 1.0)
[2024-11-08 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:24][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.033913593739271164, acc: 1.0)
[2024-11-08 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:25][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.0031863690819591284, acc: 1.0)
[2024-11-08 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:25][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.014972490258514881, acc: 1.0)
[2024-11-08 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:25][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.03223247826099396, acc: 1.0)
[2024-11-08 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:26][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.009945523925125599, acc: 1.0)
[2024-11-08 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:26][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.003647720441222191, acc: 1.0)
[2024-11-08 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:26][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.17289409041404724, acc: 0.9545454382896423)
[2024-11-08 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:26][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.05425373464822769, acc: 0.949999988079071)
[2024-11-08 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:27][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.2129221111536026, acc: 0.9130434989929199)
[2024-11-08 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:27][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.028953973203897476, acc: 1.0)
[2024-11-08 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:27][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.16104230284690857, acc: 0.9047619104385376)
[2024-11-08 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:28][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.0035472935996949673, acc: 1.0)
[2024-11-08 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:28][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.40982216596603394, acc: 0.9473684430122375)
[2024-11-08 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:28][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.027441196143627167, acc: 1.0)
[2024-11-08 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:29][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.30236712098121643, acc: 0.9047619104385376)
[2024-11-08 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:29][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.05812186002731323, acc: 0.9583333134651184)
[2024-11-08 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:29][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.011000449769198895, acc: 1.0)
[2024-11-08 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:30][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.009552220813930035, acc: 1.0)
[2024-11-08 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:30][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.19059208035469055, acc: 0.9615384340286255)
[2024-11-08 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:31][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.019973427057266235, acc: 1.0)
[2024-11-08 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:31][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.04791612550616264, acc: 1.0)
[2024-11-08 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:31][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.022037558257579803, acc: 1.0)
[2024-11-08 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:32][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.022521305829286575, acc: 1.0)
[2024-11-08 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:32][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.03505915403366089, acc: 1.0)
[2024-11-08 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:32][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.011078380048274994, acc: 1.0)
[2024-11-08 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:33][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.011937170289456844, acc: 1.0)
[2024-11-08 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:33][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.0009286417625844479, acc: 1.0)
[2024-11-08 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:33][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.05029543489217758, acc: 1.0)
[2024-11-08 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:34][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.00609624246135354, acc: 1.0)
[2024-11-08 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:34][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.035072095692157745, acc: 1.0)
[2024-11-08 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:35][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.002458809642121196, acc: 1.0)
[2024-11-08 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:35][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.01704561524093151, acc: 1.0)
[2024-11-08 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:36][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.023282522335648537, acc: 1.0)
[2024-11-08 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:36][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.12664520740509033, acc: 0.9583333134651184)
[2024-11-08 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:36][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.20524634420871735, acc: 0.8260869383811951)
[2024-11-08 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:37][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.1164255291223526, acc: 0.9523809552192688)
[2024-11-08 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:37][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.12355563789606094, acc: 1.0)
[2024-11-08 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:37][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.09210800379514694, acc: 1.0)
[2024-11-08 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:38][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 0.22050441801548004, acc: 0.9090909361839294)
[2024-11-08 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:38][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.2983859181404114, acc: 0.8636363744735718)
[2024-11-08 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:39][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.0985795110464096, acc: 0.9642857313156128)
[2024-11-08 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:39][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.03052862547338009, acc: 0.9629629850387573)
[2024-11-08 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:39][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.07866382598876953, acc: 0.9696969985961914)
[2024-11-08 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:39][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.06344962865114212, acc: 1.0)
[2024-11-08 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:40][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.017631907016038895, acc: 1.0)
[2024-11-08 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:40][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.009116319939494133, acc: 1.0)
[2024-11-08 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:40][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.0017797426553443074, acc: 1.0)
[2024-11-08 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:41][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.031136764213442802, acc: 1.0)
[2024-11-08 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:42][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.023738490417599678, acc: 1.0)
[2024-11-08 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:42][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.0002780551731120795, acc: 1.0)
[2024-11-08 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:42][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.11354439705610275, acc: 0.9666666388511658)
[2024-11-08 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:42][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.010397616773843765, acc: 1.0)
[2024-11-08 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:43][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.002035814570263028, acc: 1.0)
[2024-11-08 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:43][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.3843325078487396, acc: 0.9523809552192688)
[2024-11-08 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:43][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.021103912964463234, acc: 1.0)
[2024-11-08 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:44][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.004095408134162426, acc: 1.0)
[2024-11-08 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:44][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.0363466814160347, acc: 1.0)
[2024-11-08 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:44][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.0006877690902911127, acc: 1.0)
[2024-11-08 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:45][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.006905890069901943, acc: 1.0)
[2024-11-08 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:45][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.007615110371261835, acc: 1.0)
[2024-11-08 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:45][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.020103875547647476, acc: 1.0)
[2024-11-08 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:46][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.0006264977855607867, acc: 1.0)
[2024-11-08 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:46][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.018392503261566162, acc: 1.0)
[2024-11-08 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:46][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.006590439938008785, acc: 1.0)
[2024-11-08 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:47][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.030380891636013985, acc: 1.0)
[2024-11-08 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:47][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.0011419301154091954, acc: 1.0)
[2024-11-08 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:47][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.016735224053263664, acc: 1.0)
[2024-11-08 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:48][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.10641244053840637, acc: 0.970588207244873)
[2024-11-08 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:48][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.01972854509949684, acc: 1.0)
[2024-11-08 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:48][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.003189359325915575, acc: 1.0)
[2024-11-08 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:49][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.015201397240161896, acc: 1.0)
[2024-11-08 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:50][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.009797782637178898, acc: 1.0)
[2024-11-08 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:50][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.01625390723347664, acc: 1.0)
[2024-11-08 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:50][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.1447085291147232, acc: 0.931034505367279)
[2024-11-08 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:51][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.002409466542303562, acc: 1.0)
[2024-11-08 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:19][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3254, device='cuda:0') eval_epoch_loss=tensor(0.2817, device='cuda:0') eval_epoch_acc=tensor(0.9384, device='cuda:0')
[2024-11-08 01:11:19][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:11:19][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:11:20][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_9_step_270_loss_0.2817009687423706/model.pt
[2024-11-08 01:11:20][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:20][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.004549211356788874, acc: 1.0)
[2024-11-08 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:20][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.00035705347545444965, acc: 1.0)
[2024-11-08 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:21][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.00046272575855255127, acc: 1.0)
[2024-11-08 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:21][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.02928340435028076, acc: 1.0)
[2024-11-08 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:21][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.0016526528634130955, acc: 1.0)
[2024-11-08 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:22][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.002796335145831108, acc: 1.0)
[2024-11-08 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:22][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.001513560418970883, acc: 1.0)
[2024-11-08 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:22][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.04775528982281685, acc: 0.9629629850387573)
[2024-11-08 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:23][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.00287770782597363, acc: 1.0)
[2024-11-08 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:23][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.0003181259089615196, acc: 1.0)
[2024-11-08 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:23][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.03727882355451584, acc: 1.0)
[2024-11-08 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:24][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.004606795031577349, acc: 1.0)
[2024-11-08 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:24][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.05500644072890282, acc: 0.9523809552192688)
[2024-11-08 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:24][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.15684570372104645, acc: 0.9655172228813171)
[2024-11-08 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:25][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.1006498858332634, acc: 0.9333333373069763)
[2024-11-08 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:25][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.21032267808914185, acc: 0.931034505367279)
[2024-11-08 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:25][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.002959050238132477, acc: 1.0)
[2024-11-08 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:26][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.04549901559948921, acc: 1.0)
[2024-11-08 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:26][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.019016887992620468, acc: 1.0)
[2024-11-08 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:26][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.09588558226823807, acc: 0.9545454382896423)
[2024-11-08 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:27][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.03759722784161568, acc: 1.0)
[2024-11-08 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:27][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.0004523876996245235, acc: 1.0)
[2024-11-08 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:27][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.026005994528532028, acc: 1.0)
[2024-11-08 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:28][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.02178707905113697, acc: 1.0)
[2024-11-08 01:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:28][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.0030058613047003746, acc: 1.0)
[2024-11-08 01:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:29][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.01937558688223362, acc: 1.0)
[2024-11-08 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:29][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.003030591644346714, acc: 1.0)
[2024-11-08 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:29][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.032932743430137634, acc: 1.0)
[2024-11-08 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:30][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.0010087020928040147, acc: 1.0)
[2024-11-08 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:30][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.049545202404260635, acc: 0.9677419066429138)
[2024-11-08 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:30][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.004099106416106224, acc: 1.0)
[2024-11-08 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:30][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.010355695150792599, acc: 1.0)
[2024-11-08 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:31][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.13917873799800873, acc: 0.9523809552192688)
[2024-11-08 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:31][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.09114524722099304, acc: 1.0)
[2024-11-08 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:31][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.0828080028295517, acc: 0.9473684430122375)
[2024-11-08 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:32][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.02148871123790741, acc: 1.0)
[2024-11-08 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:32][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.2034904509782791, acc: 0.9545454382896423)
[2024-11-08 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:32][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.04495101422071457, acc: 0.9615384340286255)
[2024-11-08 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:33][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.013226967304944992, acc: 1.0)
[2024-11-08 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:33][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.031063776463270187, acc: 1.0)
[2024-11-08 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:33][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.03227439522743225, acc: 1.0)
[2024-11-08 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:34][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.06929133832454681, acc: 0.9545454382896423)
[2024-11-08 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:34][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.015633076429367065, acc: 1.0)
[2024-11-08 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:34][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.0016252666246145964, acc: 1.0)
[2024-11-08 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:35][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.11936122179031372, acc: 0.9629629850387573)
[2024-11-08 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:35][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.14152254164218903, acc: 0.9714285731315613)
[2024-11-08 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:35][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.028218675404787064, acc: 1.0)
[2024-11-08 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:36][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.11531820893287659, acc: 0.9523809552192688)
[2024-11-08 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:36][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.0005966949975118041, acc: 1.0)
[2024-11-08 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:36][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.0007521549123339355, acc: 1.0)
[2024-11-08 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:37][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.019382625818252563, acc: 1.0)
[2024-11-08 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:37][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.005967495031654835, acc: 1.0)
[2024-11-08 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:37][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.3819449841976166, acc: 0.9130434989929199)
[2024-11-08 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:38][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.10142777115106583, acc: 0.9523809552192688)
[2024-11-08 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:38][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.21360750496387482, acc: 0.9473684430122375)
[2024-11-08 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:38][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.4301406145095825, acc: 0.7727272510528564)
[2024-11-08 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:39][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.46029016375541687, acc: 0.8421052694320679)
[2024-11-08 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:39][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.11557132005691528, acc: 0.9583333134651184)
[2024-11-08 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:39][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.09462429583072662, acc: 0.9655172228813171)
[2024-11-08 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:40][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.004340526647865772, acc: 1.0)
[2024-11-08 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:40][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.11859926581382751, acc: 0.9523809552192688)
[2024-11-08 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:40][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.0023480234667658806, acc: 1.0)
[2024-11-08 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:41][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.010903759859502316, acc: 1.0)
[2024-11-08 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:41][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.015037190169095993, acc: 1.0)
[2024-11-08 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:42][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.02654297463595867, acc: 1.0)
[2024-11-08 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:42][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.07505960762500763, acc: 0.9615384340286255)
[2024-11-08 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:42][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.07737906277179718, acc: 0.9583333134651184)
[2024-11-08 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:43][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.10248749703168869, acc: 0.9523809552192688)
[2024-11-08 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:43][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.021562715992331505, acc: 1.0)
[2024-11-08 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:43][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.010785781778395176, acc: 1.0)
[2024-11-08 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:43][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.010072560049593449, acc: 1.0)
[2024-11-08 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:44][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.04161541908979416, acc: 0.9696969985961914)
[2024-11-08 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:44][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.005120228044688702, acc: 1.0)
[2024-11-08 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:44][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.1319347620010376, acc: 0.949999988079071)
[2024-11-08 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:45][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.006049646530300379, acc: 1.0)
[2024-11-08 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:45][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.002371877199038863, acc: 1.0)
[2024-11-08 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:45][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.06708838045597076, acc: 0.949999988079071)
[2024-11-08 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:46][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.0040702191181480885, acc: 1.0)
[2024-11-08 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:46][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.01507148053497076, acc: 1.0)
[2024-11-08 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:46][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.16091589629650116, acc: 0.949999988079071)
[2024-11-08 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:47][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.005517716519534588, acc: 1.0)
[2024-11-08 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:47][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.037878602743148804, acc: 0.9545454382896423)
[2024-11-08 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:47][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.23342029750347137, acc: 0.949999988079071)
[2024-11-08 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:48][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.2072695791721344, acc: 0.9599999785423279)
[2024-11-08 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:48][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.05496703088283539, acc: 0.9599999785423279)
[2024-11-08 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:48][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.003406181465834379, acc: 1.0)
[2024-11-08 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:49][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.08169908821582794, acc: 0.949999988079071)
[2024-11-08 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:49][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.11403517425060272, acc: 0.9523809552192688)
[2024-11-08 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:49][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.044743914157152176, acc: 0.9545454382896423)
[2024-11-08 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:50][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.14199978113174438, acc: 0.9642857313156128)
[2024-11-08 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:50][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.006464206147938967, acc: 1.0)
[2024-11-08 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:50][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.19815748929977417, acc: 0.9714285731315613)
[2024-11-08 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:50][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.15960396826267242, acc: 0.9545454382896423)
[2024-11-08 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:51][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.00535805057734251, acc: 1.0)
[2024-11-08 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:51][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.005928287748247385, acc: 1.0)
[2024-11-08 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:51][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.006369645241647959, acc: 1.0)
[2024-11-08 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:52][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.001350168022327125, acc: 1.0)
[2024-11-08 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:52][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.026883244514465332, acc: 1.0)
[2024-11-08 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:52][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.10047388821840286, acc: 0.96875)
[2024-11-08 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:53][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.005896905902773142, acc: 1.0)
[2024-11-08 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:53][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.18149130046367645, acc: 0.9523809552192688)
[2024-11-08 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:54][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.09645113348960876, acc: 0.9473684430122375)
[2024-11-08 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:54][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.002427031984552741, acc: 1.0)
[2024-11-08 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:55][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.06450667232275009, acc: 0.9473684430122375)
[2024-11-08 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:55][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.016744637861847878, acc: 1.0)
[2024-11-08 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:55][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.054326124489307404, acc: 0.9655172228813171)
[2024-11-08 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:55][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.09398971498012543, acc: 0.9677419066429138)
[2024-11-08 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:56][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.01250867173075676, acc: 1.0)
[2024-11-08 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:56][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.15176986157894135, acc: 0.9523809552192688)
[2024-11-08 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:57][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.049299340695142746, acc: 1.0)
[2024-11-08 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:57][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.19542913138866425, acc: 0.9090909361839294)
[2024-11-08 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:58][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.0011497561354190111, acc: 1.0)
[2024-11-08 01:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:59][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.006215323694050312, acc: 1.0)
[2024-11-08 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:59][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.06958641856908798, acc: 0.9642857313156128)
[2024-11-08 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:59][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.013936094008386135, acc: 1.0)
[2024-11-08 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:11:59][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.024838630110025406, acc: 0.9714285731315613)
[2024-11-08 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:00][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.0006265903357416391, acc: 1.0)
[2024-11-08 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:00][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.0002666005166247487, acc: 1.0)
[2024-11-08 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:00][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.0054183173924684525, acc: 1.0)
[2024-11-08 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:01][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.015939924865961075, acc: 1.0)
[2024-11-08 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:01][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.007653963752090931, acc: 1.0)
[2024-11-08 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:01][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.11492522805929184, acc: 0.9047619104385376)
[2024-11-08 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:02][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.016866410151124, acc: 1.0)
[2024-11-08 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:02][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.020824693143367767, acc: 1.0)
[2024-11-08 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:02][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.13414840400218964, acc: 0.9090909361839294)
[2024-11-08 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:03][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.0275262501090765, acc: 1.0)
[2024-11-08 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:03][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.011600189842283726, acc: 1.0)
[2024-11-08 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:03][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.2261204570531845, acc: 0.8999999761581421)
[2024-11-08 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:04][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.15356552600860596, acc: 0.9047619104385376)
[2024-11-08 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:04][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.008189409971237183, acc: 1.0)
[2024-11-08 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:05][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.012189110741019249, acc: 1.0)
[2024-11-08 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:05][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.004218761343508959, acc: 1.0)
[2024-11-08 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:05][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.0518619567155838, acc: 0.9583333134651184)
[2024-11-08 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:06][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.018966106697916985, acc: 1.0)
[2024-11-08 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:06][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.06269778311252594, acc: 0.9714285731315613)
[2024-11-08 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:06][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.009444399736821651, acc: 1.0)
[2024-11-08 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:06][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.0014373523881658912, acc: 1.0)
[2024-11-08 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:07][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.002304718131199479, acc: 1.0)
[2024-11-08 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:07][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.016631383448839188, acc: 1.0)
[2024-11-08 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:07][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.0525343231856823, acc: 1.0)
[2024-11-08 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:08][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.0019273131620138884, acc: 1.0)
[2024-11-08 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:08][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.004930882714688778, acc: 1.0)
[2024-11-08 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:08][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.016156533733010292, acc: 1.0)
[2024-11-08 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:38][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3671, device='cuda:0') eval_epoch_loss=tensor(0.3127, device='cuda:0') eval_epoch_acc=tensor(0.9298, device='cuda:0')
[2024-11-08 01:12:38][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:12:38][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:12:38][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_9_step_413_loss_0.31265607476234436/model.pt
[2024-11-08 01:12:38][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:38][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.007469767238944769, acc: 1.0)
[2024-11-08 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:39][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.05820145457983017, acc: 0.9615384340286255)
[2024-11-08 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:39][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.050452493131160736, acc: 0.9523809552192688)
[2024-11-08 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:39][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.09549158811569214, acc: 0.9473684430122375)
[2024-11-08 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:40][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.007037419360131025, acc: 1.0)
[2024-11-08 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:40][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.02189069800078869, acc: 1.0)
[2024-11-08 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:40][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.30920758843421936, acc: 0.9047619104385376)
[2024-11-08 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:41][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.008720927871763706, acc: 1.0)
[2024-11-08 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:41][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.019402675330638885, acc: 1.0)
[2024-11-08 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:41][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.022353895008563995, acc: 1.0)
[2024-11-08 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:42][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.01591900922358036, acc: 1.0)
[2024-11-08 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:42][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.0028075508307665586, acc: 1.0)
[2024-11-08 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:42][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.004144860897213221, acc: 1.0)
[2024-11-08 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:43][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.002955178963020444, acc: 1.0)
[2024-11-08 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:43][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.0028458803426474333, acc: 1.0)
[2024-11-08 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:43][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.0015743601834401488, acc: 1.0)
[2024-11-08 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:44][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.09971117228269577, acc: 0.9583333134651184)
[2024-11-08 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:44][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.0225550327450037, acc: 1.0)
[2024-11-08 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:44][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 0.002519647590816021, acc: 1.0)
[2024-11-08 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:44][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.001072199665941298, acc: 1.0)
[2024-11-08 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:45][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.0005334870074875653, acc: 1.0)
[2024-11-08 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:45][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.001505172811448574, acc: 1.0)
[2024-11-08 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:45][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.026770377531647682, acc: 1.0)
[2024-11-08 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:46][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.003359544789418578, acc: 1.0)
[2024-11-08 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:46][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.0007796389400027692, acc: 1.0)
[2024-11-08 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:46][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0006992238922975957, acc: 1.0)
[2024-11-08 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:47][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.009077426977455616, acc: 1.0)
[2024-11-08 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:47][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.023651422932744026, acc: 1.0)
[2024-11-08 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:48][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.0038584887515753508, acc: 1.0)
[2024-11-08 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:48][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.11779466271400452, acc: 0.9473684430122375)
[2024-11-08 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:49][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.008673439733684063, acc: 1.0)
[2024-11-08 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:49][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.04656786099076271, acc: 0.9523809552192688)
[2024-11-08 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:49][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.007690520491451025, acc: 1.0)
[2024-11-08 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:50][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.12770487368106842, acc: 0.9677419066429138)
[2024-11-08 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:50][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.137827068567276, acc: 0.9354838728904724)
[2024-11-08 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:50][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.0010844916105270386, acc: 1.0)
[2024-11-08 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:51][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.0012230371357873082, acc: 1.0)
[2024-11-08 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:51][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.01060955598950386, acc: 1.0)
[2024-11-08 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:51][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.007193340454250574, acc: 1.0)
[2024-11-08 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:52][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.0019620517268776894, acc: 1.0)
[2024-11-08 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:52][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.0081942118704319, acc: 1.0)
[2024-11-08 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:52][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.002768983133137226, acc: 1.0)
[2024-11-08 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:53][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.02455638349056244, acc: 1.0)
[2024-11-08 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:53][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.02175634354352951, acc: 1.0)
[2024-11-08 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:53][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.0023177401162683964, acc: 1.0)
[2024-11-08 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:54][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.06406842172145844, acc: 0.9473684430122375)
[2024-11-08 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:54][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.00015617470489814878, acc: 1.0)
[2024-11-08 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:54][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.003964562900364399, acc: 1.0)
[2024-11-08 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:55][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.002715267241001129, acc: 1.0)
[2024-11-08 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:55][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.012810425832867622, acc: 1.0)
[2024-11-08 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:55][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.0028704223223030567, acc: 1.0)
[2024-11-08 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:56][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.007182473316788673, acc: 1.0)
[2024-11-08 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:56][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.018739454448223114, acc: 1.0)
[2024-11-08 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:56][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.003619783092290163, acc: 1.0)
[2024-11-08 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:56][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.1710304170846939, acc: 0.9473684430122375)
[2024-11-08 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:57][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.00016471384151373059, acc: 1.0)
[2024-11-08 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:57][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.16496369242668152, acc: 0.9523809552192688)
[2024-11-08 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:57][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.0036267254035919905, acc: 1.0)
[2024-11-08 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:58][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.0025201335083693266, acc: 1.0)
[2024-11-08 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:58][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.250004380941391, acc: 0.9166666865348816)
[2024-11-08 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:58][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.10047385841608047, acc: 0.9523809552192688)
[2024-11-08 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:59][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.05058394744992256, acc: 1.0)
[2024-11-08 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:59][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.09924201667308807, acc: 0.949999988079071)
[2024-11-08 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:12:59][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.06109246239066124, acc: 0.9523809552192688)
[2024-11-08 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:00][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.07077208161354065, acc: 0.9545454382896423)
[2024-11-08 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:00][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.0065193772315979, acc: 1.0)
[2024-11-08 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:00][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.0010090251453220844, acc: 1.0)
[2024-11-08 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:01][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.01775651052594185, acc: 1.0)
[2024-11-08 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:01][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.16270938515663147, acc: 0.9523809552192688)
[2024-11-08 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:01][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.1618906855583191, acc: 0.9473684430122375)
[2024-11-08 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:02][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.11322566866874695, acc: 0.9545454382896423)
[2024-11-08 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:02][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.02219591848552227, acc: 1.0)
[2024-11-08 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:02][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.02560754120349884, acc: 1.0)
[2024-11-08 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:02][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.05381112918257713, acc: 0.9629629850387573)
[2024-11-08 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:03][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.1541464775800705, acc: 0.9428571462631226)
[2024-11-08 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:03][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.19469359517097473, acc: 0.8846153616905212)
[2024-11-08 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:04][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.0016102134250104427, acc: 1.0)
[2024-11-08 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:04][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.05442522093653679, acc: 0.9473684430122375)
[2024-11-08 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:04][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.0028858587611466646, acc: 1.0)
[2024-11-08 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:05][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.003070505103096366, acc: 1.0)
[2024-11-08 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:05][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.4687455892562866, acc: 0.9047619104385376)
[2024-11-08 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:05][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.037161264568567276, acc: 1.0)
[2024-11-08 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:05][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.028612045571208, acc: 1.0)
[2024-11-08 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:06][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.07883719354867935, acc: 0.949999988079071)
[2024-11-08 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:06][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.013787418603897095, acc: 1.0)
[2024-11-08 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:06][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.2946215271949768, acc: 0.9473684430122375)
[2024-11-08 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:07][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.018319791182875633, acc: 1.0)
[2024-11-08 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:07][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.10430926084518433, acc: 0.949999988079071)
[2024-11-08 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:07][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.006520659197121859, acc: 1.0)
[2024-11-08 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:08][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.013655842281877995, acc: 1.0)
[2024-11-08 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:08][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.014080078341066837, acc: 1.0)
[2024-11-08 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:08][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.03132808580994606, acc: 1.0)
[2024-11-08 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:09][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.006037052255123854, acc: 1.0)
[2024-11-08 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:09][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.3365873396396637, acc: 0.8421052694320679)
[2024-11-08 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:09][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.008254791609942913, acc: 1.0)
[2024-11-08 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:10][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.31551897525787354, acc: 0.8999999761581421)
[2024-11-08 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:10][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.020266370847821236, acc: 1.0)
[2024-11-08 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:10][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.025373613461852074, acc: 1.0)
[2024-11-08 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:11][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.00340382126159966, acc: 1.0)
[2024-11-08 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:13][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.04433382302522659, acc: 0.9642857313156128)
[2024-11-08 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:14][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.07742670923471451, acc: 0.9523809552192688)
[2024-11-08 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:14][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.2322872132062912, acc: 0.8947368264198303)
[2024-11-08 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:14][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.003750626463443041, acc: 1.0)
[2024-11-08 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:15][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.0065212734043598175, acc: 1.0)
[2024-11-08 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:15][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.0009609665721654892, acc: 1.0)
[2024-11-08 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:15][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.11872889846563339, acc: 0.9642857313156128)
[2024-11-08 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:16][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.16777648031711578, acc: 0.9629629850387573)
[2024-11-08 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:16][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.05416670814156532, acc: 0.9714285731315613)
[2024-11-08 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:17][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.011977924033999443, acc: 1.0)
[2024-11-08 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:17][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.001809317385777831, acc: 1.0)
[2024-11-08 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:18][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.033150628209114075, acc: 1.0)
[2024-11-08 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:18][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.1684129387140274, acc: 0.9545454382896423)
[2024-11-08 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:19][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.2517835199832916, acc: 0.8999999761581421)
[2024-11-08 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:19][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.31011033058166504, acc: 0.9090909361839294)
[2024-11-08 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:19][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.1030036211013794, acc: 0.939393937587738)
[2024-11-08 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:19][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.16799257695674896, acc: 0.9642857313156128)
[2024-11-08 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:20][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.0037418773863464594, acc: 1.0)
[2024-11-08 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:20][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.14739394187927246, acc: 0.9473684430122375)
[2024-11-08 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:20][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.06472812592983246, acc: 1.0)
[2024-11-08 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:21][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.23548004031181335, acc: 0.9523809552192688)
[2024-11-08 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:21][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.009594242088496685, acc: 1.0)
[2024-11-08 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:21][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.07839532196521759, acc: 0.9642857313156128)
[2024-11-08 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:22][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.020666472613811493, acc: 1.0)
[2024-11-08 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:22][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.006343427114188671, acc: 1.0)
[2024-11-08 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:22][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.01795295625925064, acc: 1.0)
[2024-11-08 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:23][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.4199085533618927, acc: 0.8947368264198303)
[2024-11-08 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:23][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.09484201669692993, acc: 1.0)
[2024-11-08 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:23][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.30953067541122437, acc: 0.8571428656578064)
[2024-11-08 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:24][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.10067200660705566, acc: 0.9523809552192688)
[2024-11-08 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:24][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.0007656337693333626, acc: 1.0)
[2024-11-08 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:24][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 0.004576404578983784, acc: 1.0)
[2024-11-08 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:24][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.00525352219119668, acc: 1.0)
[2024-11-08 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:25][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.009138239547610283, acc: 1.0)
[2024-11-08 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:25][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.0014470478054136038, acc: 1.0)
[2024-11-08 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:25][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.007924407720565796, acc: 1.0)
[2024-11-08 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:26][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.1102583184838295, acc: 0.9523809552192688)
[2024-11-08 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:26][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 0.016802551224827766, acc: 1.0)
[2024-11-08 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:26][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.0014262208715081215, acc: 1.0)
[2024-11-08 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:27][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.03112829104065895, acc: 1.0)
[2024-11-08 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:27][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.0021283579990267754, acc: 1.0)
[2024-11-08 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:27][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.17151620984077454, acc: 0.9523809552192688)
[2024-11-08 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:28][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.013694323599338531, acc: 1.0)
[2024-11-08 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:28][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.00421144999563694, acc: 1.0)
[2024-11-08 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:57][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4039, device='cuda:0') eval_epoch_loss=tensor(0.3393, device='cuda:0') eval_epoch_acc=tensor(0.9214, device='cuda:0')
[2024-11-08 01:13:57][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:13:57][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:13:57][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_9_step_556_loss_0.33926618099212646/model.pt
[2024-11-08 01:13:57][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:57][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.0731012374162674, acc: 0.9523809552192688)
[2024-11-08 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:58][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.009547267109155655, acc: 1.0)
[2024-11-08 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:58][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.017618048936128616, acc: 1.0)
[2024-11-08 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:58][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.11824392527341843, acc: 0.9629629850387573)
[2024-11-08 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:59][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.007279453333467245, acc: 1.0)
[2024-11-08 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:59][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.013010402210056782, acc: 1.0)
[2024-11-08 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:13:59][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.002732550259679556, acc: 1.0)
[2024-11-08 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:00][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.0495852492749691, acc: 0.9444444179534912)
[2024-11-08 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:00][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.053963351994752884, acc: 1.0)
[2024-11-08 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:00][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.16365784406661987, acc: 0.949999988079071)
[2024-11-08 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:01][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.0058710332959890366, acc: 1.0)
[2024-11-08 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:01][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.0031668986193835735, acc: 1.0)
[2024-11-08 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:01][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.003491033799946308, acc: 1.0)
[2024-11-08 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:02][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.002557584084570408, acc: 1.0)
[2024-11-08 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:02][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.0005818236968480051, acc: 1.0)
[2024-11-08 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:02][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.003889871295541525, acc: 1.0)
[2024-11-08 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:03][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.010051850229501724, acc: 1.0)
[2024-11-08 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:03][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.019825255498290062, acc: 1.0)
[2024-11-08 01:14:03][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.0668, train_epoch_loss=0.0647, epoch time 329.24431420862675s
[2024-11-08 01:14:03][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 01:14:03][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 01:14:03][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 01:14:03][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 01:14:03][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:04][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.002793863881379366, acc: 1.0)
[2024-11-08 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:05][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.021672604605555534, acc: 1.0)
[2024-11-08 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:05][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.14638234674930573, acc: 0.9714285731315613)
[2024-11-08 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:05][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.02600465528666973, acc: 1.0)
[2024-11-08 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:06][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.014237523078918457, acc: 1.0)
[2024-11-08 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:06][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.010045478120446205, acc: 1.0)
[2024-11-08 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:06][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.009488399140536785, acc: 1.0)
[2024-11-08 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:07][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.02903095632791519, acc: 1.0)
[2024-11-08 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:07][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.0028629025910049677, acc: 1.0)
[2024-11-08 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:07][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.016396088525652885, acc: 1.0)
[2024-11-08 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:08][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.007338241674005985, acc: 1.0)
[2024-11-08 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:08][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.04045997932553291, acc: 1.0)
[2024-11-08 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:08][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.038911961019039154, acc: 0.949999988079071)
[2024-11-08 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:09][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.006442935671657324, acc: 1.0)
[2024-11-08 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:09][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.13232854008674622, acc: 0.95652174949646)
[2024-11-08 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:09][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.004800297785550356, acc: 1.0)
[2024-11-08 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:10][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.047982487827539444, acc: 0.9642857313156128)
[2024-11-08 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:10][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.007501781918108463, acc: 1.0)
[2024-11-08 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:10][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.032854363322257996, acc: 1.0)
[2024-11-08 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:10][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.019024670124053955, acc: 1.0)
[2024-11-08 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:11][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.01798248663544655, acc: 1.0)
[2024-11-08 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:11][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.05290723964571953, acc: 1.0)
[2024-11-08 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:12][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.010385625064373016, acc: 1.0)
[2024-11-08 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:12][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.0005434833001345396, acc: 1.0)
[2024-11-08 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:12][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.011504673399031162, acc: 1.0)
[2024-11-08 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:13][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.007004793267697096, acc: 1.0)
[2024-11-08 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:13][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.11310354620218277, acc: 0.9523809552192688)
[2024-11-08 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:14][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.007369035389274359, acc: 1.0)
[2024-11-08 01:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:14][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.0009021162986755371, acc: 1.0)
[2024-11-08 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:15][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.10438202321529388, acc: 0.949999988079071)
[2024-11-08 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:15][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.0031228126026690006, acc: 1.0)
[2024-11-08 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:16][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.002472194377332926, acc: 1.0)
[2024-11-08 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:16][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.017743801698088646, acc: 1.0)
[2024-11-08 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:16][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0004852908896282315, acc: 1.0)
[2024-11-08 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:17][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.06856870651245117, acc: 0.9523809552192688)
[2024-11-08 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:17][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.025865333154797554, acc: 1.0)
[2024-11-08 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:17][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.05346032232046127, acc: 0.949999988079071)
[2024-11-08 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:18][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.0019622978288680315, acc: 1.0)
[2024-11-08 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:18][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.0009848166955634952, acc: 1.0)
[2024-11-08 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:18][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.01574776880443096, acc: 1.0)
[2024-11-08 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:19][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.1212722510099411, acc: 0.9428571462631226)
[2024-11-08 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:19][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.03717977926135063, acc: 1.0)
[2024-11-08 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:19][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.09292450547218323, acc: 1.0)
[2024-11-08 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:20][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.030534973368048668, acc: 1.0)
[2024-11-08 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:20][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.27086764574050903, acc: 0.9090909361839294)
[2024-11-08 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:20][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.19907166063785553, acc: 0.9523809552192688)
[2024-11-08 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:21][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.14917117357254028, acc: 0.9583333134651184)
[2024-11-08 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:21][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.0016689090989530087, acc: 1.0)
[2024-11-08 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:21][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.0048665981739759445, acc: 1.0)
[2024-11-08 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:22][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.004802865907549858, acc: 1.0)
[2024-11-08 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:22][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.02261979505419731, acc: 1.0)
[2024-11-08 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:22][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.010392769239842892, acc: 1.0)
[2024-11-08 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:23][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.34305340051651, acc: 0.8999999761581421)
[2024-11-08 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:23][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.0039907642640173435, acc: 1.0)
[2024-11-08 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:23][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.11923260986804962, acc: 0.9545454382896423)
[2024-11-08 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:24][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.12288541346788406, acc: 0.9642857313156128)
[2024-11-08 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:26][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.08930331468582153, acc: 0.9523809552192688)
[2024-11-08 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:27][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.0047473912127316, acc: 1.0)
[2024-11-08 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:28][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.03981181979179382, acc: 1.0)
[2024-11-08 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:29][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.01567849889397621, acc: 1.0)
[2024-11-08 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:29][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.01756938360631466, acc: 1.0)
[2024-11-08 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:29][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.005477394908666611, acc: 1.0)
[2024-11-08 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:30][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.0008780361386016011, acc: 1.0)
[2024-11-08 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:30][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.282468318939209, acc: 0.9642857313156128)
[2024-11-08 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:30][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.01781090721487999, acc: 1.0)
[2024-11-08 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:31][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.01544198114424944, acc: 1.0)
[2024-11-08 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:31][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.005829362664371729, acc: 1.0)
[2024-11-08 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:31][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.09036310017108917, acc: 1.0)
[2024-11-08 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:32][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.009721706621348858, acc: 1.0)
[2024-11-08 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:32][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.0021102603059262037, acc: 1.0)
[2024-11-08 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:32][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.0218211617320776, acc: 1.0)
[2024-11-08 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:33][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.04700183495879173, acc: 1.0)
[2024-11-08 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:33][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.06564079225063324, acc: 0.9523809552192688)
[2024-11-08 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:33][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.027010751888155937, acc: 1.0)
[2024-11-08 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:34][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.17915216088294983, acc: 0.949999988079071)
[2024-11-08 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:34][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.12859351933002472, acc: 0.9523809552192688)
[2024-11-08 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:34][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.012308258563280106, acc: 1.0)
[2024-11-08 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:35][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.00591894518584013, acc: 1.0)
[2024-11-08 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:35][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.004799296613782644, acc: 1.0)
[2024-11-08 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:35][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.07543482631444931, acc: 0.9714285731315613)
[2024-11-08 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:36][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.0005138119449838996, acc: 1.0)
[2024-11-08 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:36][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.0643363818526268, acc: 0.9523809552192688)
[2024-11-08 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:36][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.08143872022628784, acc: 0.9473684430122375)
[2024-11-08 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:37][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.005780070088803768, acc: 1.0)
[2024-11-08 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:37][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.11461734771728516, acc: 0.9545454382896423)
[2024-11-08 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:37][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.004884032532572746, acc: 1.0)
[2024-11-08 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:38][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.1281382292509079, acc: 0.9583333134651184)
[2024-11-08 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:38][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.038592930883169174, acc: 1.0)
[2024-11-08 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:38][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.09645619988441467, acc: 0.9047619104385376)
[2024-11-08 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:40][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.023907369002699852, acc: 1.0)
[2024-11-08 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:40][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.07303580641746521, acc: 0.9473684430122375)
[2024-11-08 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:41][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.019187236204743385, acc: 1.0)
[2024-11-08 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:42][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.03272903710603714, acc: 1.0)
[2024-11-08 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:43][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.11155760288238525, acc: 0.9615384340286255)
[2024-11-08 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:43][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.13234369456768036, acc: 0.9285714030265808)
[2024-11-08 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:43][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.02378515526652336, acc: 1.0)
[2024-11-08 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:44][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.014849665574729443, acc: 1.0)
[2024-11-08 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:44][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.19781222939491272, acc: 0.949999988079071)
[2024-11-08 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:45][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.13797515630722046, acc: 0.9523809552192688)
[2024-11-08 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:45][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.03898769989609718, acc: 1.0)
[2024-11-08 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:45][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.055629927664995193, acc: 0.9523809552192688)
[2024-11-08 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:45][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.2062322050333023, acc: 0.9599999785423279)
[2024-11-08 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:46][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.0026798334438353777, acc: 1.0)
[2024-11-08 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:46][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.03550638630986214, acc: 1.0)
[2024-11-08 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:47][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.15094836056232452, acc: 0.9090909361839294)
[2024-11-08 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:47][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.010373338125646114, acc: 1.0)
[2024-11-08 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:48][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.09162311255931854, acc: 0.9545454382896423)
[2024-11-08 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:48][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.004222989547997713, acc: 1.0)
[2024-11-08 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:48][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.08100369572639465, acc: 0.9333333373069763)
[2024-11-08 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:49][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.12054675817489624, acc: 0.9523809552192688)
[2024-11-08 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:49][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.0009755409555509686, acc: 1.0)
[2024-11-08 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:49][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.05672149360179901, acc: 1.0)
[2024-11-08 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:50][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.00439992593601346, acc: 1.0)
[2024-11-08 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:50][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.09838662296533585, acc: 0.9473684430122375)
[2024-11-08 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:50][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.0015564513159915805, acc: 1.0)
[2024-11-08 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:51][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.0027544547338038683, acc: 1.0)
[2024-11-08 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:51][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.06256666034460068, acc: 0.9599999785423279)
[2024-11-08 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:52][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.004811067134141922, acc: 1.0)
[2024-11-08 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:52][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.00044183581485413015, acc: 1.0)
[2024-11-08 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:53][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.003167513059452176, acc: 1.0)
[2024-11-08 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:53][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.021030401811003685, acc: 1.0)
[2024-11-08 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:53][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.0909116268157959, acc: 0.9090909361839294)
[2024-11-08 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:54][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.14893671870231628, acc: 0.9642857313156128)
[2024-11-08 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:54][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.36430788040161133, acc: 0.9230769276618958)
[2024-11-08 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:54][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.28769415616989136, acc: 0.8500000238418579)
[2024-11-08 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:24][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4150, device='cuda:0') eval_epoch_loss=tensor(0.3471, device='cuda:0') eval_epoch_acc=tensor(0.9242, device='cuda:0')
[2024-11-08 01:15:24][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:15:24][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:15:25][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_125_loss_0.3471086621284485/model.pt
[2024-11-08 01:15:25][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:25][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.06124722212553024, acc: 0.949999988079071)
[2024-11-08 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:25][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.0010138597572222352, acc: 1.0)
[2024-11-08 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:26][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.13971085846424103, acc: 0.9545454382896423)
[2024-11-08 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:26][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.142696812748909, acc: 0.8999999761581421)
[2024-11-08 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:26][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.02050989679992199, acc: 1.0)
[2024-11-08 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:27][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.06451143324375153, acc: 0.9696969985961914)
[2024-11-08 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:27][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.0703224241733551, acc: 0.9629629850387573)
[2024-11-08 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:27][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.19560958445072174, acc: 0.939393937587738)
[2024-11-08 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:28][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.1740601658821106, acc: 0.949999988079071)
[2024-11-08 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:28][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.03358473628759384, acc: 1.0)
[2024-11-08 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:28][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.04302763566374779, acc: 1.0)
[2024-11-08 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:29][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.007096476852893829, acc: 1.0)
[2024-11-08 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:29][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.28094619512557983, acc: 0.8500000238418579)
[2024-11-08 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:29][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.006331691052764654, acc: 1.0)
[2024-11-08 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:30][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.006540204398334026, acc: 1.0)
[2024-11-08 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:30][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.06398317962884903, acc: 0.9629629850387573)
[2024-11-08 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:30][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.03378892317414284, acc: 0.9696969985961914)
[2024-11-08 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:31][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.025092247873544693, acc: 1.0)
[2024-11-08 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:31][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.03627317398786545, acc: 1.0)
[2024-11-08 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:31][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.05708293616771698, acc: 0.9473684430122375)
[2024-11-08 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:32][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.022891225293278694, acc: 1.0)
[2024-11-08 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:32][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.05693645402789116, acc: 0.949999988079071)
[2024-11-08 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:33][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.3780047595500946, acc: 0.9090909361839294)
[2024-11-08 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:33][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.10268127918243408, acc: 0.9696969985961914)
[2024-11-08 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:33][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.001494865631684661, acc: 1.0)
[2024-11-08 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:33][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.005195485427975655, acc: 1.0)
[2024-11-08 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:34][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.059097737073898315, acc: 0.949999988079071)
[2024-11-08 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:34][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.08930110931396484, acc: 0.949999988079071)
[2024-11-08 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:34][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.017346009612083435, acc: 1.0)
[2024-11-08 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:35][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.002473178319633007, acc: 1.0)
[2024-11-08 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:35][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.00768375163897872, acc: 1.0)
[2024-11-08 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:35][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.150496706366539, acc: 0.9545454382896423)
[2024-11-08 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:36][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.3555823266506195, acc: 0.8787878751754761)
[2024-11-08 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:37][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.43319469690322876, acc: 0.8999999761581421)
[2024-11-08 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:38][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.06836891174316406, acc: 0.949999988079071)
[2024-11-08 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:38][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.05623212084174156, acc: 1.0)
[2024-11-08 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:39][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.10815176367759705, acc: 0.9545454382896423)
[2024-11-08 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:39][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.3296389579772949, acc: 0.8999999761581421)
[2024-11-08 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:40][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.1808519810438156, acc: 0.9200000166893005)
[2024-11-08 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:40][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.010701826773583889, acc: 1.0)
[2024-11-08 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:40][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.1834079474210739, acc: 0.9444444179534912)
[2024-11-08 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:40][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.00956463348120451, acc: 1.0)
[2024-11-08 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:41][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.09438242018222809, acc: 0.9523809552192688)
[2024-11-08 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:41][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.07473438233137131, acc: 0.9473684430122375)
[2024-11-08 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:41][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.0016046599484980106, acc: 1.0)
[2024-11-08 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:42][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.12167664617300034, acc: 0.949999988079071)
[2024-11-08 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:43][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.006836576387286186, acc: 1.0)
[2024-11-08 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:43][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.12084905803203583, acc: 0.9696969985961914)
[2024-11-08 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:43][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.011623620055615902, acc: 1.0)
[2024-11-08 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:44][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.09898684173822403, acc: 0.9523809552192688)
[2024-11-08 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:44][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.1301046758890152, acc: 0.9523809552192688)
[2024-11-08 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:44][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.0009095747373066843, acc: 1.0)
[2024-11-08 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:45][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.043871041387319565, acc: 1.0)
[2024-11-08 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:46][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.09924397617578506, acc: 0.9473684430122375)
[2024-11-08 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:46][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.06463521718978882, acc: 0.9545454382896423)
[2024-11-08 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:47][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.0027766209095716476, acc: 1.0)
[2024-11-08 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:47][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.03214766085147858, acc: 1.0)
[2024-11-08 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:47][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.012563923373818398, acc: 1.0)
[2024-11-08 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:48][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.005560333840548992, acc: 1.0)
[2024-11-08 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:48][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.009335909970104694, acc: 1.0)
[2024-11-08 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:48][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.003657284891232848, acc: 1.0)
[2024-11-08 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:49][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.006344686262309551, acc: 1.0)
[2024-11-08 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:49][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.009748471900820732, acc: 1.0)
[2024-11-08 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:50][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.010852248407900333, acc: 1.0)
[2024-11-08 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:50][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.14829972386360168, acc: 0.9200000166893005)
[2024-11-08 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:50][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.016557618975639343, acc: 1.0)
[2024-11-08 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:51][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.1732664704322815, acc: 0.9047619104385376)
[2024-11-08 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:52][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.024964606389403343, acc: 1.0)
[2024-11-08 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:53][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.12384950369596481, acc: 0.9545454382896423)
[2024-11-08 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:54][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.07159586995840073, acc: 1.0)
[2024-11-08 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:55][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.0026300959289073944, acc: 1.0)
[2024-11-08 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:55][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.004208150319755077, acc: 1.0)
[2024-11-08 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:55][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.18284179270267487, acc: 0.9333333373069763)
[2024-11-08 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:56][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.0022754031233489513, acc: 1.0)
[2024-11-08 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:56][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.0687030479311943, acc: 0.949999988079071)
[2024-11-08 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:56][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.0015310198068618774, acc: 1.0)
[2024-11-08 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:56][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.01808970980346203, acc: 1.0)
[2024-11-08 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:57][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.011885570362210274, acc: 1.0)
[2024-11-08 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:57][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.034359320998191833, acc: 0.95652174949646)
[2024-11-08 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:57][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.1908138543367386, acc: 0.9642857313156128)
[2024-11-08 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:58][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.0005989274359308183, acc: 1.0)
[2024-11-08 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:58][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.010491185821592808, acc: 1.0)
[2024-11-08 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:58][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.01815553940832615, acc: 1.0)
[2024-11-08 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:59][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.17920812964439392, acc: 0.9090909361839294)
[2024-11-08 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:59][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.006823301315307617, acc: 1.0)
[2024-11-08 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:15:59][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.027175046503543854, acc: 1.0)
[2024-11-08 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:00][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.04703836143016815, acc: 0.9677419066429138)
[2024-11-08 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:00][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.02408820204436779, acc: 1.0)
[2024-11-08 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:00][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.19187019765377045, acc: 0.9615384340286255)
[2024-11-08 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:01][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.1088055670261383, acc: 0.9523809552192688)
[2024-11-08 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:01][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.0012956307036802173, acc: 1.0)
[2024-11-08 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:02][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.005488293245434761, acc: 1.0)
[2024-11-08 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:02][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.007169515360146761, acc: 1.0)
[2024-11-08 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:03][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.026872048154473305, acc: 1.0)
[2024-11-08 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:03][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.04577486217021942, acc: 1.0)
[2024-11-08 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:03][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.003826629603281617, acc: 1.0)
[2024-11-08 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:03][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.0037388482596725225, acc: 1.0)
[2024-11-08 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:04][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.005198412574827671, acc: 1.0)
[2024-11-08 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:04][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.08853210508823395, acc: 0.9523809552192688)
[2024-11-08 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:05][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.28475865721702576, acc: 0.9473684430122375)
[2024-11-08 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:05][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.024307209998369217, acc: 1.0)
[2024-11-08 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:06][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.020546823740005493, acc: 1.0)
[2024-11-08 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:06][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.1329474002122879, acc: 0.9047619104385376)
[2024-11-08 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:07][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.18752039968967438, acc: 0.9166666865348816)
[2024-11-08 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:07][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.18819750845432281, acc: 0.8695651888847351)
[2024-11-08 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:07][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.03994380682706833, acc: 1.0)
[2024-11-08 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:08][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.042912136763334274, acc: 1.0)
[2024-11-08 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:08][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.0029119616374373436, acc: 1.0)
[2024-11-08 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:08][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 0.11141650378704071, acc: 0.9545454382896423)
[2024-11-08 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:09][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.23806244134902954, acc: 0.8181818127632141)
[2024-11-08 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:09][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.011243432760238647, acc: 1.0)
[2024-11-08 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:09][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.35488075017929077, acc: 0.9629629850387573)
[2024-11-08 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:10][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.5365272164344788, acc: 0.8787878751754761)
[2024-11-08 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:10][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.18674592673778534, acc: 0.9090909361839294)
[2024-11-08 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:10][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.10179655998945236, acc: 0.9523809552192688)
[2024-11-08 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:11][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.02667546644806862, acc: 1.0)
[2024-11-08 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:11][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.09909052401781082, acc: 0.9545454382896423)
[2024-11-08 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:12][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.07174486666917801, acc: 1.0)
[2024-11-08 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:12][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.005576919764280319, acc: 1.0)
[2024-11-08 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:13][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.0008209854713641107, acc: 1.0)
[2024-11-08 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:13][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.06132691353559494, acc: 0.9666666388511658)
[2024-11-08 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:13][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.010272059589624405, acc: 1.0)
[2024-11-08 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:13][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.09380555152893066, acc: 0.95652174949646)
[2024-11-08 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:14][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.02021089754998684, acc: 1.0)
[2024-11-08 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:14][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.007719993591308594, acc: 1.0)
[2024-11-08 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:15][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.18173399567604065, acc: 0.949999988079071)
[2024-11-08 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:15][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.12626345455646515, acc: 0.9523809552192688)
[2024-11-08 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:15][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.003939126618206501, acc: 1.0)
[2024-11-08 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:15][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.01969864033162594, acc: 1.0)
[2024-11-08 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:16][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.002327150898054242, acc: 1.0)
[2024-11-08 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:16][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.01409394945949316, acc: 1.0)
[2024-11-08 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:16][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.011875529773533344, acc: 1.0)
[2024-11-08 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:17][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.3625834882259369, acc: 0.9523809552192688)
[2024-11-08 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:17][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.0022671481128782034, acc: 1.0)
[2024-11-08 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:18][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.03233722224831581, acc: 1.0)
[2024-11-08 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:18][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.08310776203870773, acc: 0.9473684430122375)
[2024-11-08 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:18][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.1073840856552124, acc: 0.9545454382896423)
[2024-11-08 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:19][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.013985172845423222, acc: 1.0)
[2024-11-08 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:19][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.018358515575528145, acc: 1.0)
[2024-11-08 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:19][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.018064219504594803, acc: 1.0)
[2024-11-08 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:20][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.1703159511089325, acc: 0.9473684430122375)
[2024-11-08 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:21][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.0692434310913086, acc: 0.95652174949646)
[2024-11-08 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:21][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.017877165228128433, acc: 1.0)
[2024-11-08 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:50][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3102, device='cuda:0') eval_epoch_loss=tensor(0.2702, device='cuda:0') eval_epoch_acc=tensor(0.9306, device='cuda:0')
[2024-11-08 01:16:50][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:16:50][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:16:50][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_268_loss_0.27018120884895325/model.pt
[2024-11-08 01:16:50][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:51][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.02316063828766346, acc: 1.0)
[2024-11-08 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:51][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.19093266129493713, acc: 0.9629629850387573)
[2024-11-08 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:52][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.031167171895503998, acc: 1.0)
[2024-11-08 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:52][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.04910830035805702, acc: 0.9523809552192688)
[2024-11-08 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:52][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.00142964581027627, acc: 1.0)
[2024-11-08 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:53][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.004791111219674349, acc: 1.0)
[2024-11-08 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:53][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.13211552798748016, acc: 0.9545454382896423)
[2024-11-08 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:53][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.015674110502004623, acc: 1.0)
[2024-11-08 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:54][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.002863002475351095, acc: 1.0)
[2024-11-08 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:54][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.0131240738555789, acc: 1.0)
[2024-11-08 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:54][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.007899404503405094, acc: 1.0)
[2024-11-08 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:54][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.0048096138052642345, acc: 1.0)
[2024-11-08 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:55][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.01145889237523079, acc: 1.0)
[2024-11-08 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:55][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.026172196492552757, acc: 1.0)
[2024-11-08 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:55][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.02332647144794464, acc: 1.0)
[2024-11-08 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:56][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.10952144861221313, acc: 0.9655172228813171)
[2024-11-08 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:56][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.13236258924007416, acc: 0.8999999761581421)
[2024-11-08 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:56][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.04061755910515785, acc: 0.9655172228813171)
[2024-11-08 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:57][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.008267071098089218, acc: 1.0)
[2024-11-08 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:57][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.011609146371483803, acc: 1.0)
[2024-11-08 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:57][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.10902382433414459, acc: 0.9473684430122375)
[2024-11-08 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:58][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.014172464609146118, acc: 1.0)
[2024-11-08 01:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:58][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.06257104128599167, acc: 0.9545454382896423)
[2024-11-08 01:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:58][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.0022008689120411873, acc: 1.0)
[2024-11-08 01:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:59][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.1452363133430481, acc: 0.9333333373069763)
[2024-11-08 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:59][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.04603946954011917, acc: 1.0)
[2024-11-08 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:16:59][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.006290482822805643, acc: 1.0)
[2024-11-08 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:00][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.04056901857256889, acc: 1.0)
[2024-11-08 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:00][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.005169850308448076, acc: 1.0)
[2024-11-08 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:01][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.09137004613876343, acc: 0.9523809552192688)
[2024-11-08 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:01][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.005067850928753614, acc: 1.0)
[2024-11-08 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:01][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.008052765391767025, acc: 1.0)
[2024-11-08 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:01][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.010832669213414192, acc: 1.0)
[2024-11-08 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:02][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.013061410747468472, acc: 1.0)
[2024-11-08 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:02][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.00679996469989419, acc: 1.0)
[2024-11-08 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:02][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.010416090488433838, acc: 1.0)
[2024-11-08 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:03][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.006710903253406286, acc: 1.0)
[2024-11-08 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:03][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.14436490833759308, acc: 0.95652174949646)
[2024-11-08 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:03][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.045516494661569595, acc: 0.9545454382896423)
[2024-11-08 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:04][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.05136473849415779, acc: 0.9615384340286255)
[2024-11-08 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:04][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.006663729902356863, acc: 1.0)
[2024-11-08 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:04][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.048136722296476364, acc: 1.0)
[2024-11-08 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:05][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.007946452125906944, acc: 1.0)
[2024-11-08 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:05][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.00791165977716446, acc: 1.0)
[2024-11-08 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:05][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.0755763128399849, acc: 0.9545454382896423)
[2024-11-08 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:06][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.0014221370220184326, acc: 1.0)
[2024-11-08 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:06][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.08003105223178864, acc: 0.9629629850387573)
[2024-11-08 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:06][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.030994050204753876, acc: 0.9714285731315613)
[2024-11-08 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:07][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.1851784586906433, acc: 0.9615384340286255)
[2024-11-08 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:07][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.00015725371486041695, acc: 1.0)
[2024-11-08 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:07][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.003696303814649582, acc: 1.0)
[2024-11-08 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:08][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.009101380594074726, acc: 1.0)
[2024-11-08 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:08][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.01640903577208519, acc: 1.0)
[2024-11-08 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:08][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.00027803427656181157, acc: 1.0)
[2024-11-08 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:09][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.18809041380882263, acc: 0.95652174949646)
[2024-11-08 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:09][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.22389356791973114, acc: 0.9047619104385376)
[2024-11-08 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:09][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.1554940640926361, acc: 0.9473684430122375)
[2024-11-08 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:10][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.1603613793849945, acc: 0.9545454382896423)
[2024-11-08 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:10][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.10751773416996002, acc: 0.9473684430122375)
[2024-11-08 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:10][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.21966856718063354, acc: 0.9583333134651184)
[2024-11-08 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:10][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.02959849312901497, acc: 0.9655172228813171)
[2024-11-08 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:11][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.004087439738214016, acc: 1.0)
[2024-11-08 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:11][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.01253565028309822, acc: 1.0)
[2024-11-08 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:11][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.000595015415456146, acc: 1.0)
[2024-11-08 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:12][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.06641469150781631, acc: 0.9545454382896423)
[2024-11-08 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:12][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.015951883047819138, acc: 1.0)
[2024-11-08 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:12][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.18833889067173004, acc: 0.9545454382896423)
[2024-11-08 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:13][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.026282908394932747, acc: 1.0)
[2024-11-08 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:13][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.010596307925879955, acc: 1.0)
[2024-11-08 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:13][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.01740267500281334, acc: 1.0)
[2024-11-08 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:14][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.22028520703315735, acc: 0.9130434989929199)
[2024-11-08 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:14][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.10817086696624756, acc: 0.9545454382896423)
[2024-11-08 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:14][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.00225614826194942, acc: 1.0)
[2024-11-08 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:15][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.003210444236174226, acc: 1.0)
[2024-11-08 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:15][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.04003448784351349, acc: 0.949999988079071)
[2024-11-08 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:15][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.13677266240119934, acc: 0.949999988079071)
[2024-11-08 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:15][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.017722653225064278, acc: 1.0)
[2024-11-08 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:16][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.048168398439884186, acc: 1.0)
[2024-11-08 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:16][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.002109526190906763, acc: 1.0)
[2024-11-08 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:16][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.002838993212208152, acc: 1.0)
[2024-11-08 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:17][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.09472696483135223, acc: 0.95652174949646)
[2024-11-08 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:17][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.006079505197703838, acc: 1.0)
[2024-11-08 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:18][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.05809127539396286, acc: 0.9473684430122375)
[2024-11-08 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:18][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.0034319693222641945, acc: 1.0)
[2024-11-08 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:18][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.005221293307840824, acc: 1.0)
[2024-11-08 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:19][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.06724271923303604, acc: 0.9599999785423279)
[2024-11-08 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:19][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.456400990486145, acc: 0.8799999952316284)
[2024-11-08 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:19][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.08176326006650925, acc: 0.9523809552192688)
[2024-11-08 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:20][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.009389237500727177, acc: 1.0)
[2024-11-08 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:20][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.10292597860097885, acc: 0.9523809552192688)
[2024-11-08 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:20][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.008785179816186428, acc: 1.0)
[2024-11-08 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:21][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.02625301666557789, acc: 1.0)
[2024-11-08 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:21][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.04054325446486473, acc: 1.0)
[2024-11-08 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:21][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.12181321531534195, acc: 0.9428571462631226)
[2024-11-08 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:22][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.0049741524271667, acc: 1.0)
[2024-11-08 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:22][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.013171839527785778, acc: 1.0)
[2024-11-08 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:22][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.008172865025699139, acc: 1.0)
[2024-11-08 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:23][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.0025572306476533413, acc: 1.0)
[2024-11-08 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:23][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.0038130702450871468, acc: 1.0)
[2024-11-08 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:23][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.05722194164991379, acc: 0.9629629850387573)
[2024-11-08 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:24][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.02834143117070198, acc: 1.0)
[2024-11-08 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:24][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.008167995139956474, acc: 1.0)
[2024-11-08 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:25][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.017177822068333626, acc: 1.0)
[2024-11-08 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:25][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.008726089261472225, acc: 1.0)
[2024-11-08 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:26][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.03075973503291607, acc: 1.0)
[2024-11-08 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:26][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.0040984321385622025, acc: 1.0)
[2024-11-08 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:26][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.03671935945749283, acc: 1.0)
[2024-11-08 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:27][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.006372929085046053, acc: 1.0)
[2024-11-08 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:27][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.0021375135984271765, acc: 1.0)
[2024-11-08 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:27][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.03184700012207031, acc: 1.0)
[2024-11-08 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:28][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.00403182627633214, acc: 1.0)
[2024-11-08 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:28][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.03946712240576744, acc: 1.0)
[2024-11-08 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:29][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.04889598861336708, acc: 0.9545454382896423)
[2024-11-08 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:30][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.006101982202380896, acc: 1.0)
[2024-11-08 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:30][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.043052539229393005, acc: 0.9545454382896423)
[2024-11-08 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:30][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.0012764068087562919, acc: 1.0)
[2024-11-08 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:31][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.003965921700000763, acc: 1.0)
[2024-11-08 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:31][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.0027431300841271877, acc: 1.0)
[2024-11-08 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:31][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.028746232390403748, acc: 1.0)
[2024-11-08 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:32][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.07785811275243759, acc: 0.9473684430122375)
[2024-11-08 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:32][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.023293469101190567, acc: 1.0)
[2024-11-08 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:32][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.0030376543290913105, acc: 1.0)
[2024-11-08 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:33][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.041621338576078415, acc: 1.0)
[2024-11-08 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:33][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.004664311185479164, acc: 1.0)
[2024-11-08 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:33][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.10054683685302734, acc: 0.9473684430122375)
[2024-11-08 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:34][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.0008896415820345283, acc: 1.0)
[2024-11-08 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:34][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.13326086103916168, acc: 0.9545454382896423)
[2024-11-08 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:34][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.09806390851736069, acc: 0.9523809552192688)
[2024-11-08 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:35][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.0014908090233802795, acc: 1.0)
[2024-11-08 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:35][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.10965048521757126, acc: 0.9666666388511658)
[2024-11-08 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:35][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.01440485566854477, acc: 1.0)
[2024-11-08 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:36][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.012959640473127365, acc: 1.0)
[2024-11-08 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:36][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.5052441358566284, acc: 0.9545454382896423)
[2024-11-08 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:37][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.0009596000891178846, acc: 1.0)
[2024-11-08 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:37][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.003399804001674056, acc: 1.0)
[2024-11-08 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:37][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.007307280786335468, acc: 1.0)
[2024-11-08 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:38][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.07365944236516953, acc: 0.9428571462631226)
[2024-11-08 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:38][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.007501016836613417, acc: 1.0)
[2024-11-08 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:38][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.0286804661154747, acc: 1.0)
[2024-11-08 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:38][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.004623400513082743, acc: 1.0)
[2024-11-08 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:39][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.0029909969307482243, acc: 1.0)
[2024-11-08 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:39][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.26874423027038574, acc: 0.9047619104385376)
[2024-11-08 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:39][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.006852905731648207, acc: 1.0)
[2024-11-08 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:08][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3278, device='cuda:0') eval_epoch_loss=tensor(0.2835, device='cuda:0') eval_epoch_acc=tensor(0.9376, device='cuda:0')
[2024-11-08 01:18:08][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:18:08][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:18:08][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_411_loss_0.283540278673172/model.pt
[2024-11-08 01:18:08][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:09][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.011172084137797356, acc: 1.0)
[2024-11-08 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:09][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.09995709359645844, acc: 0.9629629850387573)
[2024-11-08 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:09][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.1316981017589569, acc: 0.9428571462631226)
[2024-11-08 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:10][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.13417977094650269, acc: 0.9615384340286255)
[2024-11-08 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:10][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.0011643300531432033, acc: 1.0)
[2024-11-08 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:10][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.0010587569558992982, acc: 1.0)
[2024-11-08 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:11][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.12303174287080765, acc: 0.8947368264198303)
[2024-11-08 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:11][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.03535792976617813, acc: 1.0)
[2024-11-08 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:11][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.1517857164144516, acc: 0.9047619104385376)
[2024-11-08 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:12][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.09056644886732101, acc: 0.9583333134651184)
[2024-11-08 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:12][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.03997795283794403, acc: 0.9677419066429138)
[2024-11-08 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:12][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.04787709191441536, acc: 0.9677419066429138)
[2024-11-08 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:13][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.03392212837934494, acc: 1.0)
[2024-11-08 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:13][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.012869552709162235, acc: 1.0)
[2024-11-08 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:13][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.0005217916332185268, acc: 1.0)
[2024-11-08 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:14][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.006678303703665733, acc: 1.0)
[2024-11-08 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:14][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.1915772557258606, acc: 0.9090909361839294)
[2024-11-08 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:14][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.011572664603590965, acc: 1.0)
[2024-11-08 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:15][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.008257969282567501, acc: 1.0)
[2024-11-08 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:15][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.08178367465734482, acc: 0.9677419066429138)
[2024-11-08 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:15][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.06901992857456207, acc: 0.9677419066429138)
[2024-11-08 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:15][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.007122238632291555, acc: 1.0)
[2024-11-08 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:16][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.005691928323358297, acc: 1.0)
[2024-11-08 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:16][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.007441799156367779, acc: 1.0)
[2024-11-08 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:16][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.003702905960381031, acc: 1.0)
[2024-11-08 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:17][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.01642557419836521, acc: 1.0)
[2024-11-08 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:17][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.005059418268501759, acc: 1.0)
[2024-11-08 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:17][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.005217293743044138, acc: 1.0)
[2024-11-08 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:18][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.01753954216837883, acc: 1.0)
[2024-11-08 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:18][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.00493957893922925, acc: 1.0)
[2024-11-08 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:19][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.14876356720924377, acc: 0.9473684430122375)
[2024-11-08 01:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:19][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.00858819205313921, acc: 1.0)
[2024-11-08 01:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:20][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.008049853146076202, acc: 1.0)
[2024-11-08 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:20][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.002201620489358902, acc: 1.0)
[2024-11-08 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:21][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.2742193639278412, acc: 0.9166666865348816)
[2024-11-08 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:21][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.061476659029722214, acc: 0.9677419066429138)
[2024-11-08 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:21][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.06367719918489456, acc: 1.0)
[2024-11-08 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:22][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.008349350653588772, acc: 1.0)
[2024-11-08 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:22][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.02190840244293213, acc: 1.0)
[2024-11-08 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:22][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.17475353181362152, acc: 0.949999988079071)
[2024-11-08 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:23][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.021243823692202568, acc: 1.0)
[2024-11-08 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:23][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.022897817194461823, acc: 1.0)
[2024-11-08 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:23][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.004572498612105846, acc: 1.0)
[2024-11-08 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:24][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.015419087372720242, acc: 1.0)
[2024-11-08 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:24][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.686460554599762, acc: 0.8709677457809448)
[2024-11-08 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:24][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.002766122343018651, acc: 1.0)
[2024-11-08 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:24][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.0032782345078885555, acc: 1.0)
[2024-11-08 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:25][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.0012408910552039742, acc: 1.0)
[2024-11-08 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:25][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.020744668319821358, acc: 1.0)
[2024-11-08 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:26][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.16496630012989044, acc: 0.8999999761581421)
[2024-11-08 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:26][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.0022300875280052423, acc: 1.0)
[2024-11-08 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:26][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.06291443109512329, acc: 0.939393937587738)
[2024-11-08 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:27][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.018679138273000717, acc: 1.0)
[2024-11-08 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:27][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.02035309188067913, acc: 1.0)
[2024-11-08 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:27][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.002716146409511566, acc: 1.0)
[2024-11-08 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:28][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.03692557290196419, acc: 1.0)
[2024-11-08 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:28][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.006087136920541525, acc: 1.0)
[2024-11-08 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:28][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.0006846414180472493, acc: 1.0)
[2024-11-08 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:28][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.0018553148256614804, acc: 1.0)
[2024-11-08 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:29][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.021349309012293816, acc: 1.0)
[2024-11-08 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:29][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.02094344235956669, acc: 1.0)
[2024-11-08 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:29][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.03640398010611534, acc: 1.0)
[2024-11-08 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:30][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.009723273105919361, acc: 1.0)
[2024-11-08 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:30][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.0019005972426384687, acc: 1.0)
[2024-11-08 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:30][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.023781070485711098, acc: 1.0)
[2024-11-08 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:31][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.002713950118049979, acc: 1.0)
[2024-11-08 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:31][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.00665911752730608, acc: 1.0)
[2024-11-08 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:31][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.0017027166904881597, acc: 1.0)
[2024-11-08 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:32][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.004596355836838484, acc: 1.0)
[2024-11-08 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:32][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.10930981487035751, acc: 0.9677419066429138)
[2024-11-08 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:32][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.11399229615926743, acc: 0.9523809552192688)
[2024-11-08 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:33][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.14990253746509552, acc: 0.9473684430122375)
[2024-11-08 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:33][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.01511857844889164, acc: 1.0)
[2024-11-08 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:34][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.009717277251183987, acc: 1.0)
[2024-11-08 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:34][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.002840990200638771, acc: 1.0)
[2024-11-08 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:34][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.06816905736923218, acc: 0.9629629850387573)
[2024-11-08 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:34][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.023012474179267883, acc: 1.0)
[2024-11-08 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:35][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.023576924577355385, acc: 1.0)
[2024-11-08 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:35][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.04631023854017258, acc: 0.9523809552192688)
[2024-11-08 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:35][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.13504497706890106, acc: 0.9473684430122375)
[2024-11-08 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:36][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.0074911476112902164, acc: 1.0)
[2024-11-08 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:36][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.005162002518773079, acc: 1.0)
[2024-11-08 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:36][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.010130171664059162, acc: 1.0)
[2024-11-08 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:37][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.02746611274778843, acc: 1.0)
[2024-11-08 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:37][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.054111164063215256, acc: 0.9642857313156128)
[2024-11-08 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:37][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.0018980393651872873, acc: 1.0)
[2024-11-08 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:38][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.13924428820610046, acc: 0.949999988079071)
[2024-11-08 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:38][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.016018783673644066, acc: 1.0)
[2024-11-08 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:38][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.30360209941864014, acc: 0.9090909361839294)
[2024-11-08 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:39][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.009268296882510185, acc: 1.0)
[2024-11-08 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:39][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.05053560808300972, acc: 0.9583333134651184)
[2024-11-08 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:39][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.000886139168869704, acc: 1.0)
[2024-11-08 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:40][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.00335457525216043, acc: 1.0)
[2024-11-08 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:40][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.011934038251638412, acc: 1.0)
[2024-11-08 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:40][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.14237293601036072, acc: 0.9523809552192688)
[2024-11-08 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:41][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.25928255915641785, acc: 0.9473684430122375)
[2024-11-08 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:41][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.00030971039086580276, acc: 1.0)
[2024-11-08 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:42][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.4426305294036865, acc: 0.8999999761581421)
[2024-11-08 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:42][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.00405327882617712, acc: 1.0)
[2024-11-08 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:42][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.0018106469651684165, acc: 1.0)
[2024-11-08 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:43][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.0009408359765075147, acc: 1.0)
[2024-11-08 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:45][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.18855437636375427, acc: 0.9285714030265808)
[2024-11-08 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:45][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.030420128256082535, acc: 1.0)
[2024-11-08 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:46][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.22453193366527557, acc: 0.9473684430122375)
[2024-11-08 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:46][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.029873421415686607, acc: 1.0)
[2024-11-08 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:47][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.0005242233746685088, acc: 1.0)
[2024-11-08 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:47][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 0.0009023748571053147, acc: 1.0)
[2024-11-08 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:47][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.006905016954988241, acc: 1.0)
[2024-11-08 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:48][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.08845999836921692, acc: 0.9629629850387573)
[2024-11-08 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:48][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.012759971432387829, acc: 1.0)
[2024-11-08 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:49][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.3128734230995178, acc: 0.95652174949646)
[2024-11-08 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:49][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.02056422457098961, acc: 1.0)
[2024-11-08 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:49][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.01730092614889145, acc: 1.0)
[2024-11-08 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:50][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.008501843549311161, acc: 1.0)
[2024-11-08 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:50][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.001419840264134109, acc: 1.0)
[2024-11-08 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:51][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.014254853129386902, acc: 1.0)
[2024-11-08 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:51][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.04698330536484718, acc: 1.0)
[2024-11-08 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:51][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.07249761372804642, acc: 0.9642857313156128)
[2024-11-08 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:51][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.015263674780726433, acc: 1.0)
[2024-11-08 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:52][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.1813419759273529, acc: 0.8947368264198303)
[2024-11-08 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:52][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.093513622879982, acc: 0.949999988079071)
[2024-11-08 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:52][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.20250876247882843, acc: 0.9523809552192688)
[2024-11-08 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:53][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.013433288782835007, acc: 1.0)
[2024-11-08 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:53][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.15073461830615997, acc: 0.9642857313156128)
[2024-11-08 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:53][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.005301116965711117, acc: 1.0)
[2024-11-08 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:54][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.016922010108828545, acc: 1.0)
[2024-11-08 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:54][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.1100517138838768, acc: 0.9047619104385376)
[2024-11-08 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:54][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.01835339330136776, acc: 1.0)
[2024-11-08 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:55][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.029168415814638138, acc: 1.0)
[2024-11-08 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:55][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.2762356996536255, acc: 0.9047619104385376)
[2024-11-08 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:55][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.008807404898107052, acc: 1.0)
[2024-11-08 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:56][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.0030064270831644535, acc: 1.0)
[2024-11-08 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:56][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.0015276926569640636, acc: 1.0)
[2024-11-08 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:56][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.013634511269629002, acc: 1.0)
[2024-11-08 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:57][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.002044660970568657, acc: 1.0)
[2024-11-08 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:57][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.00472255190834403, acc: 1.0)
[2024-11-08 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:57][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.11080938577651978, acc: 0.9523809552192688)
[2024-11-08 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:57][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.01085678767412901, acc: 1.0)
[2024-11-08 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:58][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.0019145269179716706, acc: 1.0)
[2024-11-08 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:58][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.001221644226461649, acc: 1.0)
[2024-11-08 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:58][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.004513623658567667, acc: 1.0)
[2024-11-08 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:59][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.00108215375803411, acc: 1.0)
[2024-11-08 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:18:59][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.007649321109056473, acc: 1.0)
[2024-11-08 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:28][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3342, device='cuda:0') eval_epoch_loss=tensor(0.2883, device='cuda:0') eval_epoch_acc=tensor(0.9325, device='cuda:0')
[2024-11-08 01:19:28][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 01:19:28][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 01:19:28][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_0.2883189916610718/model.pt
[2024-11-08 01:19:28][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft directory
[2024-11-08 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:29][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.009763889014720917, acc: 1.0)
[2024-11-08 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:29][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.009310848079621792, acc: 1.0)
[2024-11-08 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:29][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.0008304557995870709, acc: 1.0)
[2024-11-08 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:30][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.0018177204765379429, acc: 1.0)
[2024-11-08 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:30][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.014947150833904743, acc: 1.0)
[2024-11-08 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:30][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.007041912525892258, acc: 1.0)
[2024-11-08 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:31][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.007864280603826046, acc: 1.0)
[2024-11-08 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:31][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.0011985002784058452, acc: 1.0)
[2024-11-08 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:31][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.005385506898164749, acc: 1.0)
[2024-11-08 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:32][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.01914353296160698, acc: 1.0)
[2024-11-08 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:32][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.002131681190803647, acc: 1.0)
[2024-11-08 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:32][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.031503550708293915, acc: 1.0)
[2024-11-08 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:32][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.0017778523033484817, acc: 1.0)
[2024-11-08 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:33][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.00786649901419878, acc: 1.0)
[2024-11-08 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:33][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.016834063455462456, acc: 1.0)
[2024-11-08 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:33][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.0007609466556459665, acc: 1.0)
[2024-11-08 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:34][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.0007180975517258048, acc: 1.0)
[2024-11-08 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:34][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.002758326940238476, acc: 1.0)
[2024-11-08 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:34][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.11639587581157684, acc: 0.9545454382896423)
[2024-11-08 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:19:35][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.009308875538408756, acc: 1.0)
[2024-11-08 01:19:35][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0601, train_epoch_loss=0.0584, epoch time 331.71960255876184s
[2024-11-08 01:19:35][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2024-11-08 01:19:35][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 01:19:35][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2024-11-08 01:19:35][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 01:19:35][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 01:19:35][root][INFO] - Key: avg_train_prep, Value: 1.5693790912628174
[2024-11-08 01:19:35][root][INFO] - Key: avg_train_loss, Value: 0.2954082489013672
[2024-11-08 01:19:35][root][INFO] - Key: avg_train_acc, Value: 0.9206121563911438
[2024-11-08 01:19:35][root][INFO] - Key: avg_eval_prep, Value: 1.4726961851119995
[2024-11-08 01:19:35][root][INFO] - Key: avg_eval_loss, Value: 0.33110639452934265
[2024-11-08 01:19:35][root][INFO] - Key: avg_eval_acc, Value: 0.9051058888435364
[2024-11-08 01:19:35][root][INFO] - Key: avg_epoch_time, Value: 334.49150775671006
[2024-11-08 01:19:35][root][INFO] - Key: avg_checkpoint_time, Value: 0.3776799970306456
Selected lowest loss checkpoint: asr_epoch_5_step_278_loss_0.2092050015926361
Latest file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_5_step_278_loss_0.2092050015926361/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_5_step_278_loss_0.2092050015926361
[2024-11-08 01:20:07][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-08 01:20:07][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-08 01:20:07][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-08 01:20:08][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-08 01:20:14][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 01:20:14][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-08 01:20:14][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 01:20:14][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-08 01:20:18][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 01:20:18][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-08 01:20:18][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-08 01:20:18][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 01:20:18][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-08 01:20:19][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-08 01:20:19][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-08 01:20:19][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_5_step_278_loss_0.2092050015926361/model.pt
[2024-11-08 01:20:19][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-08 01:20:19][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-08 01:20:21][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-08 01:20:22][root][INFO] - --> Training Set Length = 652
[2024-11-08 01:20:22][root][INFO] - =====================================
[2024-11-08 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
Initial Word Error Rate (WER) before filtering: 0.8003717472118959
Number of GT lines after filtering: 652
Number of original PRED lines: 652
Number of filtered repeated lines: 0 out of 652
Filtered Word Error Rate (WER) after removing repeated lines: 0.8003717472118959
