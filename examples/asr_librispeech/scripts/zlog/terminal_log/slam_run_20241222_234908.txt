Configuration:
Task: test
Config File: wavlm-mono
Epochs: 10
Batch Size: 4
Data Folder: psst_phoneme
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: true
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
Final identifier: psst_phoneme_wavlm_llama32_1b_linear_peft
Latest file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231
Resume epoch: 10
Resume step: 554
Selected lowest loss checkpoint: asr_epoch_4_step_137_loss_0.7657225131988525
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_4_step_137_loss_0.7657225131988525/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_4_step_137_loss_0.7657225131988525
Traceback (most recent call last):
  File "examples/asr_librispeech/inference_asr_batch.py", line 1, in <module>
    from slam_llm.pipeline.inference_batch import main as inference
ModuleNotFoundError: No module named 'slam_llm'
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/decode_test_beam4_gt_20241217_010412
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/decode_test_beam4_pred_20241217_010412
Traceback (most recent call last):
  File "examples/asr_librispeech/scripts/wer.py", line 149, in <module>
    main(args.folder, args.separate)
  File "examples/asr_librispeech/scripts/wer.py", line 120, in main
    calculate_wer(gt_combined, pred_combined, "Combined")
  File "examples/asr_librispeech/scripts/wer.py", line 68, in calculate_wer
    score = wer(gt, pred)
  File "/home/zhang.jinda1/.local/lib/python3.7/site-packages/jiwer/measures.py", line 112, in wer
    reference, hypothesis, reference_transform, hypothesis_transform
  File "/home/zhang.jinda1/.local/lib/python3.7/site-packages/jiwer/process.py", line 163, in process_words
    reference, reference_transform, is_reference=True
  File "/home/zhang.jinda1/.local/lib/python3.7/site-packages/jiwer/process.py", line 356, in _apply_transform
    "After applying the transformation, each reference should be a "
ValueError: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.
Configuration:
Task: test
Config File: wavlm-mono
Epochs: 10
Batch Size: 4
Data Folder: ami
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: true
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
Final identifier: ami_wavlm_llama32_1b_linear_peft
Latest file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.6441742181777954/model.pt
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.6441742181777954
Resume epoch: 2
Resume step: 26970
Selected lowest loss checkpoint: asr_epoch_2_step_20227_loss_0.6369916200637817
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_20227_loss_0.6369916200637817/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_20227_loss_0.6369916200637817
Traceback (most recent call last):
  File "examples/asr_librispeech/inference_asr_batch.py", line 1, in <module>
    from slam_llm.pipeline.inference_batch import main as inference
ModuleNotFoundError: No module named 'slam_llm'
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/decode_test_beam4_gt_20241219_192247
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/decode_test_beam4_pred_20241219_192247
Combined WER: 0.0

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.0
Configuration:
Task: test
Config File: wavlm-mono
Epochs: 10
Batch Size: 4
Data Folder: librispeech-100
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: true
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
Final identifier: librispeech-100_wavlm_llama32_1b_linear_peft
Latest file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434/model.pt
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434
Resume epoch: 2
Resume step: 7130
Selected lowest loss checkpoint: asr_epoch_2_step_7130_loss_0.2453521341085434
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434
Traceback (most recent call last):
  File "examples/asr_librispeech/inference_asr_batch.py", line 1, in <module>
    from slam_llm.pipeline.inference_batch import main as inference
ModuleNotFoundError: No module named 'slam_llm'
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/decode_test_beam4_gt_20241219_192348
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/decode_test_beam4_pred_20241219_192348
Combined WER: 0.03125

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.03125
Configuration:
Task: test
Config File: wavlm-mono
Epochs: 10
Batch Size: 4
Data Folder: librispeech-100_phoneme
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: true
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
Final identifier: librispeech-100_phoneme_wavlm_llama32_1b_linear_peft
Latest file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555/model.pt
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555
Resume epoch: 2
Resume step: 7130
Selected lowest loss checkpoint: asr_epoch_2_step_5347_loss_0.048072949051856995
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_5347_loss_0.048072949051856995/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_5347_loss_0.048072949051856995
Traceback (most recent call last):
  File "examples/asr_librispeech/inference_asr_batch.py", line 1, in <module>
    from slam_llm.pipeline.inference_batch import main as inference
ModuleNotFoundError: No module named 'slam_llm'
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/decode_test_beam4_gt_20241219_192552
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/decode_test_beam4_pred_20241219_192552
Combined WER: 0.2150313152400835

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.2150313152400835
Configuration:
Task: test
Config File: wavlm-mono
Epochs: 10
Batch Size: 4
Data Folder: ami_phoneme
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: true
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
Final identifier: ami_phoneme_wavlm_llama32_1b_linear_peft
Latest file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792/model.pt
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792
Resume epoch: 2
Resume step: 26970
Selected lowest loss checkpoint: asr_epoch_2_step_20227_loss_0.22307272255420685
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_20227_loss_0.22307272255420685/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_20227_loss_0.22307272255420685
Traceback (most recent call last):
  File "examples/asr_librispeech/inference_asr_batch.py", line 1, in <module>
    from slam_llm.pipeline.inference_batch import main as inference
ModuleNotFoundError: No module named 'slam_llm'
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/decode_test_beam4_gt_20241219_193820
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/decode_test_beam4_pred_20241219_193820
Combined WER: 0.14285714285714285

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.14285714285714285
