{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67bc805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/xphonebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from text2phonemesequence import Text2PhonemeSequence\n",
    "import torch\n",
    "\n",
    "# Load XPhoneBERT model and its tokenizer\n",
    "xphonebert = AutoModel.from_pretrained(\"vinai/xphonebert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/xphonebert-base\")\n",
    "\n",
    "# Load Text2PhonemeSequence\n",
    "# text2phone_model = Text2PhonemeSequence(language='eng-us', is_cuda=True)\n",
    "text2phone_model = Text2PhonemeSequence(language='jpn', is_cuda=True)\n",
    "\n",
    "# Input sequence that is already WORD-SEGMENTED (and text-normalized if applicable)\n",
    "# sentence = \"That is , it is a testing text .\"  \n",
    "sentence = \"これ は 、 テスト テキスト です .\"\n",
    "\n",
    "input_phonemes = text2phone_model.infer_sentence(sentence)\n",
    "\n",
    "input_ids = tokenizer(input_phonemes, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = xphonebert(**input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea31e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wget: /shared/centos7/anaconda3/2022.05/lib/libuuid.so.1: no version information available (required by wget)\n",
      "--2024-10-29 01:07:36--  https://raw.githubusercontent.com/lingjzhu/CharsiuG2P/main/dicts/eng-us.tsv\n",
      "Connecting to 10.99.0.130:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 2901076 (2.8M) [text/plain]\n",
      "Saving to: ‘eng-us.tsv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1% 5.39M 1s\n",
      "    50K .......... .......... .......... .......... ..........  3% 14.8M 0s\n",
      "   100K .......... .......... .......... .......... ..........  5% 10.7M 0s\n",
      "   150K .......... .......... .......... .......... ..........  7% 10.6M 0s\n",
      "   200K .......... .......... .......... .......... ..........  8% 6.27M 0s\n",
      "   250K .......... .......... .......... .......... .......... 10% 16.4M 0s\n",
      "   300K .......... .......... .......... .......... .......... 12% 17.1M 0s\n",
      "   350K .......... .......... .......... .......... .......... 14% 16.8M 0s\n",
      "   400K .......... .......... .......... .......... .......... 15% 15.8M 0s\n",
      "   450K .......... .......... .......... .......... .......... 17% 17.1M 0s\n",
      "   500K .......... .......... .......... .......... .......... 19% 17.4M 0s\n",
      "   550K .......... .......... .......... .......... .......... 21% 19.4M 0s\n",
      "   600K .......... .......... .......... .......... .......... 22% 16.0M 0s\n",
      "   650K .......... .......... .......... .......... .......... 24% 18.1M 0s\n",
      "   700K .......... .......... .......... .......... .......... 26% 17.6M 0s\n",
      "   750K .......... .......... .......... .......... .......... 28% 19.8M 0s\n",
      "   800K .......... .......... .......... .......... .......... 30% 18.7M 0s\n",
      "   850K .......... .......... .......... .......... .......... 31% 21.9M 0s\n",
      "   900K .......... .......... .......... .......... .......... 33% 22.1M 0s\n",
      "   950K .......... .......... .......... .......... .......... 35% 21.2M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 37% 19.5M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 38% 22.3M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 40% 22.2M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 42% 22.3M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 44% 23.1M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 45%  151M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 47%  142M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 49%  154M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 51%  156M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 52%  157M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 54%  173M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 56%  157M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 58%  142M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 60%  176M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 61%  178M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 63%  159M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 65%  161M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 67%  194M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 68%  187M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 70%  192M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 72% 15.6M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 74% 10.9M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 75% 15.8M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 77% 37.8M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 79% 33.8M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 81% 26.5M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 82% 26.5M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 84% 27.8M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 86% 23.5M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 88% 27.8M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 90% 27.6M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 91% 26.8M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 93% 25.0M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 95% 29.4M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 97% 27.3M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 98% 28.0M 0s\n",
      "  2800K .......... .......... .......... ...                  100% 23.1M=0.1s\n",
      "\n",
      "2024-10-29 01:07:36 (23.0 MB/s) - ‘eng-us.tsv’ saved [2901076/2901076]\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/absolute/path/to/input/file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m Text2PhonemeSequence(pretrained_g2p_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcharsiu/g2p_multilingual_byT5_small_100\u001b[39m\u001b[39m'\u001b[39m, language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meng-us\u001b[39m\u001b[39m'\u001b[39m, is_cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Convert a raw corpus\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model\u001b[39m.\u001b[39;49minfer_dataset(input_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/absolute/path/to/input/file\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/absolute/path/to/output/file\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m) \u001b[39m# batch_size is the number of words fed into the CharsiuG2P toolkit per times. \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Convert a raw sentence\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39minfer_sentence(\u001b[39m\"\u001b[39m\u001b[39mThe overwhelming majority of people in this country know how to sift the wheat from the chaff in what they hear and what they read .\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/text2phonemesequence/text2phonemesequence.py:33\u001b[0m, in \u001b[0;36mText2PhonemeSequence.infer_dataset\u001b[0;34m(self, input_file, seperate_syllabel_token, output_file, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer_dataset\u001b[39m(\u001b[39mself\u001b[39m, input_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, seperate_syllabel_token\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, output_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(input_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     34\u001b[0m     list_lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     35\u001b[0m     f\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/absolute/path/to/input/file'"
     ]
    }
   ],
   "source": [
    "from text2phonemesequence import Text2PhonemeSequence\n",
    "\n",
    "# Load Text2PhonemeSequence\n",
    "model = Text2PhonemeSequence(pretrained_g2p_model='charsiu/g2p_multilingual_byT5_small_100', language='eng-us', is_cuda=False)\n",
    "\n",
    "\n",
    "# Convert a raw corpus\n",
    "model.infer_dataset(input_file=\"/absolute/path/to/input/file\", output_file=\"/absolute/path/to/output/file\", batch_size=64) # batch_size is the number of words fed into the CharsiuG2P toolkit per times. \n",
    "\n",
    "# Convert a raw sentence\n",
    "model.infer_sentence(\"The overwhelming majority of people in this country know how to sift the wheat from the chaff in what they hear and what they read .\")\n",
    "##Output: \"ˈθ i ▁ ˈo ʊ v ɝ ˌw ɛ ɫ m ɪ ŋ ▁ m ə ˈd ʒ ɔ ɹ ə t i ▁ ˈɑ f ▁ ˈp i p ə ɫ ▁ ˈɪ n ▁ ˈθ ɪ s ▁ ˈk a ʊ n t ɹ i ▁ ˈn o ʊ ▁ ˈh o ʊ ▁ ˈt o ʊ ▁ ˈs ɪ f t ▁ ˈθ i ▁ ˈw i t ▁ ˈf ɹ ɑ m ▁ ˈθ i ▁ ˈt ʃ æ f ▁ ˈɪ n ▁ ˈw æ t ▁ ˈθ e ɪ ▁ ˈh ɪ ɹ ▁ ˈæ n d ▁ ˈw æ t ▁ ˈθ e ɪ ▁ ˈɹ ɛ d ▁ .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7095759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text2phonemesequence\n",
      "  Downloading text2phonemesequence-0.1.4.tar.gz (4.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from text2phonemesequence) (4.46.0)\n",
      "Requirement already satisfied: tqdm in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from text2phonemesequence) (4.66.5)\n",
      "Requirement already satisfied: segments in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from text2phonemesequence) (2.2.1)\n",
      "Requirement already satisfied: clldutils>=1.7.3 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from segments->text2phonemesequence) (3.22.2)\n",
      "Requirement already satisfied: csvw>=1.5.6 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from segments->text2phonemesequence) (3.3.0)\n",
      "Requirement already satisfied: regex in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from segments->text2phonemesequence) (2024.5.15)\n",
      "Requirement already satisfied: filelock in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (6.0.2rc1)\n",
      "Requirement already satisfied: requests in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from transformers->text2phonemesequence) (0.20.1)\n",
      "Requirement already satisfied: python-dateutil in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (2.9.0.post0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (0.9.0)\n",
      "Requirement already satisfied: colorlog in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (6.8.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (23.1.0)\n",
      "Requirement already satisfied: bibtexparser>=2.0.0b4 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (2.0.0b7)\n",
      "Requirement already satisfied: pylatexenc in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (2.10)\n",
      "Requirement already satisfied: markdown in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (3.4.1)\n",
      "Requirement already satisfied: lxml in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (5.2.2)\n",
      "Requirement already satisfied: markupsafe in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from clldutils>=1.7.3->segments->text2phonemesequence) (2.1.5)\n",
      "Requirement already satisfied: babel in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (2.11.0)\n",
      "Requirement already satisfied: colorama in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (0.4.6)\n",
      "Requirement already satisfied: isodate in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (0.6.1)\n",
      "Requirement already satisfied: jsonschema in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (4.23.0)\n",
      "Requirement already satisfied: language-tags in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (1.2.0)\n",
      "Requirement already satisfied: rdflib in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (7.0.0)\n",
      "Requirement already satisfied: rfc3986<2 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (1.5.0)\n",
      "Requirement already satisfied: uritemplate>=3.0.0 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from csvw>=1.5.6->segments->text2phonemesequence) (4.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->text2phonemesequence) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->text2phonemesequence) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from requests->transformers->text2phonemesequence) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from requests->transformers->text2phonemesequence) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from requests->transformers->text2phonemesequence) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from requests->transformers->text2phonemesequence) (2024.6.2)\n",
      "Requirement already satisfied: pytz>=2015.7 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from babel->csvw>=1.5.6->segments->text2phonemesequence) (2024.1)\n",
      "Requirement already satisfied: six in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from isodate->csvw>=1.5.6->segments->text2phonemesequence) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from jsonschema->csvw>=1.5.6->segments->text2phonemesequence) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from jsonschema->csvw>=1.5.6->segments->text2phonemesequence) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from jsonschema->csvw>=1.5.6->segments->text2phonemesequence) (0.10.6)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages (from rdflib->csvw>=1.5.6->segments->text2phonemesequence) (3.0.9)\n",
      "Building wheels for collected packages: text2phonemesequence\n",
      "  Building wheel for text2phonemesequence (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for text2phonemesequence: filename=text2phonemesequence-0.1.4-py3-none-any.whl size=5081 sha256=1902d9b150633fbb101986406550f31a6aa6089fed63f1ff13d581b72ed287f0\n",
      "  Stored in directory: /home/zhang.jinda1/.cache/pip/wheels/c8/78/56/6492dd9df8e6986dbfb5c8331f29500842287355a6f30037b3\n",
      "Successfully built text2phonemesequence\n",
      "Installing collected packages: text2phonemesequence\n",
      "Successfully installed text2phonemesequence-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install text2phonemesequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549525a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: ami-10h_w2v2_TinyLlama_linear_phoneme_freeze, Last Modified: Sat Sep 28 01:39:35 2024\n",
      "Folder: _wavlm_TinyLlama_dual, Last Modified: Fri Sep 27 17:53:39 2024\n",
      "Folder: ami-10h_w2v2_phi-2_linear_phoneme_freeze, Last Modified: Thu Sep 26 04:08:10 2024\n",
      "Folder: ami-10h_w2v2_TinyLlama_linear_phoneme_unfreeze, Last Modified: Thu Sep 26 03:13:07 2024\n",
      "Folder: ami-10h_wavlm_TinyLlama_dual_freeze, Last Modified: Tue Sep 24 17:40:57 2024\n",
      "Folder: ami-10h_wavlm_TinyLlama_dual, Last Modified: Mon Sep 23 00:07:58 2024\n",
      "Folder: ami-10h_w2v2_TinyLlama_linear, Last Modified: Tue Sep 17 10:16:56 2024\n",
      "Folder: ami-10h_w2v2_TinyLlama_linear_phoneme, Last Modified: Thu Sep  5 06:42:09 2024\n",
      "Folder: ami-10h_w2v2_phi-2_linear_phoneme, Last Modified: Thu Sep  5 02:24:36 2024\n",
      "Folder: ami-10h_w2v2_phi-2_linear, Last Modified: Thu Sep  5 02:18:29 2024\n",
      "Folder: ami_w2v2_phi2, Last Modified: Tue Sep  3 05:28:24 2024\n",
      "Folder: phi-2-librispeech-linear-steplrwarmupkeep1e-4-whisper-largev3-20240829, Last Modified: Thu Aug 29 01:09:46 2024\n",
      "Folder: phi-2-librispeech-linear-steplrwarmupkeep1e-4-whisper-largev3-20240827, Last Modified: Tue Aug 27 13:58:19 2024\n",
      "Folder: ami_dual_100h_phi2, Last Modified: Wed Aug 21 08:50:49 2024\n",
      "Folder: ami_dual_100h-20240819, Last Modified: Tue Aug 20 19:06:07 2024\n",
      "Folder: ami_wavlm_mono-20240819, Last Modified: Tue Aug 20 18:10:52 2024\n",
      "Folder: ami_pho_mono-20240819, Last Modified: Tue Aug 20 13:47:35 2024\n",
      "Folder: M03, Last Modified: Sat Aug 17 22:23:28 2024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def get_folders_by_mod_time(directory):\n",
    "    # List all files and folders in the given directory\n",
    "    all_items = os.listdir(directory)\n",
    "\n",
    "    # Filter out directories\n",
    "    folders = [item for item in all_items if os.path.isdir(os.path.join(directory, item))]\n",
    "\n",
    "    # Get folder modification times\n",
    "    folder_mod_times = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        mod_time = os.path.getmtime(folder_path)  # Get modification time\n",
    "        folder_mod_times.append((folder, mod_time))\n",
    "\n",
    "    # Sort folders by modification time (newest to oldest)\n",
    "    sorted_folders = sorted(folder_mod_times, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Display folders and their modification times in a readable format\n",
    "    for folder, mod_time in sorted_folders:\n",
    "        print(f\"Folder: {folder}, Last Modified: {time.ctime(mod_time)}\")\n",
    "\n",
    "# Example usage\n",
    "directory_path = '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/train'  # Replace with your target directory\n",
    "get_folders_by_mod_time(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898410a4-7512-43c0-bcd6-ee6daa8652d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca2b9d-67b9-4841-904e-2497b2da4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412b60e-76dc-40d7-af96-61e15c266a6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54455652",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba35ec-495f-45ca-afb3-b4cc1ef028a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2052fec-e6c5-4bd7-9fb7-9e6ff91dd21e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cde01-5f51-45d6-bbef-777470f0003a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa00e8-d5aa-424c-b4f2-834efa227e7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b939dd3-c042-4b87-a7d2-d3f76479246d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283eeff-3429-4a68-87bb-0dc8c56b49c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c744a-2f23-4234-8153-2f6c4a0fc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001656f-3f83-485c-8a98-5660b3043b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --editable ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2663e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07416827-64d6-43d5-8ae4-46d6110f3362",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8db336",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60981a3c-67c1-4be3-87c9-b33865dcdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f69fd4-3a36-4383-b99d-bfad1071301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall slam-llm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2411c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PATH | grep espeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "class MusicFMEncoder(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        from musicfm.model.musicfm_25hz import MusicFM25Hz\n",
    "        model = MusicFM25Hz(\n",
    "            stat_path=model_config['encoder_stat_path'],\n",
    "            model_path=model_config['encoder_path'],\n",
    "            w2v2_config_path=model_config.get('encoder_config_path', \"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "        )\n",
    "        return cls(model_config, model)\n",
    "\n",
    "    def extract_features(self, source, padding_mask=None):\n",
    "        _, hidden_states = self.model.get_predictions(source)\n",
    "        out = hidden_states[self.config['encoder_layer_idx']]\n",
    "        return out\n",
    "\n",
    "def load_wav_file(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "def main():\n",
    "    # Example model configuration\n",
    "    model_config = {\n",
    "        'encoder_stat_path': 'path/to/encoder_stat',\n",
    "        'encoder_path': 'path/to/encoder_model',\n",
    "        'encoder_layer_idx': 12\n",
    "    }\n",
    "\n",
    "    # Path to your WAV file\n",
    "    wav_file_path = \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"\n",
    "\n",
    "    # Load the encoder\n",
    "    encoder = MusicFMEncoder.load(model_config)\n",
    "\n",
    "    # Load the WAV file\n",
    "    waveform, sample_rate = load_wav_file(wav_file_path)\n",
    "\n",
    "    # Ensure the waveform is in the correct format (batch size, num_channels, num_frames)\n",
    "    if waveform.dim() == 2:\n",
    "        waveform = waveform.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Extract features\n",
    "    features = encoder.extract_features(waveform)\n",
    "\n",
    "    # Print the extracted features\n",
    "    print(\"Extracted features:\", features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "input_audio, sample_rate = librosa.load(\"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\",  sr=16000)\n",
    "\n",
    "model_name = \"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "i= feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate)\n",
    "with torch.no_grad():\n",
    "  o= model(i.input_values)\n",
    "print(o.keys())\n",
    "print(o.last_hidden_state.shape)\n",
    "print(o.extract_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119de635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio input shape: torch.Size([4, 31200])\n",
      "Extracted features shape: torch.Size([4, 97, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class Wav2PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, config, model, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        return cls(model_config, model, feature_extractor)\n",
    "\n",
    "    def extract_features(self, audio_input):\n",
    "        # Pass the processed inputs through the Wav2Vec2 model\n",
    "        outputs = self.model(audio_input)\n",
    "        # Return the last hidden state as the extracted features\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "\n",
    "# Configuration (update if necessary)\n",
    "model_config = {}\n",
    "\n",
    "# Load the model\n",
    "encoder = Wav2PhonemeEncoder.load(model_config)\n",
    "\n",
    "# Paths to test audio files\n",
    "audio_files = [\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0007.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0008.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"\n",
    "]\n",
    "\n",
    "# Target length for audio samples\n",
    "target_length = 31200\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def load_and_preprocess_audio(file_paths, target_length):\n",
    "    audio_inputs = []\n",
    "    for path in file_paths:\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        if len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:target_length]\n",
    "        audio_inputs.append(audio)\n",
    "    return np.array(audio_inputs)\n",
    "\n",
    "audio_inputs = load_and_preprocess_audio(audio_files, target_length)\n",
    "print(f\"Audio input shape: {audio_inputs_tensor.shape}\")\n",
    "\n",
    "# Convert audio inputs to PyTorch tensor\n",
    "audio_inputs_tensor = torch.tensor(audio_inputs, dtype=torch.float32)\n",
    "\n",
    "# Extract features\n",
    "features = encoder.extract_features(audio_inputs_tensor)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Extracted features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775ccf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio input shape: torch.Size([4, 31200])\n",
      "Extracted features shape: torch.Size([4, 97, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class Wav2PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, config, model, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n",
    "        return cls(model_config, model, feature_extractor)\n",
    "\n",
    "    def extract_features(self, audio_input):\n",
    "        # Pass the processed inputs through the Wav2Vec2 model\n",
    "        outputs = self.model(audio_input)\n",
    "        # Return the last hidden state as the extracted features\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "\n",
    "# Configuration (update if necessary)\n",
    "model_config = {}\n",
    "\n",
    "# Load the model\n",
    "encoder = Wav2PhonemeEncoder.load(model_config)\n",
    "\n",
    "# Paths to test audio files\n",
    "audio_files = [\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0007.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0008.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"\n",
    "]\n",
    "\n",
    "# Target length for audio samples\n",
    "target_length = 31200\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def load_and_preprocess_audio(file_paths, target_length):\n",
    "    audio_inputs = []\n",
    "    for path in file_paths:\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        if len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:target_length]\n",
    "        audio_inputs.append(audio)\n",
    "    return np.array(audio_inputs)\n",
    "\n",
    "audio_inputs = load_and_preprocess_audio(audio_files, target_length)\n",
    "\n",
    "\n",
    "# Convert audio inputs to PyTorch tensor\n",
    "audio_inputs_tensor = torch.tensor(audio_inputs, dtype=torch.float32)\n",
    "print(f\"Audio input shape: {audio_inputs_tensor.shape}\")\n",
    "\n",
    "# Extract features\n",
    "features = encoder.extract_features(audio_inputs_tensor)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Extracted features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb82a80",
   "metadata": {},
   "source": [
    "# https://huggingface.co/facebook/wav2vec2-base-960h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5022be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5591bb7e6c9e40b8aa9bf0edb45e94e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf972340fec44fff9382b0a20bb67691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a28a60a743d445c8830051ee2569491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d3ceb3d6bc4a44ba383ea271cc8d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e1b3ba146743798db7352846b70619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/309 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Prediction: ɪ k\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Define the model and processor\n",
    "MODEL_ID = \"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Path to the single audio file you want to transcribe\n",
    "audio_file_path = \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\"\n",
    "\n",
    "# Load and preprocess the audio file\n",
    "def speech_file_to_array_fn(audio_file_path):\n",
    "    speech_array, sampling_rate = librosa.load(audio_file_path, sr=16_000)\n",
    "    return speech_array\n",
    "\n",
    "speech_array = speech_file_to_array_fn(audio_file_path)\n",
    "inputs = processor(speech_array, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "# Decode the predicted ids to text\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_sentence = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "# Print the prediction\n",
    "print(\"-\" * 100)\n",
    "print(\"Prediction:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4a919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
