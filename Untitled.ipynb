{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898410a4-7512-43c0-bcd6-ee6daa8652d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca2b9d-67b9-4841-904e-2497b2da4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412b60e-76dc-40d7-af96-61e15c266a6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54455652",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba35ec-495f-45ca-afb3-b4cc1ef028a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2052fec-e6c5-4bd7-9fb7-9e6ff91dd21e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cde01-5f51-45d6-bbef-777470f0003a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa00e8-d5aa-424c-b4f2-834efa227e7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!git checkout tags/v0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b939dd3-c042-4b87-a7d2-d3f76479246d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283eeff-3429-4a68-87bb-0dc8c56b49c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c744a-2f23-4234-8153-2f6c4a0fc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001656f-3f83-485c-8a98-5660b3043b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --editable ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2663e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07416827-64d6-43d5-8ae4-46d6110f3362",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8db336",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60981a3c-67c1-4be3-87c9-b33865dcdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show slam-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f69fd4-3a36-4383-b99d-bfad1071301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall slam-llm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2411c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PATH | grep espeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "class MusicFMEncoder(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        from musicfm.model.musicfm_25hz import MusicFM25Hz\n",
    "        model = MusicFM25Hz(\n",
    "            stat_path=model_config['encoder_stat_path'],\n",
    "            model_path=model_config['encoder_path'],\n",
    "            w2v2_config_path=model_config.get('encoder_config_path', \"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
    "        )\n",
    "        return cls(model_config, model)\n",
    "\n",
    "    def extract_features(self, source, padding_mask=None):\n",
    "        _, hidden_states = self.model.get_predictions(source)\n",
    "        out = hidden_states[self.config['encoder_layer_idx']]\n",
    "        return out\n",
    "\n",
    "def load_wav_file(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "def main():\n",
    "    # Example model configuration\n",
    "    model_config = {\n",
    "        'encoder_stat_path': 'path/to/encoder_stat',\n",
    "        'encoder_path': 'path/to/encoder_model',\n",
    "        'encoder_layer_idx': 12\n",
    "    }\n",
    "\n",
    "    # Path to your WAV file\n",
    "    wav_file_path = \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"\n",
    "\n",
    "    # Load the encoder\n",
    "    encoder = MusicFMEncoder.load(model_config)\n",
    "\n",
    "    # Load the WAV file\n",
    "    waveform, sample_rate = load_wav_file(wav_file_path)\n",
    "\n",
    "    # Ensure the waveform is in the correct format (batch size, num_channels, num_frames)\n",
    "    if waveform.dim() == 2:\n",
    "        waveform = waveform.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Extract features\n",
    "    features = encoder.extract_features(waveform)\n",
    "\n",
    "    # Print the extracted features\n",
    "    print(\"Extracted features:\", features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "input_audio, sample_rate = librosa.load(\"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\",  sr=16000)\n",
    "\n",
    "model_name = \"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "i= feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate)\n",
    "with torch.no_grad():\n",
    "  o= model(i.input_values)\n",
    "print(o.keys())\n",
    "print(o.last_hidden_state.shape)\n",
    "print(o.extract_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119de635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at vitouphy/wav2vec2-xls-r-300m-timit-phoneme and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio input shape: torch.Size([4, 31200])\n",
      "Extracted features shape: torch.Size([4, 97, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class Wav2PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, config, model, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "        return cls(model_config, model, feature_extractor)\n",
    "\n",
    "    def extract_features(self, audio_input):\n",
    "        # Pass the processed inputs through the Wav2Vec2 model\n",
    "        outputs = self.model(audio_input)\n",
    "        # Return the last hidden state as the extracted features\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "\n",
    "# Configuration (update if necessary)\n",
    "model_config = {}\n",
    "\n",
    "# Load the model\n",
    "encoder = Wav2PhonemeEncoder.load(model_config)\n",
    "\n",
    "# Paths to test audio files\n",
    "audio_files = [\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0007.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0008.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"\n",
    "]\n",
    "\n",
    "# Target length for audio samples\n",
    "target_length = 31200\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def load_and_preprocess_audio(file_paths, target_length):\n",
    "    audio_inputs = []\n",
    "    for path in file_paths:\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        if len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:target_length]\n",
    "        audio_inputs.append(audio)\n",
    "    return np.array(audio_inputs)\n",
    "\n",
    "audio_inputs = load_and_preprocess_audio(audio_files, target_length)\n",
    "print(f\"Audio input shape: {audio_inputs_tensor.shape}\")\n",
    "\n",
    "# Convert audio inputs to PyTorch tensor\n",
    "audio_inputs_tensor = torch.tensor(audio_inputs, dtype=torch.float32)\n",
    "\n",
    "# Extract features\n",
    "features = encoder.extract_features(audio_inputs_tensor)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Extracted features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775ccf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio input shape: torch.Size([4, 31200])\n",
      "Extracted features shape: torch.Size([4, 97, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class Wav2PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, config, model, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_config):\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n",
    "        return cls(model_config, model, feature_extractor)\n",
    "\n",
    "    def extract_features(self, audio_input):\n",
    "        # Pass the processed inputs through the Wav2Vec2 model\n",
    "        outputs = self.model(audio_input)\n",
    "        # Return the last hidden state as the extracted features\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "\n",
    "# Configuration (update if necessary)\n",
    "model_config = {}\n",
    "\n",
    "# Load the model\n",
    "encoder = Wav2PhonemeEncoder.load(model_config)\n",
    "\n",
    "# Paths to test audio files\n",
    "audio_files = [\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0007.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0008.wav\",\n",
    "    \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav\"\n",
    "]\n",
    "\n",
    "# Target length for audio samples\n",
    "target_length = 31200\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def load_and_preprocess_audio(file_paths, target_length):\n",
    "    audio_inputs = []\n",
    "    for path in file_paths:\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        if len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:target_length]\n",
    "        audio_inputs.append(audio)\n",
    "    return np.array(audio_inputs)\n",
    "\n",
    "audio_inputs = load_and_preprocess_audio(audio_files, target_length)\n",
    "\n",
    "\n",
    "# Convert audio inputs to PyTorch tensor\n",
    "audio_inputs_tensor = torch.tensor(audio_inputs, dtype=torch.float32)\n",
    "print(f\"Audio input shape: {audio_inputs_tensor.shape}\")\n",
    "\n",
    "# Extract features\n",
    "features = encoder.extract_features(audio_inputs_tensor)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Extracted features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb82a80",
   "metadata": {},
   "source": [
    "# https://huggingface.co/facebook/wav2vec2-base-960h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5022be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcab308124fb4127a9f6c978b0b60a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9f240e51f24ddab925cb75bc8d4638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af794c09245142d489ff9b4c9e6a0205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58c875f386047809540b50f1dc2edc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf984af898947c59da0dcf98601285b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a538ce92f8654fc29ef39f70c6e838b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Prediction: AN THEATR\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Define the model and processor\n",
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Path to the single audio file you want to transcribe\n",
    "audio_file_path = \"/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0006.wav\"\n",
    "\n",
    "# Load and preprocess the audio file\n",
    "def speech_file_to_array_fn(audio_file_path):\n",
    "    speech_array, sampling_rate = librosa.load(audio_file_path, sr=16_000)\n",
    "    return speech_array\n",
    "\n",
    "speech_array = speech_file_to_array_fn(audio_file_path)\n",
    "inputs = processor(speech_array, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "# Decode the predicted ids to text\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_sentence = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "# Print the prediction\n",
    "print(\"-\" * 100)\n",
    "print(\"Prediction:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4a919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
