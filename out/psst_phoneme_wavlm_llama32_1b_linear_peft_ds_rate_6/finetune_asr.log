[2024-12-12 02:15:36,423][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True}
[2024-12-12 02:15:36,423][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-12 02:15:36,423][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 6, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-12 02:15:36,423][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-12-12_02-15-35.txt', 'log_interval': 5}
[2024-12-12 02:15:59,882][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-12 02:16:05,961][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-12 02:16:05,963][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-12 02:16:05,965][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-12 02:16:05,966][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-12 02:16:14,557][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-12 02:16:14,559][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-12 02:16:14,559][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-12 02:16:14,854][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-12 02:16:14,856][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-12 02:16:14,981][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-12 02:16:14,982][slam_llm.utils.train_utils][INFO] - --> linear has 16.781312 Million params

[2024-12-12 02:16:14,982][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-12 02:16:14,986][slam_llm.utils.train_utils][INFO] - --> asr has 22.417408 Million params

[2024-12-12 02:16:17,467][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-12 02:16:18,490][root][INFO] - --> Training Set Length = 2298
[2024-12-12 02:16:18,497][root][INFO] - --> Validation Set Length = 341
[2024-12-12 02:16:18,498][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-12-12 02:16:18,498][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-12-12 02:16:21,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:22,844][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-12-12 02:16:23,953][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 8.778504371643066, acc: 0.0)
[2024-12-12 02:16:24,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:24,297][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 7.955178737640381, acc: 0.03999999910593033)
[2024-12-12 02:16:24,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:24,730][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 7.589800834655762, acc: 0.027027027681469917)
[2024-12-12 02:16:24,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:25,274][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 7.35524320602417, acc: 0.05263157933950424)
[2024-12-12 02:16:25,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:25,682][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 6.856871128082275, acc: 0.054054055362939835)
[2024-12-12 02:16:25,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:26,049][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 7.559429168701172, acc: 0.0)
[2024-12-12 02:16:26,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:26,434][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 7.4473676681518555, acc: 0.0)
[2024-12-12 02:16:26,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:26,819][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 7.451051712036133, acc: 0.0)
[2024-12-12 02:16:26,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:27,225][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 9.549663543701172, acc: 0.0)
[2024-12-12 02:16:27,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:27,668][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 7.292933940887451, acc: 0.03846153989434242)
[2024-12-12 02:16:27,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:28,059][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 8.088187217712402, acc: 0.0)
[2024-12-12 02:16:28,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:28,438][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 6.848108768463135, acc: 0.0)
[2024-12-12 02:16:28,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:28,865][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 7.735462665557861, acc: 0.0)
[2024-12-12 02:16:29,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:29,268][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 6.768086910247803, acc: 0.0)
[2024-12-12 02:16:29,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:29,627][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 7.222898960113525, acc: 0.0)
[2024-12-12 02:16:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:30,054][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 6.336811065673828, acc: 0.06122449040412903)
[2024-12-12 02:16:30,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:30,430][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 9.361125946044922, acc: 0.0)
[2024-12-12 02:16:30,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:30,790][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 7.993798732757568, acc: 0.0)
[2024-12-12 02:16:30,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:31,189][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 6.708747863769531, acc: 0.0555555559694767)
[2024-12-12 02:16:31,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:31,615][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 8.113322257995605, acc: 0.0)
[2024-12-12 02:16:31,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:31,999][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 7.823230266571045, acc: 0.0)
[2024-12-12 02:16:32,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:32,384][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 7.208011150360107, acc: 0.03448275849223137)
[2024-12-12 02:16:32,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:32,778][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 7.269664764404297, acc: 0.03999999910593033)
[2024-12-12 02:16:32,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:33,188][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 8.428617477416992, acc: 0.0476190485060215)
[2024-12-12 02:16:33,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:33,632][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 8.271852493286133, acc: 0.0)
[2024-12-12 02:16:33,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:34,102][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 6.039013385772705, acc: 0.0)
[2024-12-12 02:16:34,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:34,481][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 5.67278528213501, acc: 0.06849315017461777)
[2024-12-12 02:16:35,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:36,036][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 3.9973642826080322, acc: 0.27272728085517883)
[2024-12-12 02:16:36,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:36,458][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 6.5427446365356445, acc: 0.04651162773370743)
[2024-12-12 02:16:36,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:36,911][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 5.304019927978516, acc: 0.14457830786705017)
[2024-12-12 02:16:37,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:37,309][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 5.102670669555664, acc: 0.12345679104328156)
[2024-12-12 02:16:37,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:37,690][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 7.383668422698975, acc: 0.0357142873108387)
[2024-12-12 02:16:37,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:38,031][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 6.922860145568848, acc: 0.0)
[2024-12-12 02:16:38,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:38,395][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 7.586089611053467, acc: 0.0)
[2024-12-12 02:16:38,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:38,838][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 4.870914459228516, acc: 0.1596638709306717)
[2024-12-12 02:16:39,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:39,259][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 5.193668842315674, acc: 0.1803278625011444)
[2024-12-12 02:16:39,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:39,680][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 5.148074150085449, acc: 0.0793650820851326)
[2024-12-12 02:16:39,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:40,075][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 6.029146194458008, acc: 0.033898305147886276)
[2024-12-12 02:16:40,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:40,462][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 5.184447765350342, acc: 0.16091954708099365)
[2024-12-12 02:16:40,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:40,858][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 7.500768661499023, acc: 0.0)
[2024-12-12 02:16:40,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:41,228][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 6.325179100036621, acc: 0.0)
[2024-12-12 02:16:41,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:41,688][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 4.941319942474365, acc: 0.20270270109176636)
[2024-12-12 02:16:41,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:42,074][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 5.280820846557617, acc: 0.16923077404499054)
[2024-12-12 02:16:42,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:42,534][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 5.208423137664795, acc: 0.1818181872367859)
[2024-12-12 02:16:42,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:42,990][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 4.653539180755615, acc: 0.25773194432258606)
[2024-12-12 02:16:43,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:43,457][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 4.901094436645508, acc: 0.16911764442920685)
[2024-12-12 02:16:43,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:43,918][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 7.332646369934082, acc: 0.0)
[2024-12-12 02:16:44,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:44,342][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 6.322299957275391, acc: 0.0)
[2024-12-12 02:16:44,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:44,714][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 6.571536540985107, acc: 0.0357142873108387)
[2024-12-12 02:16:44,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:45,132][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 5.898916244506836, acc: 0.0555555559694767)
[2024-12-12 02:16:45,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:45,503][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 5.180978775024414, acc: 0.19298245012760162)
[2024-12-12 02:16:45,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:45,864][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 4.964402675628662, acc: 0.1428571492433548)
[2024-12-12 02:16:46,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:46,267][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 5.253931999206543, acc: 0.11267605423927307)
[2024-12-12 02:16:46,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:46,787][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 4.598828315734863, acc: 0.23333333432674408)
[2024-12-12 02:16:46,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:47,257][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 6.161532878875732, acc: 0.054054055362939835)
[2024-12-12 02:16:47,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:47,718][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 6.513927936553955, acc: 0.0)
[2024-12-12 02:16:49,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:50,759][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 3.0510218143463135, acc: 0.38566553592681885)
[2024-12-12 02:16:51,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:52,072][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 3.610485315322876, acc: 0.29629629850387573)
[2024-12-12 02:16:52,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:52,812][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 3.899707555770874, acc: 0.3011363744735718)
[2024-12-12 02:16:52,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:53,444][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 4.21213960647583, acc: 0.2132352888584137)
[2024-12-12 02:16:53,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:54,063][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 3.9250903129577637, acc: 0.24637681245803833)
[2024-12-12 02:16:54,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:54,496][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 3.9989280700683594, acc: 0.3125)
[2024-12-12 02:16:54,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:54,902][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 5.104411602020264, acc: 0.029411764815449715)
[2024-12-12 02:16:55,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:55,325][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 5.443552494049072, acc: 0.0833333358168602)
[2024-12-12 02:16:55,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:55,766][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 3.824856996536255, acc: 0.328125)
[2024-12-12 02:16:55,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:56,208][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 4.53544282913208, acc: 0.27586206793785095)
[2024-12-12 02:16:56,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:56,622][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 4.357102394104004, acc: 0.25)
[2024-12-12 02:16:56,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:56,994][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 4.675021171569824, acc: 0.0833333358168602)
[2024-12-12 02:16:57,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:57,404][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 5.624103546142578, acc: 0.03999999910593033)
[2024-12-12 02:16:57,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:57,822][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 5.131809711456299, acc: 0.0833333358168602)
[2024-12-12 02:16:57,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:58,185][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 5.44248628616333, acc: 0.06060606241226196)
[2024-12-12 02:16:58,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:58,525][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 4.057981014251709, acc: 0.25735294818878174)
[2024-12-12 02:16:58,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:58,872][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 3.7156455516815186, acc: 0.2460317462682724)
[2024-12-12 02:16:58,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:59,267][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 4.060476303100586, acc: 0.23589743673801422)
[2024-12-12 02:16:59,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:59,671][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 4.718062877655029, acc: 0.12244898080825806)
[2024-12-12 02:16:59,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:00,052][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 4.418558597564697, acc: 0.11194030195474625)
[2024-12-12 02:17:00,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:00,472][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 3.663015604019165, acc: 0.262773722410202)
[2024-12-12 02:17:00,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:00,820][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 5.491710186004639, acc: 0.1428571492433548)
[2024-12-12 02:17:00,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:01,176][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 4.936279296875, acc: 0.125)
[2024-12-12 02:17:01,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:01,567][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 4.621565818786621, acc: 0.09090909361839294)
[2024-12-12 02:17:01,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:01,936][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 4.463863849639893, acc: 0.1538461595773697)
[2024-12-12 02:17:02,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:02,288][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 4.441952705383301, acc: 0.09615384787321091)
[2024-12-12 02:17:02,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:02,619][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 4.414902210235596, acc: 0.19230769574642181)
[2024-12-12 02:17:02,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:03,014][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 3.812502384185791, acc: 0.25)
[2024-12-12 02:17:03,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:03,383][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 4.332778453826904, acc: 0.15942029654979706)
[2024-12-12 02:17:03,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:03,727][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 3.7732064723968506, acc: 0.20000000298023224)
[2024-12-12 02:17:03,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:04,051][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 4.776004791259766, acc: 0.17391304671764374)
[2024-12-12 02:17:04,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:04,554][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 3.929227590560913, acc: 0.2800000011920929)
[2024-12-12 02:17:04,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:04,888][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.6490871906280518, acc: 0.28155338764190674)
[2024-12-12 02:17:05,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:06,038][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 3.476646900177002, acc: 0.3349514603614807)
[2024-12-12 02:17:06,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:06,856][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 3.505638599395752, acc: 0.2634408473968506)
[2024-12-12 02:17:07,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:07,654][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 3.25534725189209, acc: 0.3534482717514038)
[2024-12-12 02:17:07,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:08,427][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 3.3045318126678467, acc: 0.3052631616592407)
[2024-12-12 02:17:08,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:09,463][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 3.6107661724090576, acc: 0.2574257552623749)
[2024-12-12 02:17:09,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:09,861][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 3.675215005874634, acc: 0.22580644488334656)
[2024-12-12 02:17:10,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:10,310][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 3.8970518112182617, acc: 0.21739129722118378)
[2024-12-12 02:17:10,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:10,709][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 3.9386420249938965, acc: 0.16806723177433014)
[2024-12-12 02:17:10,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:11,136][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 3.6693952083587646, acc: 0.20192307233810425)
[2024-12-12 02:17:11,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:11,574][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 3.72174334526062, acc: 0.2700729966163635)
[2024-12-12 02:17:11,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:11,988][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 4.410396575927734, acc: 0.1492537260055542)
[2024-12-12 02:17:12,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:12,389][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 4.630488395690918, acc: 0.10000000149011612)
[2024-12-12 02:17:12,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:12,784][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 3.6343603134155273, acc: 0.27272728085517883)
[2024-12-12 02:17:12,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:13,190][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 2.822115421295166, acc: 0.260869562625885)
[2024-12-12 02:17:13,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:13,620][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 3.4390082359313965, acc: 0.20454545319080353)
[2024-12-12 02:17:13,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:14,071][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 3.593066453933716, acc: 0.17241379618644714)
[2024-12-12 02:17:14,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:14,450][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 4.032551288604736, acc: 0.13953489065170288)
[2024-12-12 02:17:14,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:14,813][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 3.0009732246398926, acc: 0.4000000059604645)
[2024-12-12 02:17:14,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:15,229][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 3.6378259658813477, acc: 0.29411765933036804)
[2024-12-12 02:17:15,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:15,653][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 3.432377815246582, acc: 0.26923078298568726)
[2024-12-12 02:17:15,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:16,058][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 3.728858470916748, acc: 0.190476194024086)
[2024-12-12 02:17:16,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:16,449][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 3.7451632022857666, acc: 0.2769230902194977)
[2024-12-12 02:17:16,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:16,865][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 3.5364222526550293, acc: 0.21052631735801697)
[2024-12-12 02:17:17,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:17,241][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 3.3436954021453857, acc: 0.24561403691768646)
[2024-12-12 02:17:17,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:17,583][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 3.859830856323242, acc: 0.20512820780277252)
[2024-12-12 02:17:17,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:17,979][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 3.2496964931488037, acc: 0.3469387888908386)
[2024-12-12 02:17:18,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:18,363][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 3.2790215015411377, acc: 0.27272728085517883)
[2024-12-12 02:17:18,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:18,772][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 3.092440605163574, acc: 0.3174603283405304)
[2024-12-12 02:17:18,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:19,145][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 3.1631414890289307, acc: 0.26829269528388977)
[2024-12-12 02:17:19,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:19,558][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 3.2425920963287354, acc: 0.30645161867141724)
[2024-12-12 02:17:19,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:20,496][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 3.002533435821533, acc: 0.3460076153278351)
[2024-12-12 02:17:20,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:20,852][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 3.0272300243377686, acc: 0.30666667222976685)
[2024-12-12 02:17:21,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:21,294][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 3.339263677597046, acc: 0.2884615361690521)
[2024-12-12 02:17:21,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:21,655][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 3.757025718688965, acc: 0.125)
[2024-12-12 02:17:21,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:22,110][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 2.889578104019165, acc: 0.2631579041481018)
[2024-12-12 02:17:22,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:22,514][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 3.3769500255584717, acc: 0.23926380276679993)
[2024-12-12 02:17:22,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:22,953][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 2.5795838832855225, acc: 0.3888888955116272)
[2024-12-12 02:17:23,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:23,365][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 3.117142677307129, acc: 0.21666666865348816)
[2024-12-12 02:17:23,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:23,715][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 3.1072444915771484, acc: 0.2261904776096344)
[2024-12-12 02:17:23,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:24,078][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 3.2425835132598877, acc: 0.25641027092933655)
[2024-12-12 02:17:24,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:24,539][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 2.568387031555176, acc: 0.3897058963775635)
[2024-12-12 02:17:24,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:24,944][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 3.4532253742218018, acc: 0.23076923191547394)
[2024-12-12 02:17:25,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:25,331][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.5267655849456787, acc: 0.3478260934352875)
[2024-12-12 02:17:25,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:25,721][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 3.497070074081421, acc: 0.25)
[2024-12-12 02:17:25,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:26,120][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 2.681797504425049, acc: 0.30434781312942505)
[2024-12-12 02:17:26,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:26,514][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 2.8527238368988037, acc: 0.20000000298023224)
[2024-12-12 02:17:26,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:26,865][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 2.7598228454589844, acc: 0.26923078298568726)
[2024-12-12 02:17:26,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:27,193][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 3.3179714679718018, acc: 0.261904776096344)
[2024-12-12 02:17:27,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:27,571][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 2.4146080017089844, acc: 0.4000000059604645)
[2024-12-12 02:17:27,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:27,977][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 2.3397796154022217, acc: 0.30434781312942505)
[2024-12-12 02:17:28,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:28,386][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 3.466665267944336, acc: 0.2857142984867096)
[2024-12-12 02:17:28,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:28,737][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 3.1514227390289307, acc: 0.38461539149284363)
[2024-12-12 02:17:28,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:29,122][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 3.0976154804229736, acc: 0.22580644488334656)
[2024-12-12 02:17:29,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:29,527][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 2.9435293674468994, acc: 0.21621622145175934)
[2024-12-12 02:17:30,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:30,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:31,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:31,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:32,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:32,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:32,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:33,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:33,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:34,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:34,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:34,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:35,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:35,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:36,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:36,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:37,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:37,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:37,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:38,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:38,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:38,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:39,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:39,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:39,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:40,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:40,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:41,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:41,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:41,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:42,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:42,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:42,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:43,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:43,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:44,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:44,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:44,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:45,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:45,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:45,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:46,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:46,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:46,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:47,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:47,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:47,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:48,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:48,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:48,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:49,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:49,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:49,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:50,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:50,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:51,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:51,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:51,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:51,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:52,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:52,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:53,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:53,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:53,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:54,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:54,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:54,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:55,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:55,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:56,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:56,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:56,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:57,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:57,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:57,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:58,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:58,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:59,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:59,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:59,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:00,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:00,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:01,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:01,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:01,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:02,564][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(19.2467, device='cuda:0') eval_epoch_loss=tensor(2.9573, device='cuda:0') eval_epoch_acc=tensor(0.2887, device='cuda:0')
[2024-12-12 02:18:02,566][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:18:02,566][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:18:02,838][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_1_step_143_loss_2.957340717315674/model.pt
[2024-12-12 02:18:02,848][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:18:02,849][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.957340717315674
[2024-12-12 02:18:02,850][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.2886541187763214
[2024-12-12 02:18:03,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:03,456][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 2.384371757507324, acc: 0.4649122953414917)
[2024-12-12 02:18:03,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:03,924][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 2.3660404682159424, acc: 0.447761207818985)
[2024-12-12 02:18:04,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:04,399][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 2.878028392791748, acc: 0.2857142984867096)
[2024-12-12 02:18:04,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:04,903][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 2.7016308307647705, acc: 0.3404255211353302)
[2024-12-12 02:18:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:05,301][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 2.517665147781372, acc: 0.4571428596973419)
[2024-12-12 02:18:05,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:05,672][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 3.2526912689208984, acc: 0.1785714328289032)
[2024-12-12 02:18:05,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:06,040][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 2.561002731323242, acc: 0.30434781312942505)
[2024-12-12 02:18:06,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:06,402][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 2.9128589630126953, acc: 0.2068965584039688)
[2024-12-12 02:18:06,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:06,746][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 2.714859962463379, acc: 0.30434781312942505)
[2024-12-12 02:18:06,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:07,109][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 2.5008740425109863, acc: 0.32203391194343567)
[2024-12-12 02:18:07,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:07,523][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 3.0413002967834473, acc: 0.2631579041481018)
[2024-12-12 02:18:07,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:07,955][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 2.841811180114746, acc: 0.3378378450870514)
[2024-12-12 02:18:08,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:08,357][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 2.6756246089935303, acc: 0.3928571343421936)
[2024-12-12 02:18:08,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:08,725][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 2.2377781867980957, acc: 0.3913043439388275)
[2024-12-12 02:18:08,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:09,131][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 2.567964553833008, acc: 0.2631579041481018)
[2024-12-12 02:18:09,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:10,916][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 2.9704127311706543, acc: 0.3918918967247009)
[2024-12-12 02:18:11,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:11,242][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 2.8516550064086914, acc: 0.37037035822868347)
[2024-12-12 02:18:11,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:11,672][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 2.933642625808716, acc: 0.3720930218696594)
[2024-12-12 02:18:11,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:12,269][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 2.7085649967193604, acc: 0.38823530077934265)
[2024-12-12 02:18:12,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:12,822][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 2.9516470432281494, acc: 0.33707866072654724)
[2024-12-12 02:18:12,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:13,151][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 2.641280174255371, acc: 0.3636363744735718)
[2024-12-12 02:18:13,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:13,577][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 2.571556329727173, acc: 0.3333333432674408)
[2024-12-12 02:18:13,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:13,965][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 3.1506009101867676, acc: 0.27586206793785095)
[2024-12-12 02:18:14,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:14,393][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 2.070295572280884, acc: 0.5306122303009033)
[2024-12-12 02:18:14,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:14,812][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 2.4008121490478516, acc: 0.3400000035762787)
[2024-12-12 02:18:14,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:15,258][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 2.4392788410186768, acc: 0.3888888955116272)
[2024-12-12 02:18:15,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:15,621][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 2.5748207569122314, acc: 0.343137264251709)
[2024-12-12 02:18:15,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:16,651][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 2.964430332183838, acc: 0.34246575832366943)
[2024-12-12 02:18:16,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:17,003][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 2.5795538425445557, acc: 0.375)
[2024-12-12 02:18:17,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:17,379][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 3.0627260208129883, acc: 0.14814814925193787)
[2024-12-12 02:18:17,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:17,768][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 2.670581579208374, acc: 0.2857142984867096)
[2024-12-12 02:18:17,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:18,347][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 2.225494861602783, acc: 0.48672565817832947)
[2024-12-12 02:18:18,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:18,729][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 2.687035083770752, acc: 0.2753623127937317)
[2024-12-12 02:18:18,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:19,101][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 2.879497766494751, acc: 0.2613636255264282)
[2024-12-12 02:18:19,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:20,016][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 2.8536179065704346, acc: 0.28244274854660034)
[2024-12-12 02:18:20,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:20,692][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 2.943824291229248, acc: 0.2370370328426361)
[2024-12-12 02:18:20,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:20,996][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 2.7103540897369385, acc: 0.2950819730758667)
[2024-12-12 02:18:21,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:21,374][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 2.331023693084717, acc: 0.2916666567325592)
[2024-12-12 02:18:21,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:21,719][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 2.484032392501831, acc: 0.4000000059604645)
[2024-12-12 02:18:21,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:22,076][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 2.7975687980651855, acc: 0.2142857164144516)
[2024-12-12 02:18:22,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:22,518][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 3.032583475112915, acc: 0.2195121943950653)
[2024-12-12 02:18:22,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:22,947][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 2.899144172668457, acc: 0.30513596534729004)
[2024-12-12 02:18:23,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:23,386][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 2.9551875591278076, acc: 0.24783861637115479)
[2024-12-12 02:18:23,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:23,919][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 2.9919204711914062, acc: 0.2593750059604645)
[2024-12-12 02:18:24,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:24,474][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 2.7186384201049805, acc: 0.3058161437511444)
[2024-12-12 02:18:24,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:24,925][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 2.7224457263946533, acc: 0.2918149530887604)
[2024-12-12 02:18:25,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:25,340][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 3.316979169845581, acc: 0.3199999928474426)
[2024-12-12 02:18:25,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:25,919][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 2.6628310680389404, acc: 0.3139534890651703)
[2024-12-12 02:18:26,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:26,718][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 2.678718090057373, acc: 0.3492063581943512)
[2024-12-12 02:18:26,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:27,647][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 2.613265037536621, acc: 0.3787878751754761)
[2024-12-12 02:18:27,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:28,387][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 2.231452465057373, acc: 0.4470588266849518)
[2024-12-12 02:18:28,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:29,462][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 2.298631191253662, acc: 0.40123456716537476)
[2024-12-12 02:18:29,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:30,415][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 2.3810184001922607, acc: 0.4032258093357086)
[2024-12-12 02:18:30,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:30,756][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 2.4927330017089844, acc: 0.3928571343421936)
[2024-12-12 02:18:30,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:31,101][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 2.9295990467071533, acc: 0.25)
[2024-12-12 02:18:31,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:31,546][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 3.066898822784424, acc: 0.29411765933036804)
[2024-12-12 02:18:31,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:31,907][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 2.5130276679992676, acc: 0.3897058963775635)
[2024-12-12 02:18:32,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:32,243][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 2.7537102699279785, acc: 0.3305084705352783)
[2024-12-12 02:18:32,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:32,620][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 2.693016290664673, acc: 0.36567163467407227)
[2024-12-12 02:18:32,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:33,024][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 2.8139376640319824, acc: 0.3106796145439148)
[2024-12-12 02:18:33,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:33,407][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 2.607532024383545, acc: 0.3333333432674408)
[2024-12-12 02:18:33,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:33,736][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 2.6210243701934814, acc: 0.3186813294887543)
[2024-12-12 02:18:33,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:34,136][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 2.7579286098480225, acc: 0.30493274331092834)
[2024-12-12 02:18:34,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:34,577][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 2.5850942134857178, acc: 0.35433071851730347)
[2024-12-12 02:18:34,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:35,002][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 2.710583448410034, acc: 0.31896552443504333)
[2024-12-12 02:18:35,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:35,430][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 2.435664176940918, acc: 0.41304346919059753)
[2024-12-12 02:18:35,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:35,809][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 2.7236151695251465, acc: 0.31128403544425964)
[2024-12-12 02:18:35,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:36,153][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 2.8010802268981934, acc: 0.28260868787765503)
[2024-12-12 02:18:36,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:36,480][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 2.7594716548919678, acc: 0.3913043439388275)
[2024-12-12 02:18:36,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:36,816][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 2.752588987350464, acc: 0.25)
[2024-12-12 02:18:36,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:37,248][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 2.3637163639068604, acc: 0.38297873735427856)
[2024-12-12 02:18:37,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:37,996][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 2.6408045291900635, acc: 0.3153846263885498)
[2024-12-12 02:18:38,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:38,367][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 2.569547176361084, acc: 0.3243243098258972)
[2024-12-12 02:18:38,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:38,764][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 2.6375603675842285, acc: 0.3488371968269348)
[2024-12-12 02:18:38,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:39,343][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 2.4802463054656982, acc: 0.36936935782432556)
[2024-12-12 02:18:39,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:39,748][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 2.4844436645507812, acc: 0.3333333432674408)
[2024-12-12 02:18:39,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:40,107][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 2.153744697570801, acc: 0.42424243688583374)
[2024-12-12 02:18:40,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:40,444][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 1.9540214538574219, acc: 0.4444444477558136)
[2024-12-12 02:18:40,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:40,797][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 1.7817071676254272, acc: 0.4000000059604645)
[2024-12-12 02:18:40,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:41,151][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 2.5849573612213135, acc: 0.2884615361690521)
[2024-12-12 02:18:41,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:41,936][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 2.235696792602539, acc: 0.40760868787765503)
[2024-12-12 02:18:42,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:42,493][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 2.549687385559082, acc: 0.3693181872367859)
[2024-12-12 02:18:42,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:42,939][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 2.7849819660186768, acc: 0.25531914830207825)
[2024-12-12 02:18:43,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:43,347][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 2.753739833831787, acc: 0.2830188572406769)
[2024-12-12 02:18:43,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:43,725][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 2.4022505283355713, acc: 0.4333333373069763)
[2024-12-12 02:18:43,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:44,119][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 1.9736350774765015, acc: 0.44186046719551086)
[2024-12-12 02:18:44,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:44,524][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 2.3253138065338135, acc: 0.4000000059604645)
[2024-12-12 02:18:44,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:44,965][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 2.976083278656006, acc: 0.24210526049137115)
[2024-12-12 02:18:45,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:45,273][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 2.3507285118103027, acc: 0.3888888955116272)
[2024-12-12 02:18:45,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:45,712][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 2.183504581451416, acc: 0.4833333194255829)
[2024-12-12 02:18:45,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:46,236][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 2.206038475036621, acc: 0.4357798099517822)
[2024-12-12 02:18:46,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:46,708][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 2.2136082649230957, acc: 0.4307692348957062)
[2024-12-12 02:18:46,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:47,036][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 2.262409210205078, acc: 0.3684210479259491)
[2024-12-12 02:18:47,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:47,453][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 2.1249234676361084, acc: 0.375)
[2024-12-12 02:18:47,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:47,865][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 2.525799512863159, acc: 0.1818181872367859)
[2024-12-12 02:18:47,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:48,224][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 2.112830877304077, acc: 0.4444444477558136)
[2024-12-12 02:18:48,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:48,566][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 2.2767956256866455, acc: 0.4000000059604645)
[2024-12-12 02:18:48,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:48,961][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 2.018880605697632, acc: 0.40909090638160706)
[2024-12-12 02:18:49,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:49,309][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 2.3194007873535156, acc: 0.4318181872367859)
[2024-12-12 02:18:49,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:49,923][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 2.350834369659424, acc: 0.35483869910240173)
[2024-12-12 02:18:50,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:50,473][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 2.2213947772979736, acc: 0.40909090638160706)
[2024-12-12 02:18:50,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:50,838][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 1.9400683641433716, acc: 0.4285714328289032)
[2024-12-12 02:18:50,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:51,224][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 2.5210533142089844, acc: 0.3076923191547394)
[2024-12-12 02:18:51,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:51,617][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 2.8565266132354736, acc: 0.25806450843811035)
[2024-12-12 02:18:51,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:51,967][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 1.846416711807251, acc: 0.44999998807907104)
[2024-12-12 02:18:52,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:52,375][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 2.4252312183380127, acc: 0.37837839126586914)
[2024-12-12 02:18:52,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:52,753][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 2.361356019973755, acc: 0.3243243098258972)
[2024-12-12 02:18:52,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:53,105][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 2.399932622909546, acc: 0.29729729890823364)
[2024-12-12 02:18:53,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:53,458][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 2.6125242710113525, acc: 0.3382352888584137)
[2024-12-12 02:18:53,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:53,783][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 1.6705493927001953, acc: 0.5853658318519592)
[2024-12-12 02:18:53,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:54,107][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 1.7236900329589844, acc: 0.5600000023841858)
[2024-12-12 02:18:54,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:54,412][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 1.5313246250152588, acc: 0.6000000238418579)
[2024-12-12 02:18:54,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:54,741][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 2.304713249206543, acc: 0.32258063554763794)
[2024-12-12 02:18:54,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:55,066][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 2.616093873977661, acc: 0.2982456088066101)
[2024-12-12 02:18:55,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:55,452][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 2.6316184997558594, acc: 0.34285715222358704)
[2024-12-12 02:18:55,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:55,839][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 2.424294948577881, acc: 0.3815789520740509)
[2024-12-12 02:18:56,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:56,408][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 2.4820241928100586, acc: 0.37735849618911743)
[2024-12-12 02:18:56,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:56,992][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 2.481595516204834, acc: 0.38333332538604736)
[2024-12-12 02:18:57,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:57,308][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 2.156601905822754, acc: 0.4444444477558136)
[2024-12-12 02:18:57,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:57,642][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 2.422353744506836, acc: 0.4838709533214569)
[2024-12-12 02:18:57,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:58,075][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 2.86523175239563, acc: 0.2666666805744171)
[2024-12-12 02:18:58,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:58,467][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 2.5624215602874756, acc: 0.4166666567325592)
[2024-12-12 02:18:58,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:59,371][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 2.737260341644287, acc: 0.30399999022483826)
[2024-12-12 02:18:59,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:59,762][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 2.472628355026245, acc: 0.3932584226131439)
[2024-12-12 02:18:59,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:00,170][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 2.601820230484009, acc: 0.3513513505458832)
[2024-12-12 02:19:00,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:00,626][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 1.9982720613479614, acc: 0.4655172526836395)
[2024-12-12 02:19:00,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:00,971][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 2.232764720916748, acc: 0.5)
[2024-12-12 02:19:01,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:01,362][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 1.8981796503067017, acc: 0.40909090638160706)
[2024-12-12 02:19:01,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:01,696][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 1.7255922555923462, acc: 0.53125)
[2024-12-12 02:19:01,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:02,112][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 2.091801166534424, acc: 0.46666666865348816)
[2024-12-12 02:19:02,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:02,540][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 2.478224277496338, acc: 0.36666667461395264)
[2024-12-12 02:19:02,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:02,939][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 2.119215488433838, acc: 0.46875)
[2024-12-12 02:19:03,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:03,298][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 1.7285408973693848, acc: 0.6000000238418579)
[2024-12-12 02:19:03,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:03,686][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 2.301872730255127, acc: 0.3448275923728943)
[2024-12-12 02:19:03,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:04,062][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 1.7980765104293823, acc: 0.36000001430511475)
[2024-12-12 02:19:04,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:04,415][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 2.5472490787506104, acc: 0.3404255211353302)
[2024-12-12 02:19:04,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:04,808][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 2.3103959560394287, acc: 0.4583333432674408)
[2024-12-12 02:19:04,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:05,250][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 2.03592848777771, acc: 0.47727271914482117)
[2024-12-12 02:19:05,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:05,700][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 2.493971347808838, acc: 0.3253012001514435)
[2024-12-12 02:19:05,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:06,086][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 2.551750659942627, acc: 0.34259259700775146)
[2024-12-12 02:19:06,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:06,434][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 2.573605537414551, acc: 0.2368421107530594)
[2024-12-12 02:19:06,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:06,884][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 2.532215118408203, acc: 0.23529411852359772)
[2024-12-12 02:19:07,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:07,254][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 2.3056302070617676, acc: 0.25)
[2024-12-12 02:19:07,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:08,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:08,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:09,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:09,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:09,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:10,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:10,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:10,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:11,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:11,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:12,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:12,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:12,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:13,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:13,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:13,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:14,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:14,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:15,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:15,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:15,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:15,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:16,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:16,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:17,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:17,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:17,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:18,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:18,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:18,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:19,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:19,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:20,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:20,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:20,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:21,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:21,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:22,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:22,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:23,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:23,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:23,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:24,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:24,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:24,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:25,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:25,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:25,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:26,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:26,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:26,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:27,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:27,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:27,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:28,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:28,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:28,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:29,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:29,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:30,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:30,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:30,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:31,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:31,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:31,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:32,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:32,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:32,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:33,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:33,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:33,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:34,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:34,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:35,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:35,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:35,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:35,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:36,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:36,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:37,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:37,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:37,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:38,683][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(10.5638, device='cuda:0') eval_epoch_loss=tensor(2.3574, device='cuda:0') eval_epoch_acc=tensor(0.3911, device='cuda:0')
[2024-12-12 02:19:38,684][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:19:38,685][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:19:38,952][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_1_step_286_loss_2.357429027557373/model.pt
[2024-12-12 02:19:38,958][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:19:38,959][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.357429027557373
[2024-12-12 02:19:38,959][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.391144335269928
[2024-12-12 02:19:39,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:39,382][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 2.4863572120666504, acc: 0.328125)
[2024-12-12 02:19:39,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:39,743][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 2.6311776638031006, acc: 0.25600001215934753)
[2024-12-12 02:19:39,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:40,131][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 2.2518506050109863, acc: 0.4175824224948883)
[2024-12-12 02:19:40,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:40,505][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 2.659984588623047, acc: 0.29192546010017395)
[2024-12-12 02:19:40,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:40,892][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 2.708667516708374, acc: 0.30927833914756775)
[2024-12-12 02:19:40,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:41,224][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 1.5825930833816528, acc: 0.6363636255264282)
[2024-12-12 02:19:41,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:41,613][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 2.4362688064575195, acc: 0.380952388048172)
[2024-12-12 02:19:41,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:41,966][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 2.0307116508483887, acc: 0.5)
[2024-12-12 02:19:42,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:42,458][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 1.7508858442306519, acc: 0.581818163394928)
[2024-12-12 02:19:42,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:43,016][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 2.114528179168701, acc: 0.48969072103500366)
[2024-12-12 02:19:43,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:43,394][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 2.5074570178985596, acc: 0.36206895112991333)
[2024-12-12 02:19:43,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:43,765][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 2.0700883865356445, acc: 0.48148149251937866)
[2024-12-12 02:19:43,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:44,153][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 2.239468812942505, acc: 0.44736841320991516)
[2024-12-12 02:19:44,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:44,554][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 2.434837579727173, acc: 0.3392857015132904)
[2024-12-12 02:19:44,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:44,900][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 2.2427828311920166, acc: 0.40625)
[2024-12-12 02:19:45,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:45,317][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 2.563612461090088, acc: 0.30188679695129395)
[2024-12-12 02:19:45,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:45,726][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 1.701541543006897, acc: 0.5660377144813538)
[2024-12-12 02:19:45,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:46,057][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 1.9109364748001099, acc: 0.529411792755127)
[2024-12-12 02:19:46,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:46,338][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 2.5561716556549072, acc: 0.28125)
[2024-12-12 02:19:46,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:46,722][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 1.8521504402160645, acc: 0.5573770403862)
[2024-12-12 02:19:46,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:47,100][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 1.5835052728652954, acc: 0.6000000238418579)
[2024-12-12 02:19:47,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:47,430][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 1.3780853748321533, acc: 0.6315789222717285)
[2024-12-12 02:19:47,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:47,771][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 2.5736422538757324, acc: 0.3478260934352875)
[2024-12-12 02:19:47,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:48,219][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 2.191828966140747, acc: 0.4305555522441864)
[2024-12-12 02:19:48,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:48,628][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 2.2288548946380615, acc: 0.3855421543121338)
[2024-12-12 02:19:48,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:48,928][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 2.6935441493988037, acc: 0.3076923191547394)
[2024-12-12 02:19:49,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:49,300][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 2.738255739212036, acc: 0.2857142984867096)
[2024-12-12 02:19:49,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:49,645][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 1.0067840814590454, acc: 0.75)
[2024-12-12 02:19:49,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:49,981][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 1.9486498832702637, acc: 0.4166666567325592)
[2024-12-12 02:19:50,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:50,379][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 2.05460262298584, acc: 0.35483869910240173)
[2024-12-12 02:19:50,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:50,786][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 2.4554288387298584, acc: 0.32258063554763794)
[2024-12-12 02:19:50,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:51,144][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 1.968309998512268, acc: 0.5074626803398132)
[2024-12-12 02:19:51,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:51,489][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 1.89031982421875, acc: 0.5192307829856873)
[2024-12-12 02:19:51,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:51,867][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 2.656961679458618, acc: 0.24444444477558136)
[2024-12-12 02:19:52,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:52,258][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 2.1856560707092285, acc: 0.3709677457809448)
[2024-12-12 02:19:52,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:52,608][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 1.5682941675186157, acc: 0.7200000286102295)
[2024-12-12 02:19:52,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:52,997][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 2.8436293601989746, acc: 0.29629629850387573)
[2024-12-12 02:19:53,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:53,345][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 3.6737561225891113, acc: 0.05714285746216774)
[2024-12-12 02:19:53,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:53,745][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 2.7570502758026123, acc: 0.25641027092933655)
[2024-12-12 02:19:53,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:54,102][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 2.7477948665618896, acc: 0.39024388790130615)
[2024-12-12 02:19:54,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:54,422][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 2.3445441722869873, acc: 0.3947368562221527)
[2024-12-12 02:19:54,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:54,738][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 1.746778130531311, acc: 0.5263158082962036)
[2024-12-12 02:19:54,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:55,098][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 1.73408043384552, acc: 0.5)
[2024-12-12 02:19:55,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:55,466][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 2.2882866859436035, acc: 0.37037035822868347)
[2024-12-12 02:19:55,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:55,872][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 1.5546010732650757, acc: 0.53125)
[2024-12-12 02:19:56,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:56,238][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 2.269110679626465, acc: 0.3870967626571655)
[2024-12-12 02:19:56,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:56,646][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 1.945513367652893, acc: 0.42105263471603394)
[2024-12-12 02:19:56,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:57,050][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 2.700721025466919, acc: 0.21875)
[2024-12-12 02:19:57,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:57,461][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 1.9026916027069092, acc: 0.6000000238418579)
[2024-12-12 02:19:57,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:57,869][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 2.2275378704071045, acc: 0.42105263471603394)
[2024-12-12 02:19:58,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:58,304][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 2.3515634536743164, acc: 0.2800000011920929)
[2024-12-12 02:19:58,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:58,761][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 2.481358289718628, acc: 0.3563218414783478)
[2024-12-12 02:19:58,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:59,149][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 2.6281352043151855, acc: 0.3510638177394867)
[2024-12-12 02:19:59,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:59,482][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 2.5951976776123047, acc: 0.3734939694404602)
[2024-12-12 02:19:59,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:59,914][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 1.875259280204773, acc: 0.6086956262588501)
[2024-12-12 02:20:00,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:00,240][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 2.6330134868621826, acc: 0.3076923191547394)
[2024-12-12 02:20:00,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:00,644][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 2.8825294971466064, acc: 0.2650602459907532)
[2024-12-12 02:20:00,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:01,046][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 2.24788236618042, acc: 0.4150943458080292)
[2024-12-12 02:20:01,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:01,404][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 2.580430507659912, acc: 0.29113924503326416)
[2024-12-12 02:20:01,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:01,744][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 2.462153673171997, acc: 0.3333333432674408)
[2024-12-12 02:20:01,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:02,131][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 2.796618700027466, acc: 0.28358209133148193)
[2024-12-12 02:20:02,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:02,479][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 1.845176100730896, acc: 0.6000000238418579)
[2024-12-12 02:20:02,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:02,868][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 1.8641080856323242, acc: 0.5199999809265137)
[2024-12-12 02:20:03,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:03,305][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 1.6775298118591309, acc: 0.6388888955116272)
[2024-12-12 02:20:03,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:03,648][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 2.3381974697113037, acc: 0.3720930218696594)
[2024-12-12 02:20:03,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:04,039][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 2.207275867462158, acc: 0.38461539149284363)
[2024-12-12 02:20:04,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:04,428][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 2.2574257850646973, acc: 0.3333333432674408)
[2024-12-12 02:20:04,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:04,728][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 1.340855598449707, acc: 0.47826087474823)
[2024-12-12 02:20:04,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:05,122][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 2.8595032691955566, acc: 0.23076923191547394)
[2024-12-12 02:20:05,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:05,515][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 2.7513391971588135, acc: 0.2857142984867096)
[2024-12-12 02:20:05,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:06,033][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 2.143714666366577, acc: 0.40869563817977905)
[2024-12-12 02:20:06,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:06,439][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 2.363318920135498, acc: 0.32608696818351746)
[2024-12-12 02:20:06,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:06,814][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 2.374000310897827, acc: 0.3877550959587097)
[2024-12-12 02:20:06,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:07,171][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 0.8518239855766296, acc: 0.6666666865348816)
[2024-12-12 02:20:07,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:07,536][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 1.8045830726623535, acc: 0.42307692766189575)
[2024-12-12 02:20:07,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:07,930][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 2.332089900970459, acc: 0.39024388790130615)
[2024-12-12 02:20:08,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:08,301][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 2.026942014694214, acc: 0.42222222685813904)
[2024-12-12 02:20:08,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:08,724][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 2.4423325061798096, acc: 0.3947368562221527)
[2024-12-12 02:20:08,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:09,092][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 2.509293794631958, acc: 0.39024388790130615)
[2024-12-12 02:20:09,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:09,470][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 2.4671173095703125, acc: 0.4545454680919647)
[2024-12-12 02:20:09,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:09,813][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 1.1715147495269775, acc: 0.6666666865348816)
[2024-12-12 02:20:09,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:10,185][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 1.0082532167434692, acc: 0.6521739363670349)
[2024-12-12 02:20:10,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:10,597][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 1.7050210237503052, acc: 0.4642857015132904)
[2024-12-12 02:20:10,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:11,027][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 1.8713409900665283, acc: 0.46875)
[2024-12-12 02:20:11,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:11,697][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 2.265476703643799, acc: 0.4484848380088806)
[2024-12-12 02:20:12,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:12,605][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 1.7280299663543701, acc: 0.5943396091461182)
[2024-12-12 02:20:12,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:13,007][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 2.1149821281433105, acc: 0.42222222685813904)
[2024-12-12 02:20:13,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:13,383][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 2.231572389602661, acc: 0.4464285671710968)
[2024-12-12 02:20:13,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:13,715][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 1.3022886514663696, acc: 0.6857143044471741)
[2024-12-12 02:20:13,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:14,021][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.9286401867866516, acc: 0.800000011920929)
[2024-12-12 02:20:14,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:14,332][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 1.1607542037963867, acc: 0.5652173757553101)
[2024-12-12 02:20:14,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:14,648][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 2.653562068939209, acc: 0.2083333283662796)
[2024-12-12 02:20:14,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:15,024][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 2.1177656650543213, acc: 0.4421052634716034)
[2024-12-12 02:20:15,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:15,616][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 2.06834077835083, acc: 0.473053902387619)
[2024-12-12 02:20:15,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:16,018][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 1.982470989227295, acc: 0.48120301961898804)
[2024-12-12 02:20:16,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:17,275][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 2.0301175117492676, acc: 0.47058823704719543)
[2024-12-12 02:20:17,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:17,833][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 1.755505919456482, acc: 0.5315315127372742)
[2024-12-12 02:20:17,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:18,148][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 1.345715880393982, acc: 0.6071428656578064)
[2024-12-12 02:20:18,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:18,502][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 0.9722892642021179, acc: 0.7857142686843872)
[2024-12-12 02:20:18,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:18,865][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 2.082329034805298, acc: 0.40625)
[2024-12-12 02:20:19,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:19,234][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 2.2460567951202393, acc: 0.4166666567325592)
[2024-12-12 02:20:19,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:19,579][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 2.0016777515411377, acc: 0.44736841320991516)
[2024-12-12 02:20:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:19,937][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 1.6110787391662598, acc: 0.5909090638160706)
[2024-12-12 02:20:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:20,291][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 2.279438018798828, acc: 0.44999998807907104)
[2024-12-12 02:20:20,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:20,598][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 1.9201427698135376, acc: 0.4285714328289032)
[2024-12-12 02:20:20,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:20,937][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 2.4754273891448975, acc: 0.3888888955116272)
[2024-12-12 02:20:21,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:21,307][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 2.6273484230041504, acc: 0.33980581164360046)
[2024-12-12 02:20:21,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:21,843][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 2.2409608364105225, acc: 0.43382352590560913)
[2024-12-12 02:20:21,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:22,217][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 2.5222508907318115, acc: 0.3866666555404663)
[2024-12-12 02:20:22,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:22,614][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 2.342327356338501, acc: 0.4375)
[2024-12-12 02:20:22,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:23,004][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 2.2386012077331543, acc: 0.4651162922382355)
[2024-12-12 02:20:23,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:23,309][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 1.3052996397018433, acc: 0.6666666865348816)
[2024-12-12 02:20:23,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:23,700][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 1.9044066667556763, acc: 0.4651162922382355)
[2024-12-12 02:20:23,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:24,105][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 1.9680731296539307, acc: 0.5199999809265137)
[2024-12-12 02:20:24,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:24,701][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 2.150858163833618, acc: 0.45588234066963196)
[2024-12-12 02:20:24,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:25,066][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 2.1977946758270264, acc: 0.46666666865348816)
[2024-12-12 02:20:25,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:25,382][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 1.5670057535171509, acc: 0.6363636255264282)
[2024-12-12 02:20:25,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:25,720][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 2.0455284118652344, acc: 0.5151515007019043)
[2024-12-12 02:20:25,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:26,081][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 1.5608717203140259, acc: 0.5161290168762207)
[2024-12-12 02:20:26,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:26,457][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 2.0823094844818115, acc: 0.4444444477558136)
[2024-12-12 02:20:26,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:26,792][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 1.2223944664001465, acc: 0.7599999904632568)
[2024-12-12 02:20:26,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:27,169][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 1.308464527130127, acc: 0.6111111044883728)
[2024-12-12 02:20:27,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:27,478][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 1.4855934381484985, acc: 0.5555555820465088)
[2024-12-12 02:20:27,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:27,803][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 1.6896793842315674, acc: 0.5769230723381042)
[2024-12-12 02:20:27,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:28,177][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 1.7526870965957642, acc: 0.5344827771186829)
[2024-12-12 02:20:28,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:28,515][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 1.1645936965942383, acc: 0.7142857313156128)
[2024-12-12 02:20:28,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:28,831][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 1.4129807949066162, acc: 0.5666666626930237)
[2024-12-12 02:20:28,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:29,199][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 1.728751540184021, acc: 0.5454545617103577)
[2024-12-12 02:20:29,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:29,511][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 2.019841194152832, acc: 0.4545454680919647)
[2024-12-12 02:20:29,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:29,918][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 2.368244171142578, acc: 0.47058823704719543)
[2024-12-12 02:20:30,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:30,340][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 2.2062878608703613, acc: 0.3461538553237915)
[2024-12-12 02:20:30,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:30,680][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 1.8979663848876953, acc: 0.6111111044883728)
[2024-12-12 02:20:30,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:31,083][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 1.9829657077789307, acc: 0.550000011920929)
[2024-12-12 02:20:31,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:31,434][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 2.900432586669922, acc: 0.3499999940395355)
[2024-12-12 02:20:31,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:31,771][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 1.0095667839050293, acc: 0.7142857313156128)
[2024-12-12 02:20:31,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:32,098][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 2.3125522136688232, acc: 0.36666667461395264)
[2024-12-12 02:20:32,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:32,415][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 2.0586843490600586, acc: 0.46875)
[2024-12-12 02:20:32,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:32,750][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 1.986491322517395, acc: 0.4722222089767456)
[2024-12-12 02:20:32,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:33,099][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 1.859472393989563, acc: 0.48148149251937866)
[2024-12-12 02:20:33,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:33,459][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 1.6575902700424194, acc: 0.5757575631141663)
[2024-12-12 02:20:33,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:33,838][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 1.4249639511108398, acc: 0.52173912525177)
[2024-12-12 02:20:33,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:34,184][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 1.4891815185546875, acc: 0.5945945978164673)
[2024-12-12 02:20:34,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:34,570][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 1.3169019222259521, acc: 0.7037037014961243)
[2024-12-12 02:20:35,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:35,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:35,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:36,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:36,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:37,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:37,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:37,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:38,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:38,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:39,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:39,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:39,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:40,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:40,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:41,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:41,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:41,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:41,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:42,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:42,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:43,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:43,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:43,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:44,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:44,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:44,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:45,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:45,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:46,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:46,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:46,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:47,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:47,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:48,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:48,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:49,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:49,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:49,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:50,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:50,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:50,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:51,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:51,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:52,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:52,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:52,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:53,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:53,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:53,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:54,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:54,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:54,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:55,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:55,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:55,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:56,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:56,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:56,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:57,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:57,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:58,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:58,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:58,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:59,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:59,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:59,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:00,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:00,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:01,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:01,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:01,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:02,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:02,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:03,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:03,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:03,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:04,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:04,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:04,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:05,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:05,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:05,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:06,419][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(10.5915, device='cuda:0') eval_epoch_loss=tensor(2.3601, device='cuda:0') eval_epoch_acc=tensor(0.3978, device='cuda:0')
[2024-12-12 02:21:06,420][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:21:06,420][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:21:06,709][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_1_step_429_loss_2.360055685043335/model.pt
[2024-12-12 02:21:06,716][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:21:06,717][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.39782872796058655
[2024-12-12 02:21:06,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:07,119][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 1.915711760520935, acc: 0.52173912525177)
[2024-12-12 02:21:07,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:07,457][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.9822410345077515, acc: 0.6666666865348816)
[2024-12-12 02:21:07,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:07,816][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 1.0739752054214478, acc: 0.6666666865348816)
[2024-12-12 02:21:07,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:08,223][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 1.9616529941558838, acc: 0.5652173757553101)
[2024-12-12 02:21:08,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:08,664][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 1.7176003456115723, acc: 0.5555555820465088)
[2024-12-12 02:21:08,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:09,089][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.8733227252960205, acc: 0.800000011920929)
[2024-12-12 02:21:09,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:09,497][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 1.7810975313186646, acc: 0.4848484992980957)
[2024-12-12 02:21:09,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:09,859][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 1.6879709959030151, acc: 0.5)
[2024-12-12 02:21:09,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:10,238][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 1.7902480363845825, acc: 0.5454545617103577)
[2024-12-12 02:21:10,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:10,509][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.5751160383224487, acc: 0.8095238208770752)
[2024-12-12 02:21:10,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:10,907][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 2.2946369647979736, acc: 0.41025641560554504)
[2024-12-12 02:21:11,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:11,404][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 2.1785075664520264, acc: 0.4545454680919647)
[2024-12-12 02:21:11,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:12,143][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 2.8240230083465576, acc: 0.30399999022483826)
[2024-12-12 02:21:12,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:12,569][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 2.6527254581451416, acc: 0.3145161271095276)
[2024-12-12 02:21:12,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:13,245][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 2.4971604347229004, acc: 0.34328359365463257)
[2024-12-12 02:21:13,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:13,630][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 2.532569408416748, acc: 0.3396226465702057)
[2024-12-12 02:21:13,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:14,103][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 1.664940595626831, acc: 0.6136363744735718)
[2024-12-12 02:21:14,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:14,516][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 2.2816262245178223, acc: 0.52173912525177)
[2024-12-12 02:21:14,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:14,856][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 1.8997266292572021, acc: 0.6153846383094788)
[2024-12-12 02:21:14,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:15,245][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 1.6722303628921509, acc: 0.5714285969734192)
[2024-12-12 02:21:15,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:15,608][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 2.5447707176208496, acc: 0.3283582031726837)
[2024-12-12 02:21:15,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:15,989][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 2.070452928543091, acc: 0.4861111044883728)
[2024-12-12 02:21:16,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:16,411][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 2.4484188556671143, acc: 0.3804347813129425)
[2024-12-12 02:21:16,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:16,866][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 2.5142476558685303, acc: 0.3461538553237915)
[2024-12-12 02:21:17,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:17,249][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 2.579692840576172, acc: 0.42105263471603394)
[2024-12-12 02:21:17,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:17,630][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 1.8899770975112915, acc: 0.4897959232330322)
[2024-12-12 02:21:17,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:17,989][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 1.7233679294586182, acc: 0.5454545617103577)
[2024-12-12 02:21:18,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:18,378][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 2.3232927322387695, acc: 0.34020617604255676)
[2024-12-12 02:21:18,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:18,826][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 2.163865327835083, acc: 0.4000000059604645)
[2024-12-12 02:21:18,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:19,246][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 2.286987066268921, acc: 0.3604651093482971)
[2024-12-12 02:21:19,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:19,606][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 2.6289210319519043, acc: 0.3571428656578064)
[2024-12-12 02:21:19,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:19,979][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 2.2925000190734863, acc: 0.37037035822868347)
[2024-12-12 02:21:20,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:20,345][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 1.7884398698806763, acc: 0.5)
[2024-12-12 02:21:20,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:20,693][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 1.742218255996704, acc: 0.5)
[2024-12-12 02:21:20,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:21,051][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 1.7967441082000732, acc: 0.5769230723381042)
[2024-12-12 02:21:21,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:21,362][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 2.143184185028076, acc: 0.47826087474823)
[2024-12-12 02:21:21,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:21,743][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 2.4383323192596436, acc: 0.2380952388048172)
[2024-12-12 02:21:21,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:22,159][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 2.4208972454071045, acc: 0.3012048304080963)
[2024-12-12 02:21:22,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:22,544][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 2.211684226989746, acc: 0.4144144058227539)
[2024-12-12 02:21:22,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:22,931][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 2.241010904312134, acc: 0.446601927280426)
[2024-12-12 02:21:23,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:23,226][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 2.064751386642456, acc: 0.4390243887901306)
[2024-12-12 02:21:23,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:23,611][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 1.7399779558181763, acc: 0.625)
[2024-12-12 02:21:23,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:24,022][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 2.5078234672546387, acc: 0.2857142984867096)
[2024-12-12 02:21:24,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:24,503][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 2.316971778869629, acc: 0.3529411852359772)
[2024-12-12 02:21:24,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:24,859][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 2.4900710582733154, acc: 0.34061136841773987)
[2024-12-12 02:21:25,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:25,250][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 2.377692937850952, acc: 0.3333333432674408)
[2024-12-12 02:21:25,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:25,676][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 2.4207401275634766, acc: 0.3619631826877594)
[2024-12-12 02:21:25,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:26,077][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 2.542248249053955, acc: 0.32374101877212524)
[2024-12-12 02:21:26,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:26,478][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 2.419919967651367, acc: 0.3467336595058441)
[2024-12-12 02:21:26,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:26,846][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 1.6859993934631348, acc: 0.5555555820465088)
[2024-12-12 02:21:26,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:27,229][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 1.713867425918579, acc: 0.5151515007019043)
[2024-12-12 02:21:27,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:27,575][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 1.7568176984786987, acc: 0.37037035822868347)
[2024-12-12 02:21:27,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:27,919][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 1.9998867511749268, acc: 0.44999998807907104)
[2024-12-12 02:21:28,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:28,251][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 1.0301244258880615, acc: 0.75)
[2024-12-12 02:21:28,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:28,630][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.8547163009643555, acc: 0.5344827771186829)
[2024-12-12 02:21:28,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:28,967][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 1.5873574018478394, acc: 0.6129032373428345)
[2024-12-12 02:21:29,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:29,378][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 1.1249117851257324, acc: 0.7894737124443054)
[2024-12-12 02:21:29,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:29,747][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 2.245429754257202, acc: 0.48148149251937866)
[2024-12-12 02:21:29,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:30,082][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 2.3019208908081055, acc: 0.380952388048172)
[2024-12-12 02:21:30,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:30,414][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 1.7863574028015137, acc: 0.5)
[2024-12-12 02:21:30,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:30,804][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 2.102731943130493, acc: 0.4153846204280853)
[2024-12-12 02:21:30,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:31,182][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 1.5641874074935913, acc: 0.6333333253860474)
[2024-12-12 02:21:31,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:31,580][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 1.7173584699630737, acc: 0.5517241358757019)
[2024-12-12 02:21:31,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:31,979][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 2.233919382095337, acc: 0.37254902720451355)
[2024-12-12 02:21:32,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:32,357][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 1.8237783908843994, acc: 0.517241358757019)
[2024-12-12 02:21:32,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:32,759][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.9762758016586304, acc: 0.7894737124443054)
[2024-12-12 02:21:32,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:33,111][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 3.1742515563964844, acc: 0.2631579041481018)
[2024-12-12 02:21:33,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:33,475][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 2.243304967880249, acc: 0.3928571343421936)
[2024-12-12 02:21:33,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:33,869][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 2.1312754154205322, acc: 0.40449437499046326)
[2024-12-12 02:21:33,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:34,200][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 2.4229397773742676, acc: 0.3483146131038666)
[2024-12-12 02:21:34,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:34,586][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 2.5601296424865723, acc: 0.326241135597229)
[2024-12-12 02:21:34,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:34,982][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 2.588304281234741, acc: 0.33695653080940247)
[2024-12-12 02:21:35,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:35,371][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.918677568435669, acc: 0.7599999904632568)
[2024-12-12 02:21:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:35,757][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 1.2746460437774658, acc: 0.7307692170143127)
[2024-12-12 02:21:35,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:36,129][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 1.1537249088287354, acc: 0.6666666865348816)
[2024-12-12 02:21:36,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:36,473][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 1.949675440788269, acc: 0.5555555820465088)
[2024-12-12 02:21:36,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:36,791][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 1.9158529043197632, acc: 0.49056604504585266)
[2024-12-12 02:21:36,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:37,184][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 1.642425537109375, acc: 0.517241358757019)
[2024-12-12 02:21:37,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:37,800][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 2.2027244567871094, acc: 0.44144144654273987)
[2024-12-12 02:21:37,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:38,260][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 2.1995809078216553, acc: 0.4507042169570923)
[2024-12-12 02:21:38,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:38,582][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.4881141781806946, acc: 0.8500000238418579)
[2024-12-12 02:21:38,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:38,934][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.7876664996147156, acc: 0.800000011920929)
[2024-12-12 02:21:39,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:39,272][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 1.2955518960952759, acc: 0.5769230723381042)
[2024-12-12 02:21:40,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:42,015][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 2.2958943843841553, acc: 0.4357142746448517)
[2024-12-12 02:21:42,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:42,776][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 2.289067268371582, acc: 0.4523809552192688)
[2024-12-12 02:21:42,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:43,119][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 1.8201814889907837, acc: 0.5714285969734192)
[2024-12-12 02:21:43,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:43,484][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 1.8714179992675781, acc: 0.5333333611488342)
[2024-12-12 02:21:43,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:44,232][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 2.0518901348114014, acc: 0.5138888955116272)
[2024-12-12 02:21:44,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:44,588][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.7051617503166199, acc: 0.7692307829856873)
[2024-12-12 02:21:44,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:44,958][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 2.2745468616485596, acc: 0.4193548262119293)
[2024-12-12 02:21:45,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:45,307][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 2.647977113723755, acc: 0.44999998807907104)
[2024-12-12 02:21:45,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:45,702][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 2.3211283683776855, acc: 0.40740740299224854)
[2024-12-12 02:21:46,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:46,824][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 2.3165788650512695, acc: 0.41525423526763916)
[2024-12-12 02:21:46,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:47,228][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 2.310171127319336, acc: 0.4029850661754608)
[2024-12-12 02:21:47,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:47,652][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 2.3510499000549316, acc: 0.35036495327949524)
[2024-12-12 02:21:47,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:48,286][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 2.192795991897583, acc: 0.4000000059604645)
[2024-12-12 02:21:48,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:48,649][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 2.261988401412964, acc: 0.31481480598449707)
[2024-12-12 02:21:48,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:49,019][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 2.1043143272399902, acc: 0.4038461446762085)
[2024-12-12 02:21:49,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:49,351][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 2.483311414718628, acc: 0.2380952388048172)
[2024-12-12 02:21:49,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:49,752][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 2.9880738258361816, acc: 0.21311475336551666)
[2024-12-12 02:21:49,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:50,153][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 1.981598973274231, acc: 0.5254237055778503)
[2024-12-12 02:21:50,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:50,542][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 2.599337100982666, acc: 0.302325576543808)
[2024-12-12 02:21:50,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:50,912][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 2.4604413509368896, acc: 0.4545454680919647)
[2024-12-12 02:21:51,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:51,243][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 2.5169854164123535, acc: 0.3396226465702057)
[2024-12-12 02:21:51,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:51,618][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 2.1369364261627197, acc: 0.5)
[2024-12-12 02:21:51,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:51,921][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 1.6540898084640503, acc: 0.6800000071525574)
[2024-12-12 02:21:52,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:52,253][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 2.0447099208831787, acc: 0.44999998807907104)
[2024-12-12 02:21:52,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:52,613][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 1.5650248527526855, acc: 0.5)
[2024-12-12 02:21:52,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:53,052][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 2.0978145599365234, acc: 0.4769230782985687)
[2024-12-12 02:21:53,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:53,416][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 1.9347245693206787, acc: 0.5)
[2024-12-12 02:21:53,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:53,819][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 1.2832567691802979, acc: 0.6875)
[2024-12-12 02:21:53,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:54,143][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 2.1019928455352783, acc: 0.42424243688583374)
[2024-12-12 02:21:54,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:54,479][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.8367161154747009, acc: 0.75)
[2024-12-12 02:21:54,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:54,794][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 1.098924994468689, acc: 0.6129032373428345)
[2024-12-12 02:21:54,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:55,121][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.7654218077659607, acc: 0.739130437374115)
[2024-12-12 02:21:55,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:55,499][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 2.254189968109131, acc: 0.46666666865348816)
[2024-12-12 02:21:55,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:55,929][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 1.9577363729476929, acc: 0.4146341383457184)
[2024-12-12 02:21:56,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:56,325][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 1.265176773071289, acc: 0.6571428775787354)
[2024-12-12 02:21:56,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:56,668][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 1.8166486024856567, acc: 0.5789473652839661)
[2024-12-12 02:21:56,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:57,054][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 1.923195242881775, acc: 0.5161290168762207)
[2024-12-12 02:21:57,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:57,396][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.787525475025177, acc: 0.800000011920929)
[2024-12-12 02:21:57,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:57,717][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 1.3970435857772827, acc: 0.6363636255264282)
[2024-12-12 02:21:57,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:58,070][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 1.3692529201507568, acc: 0.625)
[2024-12-12 02:21:58,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:58,379][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 1.5862337350845337, acc: 0.6000000238418579)
[2024-12-12 02:21:58,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:58,734][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 2.604644775390625, acc: 0.277372270822525)
[2024-12-12 02:21:58,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:59,069][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 2.1890709400177, acc: 0.4482758641242981)
[2024-12-12 02:21:59,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:59,450][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 2.921696662902832, acc: 0.2142857164144516)
[2024-12-12 02:21:59,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:59,811][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 2.817631244659424, acc: 0.21192052960395813)
[2024-12-12 02:21:59,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:00,152][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 2.270045518875122, acc: 0.45299145579338074)
[2024-12-12 02:22:00,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:00,497][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.6876003742218018, acc: 0.8399999737739563)
[2024-12-12 02:22:00,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:00,860][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 1.5098596811294556, acc: 0.6153846383094788)
[2024-12-12 02:22:00,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:01,195][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 1.2810113430023193, acc: 0.6153846383094788)
[2024-12-12 02:22:01,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:01,544][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 1.9315475225448608, acc: 0.4871794879436493)
[2024-12-12 02:22:01,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:01,883][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 1.9422417879104614, acc: 0.5111111402511597)
[2024-12-12 02:22:02,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:02,231][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 2.2139179706573486, acc: 0.41558441519737244)
[2024-12-12 02:22:02,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:02,644][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 2.075500726699829, acc: 0.4166666567325592)
[2024-12-12 02:22:02,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:03,029][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 2.3372726440429688, acc: 0.3448275923728943)
[2024-12-12 02:22:03,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:03,410][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 2.1064953804016113, acc: 0.3928571343421936)
[2024-12-12 02:22:03,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:03,757][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 1.7255076169967651, acc: 0.44736841320991516)
[2024-12-12 02:22:03,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:04,107][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 1.807935357093811, acc: 0.4444444477558136)
[2024-12-12 02:22:04,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:04,520][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 2.2421934604644775, acc: 0.3636363744735718)
[2024-12-12 02:22:04,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:04,896][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 1.742853045463562, acc: 0.5645161271095276)
[2024-12-12 02:22:05,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:05,276][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 2.318662405014038, acc: 0.4188034236431122)
[2024-12-12 02:22:06,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:06,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:06,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:07,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:07,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:07,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:08,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:08,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:08,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:09,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:09,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:10,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:10,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:10,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:11,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:11,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:12,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:12,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:13,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:13,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:14,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:14,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:14,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:15,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:15,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:15,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:16,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:16,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:16,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:17,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:17,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:17,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:18,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:18,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:18,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:19,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:19,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:20,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:20,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:20,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:21,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:21,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:21,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:22,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:22,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:22,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:23,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:23,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:23,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:24,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:24,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:25,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:25,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:25,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:27,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:27,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:27,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:28,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:28,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:28,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:29,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:29,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:30,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:30,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:31,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:31,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:31,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:32,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:32,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:32,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:33,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:33,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:33,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:34,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:34,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:34,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:35,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:35,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:35,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:36,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:36,772][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(7.2289, device='cuda:0') eval_epoch_loss=tensor(1.9781, device='cuda:0') eval_epoch_acc=tensor(0.4736, device='cuda:0')
[2024-12-12 02:22:36,773][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:22:36,774][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:22:37,097][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_1_step_572_loss_1.9780890941619873/model.pt
[2024-12-12 02:22:37,106][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:22:37,107][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.9780890941619873
[2024-12-12 02:22:37,108][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.47359156608581543
[2024-12-12 02:22:37,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:37,477][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 2.5555238723754883, acc: 0.3520408272743225)
[2024-12-12 02:22:37,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:37,852][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 2.4226315021514893, acc: 0.3333333432674408)
[2024-12-12 02:22:38,351][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=17.1695, train_epoch_loss=2.8431, epoch time 379.8459742553532s
[2024-12-12 02:22:38,352][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-12-12 02:22:38,352][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:22:38,352][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-12-12 02:22:38,352][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2024-12-12 02:22:38,352][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:22:38,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:39,201][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 1.4835302829742432, acc: 0.5555555820465088)
[2024-12-12 02:22:39,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:39,566][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 2.226897954940796, acc: 0.47999998927116394)
[2024-12-12 02:22:39,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:39,906][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 2.890416383743286, acc: 0.37837839126586914)
[2024-12-12 02:22:40,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:40,311][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 2.5230178833007812, acc: 0.28947368264198303)
[2024-12-12 02:22:40,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:40,725][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 2.2180230617523193, acc: 0.4054054021835327)
[2024-12-12 02:22:40,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:41,084][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 2.255943775177002, acc: 0.3571428656578064)
[2024-12-12 02:22:41,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:41,436][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 2.4003405570983887, acc: 0.3469387888908386)
[2024-12-12 02:22:41,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:41,804][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 2.1180155277252197, acc: 0.3333333432674408)
[2024-12-12 02:22:41,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:42,166][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.4942381978034973, acc: 0.8636363744735718)
[2024-12-12 02:22:42,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:42,507][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.8997329473495483, acc: 0.692307710647583)
[2024-12-12 02:22:42,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:42,871][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 1.4451395273208618, acc: 0.5185185074806213)
[2024-12-12 02:22:42,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:43,255][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 1.8832181692123413, acc: 0.5128205418586731)
[2024-12-12 02:22:43,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:43,621][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 1.7985583543777466, acc: 0.5454545617103577)
[2024-12-12 02:22:43,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:43,988][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 1.9363197088241577, acc: 0.41304346919059753)
[2024-12-12 02:22:44,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:44,354][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 2.3772358894348145, acc: 0.4117647111415863)
[2024-12-12 02:22:44,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:44,730][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 1.788522720336914, acc: 0.4897959232330322)
[2024-12-12 02:22:44,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:45,117][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 1.045725703239441, acc: 0.7894737124443054)
[2024-12-12 02:22:45,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:45,478][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 2.068894624710083, acc: 0.3333333432674408)
[2024-12-12 02:22:45,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:45,856][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 2.344604253768921, acc: 0.3888888955116272)
[2024-12-12 02:22:45,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:46,234][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 1.7870274782180786, acc: 0.4736842215061188)
[2024-12-12 02:22:46,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:46,599][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 2.011056423187256, acc: 0.5)
[2024-12-12 02:22:46,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:46,933][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 2.1787874698638916, acc: 0.5517241358757019)
[2024-12-12 02:22:47,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:47,290][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 2.0795748233795166, acc: 0.36000001430511475)
[2024-12-12 02:22:47,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:47,609][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 1.124131441116333, acc: 0.7142857313156128)
[2024-12-12 02:22:47,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:47,954][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 1.8795126676559448, acc: 0.4375)
[2024-12-12 02:22:48,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:48,353][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 2.8272342681884766, acc: 0.24528302252292633)
[2024-12-12 02:22:48,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:48,723][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 2.5494184494018555, acc: 0.39726027846336365)
[2024-12-12 02:22:49,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:50,038][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 2.447664260864258, acc: 0.33596837520599365)
[2024-12-12 02:22:50,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:50,309][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 2.25186824798584, acc: 0.3720930218696594)
[2024-12-12 02:22:50,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:50,662][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 2.24379563331604, acc: 0.39759036898612976)
[2024-12-12 02:22:50,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:51,032][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 2.2423906326293945, acc: 0.37037035822868347)
[2024-12-12 02:22:51,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:51,344][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 2.464592456817627, acc: 0.3571428656578064)
[2024-12-12 02:22:51,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:51,710][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 1.6253817081451416, acc: 0.6296296119689941)
[2024-12-12 02:22:51,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:52,111][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 1.4166293144226074, acc: 0.5652173757553101)
[2024-12-12 02:22:52,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:52,524][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 2.1541011333465576, acc: 0.42016807198524475)
[2024-12-12 02:22:52,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:52,915][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 1.9088610410690308, acc: 0.49180328845977783)
[2024-12-12 02:22:53,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:53,289][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 2.2036852836608887, acc: 0.380952388048172)
[2024-12-12 02:22:53,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:53,650][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 2.2956063747406006, acc: 0.37288135290145874)
[2024-12-12 02:22:53,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:54,007][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 1.592610478401184, acc: 0.5287356376647949)
[2024-12-12 02:22:54,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:54,375][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 1.0898401737213135, acc: 0.6190476417541504)
[2024-12-12 02:22:54,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:54,792][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 2.3429203033447266, acc: 0.38461539149284363)
[2024-12-12 02:22:54,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:55,241][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 2.834001302719116, acc: 0.28378379344940186)
[2024-12-12 02:22:55,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:55,634][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 2.266028642654419, acc: 0.3692307770252228)
[2024-12-12 02:22:55,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:56,044][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 2.462766647338867, acc: 0.38383838534355164)
[2024-12-12 02:22:56,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:56,446][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 2.128124237060547, acc: 0.4536082446575165)
[2024-12-12 02:22:56,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:56,844][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 2.2681987285614014, acc: 0.4485294222831726)
[2024-12-12 02:22:56,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:57,183][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.9063012599945068, acc: 0.807692289352417)
[2024-12-12 02:22:57,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:57,489][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.9603420495986938, acc: 0.7407407164573669)
[2024-12-12 02:22:57,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:57,843][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 1.3204174041748047, acc: 0.6428571343421936)
[2024-12-12 02:22:57,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:58,195][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 1.1544969081878662, acc: 0.7777777910232544)
[2024-12-12 02:22:58,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:58,590][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 1.5874981880187988, acc: 0.5964912176132202)
[2024-12-12 02:22:58,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:59,017][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 1.672645926475525, acc: 0.5396825671195984)
[2024-12-12 02:22:59,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:59,405][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 2.2656774520874023, acc: 0.3802816867828369)
[2024-12-12 02:22:59,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:59,888][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 2.4929850101470947, acc: 0.4266666769981384)
[2024-12-12 02:23:00,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:00,247][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 1.3869038820266724, acc: 0.6756756901741028)
[2024-12-12 02:23:00,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:00,623][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.7577346563339233, acc: 0.8461538553237915)
[2024-12-12 02:23:02,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:03,648][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 2.094789743423462, acc: 0.4880546033382416)
[2024-12-12 02:23:04,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:04,929][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 2.583362579345703, acc: 0.37472766637802124)
[2024-12-12 02:23:05,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:05,553][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 2.0652215480804443, acc: 0.4829545319080353)
[2024-12-12 02:23:05,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:06,122][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 2.362379789352417, acc: 0.41911765933036804)
[2024-12-12 02:23:06,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:06,692][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 2.413907051086426, acc: 0.3550724685192108)
[2024-12-12 02:23:06,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:07,095][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 1.8098790645599365, acc: 0.5625)
[2024-12-12 02:23:07,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:07,406][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 1.6265407800674438, acc: 0.5882353186607361)
[2024-12-12 02:23:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:07,793][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 1.9768872261047363, acc: 0.4444444477558136)
[2024-12-12 02:23:07,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:08,191][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 1.9431315660476685, acc: 0.5)
[2024-12-12 02:23:08,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:08,568][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.9222232699394226, acc: 0.7931034564971924)
[2024-12-12 02:23:08,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:08,950][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 2.479301691055298, acc: 0.3928571343421936)
[2024-12-12 02:23:09,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:09,340][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 2.266676902770996, acc: 0.3333333432674408)
[2024-12-12 02:23:09,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:09,748][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.9848505258560181, acc: 0.6399999856948853)
[2024-12-12 02:23:09,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:10,124][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 1.3877114057540894, acc: 0.6111111044883728)
[2024-12-12 02:23:10,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:10,517][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 1.6691510677337646, acc: 0.6060606241226196)
[2024-12-12 02:23:10,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:10,907][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 2.230208396911621, acc: 0.44117647409439087)
[2024-12-12 02:23:11,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:11,289][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 2.093909978866577, acc: 0.4285714328289032)
[2024-12-12 02:23:11,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:11,643][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 2.3949146270751953, acc: 0.35384616255760193)
[2024-12-12 02:23:11,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:11,978][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 2.1574342250823975, acc: 0.36734694242477417)
[2024-12-12 02:23:12,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:12,331][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 2.558396100997925, acc: 0.26119402050971985)
[2024-12-12 02:23:12,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:12,727][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 2.3161659240722656, acc: 0.38686132431030273)
[2024-12-12 02:23:12,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:13,077][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.6290634870529175, acc: 0.8095238208770752)
[2024-12-12 02:23:13,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:13,414][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.7564327716827393, acc: 0.8333333134651184)
[2024-12-12 02:23:13,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:13,717][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 1.2048028707504272, acc: 0.7272727489471436)
[2024-12-12 02:23:13,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:14,027][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 1.1238023042678833, acc: 0.692307710647583)
[2024-12-12 02:23:14,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:14,341][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 2.042696237564087, acc: 0.42307692766189575)
[2024-12-12 02:23:14,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:14,707][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 2.2733871936798096, acc: 0.4423076808452606)
[2024-12-12 02:23:14,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:15,054][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 1.6608076095581055, acc: 0.5625)
[2024-12-12 02:23:15,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:15,375][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 2.1991100311279297, acc: 0.4202898442745209)
[2024-12-12 02:23:15,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:15,704][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 1.6514009237289429, acc: 0.5600000023841858)
[2024-12-12 02:23:15,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:16,079][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 1.8981335163116455, acc: 0.52173912525177)
[2024-12-12 02:23:16,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:16,541][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 2.4249982833862305, acc: 0.3799999952316284)
[2024-12-12 02:23:16,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:16,952][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 2.0820865631103516, acc: 0.5242718458175659)
[2024-12-12 02:23:17,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:18,051][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 1.9882707595825195, acc: 0.5291262269020081)
[2024-12-12 02:23:18,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:18,864][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 2.1924989223480225, acc: 0.39247313141822815)
[2024-12-12 02:23:19,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:19,660][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 1.9272363185882568, acc: 0.5301724076271057)
[2024-12-12 02:23:19,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:20,398][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 1.6274571418762207, acc: 0.5368421077728271)
[2024-12-12 02:23:20,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:21,383][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 2.428485155105591, acc: 0.3465346395969391)
[2024-12-12 02:23:21,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:21,746][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 2.410282850265503, acc: 0.32258063554763794)
[2024-12-12 02:23:21,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:22,171][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 2.5383427143096924, acc: 0.3333333432674408)
[2024-12-12 02:23:22,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:22,587][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 2.6968700885772705, acc: 0.3025210201740265)
[2024-12-12 02:23:22,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:22,966][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 2.68833065032959, acc: 0.26923078298568726)
[2024-12-12 02:23:23,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:23,371][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 2.5435266494750977, acc: 0.35036495327949524)
[2024-12-12 02:23:23,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:23,772][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 2.546579599380493, acc: 0.3731343150138855)
[2024-12-12 02:23:23,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:24,180][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 1.2462859153747559, acc: 0.75)
[2024-12-12 02:23:24,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:24,582][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 1.0511736869812012, acc: 0.7272727489471436)
[2024-12-12 02:23:24,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:25,006][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 1.2041951417922974, acc: 0.6521739363670349)
[2024-12-12 02:23:25,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:25,371][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 1.7854267358779907, acc: 0.4545454680919647)
[2024-12-12 02:23:25,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:25,744][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 2.16409969329834, acc: 0.4482758641242981)
[2024-12-12 02:23:25,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:26,103][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 1.8840157985687256, acc: 0.4883720874786377)
[2024-12-12 02:23:26,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:26,505][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 1.1990363597869873, acc: 0.7200000286102295)
[2024-12-12 02:23:26,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:26,944][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.6979497075080872, acc: 0.8235294222831726)
[2024-12-12 02:23:27,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:27,339][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.5099035501480103, acc: 0.807692289352417)
[2024-12-12 02:23:27,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:27,713][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 1.8364633321762085, acc: 0.5)
[2024-12-12 02:23:27,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:28,103][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 1.977982521057129, acc: 0.4769230782985687)
[2024-12-12 02:23:28,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:28,522][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 2.052297830581665, acc: 0.4385964870452881)
[2024-12-12 02:23:28,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:28,922][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 2.030116558074951, acc: 0.42105263471603394)
[2024-12-12 02:23:29,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:29,318][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 2.2621922492980957, acc: 0.43589743971824646)
[2024-12-12 02:23:29,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:29,692][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 1.5441625118255615, acc: 0.6938775777816772)
[2024-12-12 02:23:29,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:30,061][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.5121002197265625, acc: 0.8181818127632141)
[2024-12-12 02:23:30,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:30,413][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 2.209341049194336, acc: 0.460317462682724)
[2024-12-12 02:23:30,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:30,737][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 2.1500778198242188, acc: 0.47154471278190613)
[2024-12-12 02:23:30,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:31,144][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 1.776187777519226, acc: 0.5322580933570862)
[2024-12-12 02:23:31,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:31,985][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 2.1254079341888428, acc: 0.4486691951751709)
[2024-12-12 02:23:32,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:32,356][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 1.768178939819336, acc: 0.47999998927116394)
[2024-12-12 02:23:32,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:32,753][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 1.6274759769439697, acc: 0.5961538553237915)
[2024-12-12 02:23:32,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:33,069][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.9116948246955872, acc: 0.8333333134651184)
[2024-12-12 02:23:33,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:33,435][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 1.9419291019439697, acc: 0.42105263471603394)
[2024-12-12 02:23:33,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:33,845][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 2.222254753112793, acc: 0.4049079716205597)
[2024-12-12 02:23:33,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:34,206][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 1.8912353515625, acc: 0.5277777910232544)
[2024-12-12 02:23:34,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:34,578][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 2.3567652702331543, acc: 0.3333333432674408)
[2024-12-12 02:23:34,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:34,976][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 2.401947259902954, acc: 0.3214285671710968)
[2024-12-12 02:23:35,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:35,370][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 2.1853151321411133, acc: 0.43589743971824646)
[2024-12-12 02:23:35,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:35,812][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 1.933354377746582, acc: 0.5)
[2024-12-12 02:23:35,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:36,165][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 1.1278951168060303, acc: 0.6538461446762085)
[2024-12-12 02:23:36,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:36,488][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 1.0121389627456665, acc: 0.695652186870575)
[2024-12-12 02:23:36,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:36,859][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 1.7890968322753906, acc: 0.4375)
[2024-12-12 02:23:36,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:37,198][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 2.021710157394409, acc: 0.3913043439388275)
[2024-12-12 02:23:37,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:37,524][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 1.4353193044662476, acc: 0.5714285969734192)
[2024-12-12 02:23:37,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:37,825][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 1.4213913679122925, acc: 0.5384615659713745)
[2024-12-12 02:23:37,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:38,132][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 2.2009594440460205, acc: 0.3095238208770752)
[2024-12-12 02:23:38,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:38,486][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 1.575518012046814, acc: 0.5333333611488342)
[2024-12-12 02:23:38,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:38,815][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 1.7818111181259155, acc: 0.52173912525177)
[2024-12-12 02:23:38,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:39,221][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 2.2747485637664795, acc: 0.523809552192688)
[2024-12-12 02:23:39,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:39,641][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 2.343892812728882, acc: 0.42307692766189575)
[2024-12-12 02:23:40,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:40,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:41,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:41,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:41,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:42,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:42,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:42,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:43,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:43,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:43,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:44,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:44,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:44,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:45,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:45,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:45,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:46,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:46,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:46,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:47,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:47,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:47,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:48,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:48,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:49,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:49,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:49,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:50,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:50,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:50,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:51,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:51,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:52,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:52,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:52,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:53,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:53,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:53,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:54,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:54,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:54,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:55,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:55,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:56,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:56,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:56,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:57,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:57,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:58,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:58,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:58,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:59,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:59,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:00,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:00,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:00,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:00,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:01,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:01,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:02,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:02,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:02,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:03,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:03,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:03,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:04,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:04,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:05,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:05,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:05,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:06,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:06,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:06,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:06,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:07,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:07,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:07,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:08,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:08,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:09,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:09,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:09,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:10,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:10,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:11,154][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(7.1732, device='cuda:0') eval_epoch_loss=tensor(1.9703, device='cuda:0') eval_epoch_acc=tensor(0.4666, device='cuda:0')
[2024-12-12 02:24:11,155][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:24:11,156][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:24:11,426][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_2_step_141_loss_1.970348596572876/model.pt
[2024-12-12 02:24:11,435][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:24:11,437][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.970348596572876
[2024-12-12 02:24:11,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:11,846][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 2.735074043273926, acc: 0.22580644488334656)
[2024-12-12 02:24:11,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:12,238][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 2.4730021953582764, acc: 0.2702702581882477)
[2024-12-12 02:24:12,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:12,761][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 2.1793971061706543, acc: 0.3684210479259491)
[2024-12-12 02:24:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:13,082][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 1.9414784908294678, acc: 0.43283581733703613)
[2024-12-12 02:24:13,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:13,484][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 2.4118492603302, acc: 0.3163265287876129)
[2024-12-12 02:24:13,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:13,917][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 2.1687605381011963, acc: 0.3404255211353302)
[2024-12-12 02:24:14,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:14,258][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 1.9393705129623413, acc: 0.48571428656578064)
[2024-12-12 02:24:14,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:14,625][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 2.1960718631744385, acc: 0.4285714328289032)
[2024-12-12 02:24:14,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:15,005][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 1.7421433925628662, acc: 0.52173912525177)
[2024-12-12 02:24:15,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:15,393][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 2.114894151687622, acc: 0.27586206793785095)
[2024-12-12 02:24:15,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:15,768][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 2.0879976749420166, acc: 0.47826087474823)
[2024-12-12 02:24:15,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:16,158][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 2.0391347408294678, acc: 0.4576271176338196)
[2024-12-12 02:24:16,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:16,498][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 2.452721118927002, acc: 0.38596490025520325)
[2024-12-12 02:24:16,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:16,851][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 2.0025057792663574, acc: 0.47297295928001404)
[2024-12-12 02:24:16,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:17,191][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 1.6717840433120728, acc: 0.5714285969734192)
[2024-12-12 02:24:17,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:17,456][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 1.3792240619659424, acc: 0.6086956262588501)
[2024-12-12 02:24:17,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:17,775][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 2.229895830154419, acc: 0.3684210479259491)
[2024-12-12 02:24:18,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:19,505][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 1.8310238122940063, acc: 0.5270270109176636)
[2024-12-12 02:24:19,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:19,808][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 2.218075752258301, acc: 0.46296295523643494)
[2024-12-12 02:24:19,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:20,193][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 2.0823419094085693, acc: 0.45348837971687317)
[2024-12-12 02:24:20,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:20,777][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 1.9577537775039673, acc: 0.4588235318660736)
[2024-12-12 02:24:20,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:21,332][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 2.258110284805298, acc: 0.43820226192474365)
[2024-12-12 02:24:21,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:21,693][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 1.8968415260314941, acc: 0.5)
[2024-12-12 02:24:21,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:22,084][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 1.7076703310012817, acc: 0.523809552192688)
[2024-12-12 02:24:22,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:22,409][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 1.9559162855148315, acc: 0.4137931168079376)
[2024-12-12 02:24:22,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:22,774][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 1.5457767248153687, acc: 0.5306122303009033)
[2024-12-12 02:24:22,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:23,103][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 1.8976963758468628, acc: 0.47999998927116394)
[2024-12-12 02:24:23,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:23,493][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 1.7483665943145752, acc: 0.5)
[2024-12-12 02:24:23,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:23,879][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 2.073047637939453, acc: 0.46078431606292725)
[2024-12-12 02:24:24,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:24,908][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 2.398037910461426, acc: 0.39726027846336365)
[2024-12-12 02:24:25,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:25,230][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 1.1845849752426147, acc: 0.75)
[2024-12-12 02:24:25,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:25,536][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 1.1372041702270508, acc: 0.6296296119689941)
[2024-12-12 02:24:25,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:25,855][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 1.7050434350967407, acc: 0.5357142686843872)
[2024-12-12 02:24:26,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:26,399][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 1.9410446882247925, acc: 0.5132743120193481)
[2024-12-12 02:24:26,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:26,741][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 1.9605540037155151, acc: 0.4202898442745209)
[2024-12-12 02:24:26,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:27,066][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 2.0859944820404053, acc: 0.375)
[2024-12-12 02:24:27,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:27,968][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 2.516631603240967, acc: 0.3435114622116089)
[2024-12-12 02:24:28,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:28,634][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 2.36846661567688, acc: 0.34074074029922485)
[2024-12-12 02:24:28,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:28,986][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 1.6631453037261963, acc: 0.5409836173057556)
[2024-12-12 02:24:29,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:29,348][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.7283990383148193, acc: 0.8333333134651184)
[2024-12-12 02:24:29,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:29,707][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 1.4246046543121338, acc: 0.6000000238418579)
[2024-12-12 02:24:29,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:30,038][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 1.2147190570831299, acc: 0.7142857313156128)
[2024-12-12 02:24:30,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:30,364][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 2.2432644367218018, acc: 0.3414634168148041)
[2024-12-12 02:24:30,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:30,746][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 2.3986213207244873, acc: 0.35951662063598633)
[2024-12-12 02:24:30,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:31,164][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 2.4792051315307617, acc: 0.35446685552597046)
[2024-12-12 02:24:31,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:31,642][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 2.4635541439056396, acc: 0.359375)
[2024-12-12 02:24:31,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:32,172][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 2.2387900352478027, acc: 0.3733583390712738)
[2024-12-12 02:24:32,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:32,606][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 2.269904136657715, acc: 0.38078293204307556)
[2024-12-12 02:24:32,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:32,972][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 2.581477165222168, acc: 0.5199999809265137)
[2024-12-12 02:24:33,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:33,517][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 2.359306573867798, acc: 0.40697672963142395)
[2024-12-12 02:24:33,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:34,307][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 2.087001323699951, acc: 0.5079365372657776)
[2024-12-12 02:24:34,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:35,223][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 2.1778628826141357, acc: 0.4318181872367859)
[2024-12-12 02:24:35,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:35,959][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 1.8586137294769287, acc: 0.529411792755127)
[2024-12-12 02:24:36,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:37,033][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 1.8353047370910645, acc: 0.5)
[2024-12-12 02:24:37,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:37,981][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 1.4992685317993164, acc: 0.5645161271095276)
[2024-12-12 02:24:38,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:38,291][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.7354835867881775, acc: 0.8571428656578064)
[2024-12-12 02:24:38,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:38,672][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 1.7687921524047852, acc: 0.574999988079071)
[2024-12-12 02:24:38,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:39,058][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 2.0134921073913574, acc: 0.47058823704719543)
[2024-12-12 02:24:39,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:39,495][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 2.074657678604126, acc: 0.47058823704719543)
[2024-12-12 02:24:39,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:39,879][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 2.36122727394104, acc: 0.3644067943096161)
[2024-12-12 02:24:39,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:40,238][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 2.401437520980835, acc: 0.4029850661754608)
[2024-12-12 02:24:40,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:40,611][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 2.2675814628601074, acc: 0.3786407709121704)
[2024-12-12 02:24:40,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:40,960][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 2.002589702606201, acc: 0.460317462682724)
[2024-12-12 02:24:41,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:41,291][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 2.0245471000671387, acc: 0.450549453496933)
[2024-12-12 02:24:41,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:41,686][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 2.2212252616882324, acc: 0.4035874307155609)
[2024-12-12 02:24:41,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:42,115][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 2.147393226623535, acc: 0.4606299102306366)
[2024-12-12 02:24:42,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:42,532][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 2.1898622512817383, acc: 0.4094827473163605)
[2024-12-12 02:24:42,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:42,884][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 2.06624436378479, acc: 0.4528985619544983)
[2024-12-12 02:24:43,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:43,277][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 2.3366992473602295, acc: 0.3774318993091583)
[2024-12-12 02:24:43,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:43,619][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 2.4503121376037598, acc: 0.3695652186870575)
[2024-12-12 02:24:43,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:43,961][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 1.4446910619735718, acc: 0.695652186870575)
[2024-12-12 02:24:44,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:44,320][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 1.7602134943008423, acc: 0.5714285969734192)
[2024-12-12 02:24:44,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:44,736][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 1.58156418800354, acc: 0.5106382966041565)
[2024-12-12 02:24:44,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:45,462][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 1.8861234188079834, acc: 0.4923076927661896)
[2024-12-12 02:24:45,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:45,777][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 1.728860855102539, acc: 0.5540540814399719)
[2024-12-12 02:24:45,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:46,082][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 1.7251276969909668, acc: 0.5581395626068115)
[2024-12-12 02:24:46,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:46,619][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 1.8567304611206055, acc: 0.522522509098053)
[2024-12-12 02:24:46,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:47,014][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 1.7969729900360107, acc: 0.4555555582046509)
[2024-12-12 02:24:47,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:47,411][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.8595123291015625, acc: 0.7878788113594055)
[2024-12-12 02:24:47,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:47,792][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.362638920545578, acc: 0.8888888955116272)
[2024-12-12 02:24:47,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:48,164][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.8490108251571655, acc: 0.8399999737739563)
[2024-12-12 02:24:48,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:48,573][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 2.1353611946105957, acc: 0.4423076808452606)
[2024-12-12 02:24:48,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:49,345][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 1.825900673866272, acc: 0.52173912525177)
[2024-12-12 02:24:49,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:49,881][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 2.030498743057251, acc: 0.4431818127632141)
[2024-12-12 02:24:50,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:50,324][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 2.256955623626709, acc: 0.40425533056259155)
[2024-12-12 02:24:50,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:50,690][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 1.7113755941390991, acc: 0.4716981053352356)
[2024-12-12 02:24:50,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:51,092][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 1.9375540018081665, acc: 0.5)
[2024-12-12 02:24:51,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:51,484][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 1.4249820709228516, acc: 0.6279069781303406)
[2024-12-12 02:24:51,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:51,814][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 1.3443107604980469, acc: 0.6333333253860474)
[2024-12-12 02:24:51,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:52,223][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 2.477961301803589, acc: 0.378947377204895)
[2024-12-12 02:24:52,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:52,616][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 1.9398105144500732, acc: 0.47777777910232544)
[2024-12-12 02:24:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:53,068][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 1.6937106847763062, acc: 0.5666666626930237)
[2024-12-12 02:24:53,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:53,577][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 1.8452414274215698, acc: 0.5321100950241089)
[2024-12-12 02:24:53,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:54,073][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 1.748975396156311, acc: 0.5384615659713745)
[2024-12-12 02:24:54,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:54,367][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 1.4313300848007202, acc: 0.6315789222717285)
[2024-12-12 02:24:54,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:54,659][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 1.4701929092407227, acc: 0.625)
[2024-12-12 02:24:54,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:55,026][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 2.9498136043548584, acc: 0.27272728085517883)
[2024-12-12 02:24:55,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:55,403][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 1.8354201316833496, acc: 0.40740740299224854)
[2024-12-12 02:24:55,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:55,739][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 1.5076947212219238, acc: 0.6000000238418579)
[2024-12-12 02:24:55,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:56,081][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 1.5960818529129028, acc: 0.5454545617103577)
[2024-12-12 02:24:56,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:56,368][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 1.8967152833938599, acc: 0.5454545617103577)
[2024-12-12 02:24:56,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:56,941][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 1.9513485431671143, acc: 0.4354838728904724)
[2024-12-12 02:24:57,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:57,465][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 1.6075466871261597, acc: 0.5681818127632141)
[2024-12-12 02:24:57,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:57,823][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.6442896723747253, acc: 0.9047619104385376)
[2024-12-12 02:24:57,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:58,147][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 1.4400862455368042, acc: 0.6153846383094788)
[2024-12-12 02:24:58,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:58,455][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 1.3347076177597046, acc: 0.7096773982048035)
[2024-12-12 02:24:58,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:58,777][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.7997505068778992, acc: 0.6499999761581421)
[2024-12-12 02:24:58,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:59,158][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 1.5132488012313843, acc: 0.5135135054588318)
[2024-12-12 02:24:59,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:59,537][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 1.6377915143966675, acc: 0.45945945382118225)
[2024-12-12 02:24:59,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:59,906][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 1.483497142791748, acc: 0.5675675868988037)
[2024-12-12 02:25:00,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:00,241][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 2.0309321880340576, acc: 0.3970588147640228)
[2024-12-12 02:25:00,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:00,644][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.7729728817939758, acc: 0.8292682766914368)
[2024-12-12 02:25:00,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:01,044][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.4957311749458313, acc: 0.8799999952316284)
[2024-12-12 02:25:01,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:01,444][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.2763976454734802, acc: 0.9599999785423279)
[2024-12-12 02:25:01,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:01,854][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.6921443343162537, acc: 0.774193525314331)
[2024-12-12 02:25:01,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:02,216][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 1.8439544439315796, acc: 0.4736842215061188)
[2024-12-12 02:25:02,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:02,568][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 2.0008435249328613, acc: 0.5285714268684387)
[2024-12-12 02:25:02,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:02,979][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 1.5888546705245972, acc: 0.5526315569877625)
[2024-12-12 02:25:03,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:03,540][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 1.942238688468933, acc: 0.40566039085388184)
[2024-12-12 02:25:03,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:04,130][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 1.9726508855819702, acc: 0.5)
[2024-12-12 02:25:04,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:04,483][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 1.3584150075912476, acc: 0.6111111044883728)
[2024-12-12 02:25:04,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:04,808][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 1.7946659326553345, acc: 0.5806451439857483)
[2024-12-12 02:25:04,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:05,136][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 2.818892002105713, acc: 0.3466666638851166)
[2024-12-12 02:25:05,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:05,421][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 2.3159892559051514, acc: 0.3958333432674408)
[2024-12-12 02:25:05,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:06,233][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 2.544949531555176, acc: 0.328000009059906)
[2024-12-12 02:25:06,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:06,585][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 2.245504379272461, acc: 0.3932584226131439)
[2024-12-12 02:25:06,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:06,928][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 2.271320343017578, acc: 0.4864864945411682)
[2024-12-12 02:25:07,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:07,370][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 1.4215550422668457, acc: 0.6034482717514038)
[2024-12-12 02:25:07,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:07,682][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 1.2217628955841064, acc: 0.6818181872367859)
[2024-12-12 02:25:07,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:08,015][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 1.0346051454544067, acc: 0.6363636255264282)
[2024-12-12 02:25:08,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:08,375][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.939763605594635, acc: 0.71875)
[2024-12-12 02:25:08,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:08,752][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 1.1122307777404785, acc: 0.6666666865348816)
[2024-12-12 02:25:08,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:09,132][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 2.068312883377075, acc: 0.4333333373069763)
[2024-12-12 02:25:09,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:09,478][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 1.4879522323608398, acc: 0.59375)
[2024-12-12 02:25:09,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:09,891][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.8913593292236328, acc: 0.7666666507720947)
[2024-12-12 02:25:09,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:10,238][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 1.4913119077682495, acc: 0.6896551847457886)
[2024-12-12 02:25:10,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:10,639][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 1.1251704692840576, acc: 0.6399999856948853)
[2024-12-12 02:25:10,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:11,009][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 2.2391390800476074, acc: 0.40425533056259155)
[2024-12-12 02:25:11,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:11,351][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 1.7693277597427368, acc: 0.5416666865348816)
[2024-12-12 02:25:11,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:11,738][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 1.6698492765426636, acc: 0.6590909361839294)
[2024-12-12 02:25:11,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:12,159][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 2.2055187225341797, acc: 0.4337349534034729)
[2024-12-12 02:25:12,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:12,489][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 2.0974011421203613, acc: 0.5)
[2024-12-12 02:25:12,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:12,817][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 2.2884278297424316, acc: 0.31578946113586426)
[2024-12-12 02:25:13,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:13,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:14,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:14,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:14,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:15,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:15,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:16,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:16,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:16,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:17,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:17,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:17,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:18,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:18,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:18,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:19,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:19,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:20,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:20,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:20,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:21,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:21,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:21,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:22,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:22,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:23,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:23,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:23,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:24,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:24,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:24,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:25,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:25,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:25,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:26,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:26,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:27,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:27,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:28,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:28,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:28,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:28,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:29,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:29,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:29,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:30,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:30,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:30,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:31,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:31,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:31,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:32,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:32,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:32,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:33,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:33,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:34,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:34,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:34,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:35,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:35,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:35,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:36,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:36,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:36,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:37,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:37,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:38,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:38,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:39,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:39,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:39,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:40,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:40,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:40,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:40,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:41,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:41,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:42,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:42,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:42,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:43,405][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(6.2167, device='cuda:0') eval_epoch_loss=tensor(1.8272, device='cuda:0') eval_epoch_acc=tensor(0.5149, device='cuda:0')
[2024-12-12 02:25:43,406][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:25:43,406][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:25:43,746][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_2_step_284_loss_1.8272395133972168/model.pt
[2024-12-12 02:25:43,750][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:25:43,750][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.8272395133972168
[2024-12-12 02:25:43,751][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.5148565173149109
[2024-12-12 02:25:43,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:44,123][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 2.353013753890991, acc: 0.3235294222831726)
[2024-12-12 02:25:44,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:44,510][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 1.9235607385635376, acc: 0.44999998807907104)
[2024-12-12 02:25:44,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:44,881][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 2.1636083126068115, acc: 0.3671875)
[2024-12-12 02:25:44,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:45,217][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 2.396432399749756, acc: 0.31200000643730164)
[2024-12-12 02:25:45,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:45,581][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 1.9642236232757568, acc: 0.5164835453033447)
[2024-12-12 02:25:45,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:45,889][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 2.3270604610443115, acc: 0.3602484464645386)
[2024-12-12 02:25:45,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:46,271][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 2.4502556324005127, acc: 0.3505154550075531)
[2024-12-12 02:25:46,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:46,672][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.49857795238494873, acc: 0.8181818127632141)
[2024-12-12 02:25:46,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:47,090][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 2.0048816204071045, acc: 0.4523809552192688)
[2024-12-12 02:25:47,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:47,504][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 1.5066323280334473, acc: 0.6034482717514038)
[2024-12-12 02:25:47,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:47,993][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 1.388797640800476, acc: 0.6363636255264282)
[2024-12-12 02:25:48,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:48,557][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 1.815443515777588, acc: 0.5257731676101685)
[2024-12-12 02:25:48,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:48,978][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 2.1262643337249756, acc: 0.4137931168079376)
[2024-12-12 02:25:49,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:49,347][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 1.7553315162658691, acc: 0.5555555820465088)
[2024-12-12 02:25:49,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:49,757][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 1.852852463722229, acc: 0.5263158082962036)
[2024-12-12 02:25:49,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:50,122][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 1.6990156173706055, acc: 0.5892857313156128)
[2024-12-12 02:25:50,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:50,412][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 1.757707118988037, acc: 0.5)
[2024-12-12 02:25:50,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:50,720][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 1.9791535139083862, acc: 0.49056604504585266)
[2024-12-12 02:25:50,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:51,110][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 1.1403067111968994, acc: 0.6415094137191772)
[2024-12-12 02:25:51,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:51,453][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 1.3102391958236694, acc: 0.7058823704719543)
[2024-12-12 02:25:51,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:51,766][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 1.9401909112930298, acc: 0.5)
[2024-12-12 02:25:51,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:52,095][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 1.3559954166412354, acc: 0.6557376980781555)
[2024-12-12 02:25:52,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:52,444][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.9496486783027649, acc: 0.7666666507720947)
[2024-12-12 02:25:52,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:52,742][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.3068723976612091, acc: 0.9473684430122375)
[2024-12-12 02:25:52,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:53,053][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 2.103703498840332, acc: 0.4637681245803833)
[2024-12-12 02:25:53,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:53,472][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 1.891385555267334, acc: 0.5138888955116272)
[2024-12-12 02:25:53,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:53,813][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 1.693270206451416, acc: 0.4939759075641632)
[2024-12-12 02:25:53,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:54,165][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 2.1702449321746826, acc: 0.39743590354919434)
[2024-12-12 02:25:54,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:54,559][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 2.348494291305542, acc: 0.3979591727256775)
[2024-12-12 02:25:54,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:54,946][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.28579404950141907, acc: 0.9583333134651184)
[2024-12-12 02:25:55,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:55,326][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 1.1039865016937256, acc: 0.6666666865348816)
[2024-12-12 02:25:55,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:55,613][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.9954507946968079, acc: 0.7419354915618896)
[2024-12-12 02:25:55,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:55,974][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 1.3774832487106323, acc: 0.5806451439857483)
[2024-12-12 02:25:56,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:56,312][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 1.422589659690857, acc: 0.611940324306488)
[2024-12-12 02:25:56,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:56,632][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 1.398460865020752, acc: 0.5865384340286255)
[2024-12-12 02:25:56,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:56,970][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 2.0024423599243164, acc: 0.42222222685813904)
[2024-12-12 02:25:57,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:57,286][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 1.552109718322754, acc: 0.5967742204666138)
[2024-12-12 02:25:57,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:57,567][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.9552274346351624, acc: 0.7400000095367432)
[2024-12-12 02:25:57,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:57,908][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 2.4844324588775635, acc: 0.29629629850387573)
[2024-12-12 02:25:58,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:58,291][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 3.2173802852630615, acc: 0.17142857611179352)
[2024-12-12 02:25:58,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:58,657][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 2.349810838699341, acc: 0.3333333432674408)
[2024-12-12 02:25:58,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:59,021][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 2.5564892292022705, acc: 0.4146341383457184)
[2024-12-12 02:25:59,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:59,392][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 2.219691514968872, acc: 0.3947368562221527)
[2024-12-12 02:25:59,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:59,724][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 1.32582426071167, acc: 0.6315789222717285)
[2024-12-12 02:25:59,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:00,039][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.5889264345169067, acc: 0.8214285969734192)
[2024-12-12 02:26:00,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:00,325][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 1.9733976125717163, acc: 0.40740740299224854)
[2024-12-12 02:26:00,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:00,703][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.7951448559761047, acc: 0.78125)
[2024-12-12 02:26:00,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:01,074][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 1.8116528987884521, acc: 0.4516128897666931)
[2024-12-12 02:26:01,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:01,471][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 1.5950634479522705, acc: 0.5087719559669495)
[2024-12-12 02:26:01,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:01,816][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 2.1105380058288574, acc: 0.34375)
[2024-12-12 02:26:01,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:02,188][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 1.2346851825714111, acc: 0.699999988079071)
[2024-12-12 02:26:02,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:02,544][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 1.6715002059936523, acc: 0.4736842215061188)
[2024-12-12 02:26:02,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:02,977][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 2.1237094402313232, acc: 0.46000000834465027)
[2024-12-12 02:26:03,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:03,384][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 2.3282182216644287, acc: 0.37931033968925476)
[2024-12-12 02:26:03,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:03,789][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 2.4595701694488525, acc: 0.3085106313228607)
[2024-12-12 02:26:03,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:04,127][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 2.4332351684570312, acc: 0.3614457845687866)
[2024-12-12 02:26:04,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:04,409][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.9130041599273682, acc: 0.6521739363670349)
[2024-12-12 02:26:04,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:04,734][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 1.735290288925171, acc: 0.5641025900840759)
[2024-12-12 02:26:04,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:05,077][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 2.337067127227783, acc: 0.42168673872947693)
[2024-12-12 02:26:05,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:05,430][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 1.7594650983810425, acc: 0.5471698045730591)
[2024-12-12 02:26:05,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:05,766][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 1.8796526193618774, acc: 0.49367088079452515)
[2024-12-12 02:26:05,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:06,111][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 1.828800082206726, acc: 0.5686274766921997)
[2024-12-12 02:26:06,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:06,453][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 2.386121988296509, acc: 0.3283582031726837)
[2024-12-12 02:26:06,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:06,803][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.6242591142654419, acc: 0.8500000238418579)
[2024-12-12 02:26:06,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:07,146][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 1.3058353662490845, acc: 0.6800000071525574)
[2024-12-12 02:26:07,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:07,521][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 1.3177212476730347, acc: 0.6944444179534912)
[2024-12-12 02:26:07,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:07,832][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 1.825153112411499, acc: 0.4883720874786377)
[2024-12-12 02:26:07,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:08,194][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 1.7722030878067017, acc: 0.4871794879436493)
[2024-12-12 02:26:08,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:08,588][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 1.9828230142593384, acc: 0.4000000059604645)
[2024-12-12 02:26:08,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:08,929][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.44431930780410767, acc: 0.9130434989929199)
[2024-12-12 02:26:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:09,275][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 2.15932559967041, acc: 0.5)
[2024-12-12 02:26:09,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:09,645][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 2.3646254539489746, acc: 0.4395604431629181)
[2024-12-12 02:26:09,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:10,158][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 1.88829505443573, acc: 0.5043478012084961)
[2024-12-12 02:26:10,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:10,529][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 1.9312299489974976, acc: 0.510869562625885)
[2024-12-12 02:26:10,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:10,882][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 1.9159834384918213, acc: 0.4693877696990967)
[2024-12-12 02:26:10,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:11,220][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.23984979093074799, acc: 0.9583333134651184)
[2024-12-12 02:26:11,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:11,599][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.9096727967262268, acc: 0.807692289352417)
[2024-12-12 02:26:11,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:11,943][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 1.6071637868881226, acc: 0.6341463327407837)
[2024-12-12 02:26:12,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:12,333][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 1.8067129850387573, acc: 0.46666666865348816)
[2024-12-12 02:26:12,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:12,667][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 1.9838708639144897, acc: 0.5263158082962036)
[2024-12-12 02:26:12,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:13,034][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 1.94573175907135, acc: 0.5609756112098694)
[2024-12-12 02:26:13,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:13,421][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 1.530929684638977, acc: 0.5757575631141663)
[2024-12-12 02:26:13,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:13,771][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.5780197978019714, acc: 0.7916666865348816)
[2024-12-12 02:26:13,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:14,120][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.31264814734458923, acc: 0.9130434989929199)
[2024-12-12 02:26:14,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:14,497][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.6334091424942017, acc: 0.8571428656578064)
[2024-12-12 02:26:14,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:14,898][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 1.4837924242019653, acc: 0.5625)
[2024-12-12 02:26:15,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:15,493][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 2.044461250305176, acc: 0.4545454680919647)
[2024-12-12 02:26:15,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:16,361][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 1.504789113998413, acc: 0.6226415038108826)
[2024-12-12 02:26:16,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:16,775][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 1.7316899299621582, acc: 0.5222222208976746)
[2024-12-12 02:26:16,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:17,117][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 1.5536717176437378, acc: 0.5714285969734192)
[2024-12-12 02:26:17,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:17,491][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.9807326197624207, acc: 0.800000011920929)
[2024-12-12 02:26:17,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:17,851][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.3377581536769867, acc: 0.9200000166893005)
[2024-12-12 02:26:17,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:18,175][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.3621460497379303, acc: 0.8695651888847351)
[2024-12-12 02:26:18,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:18,529][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 1.914656639099121, acc: 0.4166666567325592)
[2024-12-12 02:26:18,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:18,869][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 1.73014235496521, acc: 0.557894766330719)
[2024-12-12 02:26:19,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:19,438][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 1.859439492225647, acc: 0.538922131061554)
[2024-12-12 02:26:19,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:19,837][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 1.6107608079910278, acc: 0.5639097690582275)
[2024-12-12 02:26:20,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:21,045][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 1.721278429031372, acc: 0.5508021116256714)
[2024-12-12 02:26:21,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:21,606][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 1.341142177581787, acc: 0.6216216087341309)
[2024-12-12 02:26:21,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:21,981][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.6059584021568298, acc: 0.8571428656578064)
[2024-12-12 02:26:22,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:22,354][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.7942342758178711, acc: 0.7857142686843872)
[2024-12-12 02:26:22,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:22,690][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.8815169334411621, acc: 0.6875)
[2024-12-12 02:26:22,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:23,062][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 1.1437658071517944, acc: 0.7222222089767456)
[2024-12-12 02:26:23,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:23,431][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.9860666990280151, acc: 0.7105262875556946)
[2024-12-12 02:26:23,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:23,762][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.5985215902328491, acc: 0.8181818127632141)
[2024-12-12 02:26:23,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:24,067][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.6006590723991394, acc: 0.8500000238418579)
[2024-12-12 02:26:24,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:24,389][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.9849581122398376, acc: 0.8095238208770752)
[2024-12-12 02:26:24,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:24,697][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 2.514361619949341, acc: 0.42592594027519226)
[2024-12-12 02:26:24,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:25,062][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 2.3532767295837402, acc: 0.3883495032787323)
[2024-12-12 02:26:25,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:25,578][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 1.872491717338562, acc: 0.5514705777168274)
[2024-12-12 02:26:25,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:25,976][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 2.2795794010162354, acc: 0.4333333373069763)
[2024-12-12 02:26:26,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:26,386][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 2.084970235824585, acc: 0.4791666567325592)
[2024-12-12 02:26:26,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:26,764][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 1.9598308801651, acc: 0.5581395626068115)
[2024-12-12 02:26:26,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:27,123][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.6176095604896545, acc: 0.8333333134651184)
[2024-12-12 02:26:27,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:27,519][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 1.4266002178192139, acc: 0.5581395626068115)
[2024-12-12 02:26:27,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:27,844][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 1.2278056144714355, acc: 0.6800000071525574)
[2024-12-12 02:26:27,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:28,372][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 1.7617900371551514, acc: 0.5588235259056091)
[2024-12-12 02:26:28,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:28,739][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 1.5866057872772217, acc: 0.6133333444595337)
[2024-12-12 02:26:28,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:29,080][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 1.236295461654663, acc: 0.6363636255264282)
[2024-12-12 02:26:29,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:29,400][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 1.1225513219833374, acc: 0.6666666865348816)
[2024-12-12 02:26:29,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:29,698][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.8527750968933105, acc: 0.774193525314331)
[2024-12-12 02:26:29,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:29,978][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 1.0405032634735107, acc: 0.7037037014961243)
[2024-12-12 02:26:30,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:30,283][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.4578455984592438, acc: 0.9599999785423279)
[2024-12-12 02:26:30,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:30,607][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.6676235198974609, acc: 0.8055555820465088)
[2024-12-12 02:26:30,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:30,975][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.6667335629463196, acc: 0.8148148059844971)
[2024-12-12 02:26:31,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:31,359][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.7592382431030273, acc: 0.7307692170143127)
[2024-12-12 02:26:31,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:31,701][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 1.2030619382858276, acc: 0.6896551847457886)
[2024-12-12 02:26:31,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:32,077][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.5387996435165405, acc: 0.8928571343421936)
[2024-12-12 02:26:32,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:32,438][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.9934912919998169, acc: 0.800000011920929)
[2024-12-12 02:26:32,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:32,823][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.8863327503204346, acc: 0.9090909361839294)
[2024-12-12 02:26:32,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:33,149][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 1.0261515378952026, acc: 0.8181818127632141)
[2024-12-12 02:26:33,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:33,501][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 1.7339402437210083, acc: 0.529411792755127)
[2024-12-12 02:26:33,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:33,843][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 1.5784903764724731, acc: 0.692307710647583)
[2024-12-12 02:26:33,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:34,170][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 1.451119065284729, acc: 0.6111111044883728)
[2024-12-12 02:26:34,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:34,564][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 1.2921193838119507, acc: 0.699999988079071)
[2024-12-12 02:26:34,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:34,890][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 1.5188045501708984, acc: 0.699999988079071)
[2024-12-12 02:26:34,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:35,222][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.4301377236843109, acc: 0.8571428656578064)
[2024-12-12 02:26:35,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:35,572][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 1.0249205827713013, acc: 0.7666666507720947)
[2024-12-12 02:26:35,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:35,966][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 1.3499587774276733, acc: 0.6875)
[2024-12-12 02:26:36,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:36,331][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 1.4712470769882202, acc: 0.5833333134651184)
[2024-12-12 02:26:36,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:36,629][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.7795658111572266, acc: 0.7407407164573669)
[2024-12-12 02:26:36,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:37,038][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 1.0878651142120361, acc: 0.7272727489471436)
[2024-12-12 02:26:37,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:37,410][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.6558588147163391, acc: 0.8695651888847351)
[2024-12-12 02:26:38,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:38,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:38,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:39,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:39,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:39,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:40,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:41,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:41,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:41,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:42,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:42,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:42,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:43,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:43,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:43,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:44,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:44,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:45,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:45,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:45,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:46,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:46,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:46,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:47,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:47,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:48,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:48,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:48,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:49,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:49,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:49,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:50,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:50,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:50,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:51,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:51,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:51,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:52,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:52,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:53,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:53,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:53,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:54,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:54,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:54,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:55,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:55,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:55,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:56,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:56,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:56,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:58,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:58,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:58,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:59,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:59,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:00,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:00,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:00,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:01,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:01,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:02,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:02,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:02,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:03,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:03,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:03,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:04,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:04,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:04,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:05,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:05,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:05,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:06,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:06,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:07,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:07,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:08,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:08,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:09,108][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.7495, device='cuda:0') eval_epoch_loss=tensor(1.7491, device='cuda:0') eval_epoch_acc=tensor(0.5159, device='cuda:0')
[2024-12-12 02:27:09,110][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:27:09,110][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:27:09,607][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_2_step_427_loss_1.749109148979187/model.pt
[2024-12-12 02:27:09,614][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:27:09,615][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.749109148979187
[2024-12-12 02:27:09,616][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.5158596038818359
[2024-12-12 02:27:09,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:09,999][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.960318386554718, acc: 0.7297297120094299)
[2024-12-12 02:27:10,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:10,398][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.5868188738822937, acc: 0.8518518805503845)
[2024-12-12 02:27:10,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:10,762][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 1.1731340885162354, acc: 0.739130437374115)
[2024-12-12 02:27:10,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:11,141][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.31483814120292664, acc: 0.9629629850387573)
[2024-12-12 02:27:11,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:11,521][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.21529467403888702, acc: 1.0)
[2024-12-12 02:27:11,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:11,873][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.7805629968643188, acc: 0.695652186870575)
[2024-12-12 02:27:11,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:12,233][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 1.1483923196792603, acc: 0.7222222089767456)
[2024-12-12 02:27:12,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:12,629][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.3093644380569458, acc: 0.9200000166893005)
[2024-12-12 02:27:12,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:12,972][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.9320265650749207, acc: 0.7272727489471436)
[2024-12-12 02:27:13,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:13,276][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 1.0629980564117432, acc: 0.7777777910232544)
[2024-12-12 02:27:13,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:13,631][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 1.4672455787658691, acc: 0.6363636255264282)
[2024-12-12 02:27:13,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:13,953][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.22582204639911652, acc: 0.9047619104385376)
[2024-12-12 02:27:14,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:14,244][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 1.3643741607666016, acc: 0.6410256624221802)
[2024-12-12 02:27:14,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:14,715][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 1.9403804540634155, acc: 0.4848484992980957)
[2024-12-12 02:27:14,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:15,430][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 2.4300150871276855, acc: 0.328000009059906)
[2024-12-12 02:27:15,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:15,821][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 2.2057788372039795, acc: 0.42741936445236206)
[2024-12-12 02:27:15,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:16,465][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 2.1444830894470215, acc: 0.4427860677242279)
[2024-12-12 02:27:16,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:16,833][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 1.8523916006088257, acc: 0.5094339847564697)
[2024-12-12 02:27:16,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:17,266][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 1.030747890472412, acc: 0.7045454382896423)
[2024-12-12 02:27:17,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:17,597][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 1.0414477586746216, acc: 0.8260869383811951)
[2024-12-12 02:27:17,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:18,001][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 1.3608477115631104, acc: 0.692307710647583)
[2024-12-12 02:27:18,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:18,354][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.7756872177124023, acc: 0.75)
[2024-12-12 02:27:18,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:18,697][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 1.7740837335586548, acc: 0.5373134613037109)
[2024-12-12 02:27:18,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:19,085][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 1.3557977676391602, acc: 0.6388888955116272)
[2024-12-12 02:27:19,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:19,473][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 1.5636597871780396, acc: 0.554347813129425)
[2024-12-12 02:27:19,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:19,777][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 1.704404592514038, acc: 0.5512820482254028)
[2024-12-12 02:27:19,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:20,131][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 1.8217867612838745, acc: 0.5394737124443054)
[2024-12-12 02:27:20,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:20,450][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 1.3398919105529785, acc: 0.6326530575752258)
[2024-12-12 02:27:20,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:20,724][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 0.9734316468238831, acc: 0.7272727489471436)
[2024-12-12 02:27:20,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:21,104][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 1.8686376810073853, acc: 0.4845360815525055)
[2024-12-12 02:27:21,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:21,475][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 1.473871111869812, acc: 0.6142857074737549)
[2024-12-12 02:27:21,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:21,892][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 1.901613473892212, acc: 0.49418604373931885)
[2024-12-12 02:27:21,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:22,223][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 1.8160316944122314, acc: 0.4464285671710968)
[2024-12-12 02:27:22,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:22,562][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 1.7667925357818604, acc: 0.5555555820465088)
[2024-12-12 02:27:22,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:22,878][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 1.4579977989196777, acc: 0.5833333134651184)
[2024-12-12 02:27:23,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:23,256][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 1.0372416973114014, acc: 0.71875)
[2024-12-12 02:27:23,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:23,627][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.8365563750267029, acc: 0.6538461446762085)
[2024-12-12 02:27:23,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:24,021][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 1.2088755369186401, acc: 0.6521739363670349)
[2024-12-12 02:27:24,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:24,372][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 1.436905860900879, acc: 0.6071428656578064)
[2024-12-12 02:27:24,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:24,709][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 2.0099551677703857, acc: 0.4457831382751465)
[2024-12-12 02:27:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:25,143][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 1.5605489015579224, acc: 0.522522509098053)
[2024-12-12 02:27:25,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:25,529][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 1.874220609664917, acc: 0.5048543810844421)
[2024-12-12 02:27:25,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:25,877][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 1.60676908493042, acc: 0.5609756112098694)
[2024-12-12 02:27:25,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:26,145][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.9897679686546326, acc: 0.7083333134651184)
[2024-12-12 02:27:26,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:26,495][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 1.7264043092727661, acc: 0.4642857015132904)
[2024-12-12 02:27:26,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:26,917][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 2.0046801567077637, acc: 0.47058823704719543)
[2024-12-12 02:27:27,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:27,321][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 2.257005214691162, acc: 0.44541484117507935)
[2024-12-12 02:27:27,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:27,660][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 1.8322458267211914, acc: 0.4791666567325592)
[2024-12-12 02:27:27,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:27,999][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 1.7387738227844238, acc: 0.5705521702766418)
[2024-12-12 02:27:28,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:28,350][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 1.7156620025634766, acc: 0.5395683646202087)
[2024-12-12 02:27:28,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:28,742][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 2.008241653442383, acc: 0.48241207003593445)
[2024-12-12 02:27:28,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:29,131][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 1.2746903896331787, acc: 0.6666666865348816)
[2024-12-12 02:27:29,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:29,475][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 1.0580809116363525, acc: 0.6060606241226196)
[2024-12-12 02:27:29,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:29,845][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.922042965888977, acc: 0.7407407164573669)
[2024-12-12 02:27:29,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:30,189][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 1.2375385761260986, acc: 0.5)
[2024-12-12 02:27:30,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:30,547][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.6887032985687256, acc: 0.800000011920929)
[2024-12-12 02:27:30,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:30,945][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 1.2869601249694824, acc: 0.6379310488700867)
[2024-12-12 02:27:31,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:31,312][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.8673630952835083, acc: 0.8064516186714172)
[2024-12-12 02:27:31,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:31,678][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.6499969363212585, acc: 0.8947368264198303)
[2024-12-12 02:27:31,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:32,056][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 1.683676838874817, acc: 0.5925925970077515)
[2024-12-12 02:27:32,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:32,423][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 1.3808653354644775, acc: 0.4761904776096344)
[2024-12-12 02:27:32,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:32,782][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.8674845099449158, acc: 0.7272727489471436)
[2024-12-12 02:27:32,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:33,180][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 1.91950523853302, acc: 0.4923076927661896)
[2024-12-12 02:27:33,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:33,545][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 1.0248818397521973, acc: 0.699999988079071)
[2024-12-12 02:27:33,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:33,888][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 1.04270339012146, acc: 0.7241379022598267)
[2024-12-12 02:27:34,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:34,252][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 1.5121102333068848, acc: 0.529411792755127)
[2024-12-12 02:27:34,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:34,616][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 1.0356910228729248, acc: 0.6206896305084229)
[2024-12-12 02:27:34,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:34,981][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.9693726897239685, acc: 0.7368420958518982)
[2024-12-12 02:27:35,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:35,333][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 2.5308918952941895, acc: 0.42105263471603394)
[2024-12-12 02:27:35,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:35,713][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 1.7842607498168945, acc: 0.5)
[2024-12-12 02:27:35,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:36,133][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 1.40985906124115, acc: 0.6067415475845337)
[2024-12-12 02:27:36,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:36,484][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 1.7984929084777832, acc: 0.5056179761886597)
[2024-12-12 02:27:36,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:36,885][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 2.052117347717285, acc: 0.39716312289237976)
[2024-12-12 02:27:37,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:37,208][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 1.856226921081543, acc: 0.489130437374115)
[2024-12-12 02:27:37,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:37,540][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.5029206275939941, acc: 0.8399999737739563)
[2024-12-12 02:27:37,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:37,899][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.6866331696510315, acc: 0.7692307829856873)
[2024-12-12 02:27:37,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:38,214][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.8261663913726807, acc: 0.7777777910232544)
[2024-12-12 02:27:38,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:38,552][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 1.3150267601013184, acc: 0.5925925970077515)
[2024-12-12 02:27:38,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:38,868][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 1.1038026809692383, acc: 0.7169811129570007)
[2024-12-12 02:27:38,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:39,164][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 1.202243685722351, acc: 0.6896551847457886)
[2024-12-12 02:27:39,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:39,769][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 1.9158753156661987, acc: 0.5045045018196106)
[2024-12-12 02:27:39,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:40,203][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 1.6118966341018677, acc: 0.591549277305603)
[2024-12-12 02:27:40,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:40,568][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.3341773748397827, acc: 0.8500000238418579)
[2024-12-12 02:27:40,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:40,905][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.2262703776359558, acc: 0.9666666388511658)
[2024-12-12 02:27:41,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:41,272][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.9697580337524414, acc: 0.7307692170143127)
[2024-12-12 02:27:42,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:44,041][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 2.153982639312744, acc: 0.4357142746448517)
[2024-12-12 02:27:44,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:44,797][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 1.694207787513733, acc: 0.523809552192688)
[2024-12-12 02:27:44,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:45,083][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 1.3976253271102905, acc: 0.6428571343421936)
[2024-12-12 02:27:45,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:45,405][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.9162180423736572, acc: 0.7166666388511658)
[2024-12-12 02:27:45,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:46,104][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 1.3424923419952393, acc: 0.6527777910232544)
[2024-12-12 02:27:46,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:46,404][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.12039060890674591, acc: 1.0)
[2024-12-12 02:27:46,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:46,698][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 1.0557372570037842, acc: 0.7419354915618896)
[2024-12-12 02:27:46,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:47,019][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 1.6690372228622437, acc: 0.6000000238418579)
[2024-12-12 02:27:47,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:47,344][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 1.3856123685836792, acc: 0.5925925970077515)
[2024-12-12 02:27:47,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:48,351][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 1.7862226963043213, acc: 0.5338982939720154)
[2024-12-12 02:27:48,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:48,762][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 1.4749417304992676, acc: 0.611940324306488)
[2024-12-12 02:27:48,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:49,216][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 1.577291488647461, acc: 0.5912408828735352)
[2024-12-12 02:27:49,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:49,785][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 1.5769022703170776, acc: 0.5649999976158142)
[2024-12-12 02:27:49,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:50,124][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.9039351940155029, acc: 0.7777777910232544)
[2024-12-12 02:27:50,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:50,454][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.9927673935890198, acc: 0.6346153616905212)
[2024-12-12 02:27:50,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:50,770][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 1.3810052871704102, acc: 0.6190476417541504)
[2024-12-12 02:27:50,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:51,095][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 2.723900556564331, acc: 0.2950819730758667)
[2024-12-12 02:27:51,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:51,473][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 1.3226299285888672, acc: 0.6440678238868713)
[2024-12-12 02:27:51,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:51,820][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 2.1982779502868652, acc: 0.4883720874786377)
[2024-12-12 02:27:51,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:52,153][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 1.9426701068878174, acc: 0.47727271914482117)
[2024-12-12 02:27:52,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:52,487][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 2.1698410511016846, acc: 0.37735849618911743)
[2024-12-12 02:27:52,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:52,786][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 1.5624843835830688, acc: 0.6136363744735718)
[2024-12-12 02:27:52,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:53,053][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 1.2807211875915527, acc: 0.6399999856948853)
[2024-12-12 02:27:53,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:53,343][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.9219695329666138, acc: 0.8500000238418579)
[2024-12-12 02:27:53,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:53,655][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.730897843837738, acc: 0.7727272510528564)
[2024-12-12 02:27:53,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:54,052][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 1.2572160959243774, acc: 0.6307692527770996)
[2024-12-12 02:27:54,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:54,423][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 1.2876402139663696, acc: 0.640625)
[2024-12-12 02:27:54,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:54,824][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.8860209584236145, acc: 0.78125)
[2024-12-12 02:27:54,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:55,128][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 1.4098814725875854, acc: 0.6666666865348816)
[2024-12-12 02:27:55,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:55,441][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.5299789309501648, acc: 0.8125)
[2024-12-12 02:27:55,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:55,733][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.652511715888977, acc: 0.7419354915618896)
[2024-12-12 02:27:55,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:56,025][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.48470252752304077, acc: 0.8695651888847351)
[2024-12-12 02:27:56,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:56,297][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 1.569036602973938, acc: 0.6000000238418579)
[2024-12-12 02:27:56,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:56,583][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.5990501046180725, acc: 0.7804877758026123)
[2024-12-12 02:27:56,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:56,875][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.3924187123775482, acc: 0.8857142925262451)
[2024-12-12 02:27:56,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:57,220][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.6053520441055298, acc: 0.8421052694320679)
[2024-12-12 02:27:57,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:57,556][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.6091534495353699, acc: 0.8387096524238586)
[2024-12-12 02:27:57,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:57,908][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.2544008195400238, acc: 0.8799999952316284)
[2024-12-12 02:27:58,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:58,278][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.7675566673278809, acc: 0.8181818127632141)
[2024-12-12 02:27:58,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:58,657][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.7301668524742126, acc: 0.7749999761581421)
[2024-12-12 02:27:58,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:58,982][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.7547121644020081, acc: 0.7285714149475098)
[2024-12-12 02:27:59,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:59,355][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 1.4660321474075317, acc: 0.5839415788650513)
[2024-12-12 02:27:59,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:59,679][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 1.1398730278015137, acc: 0.6758620738983154)
[2024-12-12 02:27:59,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:00,034][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 1.842158555984497, acc: 0.5571428537368774)
[2024-12-12 02:28:00,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:00,441][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 1.556073784828186, acc: 0.5960264801979065)
[2024-12-12 02:28:00,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:00,804][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 1.3386478424072266, acc: 0.6666666865348816)
[2024-12-12 02:28:00,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:01,156][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.33247238397598267, acc: 0.8799999952316284)
[2024-12-12 02:28:01,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:01,464][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 1.362467885017395, acc: 0.5769230723381042)
[2024-12-12 02:28:01,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:01,784][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.3642221689224243, acc: 0.9230769276618958)
[2024-12-12 02:28:01,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:02,114][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 1.2709070444107056, acc: 0.692307710647583)
[2024-12-12 02:28:02,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:02,511][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 1.1403067111968994, acc: 0.7111111283302307)
[2024-12-12 02:28:02,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:02,925][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 1.1358906030654907, acc: 0.701298713684082)
[2024-12-12 02:28:03,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:03,331][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 1.2321768999099731, acc: 0.625)
[2024-12-12 02:28:03,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:03,718][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.8858984708786011, acc: 0.7413793206214905)
[2024-12-12 02:28:03,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:04,090][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 1.205271601676941, acc: 0.5833333134651184)
[2024-12-12 02:28:04,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:04,486][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.8479799032211304, acc: 0.7105262875556946)
[2024-12-12 02:28:04,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:04,891][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.5596711039543152, acc: 0.7777777910232544)
[2024-12-12 02:28:05,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:05,312][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 1.227874755859375, acc: 0.6737967729568481)
[2024-12-12 02:28:05,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:06,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:06,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:06,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:07,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:07,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:08,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:08,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:08,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:09,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:09,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:09,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:10,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:10,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:11,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:11,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:11,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:12,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:12,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:12,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:13,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:13,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:14,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:14,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:14,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:15,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:15,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:15,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:16,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:16,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:16,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:17,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:17,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:17,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:18,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:18,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:18,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:19,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:19,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:20,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:20,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:20,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:21,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:21,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:21,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:22,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:23,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:23,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:23,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:23,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:24,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:24,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:25,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:25,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:25,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:26,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:26,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:26,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:27,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:27,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:27,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:28,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:28,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:28,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:29,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:29,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:29,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:30,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:30,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:31,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:31,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:31,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:32,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:32,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:32,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:33,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:33,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:33,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:34,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:34,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:35,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:35,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:35,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:36,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:36,663][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.4890, device='cuda:0') eval_epoch_loss=tensor(1.2496, device='cuda:0') eval_epoch_acc=tensor(0.6576, device='cuda:0')
[2024-12-12 02:28:36,664][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:28:36,665][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:28:37,077][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_2_step_570_loss_1.2496038675308228/model.pt
[2024-12-12 02:28:37,086][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:28:37,088][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.2496038675308228
[2024-12-12 02:28:37,088][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.657554030418396
[2024-12-12 02:28:37,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:37,527][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.5954964756965637, acc: 0.8387096524238586)
[2024-12-12 02:28:37,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:37,919][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 1.2570240497589111, acc: 0.6752136945724487)
[2024-12-12 02:28:38,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:38,257][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 1.5142213106155396, acc: 0.5918367505073547)
[2024-12-12 02:28:38,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:38,589][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 1.817872405052185, acc: 0.4654088020324707)
[2024-12-12 02:28:38,985][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=5.0218, train_epoch_loss=1.6138, epoch time 360.6317468062043s
[2024-12-12 02:28:38,985][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-12-12 02:28:38,985][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:28:38,985][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-12-12 02:28:38,986][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2024-12-12 02:28:38,986][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:28:39,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:39,912][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 1.2217671871185303, acc: 0.7037037014961243)
[2024-12-12 02:28:40,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:40,235][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 1.2903424501419067, acc: 0.6800000071525574)
[2024-12-12 02:28:40,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:40,543][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 1.8218002319335938, acc: 0.5135135054588318)
[2024-12-12 02:28:40,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:40,924][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 1.0989289283752441, acc: 0.6315789222717285)
[2024-12-12 02:28:41,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:41,270][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 1.2876006364822388, acc: 0.6486486196517944)
[2024-12-12 02:28:41,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:41,641][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.950334370136261, acc: 0.6785714030265808)
[2024-12-12 02:28:41,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:41,997][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 1.3831757307052612, acc: 0.6326530575752258)
[2024-12-12 02:28:42,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:42,396][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.9410589337348938, acc: 0.7666666507720947)
[2024-12-12 02:28:42,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:42,772][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.158338263630867, acc: 0.9545454382896423)
[2024-12-12 02:28:42,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:43,079][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.3090432286262512, acc: 0.8846153616905212)
[2024-12-12 02:28:43,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:43,397][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.3935053050518036, acc: 0.9259259104728699)
[2024-12-12 02:28:43,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:43,799][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 1.4122387170791626, acc: 0.6153846383094788)
[2024-12-12 02:28:43,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:44,156][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.8614421486854553, acc: 0.6969696879386902)
[2024-12-12 02:28:44,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:44,534][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.8967183828353882, acc: 0.695652186870575)
[2024-12-12 02:28:44,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:44,851][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 1.0695713758468628, acc: 0.6470588445663452)
[2024-12-12 02:28:44,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:45,225][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 1.0633610486984253, acc: 0.7755101919174194)
[2024-12-12 02:28:45,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:45,573][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.5717820525169373, acc: 0.8947368264198303)
[2024-12-12 02:28:45,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:45,930][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.8119291663169861, acc: 0.75)
[2024-12-12 02:28:46,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:46,303][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 1.8073844909667969, acc: 0.4444444477558136)
[2024-12-12 02:28:46,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:46,702][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.7143571376800537, acc: 0.7368420958518982)
[2024-12-12 02:28:46,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:47,037][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.7213324308395386, acc: 0.7692307829856873)
[2024-12-12 02:28:47,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:47,356][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 1.1124355792999268, acc: 0.6896551847457886)
[2024-12-12 02:28:47,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:47,689][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 1.0696016550064087, acc: 0.5600000023841858)
[2024-12-12 02:28:47,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:48,104][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.7739388346672058, acc: 0.8095238208770752)
[2024-12-12 02:28:48,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:48,438][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.5754390358924866, acc: 0.875)
[2024-12-12 02:28:48,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:48,830][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 1.4898054599761963, acc: 0.6037735939025879)
[2024-12-12 02:28:48,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:49,212][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 1.6338751316070557, acc: 0.6027397513389587)
[2024-12-12 02:28:49,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:50,533][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 2.027756690979004, acc: 0.4071146249771118)
[2024-12-12 02:28:50,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:50,859][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 1.085882544517517, acc: 0.5813953280448914)
[2024-12-12 02:28:50,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:51,256][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 1.0391175746917725, acc: 0.6987951993942261)
[2024-12-12 02:28:51,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:51,633][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 1.4664781093597412, acc: 0.6172839403152466)
[2024-12-12 02:28:51,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:51,977][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 1.2665528059005737, acc: 0.6071428656578064)
[2024-12-12 02:28:52,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:52,320][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.6256378889083862, acc: 0.8518518805503845)
[2024-12-12 02:28:52,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:52,699][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.37987616658210754, acc: 0.8695651888847351)
[2024-12-12 02:28:52,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:53,067][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 1.1404659748077393, acc: 0.7226890921592712)
[2024-12-12 02:28:53,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:53,455][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.9843199849128723, acc: 0.7868852615356445)
[2024-12-12 02:28:53,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:53,852][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 1.127264380455017, acc: 0.6984127163887024)
[2024-12-12 02:28:53,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:54,214][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 1.3346270322799683, acc: 0.694915235042572)
[2024-12-12 02:28:54,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:54,620][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.7489080429077148, acc: 0.7816091775894165)
[2024-12-12 02:28:54,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:55,007][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.5877025723457336, acc: 0.8095238208770752)
[2024-12-12 02:28:55,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:55,320][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 1.039238691329956, acc: 0.692307710647583)
[2024-12-12 02:28:55,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:55,731][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 1.569262146949768, acc: 0.5810810923576355)
[2024-12-12 02:28:55,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:56,161][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 1.5973833799362183, acc: 0.6000000238418579)
[2024-12-12 02:28:56,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:56,589][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 1.3986186981201172, acc: 0.6060606241226196)
[2024-12-12 02:28:56,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:57,023][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 1.3525974750518799, acc: 0.6391752362251282)
[2024-12-12 02:28:57,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:57,450][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 1.288087010383606, acc: 0.6691176295280457)
[2024-12-12 02:28:57,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:57,770][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.3883329927921295, acc: 0.8846153616905212)
[2024-12-12 02:28:57,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:58,129][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.5589697957038879, acc: 0.8888888955116272)
[2024-12-12 02:28:58,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:58,479][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.5241652727127075, acc: 0.7857142686843872)
[2024-12-12 02:28:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:58,838][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.4853557050228119, acc: 0.8333333134651184)
[2024-12-12 02:28:58,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:59,198][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 1.0619980096817017, acc: 0.719298243522644)
[2024-12-12 02:28:59,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:59,597][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 1.2081174850463867, acc: 0.682539701461792)
[2024-12-12 02:28:59,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:00,009][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 1.743985891342163, acc: 0.5633803009986877)
[2024-12-12 02:29:00,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:00,473][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 1.8740257024765015, acc: 0.5133333206176758)
[2024-12-12 02:29:00,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:00,810][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 1.1786437034606934, acc: 0.6756756901741028)
[2024-12-12 02:29:00,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:01,134][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.15012094378471375, acc: 1.0)
[2024-12-12 02:29:02,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:04,192][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 1.744400978088379, acc: 0.5290102362632751)
[2024-12-12 02:29:04,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:05,501][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 2.02890944480896, acc: 0.48148149251937866)
[2024-12-12 02:29:05,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:06,131][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 1.5323230028152466, acc: 0.5965909361839294)
[2024-12-12 02:29:06,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:06,698][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 1.3555415868759155, acc: 0.6764705777168274)
[2024-12-12 02:29:06,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:07,266][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 1.4986259937286377, acc: 0.5289855003356934)
[2024-12-12 02:29:07,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:07,689][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 1.3741858005523682, acc: 0.637499988079071)
[2024-12-12 02:29:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:08,083][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.8473731279373169, acc: 0.7352941036224365)
[2024-12-12 02:29:08,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:08,493][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.65669846534729, acc: 0.8611111044883728)
[2024-12-12 02:29:08,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:08,897][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.8718598484992981, acc: 0.75)
[2024-12-12 02:29:08,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:09,244][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.4811238944530487, acc: 0.7931034564971924)
[2024-12-12 02:29:09,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:09,623][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 1.6624385118484497, acc: 0.5535714030265808)
[2024-12-12 02:29:09,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:09,994][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 1.170759916305542, acc: 0.7166666388511658)
[2024-12-12 02:29:10,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:10,362][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.14473161101341248, acc: 0.9599999785423279)
[2024-12-12 02:29:10,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:10,775][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 1.0426154136657715, acc: 0.6944444179534912)
[2024-12-12 02:29:10,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:11,202][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 1.3738678693771362, acc: 0.5757575631141663)
[2024-12-12 02:29:11,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:11,588][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 1.7229180335998535, acc: 0.5367646813392639)
[2024-12-12 02:29:11,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:11,969][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 1.4999566078186035, acc: 0.5873016119003296)
[2024-12-12 02:29:12,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:12,378][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 1.915408730506897, acc: 0.4923076927661896)
[2024-12-12 02:29:12,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:12,814][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 1.4635660648345947, acc: 0.6224489808082581)
[2024-12-12 02:29:12,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:13,231][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 1.9156678915023804, acc: 0.5)
[2024-12-12 02:29:13,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:13,661][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 1.8809819221496582, acc: 0.48905110359191895)
[2024-12-12 02:29:13,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:14,017][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.18463902175426483, acc: 1.0)
[2024-12-12 02:29:14,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:14,335][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.32347479462623596, acc: 0.875)
[2024-12-12 02:29:14,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:14,676][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.690792441368103, acc: 0.8787878751754761)
[2024-12-12 02:29:14,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:15,067][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.4570365250110626, acc: 0.8846153616905212)
[2024-12-12 02:29:15,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:15,433][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 1.3719004392623901, acc: 0.5961538553237915)
[2024-12-12 02:29:15,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:15,775][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 1.5394026041030884, acc: 0.6730769276618958)
[2024-12-12 02:29:15,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:16,131][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.7400365471839905, acc: 0.78125)
[2024-12-12 02:29:16,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:16,524][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.9357429146766663, acc: 0.6666666865348816)
[2024-12-12 02:29:16,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:16,874][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 1.0200068950653076, acc: 0.7200000286102295)
[2024-12-12 02:29:16,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:17,187][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.5488332509994507, acc: 0.782608687877655)
[2024-12-12 02:29:17,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:17,660][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 1.5955822467803955, acc: 0.6200000047683716)
[2024-12-12 02:29:17,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:17,954][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 1.5521628856658936, acc: 0.6116504669189453)
[2024-12-12 02:29:18,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:19,064][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 1.3461962938308716, acc: 0.6407766938209534)
[2024-12-12 02:29:19,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:19,877][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 1.6273245811462402, acc: 0.5322580933570862)
[2024-12-12 02:29:20,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:20,672][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 1.4299317598342896, acc: 0.642241358757019)
[2024-12-12 02:29:20,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:21,420][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 1.2232615947723389, acc: 0.6736842393875122)
[2024-12-12 02:29:21,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:22,407][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 2.1339621543884277, acc: 0.4455445408821106)
[2024-12-12 02:29:22,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:22,754][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 1.5716221332550049, acc: 0.5)
[2024-12-12 02:29:22,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:23,116][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 1.7823432683944702, acc: 0.5072463750839233)
[2024-12-12 02:29:23,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:23,522][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 2.0345215797424316, acc: 0.4117647111415863)
[2024-12-12 02:29:23,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:23,877][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 1.955190896987915, acc: 0.45192307233810425)
[2024-12-12 02:29:23,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:24,270][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 1.9315308332443237, acc: 0.43065693974494934)
[2024-12-12 02:29:24,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:24,606][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 1.886644721031189, acc: 0.4776119291782379)
[2024-12-12 02:29:24,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:24,977][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 1.0244567394256592, acc: 0.6000000238418579)
[2024-12-12 02:29:25,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:25,312][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.12025114893913269, acc: 1.0)
[2024-12-12 02:29:25,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:25,668][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.2881067991256714, acc: 0.9130434989929199)
[2024-12-12 02:29:25,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:26,038][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.38344427943229675, acc: 0.9318181872367859)
[2024-12-12 02:29:26,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:26,410][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.8646774888038635, acc: 0.7586206793785095)
[2024-12-12 02:29:26,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:26,692][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.5990216732025146, acc: 0.7674418687820435)
[2024-12-12 02:29:26,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:27,028][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.6995022296905518, acc: 0.800000011920929)
[2024-12-12 02:29:27,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:27,341][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.09308409690856934, acc: 1.0)
[2024-12-12 02:29:27,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:27,687][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.09702356904745102, acc: 0.9615384340286255)
[2024-12-12 02:29:27,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:28,099][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.824203610420227, acc: 0.738095223903656)
[2024-12-12 02:29:28,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:28,478][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.9002156853675842, acc: 0.8153846263885498)
[2024-12-12 02:29:28,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:28,902][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 1.0403844118118286, acc: 0.7017543911933899)
[2024-12-12 02:29:29,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:29,264][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 1.1590008735656738, acc: 0.6491228342056274)
[2024-12-12 02:29:29,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:29,619][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 1.095315933227539, acc: 0.6410256624221802)
[2024-12-12 02:29:29,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:29,997][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.6566274166107178, acc: 0.7755101919174194)
[2024-12-12 02:29:30,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:30,334][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.27288421988487244, acc: 0.9545454382896423)
[2024-12-12 02:29:30,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:30,701][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 1.2030649185180664, acc: 0.682539701461792)
[2024-12-12 02:29:30,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:31,112][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 1.2614574432373047, acc: 0.6829268336296082)
[2024-12-12 02:29:31,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:31,538][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 1.0202107429504395, acc: 0.725806474685669)
[2024-12-12 02:29:31,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:32,394][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 1.393233060836792, acc: 0.589353621006012)
[2024-12-12 02:29:32,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:32,753][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.9767594337463379, acc: 0.746666669845581)
[2024-12-12 02:29:32,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:33,174][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.935103178024292, acc: 0.7307692170143127)
[2024-12-12 02:29:33,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:33,535][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.4254094362258911, acc: 0.875)
[2024-12-12 02:29:33,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:33,908][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.45790982246398926, acc: 0.8421052694320679)
[2024-12-12 02:29:34,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:34,302][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 1.7241744995117188, acc: 0.546012282371521)
[2024-12-12 02:29:34,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:34,709][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 1.6392499208450317, acc: 0.5416666865348816)
[2024-12-12 02:29:34,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:35,129][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 1.7879748344421387, acc: 0.4833333194255829)
[2024-12-12 02:29:35,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:35,522][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 1.9082176685333252, acc: 0.4583333432674408)
[2024-12-12 02:29:35,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:35,937][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 1.4725738763809204, acc: 0.6153846383094788)
[2024-12-12 02:29:36,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:36,336][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 1.6252906322479248, acc: 0.6323529481887817)
[2024-12-12 02:29:36,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:36,695][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.8327828645706177, acc: 0.7307692170143127)
[2024-12-12 02:29:36,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:37,039][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.5314180254936218, acc: 0.9130434989929199)
[2024-12-12 02:29:37,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:37,347][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 1.2646440267562866, acc: 0.625)
[2024-12-12 02:29:37,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:37,659][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 1.8887872695922852, acc: 0.6086956262588501)
[2024-12-12 02:29:37,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:37,972][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 1.1118144989013672, acc: 0.6571428775787354)
[2024-12-12 02:29:38,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:38,277][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.9056743383407593, acc: 0.7692307829856873)
[2024-12-12 02:29:38,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:38,593][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 1.2439073324203491, acc: 0.7142857313156128)
[2024-12-12 02:29:38,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:38,971][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 1.290862798690796, acc: 0.6333333253860474)
[2024-12-12 02:29:39,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:39,303][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 1.287746548652649, acc: 0.739130437374115)
[2024-12-12 02:29:40,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:40,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:40,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:41,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:41,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:41,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:42,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:42,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:42,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:43,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:43,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:44,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:44,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:44,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:45,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:45,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:46,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:46,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:47,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:47,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:47,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:48,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:48,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:48,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:49,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:49,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:49,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:50,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:50,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:50,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:51,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:51,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:52,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:52,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:53,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:53,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:53,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:54,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:54,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:54,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:55,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:55,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:55,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:56,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:56,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:57,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:57,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:57,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:58,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:58,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:59,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:59,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:00,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:00,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:00,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:01,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:01,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:01,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:02,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:02,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:02,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:03,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:03,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:03,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:04,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:04,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:05,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:05,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:05,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:06,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:06,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:06,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:06,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:07,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:07,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:08,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:08,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:08,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:08,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:09,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:09,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:09,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:10,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:10,907][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.0934, device='cuda:0') eval_epoch_loss=tensor(1.1293, device='cuda:0') eval_epoch_acc=tensor(0.6884, device='cuda:0')
[2024-12-12 02:30:10,908][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:30:10,908][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:30:11,411][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_3_step_139_loss_1.1292654275894165/model.pt
[2024-12-12 02:30:11,419][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:30:11,420][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 1.1292654275894165
[2024-12-12 02:30:11,420][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.6884182095527649
[2024-12-12 02:30:11,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:11,834][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 1.3092788457870483, acc: 0.5714285969734192)
[2024-12-12 02:30:11,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:12,198][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 1.4857839345932007, acc: 0.5)
[2024-12-12 02:30:12,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:12,566][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 1.9169367551803589, acc: 0.4516128897666931)
[2024-12-12 02:30:12,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:12,966][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 1.3088124990463257, acc: 0.5675675868988037)
[2024-12-12 02:30:13,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:13,488][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 1.4104124307632446, acc: 0.5877193212509155)
[2024-12-12 02:30:13,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:13,809][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 1.2240012884140015, acc: 0.6641790866851807)
[2024-12-12 02:30:13,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:14,203][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 1.5011307001113892, acc: 0.6020408272743225)
[2024-12-12 02:30:14,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:14,643][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 1.6560328006744385, acc: 0.478723406791687)
[2024-12-12 02:30:14,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:14,974][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 1.2403403520584106, acc: 0.6714285612106323)
[2024-12-12 02:30:15,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:15,322][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 1.6045386791229248, acc: 0.5714285969734192)
[2024-12-12 02:30:15,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:15,700][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 1.0489600896835327, acc: 0.6521739363670349)
[2024-12-12 02:30:15,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:16,071][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 1.2199835777282715, acc: 0.6206896305084229)
[2024-12-12 02:30:16,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:16,431][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 1.5855787992477417, acc: 0.6304348111152649)
[2024-12-12 02:30:16,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:16,841][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 1.2670726776123047, acc: 0.694915235042572)
[2024-12-12 02:30:16,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:17,243][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 1.9210797548294067, acc: 0.4736842215061188)
[2024-12-12 02:30:17,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:17,565][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 1.1394022703170776, acc: 0.6756756901741028)
[2024-12-12 02:30:17,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:17,950][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.9297128319740295, acc: 0.8214285969734192)
[2024-12-12 02:30:18,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:18,328][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.6854410767555237, acc: 0.739130437374115)
[2024-12-12 02:30:18,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:18,672][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 2.1181583404541016, acc: 0.42105263471603394)
[2024-12-12 02:30:19,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:20,401][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 1.345957636833191, acc: 0.5810810923576355)
[2024-12-12 02:30:20,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:20,712][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 1.8758584260940552, acc: 0.46296295523643494)
[2024-12-12 02:30:20,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:21,153][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 1.5766724348068237, acc: 0.5)
[2024-12-12 02:30:21,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:21,742][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 1.6285253763198853, acc: 0.48235294222831726)
[2024-12-12 02:30:21,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:22,293][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 1.6463184356689453, acc: 0.550561785697937)
[2024-12-12 02:30:22,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:22,626][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.8235601782798767, acc: 0.8409090638160706)
[2024-12-12 02:30:22,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:23,000][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.8127691149711609, acc: 0.761904776096344)
[2024-12-12 02:30:23,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:23,347][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 1.0207768678665161, acc: 0.6896551847457886)
[2024-12-12 02:30:23,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:23,660][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.6764215230941772, acc: 0.8367347121238708)
[2024-12-12 02:30:23,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:24,009][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.5917909145355225, acc: 0.7799999713897705)
[2024-12-12 02:30:24,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:24,401][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 1.1581196784973145, acc: 0.7083333134651184)
[2024-12-12 02:30:24,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:24,733][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 1.4540942907333374, acc: 0.6372548937797546)
[2024-12-12 02:30:25,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:25,774][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 1.858482837677002, acc: 0.5479452013969421)
[2024-12-12 02:30:25,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:26,159][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.5163589715957642, acc: 0.9166666865348816)
[2024-12-12 02:30:26,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:26,489][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.9805079102516174, acc: 0.5925925970077515)
[2024-12-12 02:30:26,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:26,816][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.8912585973739624, acc: 0.7142857313156128)
[2024-12-12 02:30:26,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:27,355][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 1.4376111030578613, acc: 0.6548672318458557)
[2024-12-12 02:30:27,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:27,738][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 1.1449079513549805, acc: 0.695652186870575)
[2024-12-12 02:30:27,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:28,112][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.9241050481796265, acc: 0.7159090638160706)
[2024-12-12 02:30:28,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:29,017][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 1.8365974426269531, acc: 0.4885496199131012)
[2024-12-12 02:30:29,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:29,683][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 1.6238511800765991, acc: 0.5111111402511597)
[2024-12-12 02:30:29,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:30,039][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.9371971487998962, acc: 0.7213114500045776)
[2024-12-12 02:30:30,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:30,419][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.24746231734752655, acc: 0.9583333134651184)
[2024-12-12 02:30:30,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:30,797][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.5955620408058167, acc: 0.8799999952316284)
[2024-12-12 02:30:30,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:31,099][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.27612578868865967, acc: 0.8928571343421936)
[2024-12-12 02:30:31,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:31,426][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 1.0494946241378784, acc: 0.6951219439506531)
[2024-12-12 02:30:31,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:31,815][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 1.3244130611419678, acc: 0.634441077709198)
[2024-12-12 02:30:31,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:32,221][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 1.3330354690551758, acc: 0.6311239004135132)
[2024-12-12 02:30:32,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:32,700][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 1.3638430833816528, acc: 0.6312500238418579)
[2024-12-12 02:30:32,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:33,222][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 1.3945478200912476, acc: 0.6341463327407837)
[2024-12-12 02:30:33,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:33,641][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 1.2590601444244385, acc: 0.6654804348945618)
[2024-12-12 02:30:33,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:34,031][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.9592187404632568, acc: 0.800000011920929)
[2024-12-12 02:30:34,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:34,579][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 1.7682212591171265, acc: 0.4883720874786377)
[2024-12-12 02:30:34,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:35,373][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 1.761330008506775, acc: 0.5396825671195984)
[2024-12-12 02:30:35,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:36,282][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 1.65468430519104, acc: 0.5454545617103577)
[2024-12-12 02:30:36,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:37,021][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 1.0670475959777832, acc: 0.6823529601097107)
[2024-12-12 02:30:37,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:38,096][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 1.4688830375671387, acc: 0.5987654328346252)
[2024-12-12 02:30:38,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:39,053][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 1.0180926322937012, acc: 0.6935483813285828)
[2024-12-12 02:30:39,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:39,418][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.31328266859054565, acc: 0.9642857313156128)
[2024-12-12 02:30:39,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:39,792][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 1.2868441343307495, acc: 0.675000011920929)
[2024-12-12 02:30:39,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:40,205][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 1.0226209163665771, acc: 0.6764705777168274)
[2024-12-12 02:30:40,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:40,616][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 1.2613914012908936, acc: 0.6838235259056091)
[2024-12-12 02:30:40,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:41,026][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 1.1791067123413086, acc: 0.7033898234367371)
[2024-12-12 02:30:41,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:41,360][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 1.397308349609375, acc: 0.6343283653259277)
[2024-12-12 02:30:41,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:41,736][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 1.435997724533081, acc: 0.553398072719574)
[2024-12-12 02:30:41,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:42,137][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 1.3472200632095337, acc: 0.5873016119003296)
[2024-12-12 02:30:42,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:42,457][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.8318289518356323, acc: 0.8241758346557617)
[2024-12-12 02:30:42,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:42,818][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 1.1343796253204346, acc: 0.6771300435066223)
[2024-12-12 02:30:42,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:43,223][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 1.239659309387207, acc: 0.6417322754859924)
[2024-12-12 02:30:43,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:43,656][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 1.052473783493042, acc: 0.7241379022598267)
[2024-12-12 02:30:43,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:44,017][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.9392610788345337, acc: 0.7355072498321533)
[2024-12-12 02:30:44,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:44,370][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.9723476767539978, acc: 0.7081711888313293)
[2024-12-12 02:30:44,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:44,712][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 1.184701681137085, acc: 0.695652186870575)
[2024-12-12 02:30:44,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:45,031][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.35478663444519043, acc: 0.8695651888847351)
[2024-12-12 02:30:45,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:45,351][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.9125356078147888, acc: 0.6785714030265808)
[2024-12-12 02:30:45,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:45,763][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.369967520236969, acc: 0.936170220375061)
[2024-12-12 02:30:45,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:46,438][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.5577096343040466, acc: 0.8615384697914124)
[2024-12-12 02:30:46,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:46,823][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.4081863760948181, acc: 0.8783783912658691)
[2024-12-12 02:30:46,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:47,214][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.3628859221935272, acc: 0.8837209343910217)
[2024-12-12 02:30:47,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:47,746][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.6142764687538147, acc: 0.8198198080062866)
[2024-12-12 02:30:47,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:48,124][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.39099013805389404, acc: 0.8666666746139526)
[2024-12-12 02:30:48,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:48,436][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.48190364241600037, acc: 0.9090909361839294)
[2024-12-12 02:30:48,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:48,758][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.15889063477516174, acc: 0.9259259104728699)
[2024-12-12 02:30:48,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:49,057][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.526695728302002, acc: 0.8399999737739563)
[2024-12-12 02:30:49,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:49,403][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 1.236144781112671, acc: 0.7115384340286255)
[2024-12-12 02:30:49,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:50,157][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.9618043303489685, acc: 0.739130437374115)
[2024-12-12 02:30:50,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:50,697][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.9866155385971069, acc: 0.7443181872367859)
[2024-12-12 02:30:50,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:51,135][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 1.4060909748077393, acc: 0.5957446694374084)
[2024-12-12 02:30:51,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:51,497][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 1.0463488101959229, acc: 0.6792452931404114)
[2024-12-12 02:30:51,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:51,857][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.6533910632133484, acc: 0.800000011920929)
[2024-12-12 02:30:51,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:52,216][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.5882038474082947, acc: 0.8139534592628479)
[2024-12-12 02:30:52,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:52,579][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 1.1809101104736328, acc: 0.6666666865348816)
[2024-12-12 02:30:52,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:52,918][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 2.35148286819458, acc: 0.410526305437088)
[2024-12-12 02:30:52,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:53,219][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 1.6574286222457886, acc: 0.5666666626930237)
[2024-12-12 02:30:53,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:53,645][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 1.58124577999115, acc: 0.6000000238418579)
[2024-12-12 02:30:53,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:54,128][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 1.7365965843200684, acc: 0.5688073635101318)
[2024-12-12 02:30:54,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:54,595][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 1.6345129013061523, acc: 0.5692307949066162)
[2024-12-12 02:30:54,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:54,954][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.7152970433235168, acc: 0.8421052694320679)
[2024-12-12 02:30:55,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:55,279][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 1.0027538537979126, acc: 0.625)
[2024-12-12 02:30:55,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:55,633][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 1.9187672138214111, acc: 0.5909090638160706)
[2024-12-12 02:30:55,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:55,976][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 1.2634155750274658, acc: 0.5925925970077515)
[2024-12-12 02:30:56,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:56,332][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.9721705913543701, acc: 0.7428571581840515)
[2024-12-12 02:30:56,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:56,691][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 1.217997670173645, acc: 0.7272727489471436)
[2024-12-12 02:30:56,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:57,062][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 1.1668312549591064, acc: 0.6590909361839294)
[2024-12-12 02:30:57,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:57,635][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 1.750920295715332, acc: 0.5)
[2024-12-12 02:30:57,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:58,159][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 1.4810409545898438, acc: 0.5681818127632141)
[2024-12-12 02:30:58,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:58,458][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.10981503129005432, acc: 1.0)
[2024-12-12 02:30:58,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:58,835][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.6793121695518494, acc: 0.8461538553237915)
[2024-12-12 02:30:58,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:59,244][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.7354758381843567, acc: 0.7096773982048035)
[2024-12-12 02:30:59,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:59,649][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.5113088488578796, acc: 0.800000011920929)
[2024-12-12 02:30:59,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:00,056][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.8309281468391418, acc: 0.7567567825317383)
[2024-12-12 02:31:00,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:00,466][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.6906002163887024, acc: 0.8108108043670654)
[2024-12-12 02:31:00,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:00,836][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.49354982376098633, acc: 0.8918918967247009)
[2024-12-12 02:31:00,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:01,248][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.7741925120353699, acc: 0.75)
[2024-12-12 02:31:01,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:01,594][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.23474228382110596, acc: 0.9756097793579102)
[2024-12-12 02:31:01,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:01,962][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.10438913106918335, acc: 1.0)
[2024-12-12 02:31:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:02,331][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.08151423186063766, acc: 1.0)
[2024-12-12 02:31:02,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:02,679][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.1927928924560547, acc: 0.9354838728904724)
[2024-12-12 02:31:02,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:03,059][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.6167126893997192, acc: 0.8421052694320679)
[2024-12-12 02:31:03,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:03,429][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.6625909209251404, acc: 0.8285714387893677)
[2024-12-12 02:31:03,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:03,751][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.6127217411994934, acc: 0.8684210777282715)
[2024-12-12 02:31:03,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:04,318][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.9831163883209229, acc: 0.7452830076217651)
[2024-12-12 02:31:04,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:04,893][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.9274224042892456, acc: 0.7416666746139526)
[2024-12-12 02:31:04,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:05,239][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.46304750442504883, acc: 0.8611111044883728)
[2024-12-12 02:31:05,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:05,635][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.7960475087165833, acc: 0.8064516186714172)
[2024-12-12 02:31:05,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:06,064][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 1.6900992393493652, acc: 0.6000000238418579)
[2024-12-12 02:31:06,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:06,401][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 1.3093397617340088, acc: 0.625)
[2024-12-12 02:31:06,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:07,269][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 1.7006771564483643, acc: 0.5120000243186951)
[2024-12-12 02:31:07,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:07,596][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 1.4128172397613525, acc: 0.6067415475845337)
[2024-12-12 02:31:07,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:07,956][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 1.5103992223739624, acc: 0.5810810923576355)
[2024-12-12 02:31:08,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:08,406][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 1.0387614965438843, acc: 0.7068965435028076)
[2024-12-12 02:31:08,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:08,751][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.3167671859264374, acc: 0.8636363744735718)
[2024-12-12 02:31:08,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:09,134][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.20265841484069824, acc: 0.9545454382896423)
[2024-12-12 02:31:09,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:09,465][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.3627965450286865, acc: 0.90625)
[2024-12-12 02:31:09,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:09,823][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.328412264585495, acc: 0.8666666746139526)
[2024-12-12 02:31:09,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:10,196][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 1.1448554992675781, acc: 0.6666666865348816)
[2024-12-12 02:31:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:10,484][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.4758095145225525, acc: 0.84375)
[2024-12-12 02:31:10,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:10,852][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.49363529682159424, acc: 0.8666666746139526)
[2024-12-12 02:31:10,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:11,241][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.6706141233444214, acc: 0.8620689511299133)
[2024-12-12 02:31:11,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:11,637][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.3346313536167145, acc: 0.9599999785423279)
[2024-12-12 02:31:11,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:12,025][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 1.3077243566513062, acc: 0.5531914830207825)
[2024-12-12 02:31:12,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:12,391][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.6335148215293884, acc: 0.75)
[2024-12-12 02:31:12,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:12,754][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.5649023056030273, acc: 0.75)
[2024-12-12 02:31:12,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:13,166][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 1.2339009046554565, acc: 0.7228915691375732)
[2024-12-12 02:31:13,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:14,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:14,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:15,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:15,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:15,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:16,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:16,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:17,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:17,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:17,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:18,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:18,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:18,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:19,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:19,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:20,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:20,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:20,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:21,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:21,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:21,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:22,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:22,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:22,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:23,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:23,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:23,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:24,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:24,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:24,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:25,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:25,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:26,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:26,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:26,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:27,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:27,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:27,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:28,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:28,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:28,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:29,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:29,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:30,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:30,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:30,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:31,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:31,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:31,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:32,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:32,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:32,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:33,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:33,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:33,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:34,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:34,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:35,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:35,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:35,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:36,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:36,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:37,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:37,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:37,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:38,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:38,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:39,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:39,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:39,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:39,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:40,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:40,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:40,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:41,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:41,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:41,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:42,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:42,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:42,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:43,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:43,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:43,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:44,486][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4457, device='cuda:0') eval_epoch_loss=tensor(0.8943, device='cuda:0') eval_epoch_acc=tensor(0.7503, device='cuda:0')
[2024-12-12 02:31:44,487][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:31:44,487][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:31:44,797][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_3_step_282_loss_0.8943175673484802/model.pt
[2024-12-12 02:31:44,806][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:31:44,807][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.8943175673484802
[2024-12-12 02:31:44,807][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.7502582669258118
[2024-12-12 02:31:44,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:45,227][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 1.5129755735397339, acc: 0.5925925970077515)
[2024-12-12 02:31:45,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:45,563][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 1.2536115646362305, acc: 0.6578947305679321)
[2024-12-12 02:31:45,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:45,940][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 1.1554092168807983, acc: 0.6176470518112183)
[2024-12-12 02:31:46,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:46,346][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.6897050738334656, acc: 0.824999988079071)
[2024-12-12 02:31:46,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:46,712][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.9758278727531433, acc: 0.7109375)
[2024-12-12 02:31:46,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:47,121][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 1.0857434272766113, acc: 0.7279999852180481)
[2024-12-12 02:31:47,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:47,526][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.8957286477088928, acc: 0.791208803653717)
[2024-12-12 02:31:47,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:47,897][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 1.1732817888259888, acc: 0.7018633484840393)
[2024-12-12 02:31:48,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:48,280][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 1.3697599172592163, acc: 0.6288659572601318)
[2024-12-12 02:31:48,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:48,659][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.2295532375574112, acc: 0.9090909361839294)
[2024-12-12 02:31:48,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:49,013][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.970750093460083, acc: 0.6904761791229248)
[2024-12-12 02:31:49,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:49,425][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.5569634437561035, acc: 0.8448275923728943)
[2024-12-12 02:31:49,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:49,887][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.8430325388908386, acc: 0.7454545497894287)
[2024-12-12 02:31:50,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:50,430][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 1.1207822561264038, acc: 0.6907216310501099)
[2024-12-12 02:31:50,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:50,787][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 1.20169198513031, acc: 0.7068965435028076)
[2024-12-12 02:31:50,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:51,123][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.3866146206855774, acc: 0.9259259104728699)
[2024-12-12 02:31:51,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:51,480][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 1.2387839555740356, acc: 0.5789473652839661)
[2024-12-12 02:31:51,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:51,864][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.5213956236839294, acc: 0.8214285969734192)
[2024-12-12 02:31:51,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:52,213][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.22497084736824036, acc: 0.96875)
[2024-12-12 02:31:52,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:52,589][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.8890306949615479, acc: 0.849056601524353)
[2024-12-12 02:31:52,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:52,996][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.1308070868253708, acc: 0.9811320900917053)
[2024-12-12 02:31:53,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:53,377][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.2536582350730896, acc: 0.9411764740943909)
[2024-12-12 02:31:53,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:53,749][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.6590844392776489, acc: 0.75)
[2024-12-12 02:31:53,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:54,097][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.6329017281532288, acc: 0.8032786846160889)
[2024-12-12 02:31:54,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:54,470][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.3541719615459442, acc: 0.9333333373069763)
[2024-12-12 02:31:54,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:54,869][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.29652807116508484, acc: 0.9473684430122375)
[2024-12-12 02:31:54,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:55,241][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.7931851148605347, acc: 0.7971014380455017)
[2024-12-12 02:31:55,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:55,684][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.8081433773040771, acc: 0.75)
[2024-12-12 02:31:55,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:56,074][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.6747004389762878, acc: 0.7710843086242676)
[2024-12-12 02:31:56,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:56,473][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.8888624906539917, acc: 0.7435897588729858)
[2024-12-12 02:31:56,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:56,828][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.8846151232719421, acc: 0.7244898080825806)
[2024-12-12 02:31:56,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:57,155][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.051245108246803284, acc: 1.0)
[2024-12-12 02:31:57,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:57,484][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.30824407935142517, acc: 0.9166666865348816)
[2024-12-12 02:31:57,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:57,869][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.4190053343772888, acc: 0.9354838728904724)
[2024-12-12 02:31:57,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:58,236][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.630814790725708, acc: 0.8387096524238586)
[2024-12-12 02:31:58,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:58,590][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.522195041179657, acc: 0.8805969953536987)
[2024-12-12 02:31:58,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:58,916][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.41623586416244507, acc: 0.9134615659713745)
[2024-12-12 02:31:59,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:59,294][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.593757688999176, acc: 0.8222222328186035)
[2024-12-12 02:31:59,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:59,599][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.5125550031661987, acc: 0.8548387289047241)
[2024-12-12 02:31:59,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:59,917][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.1098262220621109, acc: 0.9599999785423279)
[2024-12-12 02:32:00,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:00,261][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 1.4994670152664185, acc: 0.5555555820465088)
[2024-12-12 02:32:00,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:00,654][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 2.4876785278320312, acc: 0.3142857253551483)
[2024-12-12 02:32:00,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:00,998][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 1.7515010833740234, acc: 0.4615384638309479)
[2024-12-12 02:32:01,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:01,364][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 2.172318696975708, acc: 0.3414634168148041)
[2024-12-12 02:32:01,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:01,755][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 1.5979465246200562, acc: 0.5526315569877625)
[2024-12-12 02:32:01,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:02,119][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.654596745967865, acc: 0.7894737124443054)
[2024-12-12 02:32:02,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:02,536][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.1808154433965683, acc: 0.9285714030265808)
[2024-12-12 02:32:02,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:02,889][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.48998206853866577, acc: 0.8518518805503845)
[2024-12-12 02:32:02,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:03,238][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.1818191111087799, acc: 0.96875)
[2024-12-12 02:32:03,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:03,579][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.8617870211601257, acc: 0.7903226017951965)
[2024-12-12 02:32:03,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:03,973][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.5293727517127991, acc: 0.859649121761322)
[2024-12-12 02:32:04,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:04,308][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.9084983468055725, acc: 0.6875)
[2024-12-12 02:32:04,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:04,648][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.24646659195423126, acc: 0.9666666388511658)
[2024-12-12 02:32:04,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:05,038][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.7666172981262207, acc: 0.7368420958518982)
[2024-12-12 02:32:05,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:05,455][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 1.449945330619812, acc: 0.6000000238418579)
[2024-12-12 02:32:05,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:05,848][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 1.8261139392852783, acc: 0.48275861144065857)
[2024-12-12 02:32:05,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:06,209][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 1.7702873945236206, acc: 0.5)
[2024-12-12 02:32:06,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:06,579][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 1.863017201423645, acc: 0.5542168617248535)
[2024-12-12 02:32:06,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:06,920][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.18364982306957245, acc: 0.95652174949646)
[2024-12-12 02:32:07,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:07,277][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 1.057162880897522, acc: 0.7435897588729858)
[2024-12-12 02:32:07,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:07,688][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 1.1169363260269165, acc: 0.7108433842658997)
[2024-12-12 02:32:07,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:08,063][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.8660322427749634, acc: 0.698113203048706)
[2024-12-12 02:32:08,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:08,405][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.6351863145828247, acc: 0.8227847814559937)
[2024-12-12 02:32:08,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:08,775][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.44781485199928284, acc: 0.843137264251709)
[2024-12-12 02:32:08,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:09,167][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 1.1664358377456665, acc: 0.7014925479888916)
[2024-12-12 02:32:09,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:09,528][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.19552066922187805, acc: 0.949999988079071)
[2024-12-12 02:32:09,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:09,868][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.2939869463443756, acc: 0.8799999952316284)
[2024-12-12 02:32:09,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:10,246][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 1.0881881713867188, acc: 0.7222222089767456)
[2024-12-12 02:32:10,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:10,626][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 1.1082464456558228, acc: 0.6279069781303406)
[2024-12-12 02:32:10,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:10,971][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.7703457474708557, acc: 0.7179487347602844)
[2024-12-12 02:32:11,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:11,357][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 1.4528735876083374, acc: 0.6222222447395325)
[2024-12-12 02:32:11,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:11,675][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.2189120352268219, acc: 0.9130434989929199)
[2024-12-12 02:32:11,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:12,051][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.9947947263717651, acc: 0.692307710647583)
[2024-12-12 02:32:12,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:12,461][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 1.5750956535339355, acc: 0.5604395866394043)
[2024-12-12 02:32:12,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:12,963][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 1.1474941968917847, acc: 0.7043478488922119)
[2024-12-12 02:32:13,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:13,362][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.9154226183891296, acc: 0.6847826242446899)
[2024-12-12 02:32:13,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:13,781][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 1.3112326860427856, acc: 0.6734693646430969)
[2024-12-12 02:32:13,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:14,156][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.03730475530028343, acc: 1.0)
[2024-12-12 02:32:14,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:14,563][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.18717201054096222, acc: 0.9615384340286255)
[2024-12-12 02:32:14,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:14,970][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.8033688068389893, acc: 0.7804877758026123)
[2024-12-12 02:32:15,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:15,381][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.865700364112854, acc: 0.7777777910232544)
[2024-12-12 02:32:15,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:15,742][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.6207159161567688, acc: 0.8157894611358643)
[2024-12-12 02:32:15,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:16,097][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.7310414910316467, acc: 0.7804877758026123)
[2024-12-12 02:32:16,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:16,429][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.4440627992153168, acc: 0.8484848737716675)
[2024-12-12 02:32:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:16,777][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.08971208333969116, acc: 1.0)
[2024-12-12 02:32:16,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:17,167][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.14636850357055664, acc: 0.95652174949646)
[2024-12-12 02:32:17,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:17,523][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.1376204937696457, acc: 1.0)
[2024-12-12 02:32:17,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:17,900][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.6102623343467712, acc: 0.84375)
[2024-12-12 02:32:18,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:18,496][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 1.1387840509414673, acc: 0.678787887096405)
[2024-12-12 02:32:18,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:19,367][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.6761771440505981, acc: 0.7924528121948242)
[2024-12-12 02:32:19,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:19,730][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.7104268670082092, acc: 0.7888888716697693)
[2024-12-12 02:32:19,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:20,068][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.44049403071403503, acc: 0.8571428656578064)
[2024-12-12 02:32:20,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:20,405][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.6458819508552551, acc: 0.800000011920929)
[2024-12-12 02:32:20,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:20,725][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.017249204218387604, acc: 1.0)
[2024-12-12 02:32:20,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:21,077][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.04597916454076767, acc: 1.0)
[2024-12-12 02:32:21,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:21,403][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.5405526757240295, acc: 0.8333333134651184)
[2024-12-12 02:32:21,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:21,787][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.2787276804447174, acc: 0.9473684430122375)
[2024-12-12 02:32:21,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:22,357][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.5983232259750366, acc: 0.8383233547210693)
[2024-12-12 02:32:22,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:22,764][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.6586788892745972, acc: 0.7969924807548523)
[2024-12-12 02:32:23,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:24,031][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.8497646450996399, acc: 0.7967914342880249)
[2024-12-12 02:32:24,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:24,591][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.2862550914287567, acc: 0.9099099040031433)
[2024-12-12 02:32:24,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:24,965][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.37166285514831543, acc: 0.8928571343421936)
[2024-12-12 02:32:25,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:25,321][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.2531552016735077, acc: 0.9285714030265808)
[2024-12-12 02:32:25,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:25,645][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.2978765368461609, acc: 0.90625)
[2024-12-12 02:32:25,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:26,013][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.18937236070632935, acc: 0.9722222089767456)
[2024-12-12 02:32:26,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:26,345][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.07497262954711914, acc: 1.0)
[2024-12-12 02:32:26,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:26,644][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.0641835406422615, acc: 1.0)
[2024-12-12 02:32:26,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:26,948][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.14291232824325562, acc: 1.0)
[2024-12-12 02:32:27,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:27,338][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.3802666664123535, acc: 0.9047619104385376)
[2024-12-12 02:32:27,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:27,678][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 1.7016172409057617, acc: 0.5925925970077515)
[2024-12-12 02:32:27,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:28,096][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 1.6439082622528076, acc: 0.6213592290878296)
[2024-12-12 02:32:28,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:28,615][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 1.4446690082550049, acc: 0.6985294222831726)
[2024-12-12 02:32:28,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:28,998][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 1.6244298219680786, acc: 0.6000000238418579)
[2024-12-12 02:32:29,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:29,386][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 1.2475361824035645, acc: 0.6666666865348816)
[2024-12-12 02:32:29,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:29,723][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.952511191368103, acc: 0.7906976938247681)
[2024-12-12 02:32:29,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:30,059][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.23076362907886505, acc: 0.9583333134651184)
[2024-12-12 02:32:30,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:30,474][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.5742120742797852, acc: 0.8604651093482971)
[2024-12-12 02:32:30,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:30,816][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.37261608242988586, acc: 0.9200000166893005)
[2024-12-12 02:32:30,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:31,346][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.6091386079788208, acc: 0.8382353186607361)
[2024-12-12 02:32:31,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:31,677][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.7702764868736267, acc: 0.7733333110809326)
[2024-12-12 02:32:31,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:32,049][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.8957725167274475, acc: 0.7575757503509521)
[2024-12-12 02:32:32,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:32,440][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.5128523111343384, acc: 0.8787878751754761)
[2024-12-12 02:32:32,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:32,808][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.3275030255317688, acc: 0.9032257795333862)
[2024-12-12 02:32:32,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:33,210][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.307426393032074, acc: 0.9629629850387573)
[2024-12-12 02:32:33,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:33,539][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.22377340495586395, acc: 0.9599999785423279)
[2024-12-12 02:32:33,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:33,867][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.2748568058013916, acc: 0.8888888955116272)
[2024-12-12 02:32:33,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:34,194][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.29137298464775085, acc: 0.8888888955116272)
[2024-12-12 02:32:34,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:34,520][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.21775946021080017, acc: 0.9230769276618958)
[2024-12-12 02:32:34,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:34,848][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.3572612404823303, acc: 0.8965517282485962)
[2024-12-12 02:32:34,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:35,207][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.16029752790927887, acc: 0.9642857313156128)
[2024-12-12 02:32:35,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:35,578][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.7276413440704346, acc: 0.800000011920929)
[2024-12-12 02:32:35,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:35,931][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.3555867075920105, acc: 0.939393937587738)
[2024-12-12 02:32:36,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:36,262][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.06399105489253998, acc: 1.0)
[2024-12-12 02:32:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:36,609][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 1.084185242652893, acc: 0.7058823704719543)
[2024-12-12 02:32:36,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:36,981][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.5560275912284851, acc: 0.807692289352417)
[2024-12-12 02:32:37,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:37,352][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.39377158880233765, acc: 0.8888888955116272)
[2024-12-12 02:32:37,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:37,719][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.6905889511108398, acc: 0.800000011920929)
[2024-12-12 02:32:37,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:38,098][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.43268662691116333, acc: 0.8999999761581421)
[2024-12-12 02:32:38,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:38,393][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.08342543989419937, acc: 1.0)
[2024-12-12 02:32:38,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:38,687][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.25214073061943054, acc: 0.9666666388511658)
[2024-12-12 02:32:38,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:39,037][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.6283270716667175, acc: 0.84375)
[2024-12-12 02:32:39,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:39,390][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.7665908932685852, acc: 0.8611111044883728)
[2024-12-12 02:32:39,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:39,742][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.35362982749938965, acc: 0.8888888955116272)
[2024-12-12 02:32:40,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:40,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:41,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:41,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:42,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:42,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:42,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:43,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:43,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:43,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:44,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:44,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:44,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:45,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:45,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:46,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:46,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:46,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:47,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:47,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:48,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:48,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:48,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:49,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:49,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:49,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:49,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:50,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:51,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:51,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:51,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:52,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:52,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:52,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:52,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:53,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:53,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:54,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:54,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:55,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:55,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:55,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:56,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:56,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:56,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:57,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:57,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:57,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:58,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:58,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:58,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:59,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:59,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:59,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:00,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:00,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:00,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:01,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:01,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:01,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:02,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:02,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:03,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:03,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:03,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:04,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:04,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:05,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:05,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:05,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:06,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:06,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:06,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:06,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:07,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:07,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:08,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:08,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:08,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:09,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:09,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:09,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:10,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:10,974][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4219, device='cuda:0') eval_epoch_loss=tensor(0.8846, device='cuda:0') eval_epoch_acc=tensor(0.7570, device='cuda:0')
[2024-12-12 02:33:10,975][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:33:10,976][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:33:11,243][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_3_step_425_loss_0.8845502138137817/model.pt
[2024-12-12 02:33:11,253][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:33:11,255][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.8845502138137817
[2024-12-12 02:33:11,256][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.7570106387138367
[2024-12-12 02:33:11,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:11,662][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.39824211597442627, acc: 0.9090909361839294)
[2024-12-12 02:33:11,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:12,018][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.27896881103515625, acc: 0.8695651888847351)
[2024-12-12 02:33:12,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:12,400][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.4058837890625, acc: 0.8648648858070374)
[2024-12-12 02:33:12,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:12,739][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.162867471575737, acc: 0.9259259104728699)
[2024-12-12 02:33:12,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:13,096][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.45887699723243713, acc: 0.9130434989929199)
[2024-12-12 02:33:13,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:13,465][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.010091613046824932, acc: 1.0)
[2024-12-12 02:33:13,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:13,818][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.04516157880425453, acc: 1.0)
[2024-12-12 02:33:13,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:14,180][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.2927025258541107, acc: 0.8695651888847351)
[2024-12-12 02:33:14,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:14,579][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.34476161003112793, acc: 0.8888888955116272)
[2024-12-12 02:33:14,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:15,032][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.0797259584069252, acc: 0.9599999785423279)
[2024-12-12 02:33:15,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:15,375][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.043233923614025116, acc: 1.0)
[2024-12-12 02:33:15,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:15,751][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.5215709209442139, acc: 0.8888888955116272)
[2024-12-12 02:33:15,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:16,131][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.5366925001144409, acc: 0.8863636255264282)
[2024-12-12 02:33:16,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:16,534][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.08447053283452988, acc: 0.9523809552192688)
[2024-12-12 02:33:16,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:16,902][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 1.0147511959075928, acc: 0.7692307829856873)
[2024-12-12 02:33:17,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:17,410][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.8326467275619507, acc: 0.6969696879386902)
[2024-12-12 02:33:17,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:18,119][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 1.0876771211624146, acc: 0.671999990940094)
[2024-12-12 02:33:18,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:18,545][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 1.2967110872268677, acc: 0.6532257795333862)
[2024-12-12 02:33:18,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:19,193][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.9966416358947754, acc: 0.7213930487632751)
[2024-12-12 02:33:19,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:19,497][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.5108519792556763, acc: 0.8113207817077637)
[2024-12-12 02:33:19,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:19,926][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.2108965516090393, acc: 0.9545454382896423)
[2024-12-12 02:33:20,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:20,281][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.6199904084205627, acc: 0.782608687877655)
[2024-12-12 02:33:20,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:20,671][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.5589161515235901, acc: 0.9230769276618958)
[2024-12-12 02:33:20,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:21,020][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.5190578699111938, acc: 0.8571428656578064)
[2024-12-12 02:33:21,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:21,351][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.4322168231010437, acc: 0.89552241563797)
[2024-12-12 02:33:21,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:21,734][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.24743109941482544, acc: 0.9444444179534912)
[2024-12-12 02:33:21,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:22,116][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.323567658662796, acc: 0.8913043737411499)
[2024-12-12 02:33:22,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:22,449][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.5943114757537842, acc: 0.8333333134651184)
[2024-12-12 02:33:22,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:22,770][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.7366608381271362, acc: 0.7631579041481018)
[2024-12-12 02:33:22,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:23,128][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.5020490884780884, acc: 0.8571428656578064)
[2024-12-12 02:33:23,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:23,449][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.4574078321456909, acc: 0.9090909361839294)
[2024-12-12 02:33:23,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:23,749][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 1.2148374319076538, acc: 0.6804123520851135)
[2024-12-12 02:33:23,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:24,130][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.5701663494110107, acc: 0.8142856955528259)
[2024-12-12 02:33:24,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:24,562][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 1.0854004621505737, acc: 0.6744186282157898)
[2024-12-12 02:33:24,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:24,914][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.6384724974632263, acc: 0.8571428656578064)
[2024-12-12 02:33:25,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:25,272][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.7236197590827942, acc: 0.790123462677002)
[2024-12-12 02:33:25,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:25,620][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.9176405668258667, acc: 0.7222222089767456)
[2024-12-12 02:33:25,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:25,951][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.3857690691947937, acc: 0.875)
[2024-12-12 02:33:26,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:26,321][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.3665703237056732, acc: 0.8846153616905212)
[2024-12-12 02:33:26,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:26,663][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.6371878385543823, acc: 0.804347813129425)
[2024-12-12 02:33:26,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:27,042][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.6166835427284241, acc: 0.8452380895614624)
[2024-12-12 02:33:27,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:27,403][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.926262378692627, acc: 0.7349397540092468)
[2024-12-12 02:33:27,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:27,740][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.6418485641479492, acc: 0.8288288116455078)
[2024-12-12 02:33:27,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:28,065][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 1.1232696771621704, acc: 0.7184466123580933)
[2024-12-12 02:33:28,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:28,371][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.9197219610214233, acc: 0.6991869807243347)
[2024-12-12 02:33:28,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:28,710][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.36634913086891174, acc: 0.875)
[2024-12-12 02:33:28,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:29,117][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.8534799814224243, acc: 0.7142857313156128)
[2024-12-12 02:33:29,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:29,537][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 1.1734014749526978, acc: 0.6666666865348816)
[2024-12-12 02:33:29,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:29,909][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 1.2537246942520142, acc: 0.6375545859336853)
[2024-12-12 02:33:30,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:30,323][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.9966489672660828, acc: 0.6875)
[2024-12-12 02:33:30,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:30,723][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.742599606513977, acc: 0.7484662532806396)
[2024-12-12 02:33:30,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:31,065][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.7303889989852905, acc: 0.8057553768157959)
[2024-12-12 02:33:31,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:31,449][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 1.1797276735305786, acc: 0.6633166074752808)
[2024-12-12 02:33:31,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:31,796][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.8185320496559143, acc: 0.8055555820465088)
[2024-12-12 02:33:31,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:32,171][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.4570666253566742, acc: 0.8787878751754761)
[2024-12-12 02:33:32,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:32,497][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.6743138432502747, acc: 0.8148148059844971)
[2024-12-12 02:33:32,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:32,895][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.3617454171180725, acc: 0.8999999761581421)
[2024-12-12 02:33:32,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:33,200][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.5514524579048157, acc: 0.8500000238418579)
[2024-12-12 02:33:33,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:33,569][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 1.1009814739227295, acc: 0.6896551847457886)
[2024-12-12 02:33:33,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:33,905][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.3809347450733185, acc: 0.8387096524238586)
[2024-12-12 02:33:34,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:34,316][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.4115564823150635, acc: 0.8947368264198303)
[2024-12-12 02:33:34,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:34,650][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 1.3351821899414062, acc: 0.6296296119689941)
[2024-12-12 02:33:34,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:34,941][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.682275116443634, acc: 0.761904776096344)
[2024-12-12 02:33:35,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:35,312][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.36607226729393005, acc: 0.8636363744735718)
[2024-12-12 02:33:35,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:35,727][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 1.379290223121643, acc: 0.5846154093742371)
[2024-12-12 02:33:35,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:36,107][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.5375492572784424, acc: 0.8666666746139526)
[2024-12-12 02:33:36,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:36,483][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.5222890377044678, acc: 0.7931034564971924)
[2024-12-12 02:33:36,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:36,857][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.6485193371772766, acc: 0.7647058963775635)
[2024-12-12 02:33:36,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:37,188][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.599213182926178, acc: 0.7931034564971924)
[2024-12-12 02:33:37,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:37,543][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.4702570140361786, acc: 0.8947368264198303)
[2024-12-12 02:33:37,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:37,909][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 1.1130174398422241, acc: 0.6842105388641357)
[2024-12-12 02:33:38,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:38,350][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.9973927140235901, acc: 0.7232142686843872)
[2024-12-12 02:33:38,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:38,780][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.6641620993614197, acc: 0.7528089880943298)
[2024-12-12 02:33:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:39,163][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 1.0557106733322144, acc: 0.6853932738304138)
[2024-12-12 02:33:39,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:39,535][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 1.5185447931289673, acc: 0.5531914830207825)
[2024-12-12 02:33:39,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:39,904][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 1.2988522052764893, acc: 0.695652186870575)
[2024-12-12 02:33:40,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:40,285][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.06666988879442215, acc: 1.0)
[2024-12-12 02:33:40,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:40,621][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.25816354155540466, acc: 0.9615384340286255)
[2024-12-12 02:33:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:40,981][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.1743791699409485, acc: 0.9629629850387573)
[2024-12-12 02:33:41,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:41,354][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.3026810884475708, acc: 0.9259259104728699)
[2024-12-12 02:33:41,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:41,697][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.6220408082008362, acc: 0.849056601524353)
[2024-12-12 02:33:41,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:42,033][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.6661658883094788, acc: 0.8620689511299133)
[2024-12-12 02:33:42,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:42,626][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 1.425756812095642, acc: 0.5675675868988037)
[2024-12-12 02:33:42,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:43,073][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.896748960018158, acc: 0.7887324094772339)
[2024-12-12 02:33:43,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:43,489][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.2593451142311096, acc: 0.8999999761581421)
[2024-12-12 02:33:43,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:43,821][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.10495980083942413, acc: 0.9666666388511658)
[2024-12-12 02:33:43,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:44,150][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.3790542483329773, acc: 0.8846153616905212)
[2024-12-12 02:33:45,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:46,966][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 1.2889777421951294, acc: 0.6642857193946838)
[2024-12-12 02:33:47,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:47,815][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.598301112651825, acc: 0.817460298538208)
[2024-12-12 02:33:47,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:48,181][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.8343276381492615, acc: 0.7142857313156128)
[2024-12-12 02:33:48,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:48,479][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.2583388388156891, acc: 0.9166666865348816)
[2024-12-12 02:33:48,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:49,207][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.7591652274131775, acc: 0.7777777910232544)
[2024-12-12 02:33:49,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:49,513][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.028829876333475113, acc: 1.0)
[2024-12-12 02:33:49,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:49,806][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.5161471962928772, acc: 0.8064516186714172)
[2024-12-12 02:33:49,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:50,200][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.4582543969154358, acc: 0.800000011920929)
[2024-12-12 02:33:50,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:50,537][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.5845468044281006, acc: 0.8888888955116272)
[2024-12-12 02:33:50,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:51,574][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 1.1501413583755493, acc: 0.6737288236618042)
[2024-12-12 02:33:51,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:51,942][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.6183381676673889, acc: 0.8059701323509216)
[2024-12-12 02:33:52,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:52,318][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.7695586681365967, acc: 0.7810218930244446)
[2024-12-12 02:33:52,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:52,875][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 1.0094184875488281, acc: 0.7049999833106995)
[2024-12-12 02:33:52,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:53,203][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.18270151317119598, acc: 0.9444444179534912)
[2024-12-12 02:33:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:53,566][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.36359691619873047, acc: 0.8846153616905212)
[2024-12-12 02:33:53,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:53,912][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.3755953907966614, acc: 0.9047619104385376)
[2024-12-12 02:33:54,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:54,263][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 2.089567184448242, acc: 0.4098360538482666)
[2024-12-12 02:33:54,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:54,574][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.4622829258441925, acc: 0.8813559412956238)
[2024-12-12 02:33:54,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:54,975][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 1.531020998954773, acc: 0.6279069781303406)
[2024-12-12 02:33:55,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:55,402][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 1.3943769931793213, acc: 0.6363636255264282)
[2024-12-12 02:33:55,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:55,828][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 1.5175422430038452, acc: 0.5660377144813538)
[2024-12-12 02:33:55,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:56,211][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.8172359466552734, acc: 0.7727272510528564)
[2024-12-12 02:33:56,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:56,575][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.596571147441864, acc: 0.800000011920929)
[2024-12-12 02:33:56,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:56,984][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.6454760432243347, acc: 0.800000011920929)
[2024-12-12 02:33:57,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:57,373][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.24367214739322662, acc: 0.9090909361839294)
[2024-12-12 02:33:57,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:57,772][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.7011682391166687, acc: 0.8153846263885498)
[2024-12-12 02:33:57,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:58,137][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.8148738741874695, acc: 0.828125)
[2024-12-12 02:33:58,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:58,528][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.6268611550331116, acc: 0.78125)
[2024-12-12 02:33:58,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:58,843][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.8549827337265015, acc: 0.7575757503509521)
[2024-12-12 02:33:58,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:59,173][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.18709821999073029, acc: 0.875)
[2024-12-12 02:33:59,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:59,516][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.21076609194278717, acc: 0.9354838728904724)
[2024-12-12 02:33:59,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:59,924][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.11001481115818024, acc: 1.0)
[2024-12-12 02:34:00,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:00,275][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.47704628109931946, acc: 0.8666666746139526)
[2024-12-12 02:34:00,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:00,643][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.1661272644996643, acc: 0.9756097793579102)
[2024-12-12 02:34:00,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:01,010][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.11718105524778366, acc: 1.0)
[2024-12-12 02:34:01,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:01,360][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.11839048564434052, acc: 0.9473684430122375)
[2024-12-12 02:34:01,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:01,695][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.21601524949073792, acc: 0.9354838728904724)
[2024-12-12 02:34:01,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:02,036][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.07580172270536423, acc: 1.0)
[2024-12-12 02:34:02,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:02,390][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.4859188497066498, acc: 0.9090909361839294)
[2024-12-12 02:34:02,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:02,736][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.21696853637695312, acc: 0.925000011920929)
[2024-12-12 02:34:02,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:03,105][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.2753303050994873, acc: 0.9428571462631226)
[2024-12-12 02:34:03,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:03,439][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.91485995054245, acc: 0.7810218930244446)
[2024-12-12 02:34:03,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:03,772][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.6951824426651001, acc: 0.7724137902259827)
[2024-12-12 02:34:03,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:04,102][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.8700672388076782, acc: 0.7285714149475098)
[2024-12-12 02:34:04,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:04,487][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.7118987441062927, acc: 0.8013244867324829)
[2024-12-12 02:34:04,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:04,836][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.32633545994758606, acc: 0.8803418874740601)
[2024-12-12 02:34:04,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:05,208][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.054193634539842606, acc: 1.0)
[2024-12-12 02:34:05,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:05,552][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.2010771483182907, acc: 0.9230769276618958)
[2024-12-12 02:34:05,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:05,875][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.13621388375759125, acc: 0.9615384340286255)
[2024-12-12 02:34:05,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:06,198][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.5497959852218628, acc: 0.8205128312110901)
[2024-12-12 02:34:06,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:06,598][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.7466791868209839, acc: 0.7666666507720947)
[2024-12-12 02:34:06,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:06,966][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.5939843058586121, acc: 0.8571428656578064)
[2024-12-12 02:34:07,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:07,314][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.34823843836784363, acc: 0.9166666865348816)
[2024-12-12 02:34:07,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:07,672][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.4854181110858917, acc: 0.8448275923728943)
[2024-12-12 02:34:07,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:08,010][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.43359246850013733, acc: 0.8809523582458496)
[2024-12-12 02:34:08,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:08,394][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.3434329628944397, acc: 0.8947368264198303)
[2024-12-12 02:34:09,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:09,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:09,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:10,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:10,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:11,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:11,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:11,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:12,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:12,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:13,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:13,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:13,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:14,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:14,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:14,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:15,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:15,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:15,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:16,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:16,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:17,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:17,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:17,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:18,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:18,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:18,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:19,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:19,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:19,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:20,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:20,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:20,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:21,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:21,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:22,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:22,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:22,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:23,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:23,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:23,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:24,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:24,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:24,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:25,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:25,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:26,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:26,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:26,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:27,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:27,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:27,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:28,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:28,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:28,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:29,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:29,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:29,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:30,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:30,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:31,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:31,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:31,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:32,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:32,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:33,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:33,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:33,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:34,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:34,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:34,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:35,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:35,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:35,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:36,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:36,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:36,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:37,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:37,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:37,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:38,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:38,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:38,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:39,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:39,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:40,384][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4430, device='cuda:0') eval_epoch_loss=tensor(0.8932, device='cuda:0') eval_epoch_acc=tensor(0.7660, device='cuda:0')
[2024-12-12 02:34:40,385][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:34:40,385][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:34:40,651][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_3_step_568_loss_0.8932223320007324/model.pt
[2024-12-12 02:34:40,661][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:34:40,662][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.7659968137741089
[2024-12-12 02:34:40,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:41,103][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.061783090233802795, acc: 1.0)
[2024-12-12 02:34:41,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:41,516][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.613378643989563, acc: 0.7807486653327942)
[2024-12-12 02:34:41,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:41,871][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.3034534752368927, acc: 0.9193548560142517)
[2024-12-12 02:34:41,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:42,187][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.5620303750038147, acc: 0.8717948794364929)
[2024-12-12 02:34:42,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:42,508][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.9454239010810852, acc: 0.7193877696990967)
[2024-12-12 02:34:42,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:42,847][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.8217085599899292, acc: 0.7421383857727051)
[2024-12-12 02:34:43,287][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=2.3734, train_epoch_loss=0.8643, epoch time 364.3005657531321s
[2024-12-12 02:34:43,288][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:34:43,288][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-12-12 02:34:43,288][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:34:43,288][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 9
[2024-12-12 02:34:43,288][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:34:43,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:44,189][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.3894338607788086, acc: 0.8888888955116272)
[2024-12-12 02:34:44,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:44,548][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.5191567540168762, acc: 0.800000011920929)
[2024-12-12 02:34:44,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:44,937][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.8111262917518616, acc: 0.7567567825317383)
[2024-12-12 02:34:45,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:45,327][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.5320100784301758, acc: 0.8157894611358643)
[2024-12-12 02:34:45,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:45,748][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.7237507104873657, acc: 0.7297297120094299)
[2024-12-12 02:34:45,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:46,124][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.2971702218055725, acc: 0.8928571343421936)
[2024-12-12 02:34:46,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:46,537][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 0.9572719931602478, acc: 0.7346938848495483)
[2024-12-12 02:34:46,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:46,946][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.36351993680000305, acc: 0.8999999761581421)
[2024-12-12 02:34:47,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:47,352][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.1658278852701187, acc: 0.8636363744735718)
[2024-12-12 02:34:47,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:47,734][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.1608618199825287, acc: 0.9615384340286255)
[2024-12-12 02:34:47,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:48,077][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.1815790832042694, acc: 0.9259259104728699)
[2024-12-12 02:34:48,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:48,422][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.5469670295715332, acc: 0.8205128312110901)
[2024-12-12 02:34:48,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:48,814][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.2039216011762619, acc: 0.9696969985961914)
[2024-12-12 02:34:48,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:49,257][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.2872406542301178, acc: 0.8913043737411499)
[2024-12-12 02:34:49,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:49,630][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.28129148483276367, acc: 0.9215686321258545)
[2024-12-12 02:34:49,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:50,015][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.7750473618507385, acc: 0.7755101919174194)
[2024-12-12 02:34:50,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:50,392][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.2839820384979248, acc: 0.9473684430122375)
[2024-12-12 02:34:50,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:50,789][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.30258092284202576, acc: 0.875)
[2024-12-12 02:34:50,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:51,136][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 1.0346238613128662, acc: 0.6944444179534912)
[2024-12-12 02:34:51,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:51,454][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.38642361760139465, acc: 0.8947368264198303)
[2024-12-12 02:34:51,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:51,830][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.38012459874153137, acc: 0.9230769276618958)
[2024-12-12 02:34:51,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:52,206][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.4860096573829651, acc: 0.8275862336158752)
[2024-12-12 02:34:52,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:52,536][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.48615455627441406, acc: 0.800000011920929)
[2024-12-12 02:34:52,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:52,876][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.5599877834320068, acc: 0.8571428656578064)
[2024-12-12 02:34:52,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:53,292][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.2807021737098694, acc: 0.9375)
[2024-12-12 02:34:53,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:53,723][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.9825348258018494, acc: 0.698113203048706)
[2024-12-12 02:34:53,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:54,087][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.9786253571510315, acc: 0.6849315166473389)
[2024-12-12 02:34:54,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:55,403][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 1.3070690631866455, acc: 0.6324110627174377)
[2024-12-12 02:34:55,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:55,795][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.724472165107727, acc: 0.7209302186965942)
[2024-12-12 02:34:55,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:56,161][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.8788231611251831, acc: 0.7710843086242676)
[2024-12-12 02:34:56,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:56,545][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.8698498606681824, acc: 0.7283950448036194)
[2024-12-12 02:34:56,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:56,936][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.3986225426197052, acc: 0.8571428656578064)
[2024-12-12 02:34:57,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:57,308][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.391613632440567, acc: 0.9259259104728699)
[2024-12-12 02:34:57,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:57,654][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.11247503012418747, acc: 1.0)
[2024-12-12 02:34:57,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:58,018][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.747974157333374, acc: 0.8235294222831726)
[2024-12-12 02:34:58,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:58,403][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.39147642254829407, acc: 0.8852459192276001)
[2024-12-12 02:34:58,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:58,759][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.6238924264907837, acc: 0.8253968358039856)
[2024-12-12 02:34:58,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:59,122][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.6597206592559814, acc: 0.8135592937469482)
[2024-12-12 02:34:59,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:59,474][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.45083895325660706, acc: 0.8735632300376892)
[2024-12-12 02:34:59,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:59,835][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.20263883471488953, acc: 0.9523809552192688)
[2024-12-12 02:34:59,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:00,190][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.2578172981739044, acc: 0.9615384340286255)
[2024-12-12 02:35:00,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:00,621][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.6978573203086853, acc: 0.7972972989082336)
[2024-12-12 02:35:00,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:01,031][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.9719212055206299, acc: 0.7076923251152039)
[2024-12-12 02:35:01,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:01,442][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.7895700335502625, acc: 0.7575757503509521)
[2024-12-12 02:35:01,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:01,875][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.6899165511131287, acc: 0.7938144207000732)
[2024-12-12 02:35:02,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:02,294][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.7273551821708679, acc: 0.8014705777168274)
[2024-12-12 02:35:02,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:02,680][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.3164110481739044, acc: 0.8846153616905212)
[2024-12-12 02:35:02,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:03,067][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.16654863953590393, acc: 0.9629629850387573)
[2024-12-12 02:35:03,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:03,426][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.3357565999031067, acc: 0.8928571343421936)
[2024-12-12 02:35:03,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:03,810][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.09710243344306946, acc: 0.9722222089767456)
[2024-12-12 02:35:03,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:04,162][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.6798592805862427, acc: 0.8245614171028137)
[2024-12-12 02:35:04,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:04,567][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.9110772609710693, acc: 0.7936508059501648)
[2024-12-12 02:35:04,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:04,938][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 1.1665759086608887, acc: 0.6760563254356384)
[2024-12-12 02:35:05,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:05,398][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 1.6671817302703857, acc: 0.5066666603088379)
[2024-12-12 02:35:05,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:05,766][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.8260473608970642, acc: 0.8108108043670654)
[2024-12-12 02:35:05,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:06,139][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.13063254952430725, acc: 0.9615384340286255)
[2024-12-12 02:35:07,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:09,172][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 1.3418314456939697, acc: 0.6484641432762146)
[2024-12-12 02:35:09,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:10,444][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 1.4429988861083984, acc: 0.6165577173233032)
[2024-12-12 02:35:10,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:11,066][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 1.1063910722732544, acc: 0.7215909361839294)
[2024-12-12 02:35:11,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:11,632][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.8072835206985474, acc: 0.7867646813392639)
[2024-12-12 02:35:11,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:12,202][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.9645003080368042, acc: 0.717391312122345)
[2024-12-12 02:35:12,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:12,641][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.9369901418685913, acc: 0.7749999761581421)
[2024-12-12 02:35:12,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:13,062][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.2911837100982666, acc: 0.9117646813392639)
[2024-12-12 02:35:13,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:13,502][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.26811373233795166, acc: 0.9444444179534912)
[2024-12-12 02:35:13,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:13,923][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.29874926805496216, acc: 0.875)
[2024-12-12 02:35:14,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:14,222][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.12222447246313095, acc: 1.0)
[2024-12-12 02:35:14,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:14,569][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.9522676467895508, acc: 0.7321428656578064)
[2024-12-12 02:35:14,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:14,947][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.776890754699707, acc: 0.8333333134651184)
[2024-12-12 02:35:15,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:15,323][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.03757898882031441, acc: 1.0)
[2024-12-12 02:35:15,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:15,648][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.3809618353843689, acc: 0.8888888955116272)
[2024-12-12 02:35:15,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:16,062][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.6875259280204773, acc: 0.8484848737716675)
[2024-12-12 02:35:16,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:16,415][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 1.1640405654907227, acc: 0.654411792755127)
[2024-12-12 02:35:16,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:16,787][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.9458877444267273, acc: 0.7301587462425232)
[2024-12-12 02:35:16,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:17,169][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 1.55161714553833, acc: 0.5846154093742371)
[2024-12-12 02:35:17,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:17,521][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 1.1442867517471313, acc: 0.7244898080825806)
[2024-12-12 02:35:17,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:17,838][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 1.3002806901931763, acc: 0.6268656849861145)
[2024-12-12 02:35:17,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:18,246][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 1.6687732934951782, acc: 0.540145993232727)
[2024-12-12 02:35:18,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:18,633][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.05024459958076477, acc: 1.0)
[2024-12-12 02:35:18,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:18,993][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.12309706211090088, acc: 0.9166666865348816)
[2024-12-12 02:35:19,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:19,368][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.28640708327293396, acc: 0.8787878751754761)
[2024-12-12 02:35:19,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:19,752][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.07053913176059723, acc: 0.9615384340286255)
[2024-12-12 02:35:19,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:20,132][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.7619737386703491, acc: 0.807692289352417)
[2024-12-12 02:35:20,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:20,441][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.949927031993866, acc: 0.7884615659713745)
[2024-12-12 02:35:20,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:20,829][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.4228726029396057, acc: 0.84375)
[2024-12-12 02:35:20,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:21,178][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.624839723110199, acc: 0.7971014380455017)
[2024-12-12 02:35:21,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:21,550][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.644378125667572, acc: 0.800000011920929)
[2024-12-12 02:35:21,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:21,952][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.32181376218795776, acc: 0.95652174949646)
[2024-12-12 02:35:22,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:22,432][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 1.1435420513153076, acc: 0.7200000286102295)
[2024-12-12 02:35:22,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:22,870][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 1.1341365575790405, acc: 0.708737850189209)
[2024-12-12 02:35:23,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:24,002][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 1.018018126487732, acc: 0.7524271607398987)
[2024-12-12 02:35:24,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:24,817][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 1.3165210485458374, acc: 0.6182795763015747)
[2024-12-12 02:35:25,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:25,614][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 1.104017972946167, acc: 0.7068965435028076)
[2024-12-12 02:35:25,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:26,352][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.9918244481086731, acc: 0.7157894968986511)
[2024-12-12 02:35:26,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:27,337][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 1.6466912031173706, acc: 0.5643564462661743)
[2024-12-12 02:35:27,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:27,629][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 1.126172423362732, acc: 0.6451612710952759)
[2024-12-12 02:35:27,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:27,933][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 1.0407588481903076, acc: 0.7246376872062683)
[2024-12-12 02:35:28,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:28,322][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 1.4004732370376587, acc: 0.5882353186607361)
[2024-12-12 02:35:28,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:28,724][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 1.3191519975662231, acc: 0.5865384340286255)
[2024-12-12 02:35:28,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:29,115][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 1.4845834970474243, acc: 0.540145993232727)
[2024-12-12 02:35:29,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:29,474][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 1.2757747173309326, acc: 0.6716417670249939)
[2024-12-12 02:35:29,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:29,851][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.43783506751060486, acc: 0.8500000238418579)
[2024-12-12 02:35:29,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:30,237][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.03285134211182594, acc: 1.0)
[2024-12-12 02:35:30,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:30,634][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.20749019086360931, acc: 0.95652174949646)
[2024-12-12 02:35:30,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:30,999][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.1299113780260086, acc: 0.9545454382896423)
[2024-12-12 02:35:31,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:31,368][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.5358849763870239, acc: 0.8448275923728943)
[2024-12-12 02:35:31,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:31,732][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.3104744851589203, acc: 0.9069767594337463)
[2024-12-12 02:35:31,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:32,092][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.2756246030330658, acc: 0.8799999952316284)
[2024-12-12 02:35:32,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:32,496][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.023792069405317307, acc: 1.0)
[2024-12-12 02:35:32,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:32,885][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.08482905477285385, acc: 0.9615384340286255)
[2024-12-12 02:35:32,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:33,181][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.31021931767463684, acc: 0.9047619104385376)
[2024-12-12 02:35:33,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:33,477][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.3403451442718506, acc: 0.8769230842590332)
[2024-12-12 02:35:33,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:33,889][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.6054823994636536, acc: 0.8421052694320679)
[2024-12-12 02:35:34,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:34,290][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 1.087262511253357, acc: 0.6842105388641357)
[2024-12-12 02:35:34,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:34,674][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.6412595510482788, acc: 0.8717948794364929)
[2024-12-12 02:35:34,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:35,074][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.4371236264705658, acc: 0.8367347121238708)
[2024-12-12 02:35:35,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:35,510][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.023558104410767555, acc: 1.0)
[2024-12-12 02:35:35,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:35,914][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.6116921305656433, acc: 0.8253968358039856)
[2024-12-12 02:35:36,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:36,286][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.7713813185691833, acc: 0.7804877758026123)
[2024-12-12 02:35:36,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:36,688][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.36372971534729004, acc: 0.8870967626571655)
[2024-12-12 02:35:36,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:37,542][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 1.106680154800415, acc: 0.6806083917617798)
[2024-12-12 02:35:37,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:37,914][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.42160242795944214, acc: 0.8799999952316284)
[2024-12-12 02:35:38,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:38,318][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.5881603360176086, acc: 0.8461538553237915)
[2024-12-12 02:35:38,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:38,606][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.11386209726333618, acc: 0.9583333134651184)
[2024-12-12 02:35:38,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:38,916][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.12000751495361328, acc: 0.9473684430122375)
[2024-12-12 02:35:39,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:39,330][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 1.1722196340560913, acc: 0.6748466491699219)
[2024-12-12 02:35:39,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:39,711][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 1.264207124710083, acc: 0.7013888955116272)
[2024-12-12 02:35:39,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:40,123][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 1.3002688884735107, acc: 0.6416666507720947)
[2024-12-12 02:35:40,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:40,530][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 1.304983377456665, acc: 0.6309523582458496)
[2024-12-12 02:35:40,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:40,910][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 1.0881396532058716, acc: 0.6974359154701233)
[2024-12-12 02:35:41,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:41,322][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 1.0716794729232788, acc: 0.720588207244873)
[2024-12-12 02:35:41,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:41,678][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.45281651616096497, acc: 0.807692289352417)
[2024-12-12 02:35:41,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:42,041][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.2759798467159271, acc: 0.9130434989929199)
[2024-12-12 02:35:42,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:42,388][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.37535130977630615, acc: 0.875)
[2024-12-12 02:35:42,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:42,747][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.7543269395828247, acc: 0.782608687877655)
[2024-12-12 02:35:42,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:43,089][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.6957516670227051, acc: 0.6857143044471741)
[2024-12-12 02:35:43,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:43,446][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.3661763370037079, acc: 0.8846153616905212)
[2024-12-12 02:35:43,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:43,813][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.7889944911003113, acc: 0.8095238208770752)
[2024-12-12 02:35:44,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:44,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:45,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:45,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:46,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:46,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:46,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:47,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:47,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:47,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:48,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:48,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:49,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:49,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:49,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:50,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:50,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:50,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:51,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:51,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:51,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:52,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:52,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:53,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:53,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:53,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:54,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:54,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:55,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:55,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:55,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:56,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:56,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:56,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:57,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:57,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:57,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:58,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:58,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:58,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:59,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:59,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:59,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:00,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:00,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:00,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:01,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:01,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:01,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:02,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:02,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:02,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:03,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:03,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:03,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:04,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:04,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:05,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:05,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:05,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:06,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:06,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:06,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:07,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:07,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:07,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:08,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:08,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:08,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:09,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:09,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:10,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:10,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:10,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:11,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:11,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:11,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:12,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:12,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:12,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:13,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:13,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:13,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:14,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:14,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:15,393][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4272, device='cuda:0') eval_epoch_loss=tensor(0.8867, device='cuda:0') eval_epoch_acc=tensor(0.7594, device='cuda:0')
[2024-12-12 02:36:15,394][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:36:15,395][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:36:15,657][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_4_step_137_loss_0.8867483139038086/model.pt
[2024-12-12 02:36:15,666][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:36:15,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:16,024][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.8945500254631042, acc: 0.699999988079071)
[2024-12-12 02:36:16,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:16,324][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.4123915731906891, acc: 0.8695651888847351)
[2024-12-12 02:36:16,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:16,636][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.28187212347984314, acc: 0.9047619104385376)
[2024-12-12 02:36:16,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:16,965][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.7838413715362549, acc: 0.7692307829856873)
[2024-12-12 02:36:17,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:17,361][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.7712822556495667, acc: 0.7419354915618896)
[2024-12-12 02:36:17,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:17,758][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.6064320802688599, acc: 0.7837837934494019)
[2024-12-12 02:36:17,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:18,289][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 1.0021625757217407, acc: 0.6578947305679321)
[2024-12-12 02:36:18,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:18,609][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.9551771879196167, acc: 0.6865671873092651)
[2024-12-12 02:36:18,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:18,993][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.8979580402374268, acc: 0.7653061151504517)
[2024-12-12 02:36:19,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:19,422][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 1.3319133520126343, acc: 0.563829779624939)
[2024-12-12 02:36:19,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:19,761][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.8663977384567261, acc: 0.7857142686843872)
[2024-12-12 02:36:19,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:20,119][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.9728698134422302, acc: 0.6785714030265808)
[2024-12-12 02:36:20,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:20,482][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.45897746086120605, acc: 0.782608687877655)
[2024-12-12 02:36:20,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:20,865][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.4944921135902405, acc: 0.8620689511299133)
[2024-12-12 02:36:20,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:21,230][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.998699426651001, acc: 0.782608687877655)
[2024-12-12 02:36:21,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:21,621][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.8881216645240784, acc: 0.8644067645072937)
[2024-12-12 02:36:21,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:21,940][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 1.2430306673049927, acc: 0.6315789222717285)
[2024-12-12 02:36:22,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:22,252][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.7540695667266846, acc: 0.7972972989082336)
[2024-12-12 02:36:22,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:22,602][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.4040054976940155, acc: 0.9285714030265808)
[2024-12-12 02:36:22,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:22,968][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.4014355540275574, acc: 0.8695651888847351)
[2024-12-12 02:36:23,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:23,333][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 1.4821792840957642, acc: 0.42105263471603394)
[2024-12-12 02:36:24,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:25,043][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 1.0556100606918335, acc: 0.7162162065505981)
[2024-12-12 02:36:25,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:25,361][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 1.4492448568344116, acc: 0.5740740895271301)
[2024-12-12 02:36:25,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:25,768][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 1.209327220916748, acc: 0.6627907156944275)
[2024-12-12 02:36:25,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:26,357][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 1.2941585779190063, acc: 0.6941176652908325)
[2024-12-12 02:36:26,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:26,911][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 1.5312217473983765, acc: 0.617977499961853)
[2024-12-12 02:36:26,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:27,223][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.5547433495521545, acc: 0.8636363744735718)
[2024-12-12 02:36:27,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:27,549][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.4312190115451813, acc: 0.8571428656578064)
[2024-12-12 02:36:27,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:27,878][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 1.0443623065948486, acc: 0.7241379022598267)
[2024-12-12 02:36:28,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:28,258][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.37058597803115845, acc: 0.8571428656578064)
[2024-12-12 02:36:28,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:28,664][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.5499927401542664, acc: 0.800000011920929)
[2024-12-12 02:36:28,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:29,108][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.6394965648651123, acc: 0.875)
[2024-12-12 02:36:29,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:29,443][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 1.0857667922973633, acc: 0.7352941036224365)
[2024-12-12 02:36:29,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:30,485][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 1.1805229187011719, acc: 0.7123287916183472)
[2024-12-12 02:36:30,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:30,811][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.20344246923923492, acc: 0.9166666865348816)
[2024-12-12 02:36:30,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:31,144][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.503445565700531, acc: 0.7777777910232544)
[2024-12-12 02:36:31,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:31,512][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.35832199454307556, acc: 0.9642857313156128)
[2024-12-12 02:36:31,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:32,070][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 1.165796160697937, acc: 0.6814159154891968)
[2024-12-12 02:36:32,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:32,454][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.9795836210250854, acc: 0.7681159377098083)
[2024-12-12 02:36:32,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:32,797][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.5691155195236206, acc: 0.8181818127632141)
[2024-12-12 02:36:33,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:33,715][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 1.4580163955688477, acc: 0.5954198241233826)
[2024-12-12 02:36:33,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:34,390][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 1.228810429573059, acc: 0.6074073910713196)
[2024-12-12 02:36:34,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:34,748][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.6708551049232483, acc: 0.7704917788505554)
[2024-12-12 02:36:34,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:35,069][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.04662744700908661, acc: 1.0)
[2024-12-12 02:36:35,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:35,400][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.1832686811685562, acc: 0.9599999785423279)
[2024-12-12 02:36:35,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:35,770][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.09777456521987915, acc: 1.0)
[2024-12-12 02:36:35,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:36,144][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.5139174461364746, acc: 0.8536585569381714)
[2024-12-12 02:36:36,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:36,526][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.7971139550209045, acc: 0.7885196208953857)
[2024-12-12 02:36:36,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:36,883][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 1.0260974168777466, acc: 0.7060518860816956)
[2024-12-12 02:36:37,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:37,361][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.9786151647567749, acc: 0.731249988079071)
[2024-12-12 02:36:37,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:37,882][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 1.0564619302749634, acc: 0.7260788083076477)
[2024-12-12 02:36:37,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:38,309][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.9004586338996887, acc: 0.7366548180580139)
[2024-12-12 02:36:38,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:38,696][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.24658550322055817, acc: 1.0)
[2024-12-12 02:36:38,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:39,248][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 1.2339999675750732, acc: 0.604651153087616)
[2024-12-12 02:36:39,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:40,046][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 1.4084395170211792, acc: 0.5634920597076416)
[2024-12-12 02:36:40,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:40,955][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 1.343322515487671, acc: 0.6136363744735718)
[2024-12-12 02:36:41,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:41,693][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.7721325755119324, acc: 0.7647058963775635)
[2024-12-12 02:36:42,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:42,768][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 1.1153651475906372, acc: 0.6234567761421204)
[2024-12-12 02:36:43,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:43,717][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.5988349914550781, acc: 0.7580645084381104)
[2024-12-12 02:36:43,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:44,072][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.27565833926200867, acc: 0.9285714030265808)
[2024-12-12 02:36:44,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:44,449][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.9642015695571899, acc: 0.7749999761581421)
[2024-12-12 02:36:44,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:44,778][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.9769996404647827, acc: 0.6764705777168274)
[2024-12-12 02:36:44,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:45,156][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 1.0642976760864258, acc: 0.75)
[2024-12-12 02:36:45,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:45,562][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.8151140213012695, acc: 0.7542372941970825)
[2024-12-12 02:36:45,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:45,903][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 1.0205177068710327, acc: 0.7089552283287048)
[2024-12-12 02:36:46,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:46,302][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.9179285168647766, acc: 0.7669903039932251)
[2024-12-12 02:36:46,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:46,672][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.7158627510070801, acc: 0.8253968358039856)
[2024-12-12 02:36:46,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:47,071][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.3992008566856384, acc: 0.9230769276618958)
[2024-12-12 02:36:47,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:47,454][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.6228474974632263, acc: 0.7937219738960266)
[2024-12-12 02:36:47,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:47,875][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.8782347440719604, acc: 0.7677165269851685)
[2024-12-12 02:36:48,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:48,304][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.6964681148529053, acc: 0.7758620977401733)
[2024-12-12 02:36:48,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:48,675][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.7112483978271484, acc: 0.8442028760910034)
[2024-12-12 02:36:48,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:49,036][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.6926939487457275, acc: 0.8054474592208862)
[2024-12-12 02:36:49,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:49,408][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.6457967162132263, acc: 0.8152173757553101)
[2024-12-12 02:36:49,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:49,770][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.14616428315639496, acc: 0.9130434989929199)
[2024-12-12 02:36:49,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:50,161][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.17320914566516876, acc: 0.9642857313156128)
[2024-12-12 02:36:50,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:50,552][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.19765393435955048, acc: 0.957446813583374)
[2024-12-12 02:36:50,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:51,249][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.3360453248023987, acc: 0.9153845906257629)
[2024-12-12 02:36:51,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:51,582][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.2199242115020752, acc: 0.9459459185600281)
[2024-12-12 02:36:51,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:51,946][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.1780933290719986, acc: 0.9534883499145508)
[2024-12-12 02:36:52,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:52,473][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.3884125053882599, acc: 0.8738738894462585)
[2024-12-12 02:36:52,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:52,868][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.2153787612915039, acc: 0.9333333373069763)
[2024-12-12 02:36:52,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:53,188][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.22274529933929443, acc: 0.9696969985961914)
[2024-12-12 02:36:53,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:53,502][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.04339524731040001, acc: 1.0)
[2024-12-12 02:36:53,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:53,857][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.1466904580593109, acc: 0.9599999785423279)
[2024-12-12 02:36:53,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:54,260][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.7615054249763489, acc: 0.692307710647583)
[2024-12-12 02:36:54,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:55,021][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.5071459412574768, acc: 0.8586956262588501)
[2024-12-12 02:36:55,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:55,563][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.682587206363678, acc: 0.8011363744735718)
[2024-12-12 02:36:55,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:55,996][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.9141584634780884, acc: 0.6914893388748169)
[2024-12-12 02:36:56,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:56,340][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.5624892711639404, acc: 0.8301886916160583)
[2024-12-12 02:36:56,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:56,671][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.3159700334072113, acc: 0.9166666865348816)
[2024-12-12 02:36:56,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:57,059][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.2802447974681854, acc: 0.9534883499145508)
[2024-12-12 02:36:57,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:57,358][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.9731544852256775, acc: 0.7333333492279053)
[2024-12-12 02:36:57,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:57,721][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 1.8954604864120483, acc: 0.5368421077728271)
[2024-12-12 02:36:57,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:58,021][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 1.4335169792175293, acc: 0.5888888835906982)
[2024-12-12 02:36:58,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:58,430][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 1.3185409307479858, acc: 0.6499999761581421)
[2024-12-12 02:36:58,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:58,913][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 1.713775873184204, acc: 0.5642201900482178)
[2024-12-12 02:36:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:59,379][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 1.4436516761779785, acc: 0.5846154093742371)
[2024-12-12 02:36:59,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:59,670][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.3892568051815033, acc: 0.8421052694320679)
[2024-12-12 02:36:59,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:59,974][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.2556416392326355, acc: 0.9583333134651184)
[2024-12-12 02:37:00,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:00,318][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.7019825577735901, acc: 0.7727272510528564)
[2024-12-12 02:37:00,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:00,709][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.604487955570221, acc: 0.8518518805503845)
[2024-12-12 02:37:00,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:01,041][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.635250985622406, acc: 0.7428571581840515)
[2024-12-12 02:37:01,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:01,457][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.898358941078186, acc: 0.8409090638160706)
[2024-12-12 02:37:01,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:01,834][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.7601984143257141, acc: 0.75)
[2024-12-12 02:37:01,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:02,408][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 1.394116997718811, acc: 0.5645161271095276)
[2024-12-12 02:37:02,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:02,925][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 1.0713415145874023, acc: 0.7272727489471436)
[2024-12-12 02:37:03,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:03,253][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.025499816983938217, acc: 1.0)
[2024-12-12 02:37:03,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:03,669][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.3158845901489258, acc: 0.8846153616905212)
[2024-12-12 02:37:03,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:04,051][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.3827301859855652, acc: 0.8387096524238586)
[2024-12-12 02:37:04,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:04,411][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.2069654017686844, acc: 0.949999988079071)
[2024-12-12 02:37:04,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:04,817][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.45836248993873596, acc: 0.9189189076423645)
[2024-12-12 02:37:04,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:05,132][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.38863521814346313, acc: 0.9189189076423645)
[2024-12-12 02:37:05,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:05,471][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.10650763660669327, acc: 0.9729729890823364)
[2024-12-12 02:37:05,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:05,816][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.39680951833724976, acc: 0.8676470518112183)
[2024-12-12 02:37:05,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:06,152][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.15294179320335388, acc: 0.9512194991111755)
[2024-12-12 02:37:06,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:06,477][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.03698839992284775, acc: 1.0)
[2024-12-12 02:37:06,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:06,806][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.03790377080440521, acc: 1.0)
[2024-12-12 02:37:06,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:07,140][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.06694789975881577, acc: 1.0)
[2024-12-12 02:37:07,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:07,561][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.3231640160083771, acc: 0.9122806787490845)
[2024-12-12 02:37:07,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:07,909][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.37843894958496094, acc: 0.8571428656578064)
[2024-12-12 02:37:08,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:08,277][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.32915568351745605, acc: 0.9342105388641357)
[2024-12-12 02:37:08,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:08,854][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.6214647889137268, acc: 0.8396226167678833)
[2024-12-12 02:37:08,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:09,440][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.602739155292511, acc: 0.8666666746139526)
[2024-12-12 02:37:09,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:09,767][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.13223645091056824, acc: 0.9444444179534912)
[2024-12-12 02:37:09,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:10,123][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.42206358909606934, acc: 0.8709677457809448)
[2024-12-12 02:37:10,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:10,452][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 1.395430326461792, acc: 0.6666666865348816)
[2024-12-12 02:37:10,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:10,813][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.8369477391242981, acc: 0.7708333134651184)
[2024-12-12 02:37:11,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:11,670][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 1.3257441520690918, acc: 0.5839999914169312)
[2024-12-12 02:37:11,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:11,984][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 1.1252628564834595, acc: 0.6853932738304138)
[2024-12-12 02:37:12,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:12,365][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.8814729452133179, acc: 0.7162162065505981)
[2024-12-12 02:37:12,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:12,833][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.7051975727081299, acc: 0.7931034564971924)
[2024-12-12 02:37:12,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:13,161][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.1595805287361145, acc: 0.9545454382896423)
[2024-12-12 02:37:13,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:13,505][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.2783513367176056, acc: 0.9545454382896423)
[2024-12-12 02:37:13,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:13,796][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.15114445984363556, acc: 0.96875)
[2024-12-12 02:37:13,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:14,077][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.19862645864486694, acc: 0.9666666388511658)
[2024-12-12 02:37:14,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:14,447][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.6034108996391296, acc: 0.8166666626930237)
[2024-12-12 02:37:14,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:14,835][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.22037175297737122, acc: 0.90625)
[2024-12-12 02:37:14,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:15,215][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.24691122770309448, acc: 0.9333333373069763)
[2024-12-12 02:37:15,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:15,621][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.34415203332901, acc: 0.9655172228813171)
[2024-12-12 02:37:15,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:15,970][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.22594968974590302, acc: 0.9200000166893005)
[2024-12-12 02:37:16,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:16,287][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.8757839798927307, acc: 0.7446808218955994)
[2024-12-12 02:37:16,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:16,562][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.3892918527126312, acc: 0.8958333134651184)
[2024-12-12 02:37:17,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:17,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:17,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:18,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:18,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:18,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:19,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:19,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:19,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:20,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:20,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:20,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:21,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:21,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:22,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:22,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:22,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:23,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:23,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:24,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:24,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:24,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:25,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:25,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:26,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:26,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:26,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:27,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:27,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:28,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:28,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:28,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:29,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:29,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:29,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:30,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:30,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:30,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:31,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:31,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:32,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:32,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:32,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:33,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:33,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:33,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:34,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:34,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:34,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:35,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:35,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:36,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:36,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:36,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:37,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:37,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:37,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:38,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:38,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:39,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:39,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:39,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:40,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:40,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:41,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:41,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:41,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:42,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:42,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:43,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:43,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:43,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:44,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:44,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:44,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:45,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:45,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:45,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:46,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:46,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:46,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:47,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:47,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:48,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:48,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:48,992][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6162, device='cuda:0') eval_epoch_loss=tensor(0.9617, device='cuda:0') eval_epoch_acc=tensor(0.7468, device='cuda:0')
[2024-12-12 02:37:48,993][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:37:48,994][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:37:49,273][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_4_step_280_loss_0.961729109287262/model.pt
[2024-12-12 02:37:49,281][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:37:49,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:49,649][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.39013659954071045, acc: 0.8863636255264282)
[2024-12-12 02:37:49,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:50,060][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 1.0085352659225464, acc: 0.7108433842658997)
[2024-12-12 02:37:50,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:50,403][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 1.105663537979126, acc: 0.6944444179534912)
[2024-12-12 02:37:50,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:50,718][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.11752023547887802, acc: 0.9736841917037964)
[2024-12-12 02:37:50,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:51,095][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.35090845823287964, acc: 0.9411764740943909)
[2024-12-12 02:37:51,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:51,465][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.3141610026359558, acc: 0.875)
[2024-12-12 02:37:51,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:51,788][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.6867780089378357, acc: 0.8203125)
[2024-12-12 02:37:51,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:52,159][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.907524585723877, acc: 0.8080000281333923)
[2024-12-12 02:37:52,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:52,540][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.506508469581604, acc: 0.8461538553237915)
[2024-12-12 02:37:52,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:52,921][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.6700140237808228, acc: 0.8198757767677307)
[2024-12-12 02:37:53,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:53,309][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.9298659563064575, acc: 0.7319587469100952)
[2024-12-12 02:37:53,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:53,659][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.04341496527194977, acc: 1.0)
[2024-12-12 02:37:53,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:54,028][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.529121994972229, acc: 0.8095238208770752)
[2024-12-12 02:37:54,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:54,427][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.4293331503868103, acc: 0.8620689511299133)
[2024-12-12 02:37:54,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:54,895][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.5465852618217468, acc: 0.8363636136054993)
[2024-12-12 02:37:55,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:55,436][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.8337383270263672, acc: 0.7680412530899048)
[2024-12-12 02:37:55,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:55,785][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.9599018096923828, acc: 0.7413793206214905)
[2024-12-12 02:37:55,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:56,124][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.3148309290409088, acc: 0.8888888955116272)
[2024-12-12 02:37:56,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:56,471][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.395615816116333, acc: 0.9210526347160339)
[2024-12-12 02:37:56,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:56,891][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.3674542307853699, acc: 0.8928571343421936)
[2024-12-12 02:37:56,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:57,237][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.08292200416326523, acc: 0.96875)
[2024-12-12 02:37:57,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:57,637][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.49765071272850037, acc: 0.8867924809455872)
[2024-12-12 02:37:57,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:58,052][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.07274552434682846, acc: 0.9811320900917053)
[2024-12-12 02:37:58,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:58,465][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.1410851776599884, acc: 0.970588207244873)
[2024-12-12 02:37:58,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:58,829][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.21632538735866547, acc: 0.9375)
[2024-12-12 02:37:58,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:59,223][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.5100870132446289, acc: 0.8524590134620667)
[2024-12-12 02:37:59,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:59,605][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.1193387433886528, acc: 0.9666666388511658)
[2024-12-12 02:37:59,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:59,932][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.03017142228782177, acc: 1.0)
[2024-12-12 02:38:00,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:00,302][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.33902642130851746, acc: 0.8985507488250732)
[2024-12-12 02:38:00,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:00,723][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.4459275007247925, acc: 0.9027777910232544)
[2024-12-12 02:38:00,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:01,088][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.3221698999404907, acc: 0.9036144614219666)
[2024-12-12 02:38:01,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:01,428][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.4804341197013855, acc: 0.8461538553237915)
[2024-12-12 02:38:01,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:01,789][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.4317084848880768, acc: 0.9081632494926453)
[2024-12-12 02:38:01,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:02,142][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.04275020956993103, acc: 1.0)
[2024-12-12 02:38:02,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:02,461][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.0372437946498394, acc: 1.0)
[2024-12-12 02:38:02,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:02,828][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.23027513921260834, acc: 0.9354838728904724)
[2024-12-12 02:38:02,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:03,214][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.45001858472824097, acc: 0.8709677457809448)
[2024-12-12 02:38:03,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:03,597][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.15625715255737305, acc: 0.9402984976768494)
[2024-12-12 02:38:03,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:04,003][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.21534498035907745, acc: 0.932692289352417)
[2024-12-12 02:38:04,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:04,341][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.19564858078956604, acc: 0.9555555582046509)
[2024-12-12 02:38:04,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:04,715][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.19391198456287384, acc: 0.9193548560142517)
[2024-12-12 02:38:04,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:05,119][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.03678061068058014, acc: 1.0)
[2024-12-12 02:38:05,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:05,485][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.8541969656944275, acc: 0.7037037014961243)
[2024-12-12 02:38:05,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:05,826][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 1.5319194793701172, acc: 0.5714285969734192)
[2024-12-12 02:38:05,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:06,197][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.9893107414245605, acc: 0.692307710647583)
[2024-12-12 02:38:06,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:06,547][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 1.5862280130386353, acc: 0.5609756112098694)
[2024-12-12 02:38:06,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:06,876][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 1.0429295301437378, acc: 0.7894737124443054)
[2024-12-12 02:38:06,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:07,201][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.17921210825443268, acc: 0.9473684430122375)
[2024-12-12 02:38:07,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:07,566][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.048748672008514404, acc: 1.0)
[2024-12-12 02:38:07,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:07,968][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.1780613511800766, acc: 0.9259259104728699)
[2024-12-12 02:38:08,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:08,298][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.07909274846315384, acc: 1.0)
[2024-12-12 02:38:08,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:08,675][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.36670857667922974, acc: 0.8870967626571655)
[2024-12-12 02:38:08,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:09,050][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.28292131423950195, acc: 0.9298245906829834)
[2024-12-12 02:38:09,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:09,412][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.6932763457298279, acc: 0.875)
[2024-12-12 02:38:09,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:09,787][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.11501628905534744, acc: 0.9666666388511658)
[2024-12-12 02:38:09,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:10,132][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.06421146541833878, acc: 1.0)
[2024-12-12 02:38:10,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:10,481][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 1.0816799402236938, acc: 0.699999988079071)
[2024-12-12 02:38:10,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:10,821][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 1.3879308700561523, acc: 0.6666666865348816)
[2024-12-12 02:38:10,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:11,162][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 1.2828850746154785, acc: 0.6276595592498779)
[2024-12-12 02:38:11,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:11,478][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 1.3789188861846924, acc: 0.6144578456878662)
[2024-12-12 02:38:11,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:11,810][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.034517575055360794, acc: 1.0)
[2024-12-12 02:38:11,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:12,204][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.4853500425815582, acc: 0.8974359035491943)
[2024-12-12 02:38:12,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:12,589][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.5628484487533569, acc: 0.8554216623306274)
[2024-12-12 02:38:12,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:12,958][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.5156696438789368, acc: 0.849056601524353)
[2024-12-12 02:38:13,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:13,299][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.2672290802001953, acc: 0.8860759735107422)
[2024-12-12 02:38:13,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:13,655][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.17458051443099976, acc: 0.9215686321258545)
[2024-12-12 02:38:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:14,011][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.6038532257080078, acc: 0.8059701323509216)
[2024-12-12 02:38:14,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:14,380][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.06284940987825394, acc: 0.949999988079071)
[2024-12-12 02:38:14,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:14,763][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.18248747289180756, acc: 0.9200000166893005)
[2024-12-12 02:38:14,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:15,192][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.6733530163764954, acc: 0.8333333134651184)
[2024-12-12 02:38:15,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:15,555][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.8610435128211975, acc: 0.7441860437393188)
[2024-12-12 02:38:15,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:15,909][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.3329019248485565, acc: 0.8717948794364929)
[2024-12-12 02:38:16,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:16,286][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 1.017024278640747, acc: 0.6888889074325562)
[2024-12-12 02:38:16,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:16,596][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.08263491094112396, acc: 0.95652174949646)
[2024-12-12 02:38:16,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:16,922][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.34087273478507996, acc: 0.8846153616905212)
[2024-12-12 02:38:17,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:17,276][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 1.0104328393936157, acc: 0.7362637519836426)
[2024-12-12 02:38:17,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:17,782][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.9331672191619873, acc: 0.730434775352478)
[2024-12-12 02:38:17,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:18,158][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.6938436627388, acc: 0.760869562625885)
[2024-12-12 02:38:18,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:18,506][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.7154271602630615, acc: 0.8163265585899353)
[2024-12-12 02:38:18,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:18,812][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.00920153222978115, acc: 1.0)
[2024-12-12 02:38:18,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:19,168][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.09638360887765884, acc: 0.9615384340286255)
[2024-12-12 02:38:19,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:19,505][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.44632405042648315, acc: 0.8536585569381714)
[2024-12-12 02:38:19,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:19,879][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.41822752356529236, acc: 0.9111111164093018)
[2024-12-12 02:38:20,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:20,255][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.1958162784576416, acc: 0.9210526347160339)
[2024-12-12 02:38:20,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:20,646][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.16864067316055298, acc: 0.9512194991111755)
[2024-12-12 02:38:20,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:21,007][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.1237403005361557, acc: 1.0)
[2024-12-12 02:38:21,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:21,400][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.016367627307772636, acc: 1.0)
[2024-12-12 02:38:21,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:21,749][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.0322251133620739, acc: 1.0)
[2024-12-12 02:38:21,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:22,150][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.09817911684513092, acc: 1.0)
[2024-12-12 02:38:22,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:22,551][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.4254313111305237, acc: 0.90625)
[2024-12-12 02:38:22,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:23,149][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.9813153147697449, acc: 0.7090908885002136)
[2024-12-12 02:38:23,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:24,019][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.5543146133422852, acc: 0.8396226167678833)
[2024-12-12 02:38:24,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:24,401][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.554137647151947, acc: 0.8666666746139526)
[2024-12-12 02:38:24,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:24,743][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.1795424073934555, acc: 0.9464285969734192)
[2024-12-12 02:38:24,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:25,120][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.15507330000400543, acc: 0.9714285731315613)
[2024-12-12 02:38:25,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:25,509][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.00702993618324399, acc: 1.0)
[2024-12-12 02:38:25,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:25,834][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.018804332241415977, acc: 1.0)
[2024-12-12 02:38:25,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:26,164][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.18271593749523163, acc: 0.9583333134651184)
[2024-12-12 02:38:26,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:26,484][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.22629916667938232, acc: 0.9578947424888611)
[2024-12-12 02:38:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:27,052][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.4841916561126709, acc: 0.8742514848709106)
[2024-12-12 02:38:27,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:27,450][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.4264158308506012, acc: 0.9097744226455688)
[2024-12-12 02:38:27,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:28,684][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.6581243872642517, acc: 0.7860962748527527)
[2024-12-12 02:38:28,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:29,244][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.2069736123085022, acc: 0.9279279112815857)
[2024-12-12 02:38:29,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:29,611][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.1245400533080101, acc: 0.9642857313156128)
[2024-12-12 02:38:29,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:29,997][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.026089761406183243, acc: 1.0)
[2024-12-12 02:38:30,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:30,357][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.21380411088466644, acc: 0.9375)
[2024-12-12 02:38:30,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:30,686][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.10661748051643372, acc: 0.9722222089767456)
[2024-12-12 02:38:30,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:31,034][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.024841807782649994, acc: 1.0)
[2024-12-12 02:38:31,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:31,372][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.004747036378830671, acc: 1.0)
[2024-12-12 02:38:31,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:31,775][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.02427687868475914, acc: 1.0)
[2024-12-12 02:38:31,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:32,110][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.2517207860946655, acc: 0.9047619104385376)
[2024-12-12 02:38:32,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:32,416][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 1.0660394430160522, acc: 0.7222222089767456)
[2024-12-12 02:38:32,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:32,823][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 1.1452455520629883, acc: 0.7281553149223328)
[2024-12-12 02:38:32,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:33,350][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 1.008631706237793, acc: 0.7279411554336548)
[2024-12-12 02:38:33,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:33,775][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 1.003751277923584, acc: 0.7333333492279053)
[2024-12-12 02:38:33,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:34,176][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.8691619634628296, acc: 0.7291666865348816)
[2024-12-12 02:38:34,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:34,565][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.45454543828964233, acc: 0.8837209343910217)
[2024-12-12 02:38:34,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:34,915][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.10153397917747498, acc: 0.9583333134651184)
[2024-12-12 02:38:35,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:35,340][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.3252863585948944, acc: 0.8837209343910217)
[2024-12-12 02:38:35,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:35,642][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.03995693102478981, acc: 1.0)
[2024-12-12 02:38:35,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:36,169][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.32815903425216675, acc: 0.8970588445663452)
[2024-12-12 02:38:36,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:36,489][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.5078443884849548, acc: 0.8533333539962769)
[2024-12-12 02:38:36,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:36,791][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.425141304731369, acc: 0.8787878751754761)
[2024-12-12 02:38:36,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:37,151][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.39089465141296387, acc: 0.9090909361839294)
[2024-12-12 02:38:37,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:37,538][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.05588147044181824, acc: 1.0)
[2024-12-12 02:38:37,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:37,866][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.11756902933120728, acc: 0.9629629850387573)
[2024-12-12 02:38:37,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:38,229][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.09902845323085785, acc: 1.0)
[2024-12-12 02:38:38,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:38,601][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.06323310732841492, acc: 0.9722222089767456)
[2024-12-12 02:38:38,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:38,943][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.1606423258781433, acc: 0.9629629850387573)
[2024-12-12 02:38:39,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:39,250][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.04930447041988373, acc: 1.0)
[2024-12-12 02:38:39,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:39,551][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.15362615883350372, acc: 0.931034505367279)
[2024-12-12 02:38:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:39,853][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.6171827912330627, acc: 0.8214285969734192)
[2024-12-12 02:38:39,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:40,242][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.12931707501411438, acc: 0.9666666388511658)
[2024-12-12 02:38:40,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:40,618][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.11376332491636276, acc: 0.9696969985961914)
[2024-12-12 02:38:40,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:41,018][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.14949862658977509, acc: 0.9545454382896423)
[2024-12-12 02:38:41,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:41,371][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.4524385929107666, acc: 0.843137264251709)
[2024-12-12 02:38:41,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:41,700][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.06148900091648102, acc: 1.0)
[2024-12-12 02:38:41,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:42,019][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.14654450118541718, acc: 0.9444444179534912)
[2024-12-12 02:38:42,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:42,363][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.20152616500854492, acc: 0.925000011920929)
[2024-12-12 02:38:42,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:42,690][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.15405899286270142, acc: 0.949999988079071)
[2024-12-12 02:38:42,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:43,057][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.04119438678026199, acc: 1.0)
[2024-12-12 02:38:43,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:43,393][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.3034118115901947, acc: 0.9333333373069763)
[2024-12-12 02:38:43,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:43,756][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.3113248944282532, acc: 0.90625)
[2024-12-12 02:38:44,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:44,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:45,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:45,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:45,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:45,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:46,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:46,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:47,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:47,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:47,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:48,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:48,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:49,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:49,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:50,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:50,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:50,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:51,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:51,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:51,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:52,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:52,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:53,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:53,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:53,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:54,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:54,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:55,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:55,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:55,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:56,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:56,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:56,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:57,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:57,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:57,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:58,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:58,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:58,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:59,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:59,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:00,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:00,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:00,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:01,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:01,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:01,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:02,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:02,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:02,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:03,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:03,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:03,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:04,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:04,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:04,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:05,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:05,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:05,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:06,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:06,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:07,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:07,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:07,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:08,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:08,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:09,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:09,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:09,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:10,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:10,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:10,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:11,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:11,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:11,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:12,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:12,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:12,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:13,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:13,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:13,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:14,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:14,941][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2544, device='cuda:0') eval_epoch_loss=tensor(0.8129, device='cuda:0') eval_epoch_acc=tensor(0.7852, device='cuda:0')
[2024-12-12 02:39:14,942][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:39:14,942][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:39:15,206][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_4_step_423_loss_0.812889575958252/model.pt
[2024-12-12 02:39:15,212][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:39:15,213][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 4 is 0.812889575958252
[2024-12-12 02:39:15,214][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.7852249145507812
[2024-12-12 02:39:15,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:15,673][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.29738476872444153, acc: 0.9444444179534912)
[2024-12-12 02:39:15,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:15,988][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.2138504981994629, acc: 0.9629629850387573)
[2024-12-12 02:39:16,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:16,312][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.1972886621952057, acc: 0.9696969985961914)
[2024-12-12 02:39:16,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:16,624][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.03316367045044899, acc: 1.0)
[2024-12-12 02:39:16,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:16,986][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.062281686812639236, acc: 0.9729729890823364)
[2024-12-12 02:39:17,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:17,337][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.05256946012377739, acc: 0.9629629850387573)
[2024-12-12 02:39:17,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:17,677][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.01477187778800726, acc: 1.0)
[2024-12-12 02:39:17,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:18,015][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.01145139243453741, acc: 1.0)
[2024-12-12 02:39:18,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:18,377][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.006925167981535196, acc: 1.0)
[2024-12-12 02:39:18,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:18,711][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.015269865281879902, acc: 1.0)
[2024-12-12 02:39:18,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:19,071][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.1722744107246399, acc: 0.9444444179534912)
[2024-12-12 02:39:19,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:19,424][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.015631817281246185, acc: 1.0)
[2024-12-12 02:39:19,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:19,759][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.034369684755802155, acc: 0.9696969985961914)
[2024-12-12 02:39:19,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:20,083][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.11786879599094391, acc: 0.9722222089767456)
[2024-12-12 02:39:20,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:20,461][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.18193091452121735, acc: 0.9545454382896423)
[2024-12-12 02:39:20,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:20,862][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.003296694252640009, acc: 1.0)
[2024-12-12 02:39:20,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:21,258][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.20920486748218536, acc: 0.9230769276618958)
[2024-12-12 02:39:21,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:21,721][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.366352379322052, acc: 0.8484848737716675)
[2024-12-12 02:39:21,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:22,422][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.8172831535339355, acc: 0.7680000066757202)
[2024-12-12 02:39:22,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:22,839][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.9232518672943115, acc: 0.725806474685669)
[2024-12-12 02:39:22,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:23,494][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.845032274723053, acc: 0.7611940503120422)
[2024-12-12 02:39:23,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:23,843][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.14752881228923798, acc: 0.9622641801834106)
[2024-12-12 02:39:23,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:24,231][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.0851718857884407, acc: 1.0)
[2024-12-12 02:39:24,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:24,583][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.20714712142944336, acc: 0.95652174949646)
[2024-12-12 02:39:24,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:24,991][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.44700998067855835, acc: 0.807692289352417)
[2024-12-12 02:39:25,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:25,355][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.10453976690769196, acc: 0.9642857313156128)
[2024-12-12 02:39:25,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:25,743][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.3136008679866791, acc: 0.9402984976768494)
[2024-12-12 02:39:25,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:26,118][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.08502956479787827, acc: 0.9583333134651184)
[2024-12-12 02:39:26,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:26,507][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.1453930288553238, acc: 0.9347826242446899)
[2024-12-12 02:39:26,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:26,901][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.3294966518878937, acc: 0.9230769276618958)
[2024-12-12 02:39:27,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:27,239][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.5364197492599487, acc: 0.8684210777282715)
[2024-12-12 02:39:27,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:27,605][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.36046335101127625, acc: 0.918367326259613)
[2024-12-12 02:39:27,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:27,981][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.24901993572711945, acc: 0.9090909361839294)
[2024-12-12 02:39:28,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:28,387][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.9998117089271545, acc: 0.7113401889801025)
[2024-12-12 02:39:28,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:28,752][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.2527424693107605, acc: 0.9142857193946838)
[2024-12-12 02:39:28,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:29,159][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.734235405921936, acc: 0.7965116500854492)
[2024-12-12 02:39:29,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:29,520][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.2763417661190033, acc: 0.9107142686843872)
[2024-12-12 02:39:29,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:29,811][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.4744601249694824, acc: 0.8641975522041321)
[2024-12-12 02:39:29,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:30,145][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.5531550645828247, acc: 0.8333333134651184)
[2024-12-12 02:39:30,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:30,507][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.07582428306341171, acc: 0.96875)
[2024-12-12 02:39:30,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:30,841][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.23396334052085876, acc: 0.9230769276618958)
[2024-12-12 02:39:30,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:31,158][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.22469192743301392, acc: 0.95652174949646)
[2024-12-12 02:39:31,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:31,499][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.46046075224876404, acc: 0.8690476417541504)
[2024-12-12 02:39:31,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:31,822][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.7052288055419922, acc: 0.8192771077156067)
[2024-12-12 02:39:31,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:32,223][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.3455301523208618, acc: 0.8918918967247009)
[2024-12-12 02:39:32,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:32,604][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.7845108509063721, acc: 0.7766990065574646)
[2024-12-12 02:39:32,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:32,943][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.6315877437591553, acc: 0.8048780560493469)
[2024-12-12 02:39:33,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:33,288][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.10007419437170029, acc: 1.0)
[2024-12-12 02:39:33,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:33,627][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.20981107652187347, acc: 0.8928571343421936)
[2024-12-12 02:39:33,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:34,068][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.6752499341964722, acc: 0.7941176295280457)
[2024-12-12 02:39:34,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:34,436][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.9159700870513916, acc: 0.7161571979522705)
[2024-12-12 02:39:34,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:34,765][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.6196080446243286, acc: 0.8229166865348816)
[2024-12-12 02:39:34,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:35,130][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.44808369874954224, acc: 0.8527607321739197)
[2024-12-12 02:39:35,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:35,502][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.38831058144569397, acc: 0.8848921060562134)
[2024-12-12 02:39:35,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:35,890][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.831508994102478, acc: 0.7638190984725952)
[2024-12-12 02:39:36,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:36,259][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.3343104422092438, acc: 0.8611111044883728)
[2024-12-12 02:39:36,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:36,633][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.26144078373908997, acc: 0.9090909361839294)
[2024-12-12 02:39:36,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:36,965][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.21214713156223297, acc: 0.9629629850387573)
[2024-12-12 02:39:37,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:37,304][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.19588284194469452, acc: 0.949999988079071)
[2024-12-12 02:39:37,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:37,657][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.3559875786304474, acc: 0.8999999761581421)
[2024-12-12 02:39:37,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:38,075][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.699880063533783, acc: 0.7586206793785095)
[2024-12-12 02:39:38,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:38,462][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.11850994825363159, acc: 0.9677419066429138)
[2024-12-12 02:39:38,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:38,792][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.23115883767604828, acc: 0.9473684430122375)
[2024-12-12 02:39:38,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:39,088][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.3686951994895935, acc: 0.8888888955116272)
[2024-12-12 02:39:39,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:39,456][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.38762766122817993, acc: 0.9047619104385376)
[2024-12-12 02:39:39,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:39,797][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.3477463126182556, acc: 0.9090909361839294)
[2024-12-12 02:39:39,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:40,169][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.9621186256408691, acc: 0.7230769395828247)
[2024-12-12 02:39:40,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:40,450][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.09792863577604294, acc: 1.0)
[2024-12-12 02:39:40,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:40,846][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.29763346910476685, acc: 0.8965517282485962)
[2024-12-12 02:39:40,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:41,209][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.5433567762374878, acc: 0.8039215803146362)
[2024-12-12 02:39:41,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:41,555][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.406546413898468, acc: 0.8620689511299133)
[2024-12-12 02:39:41,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:41,921][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.5590209364891052, acc: 0.8421052694320679)
[2024-12-12 02:39:42,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:42,287][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.2340531051158905, acc: 0.8947368264198303)
[2024-12-12 02:39:42,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:42,657][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.7688361406326294, acc: 0.8125)
[2024-12-12 02:39:42,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:43,027][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.5833691954612732, acc: 0.8651685118675232)
[2024-12-12 02:39:43,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:43,359][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.822942852973938, acc: 0.7415730357170105)
[2024-12-12 02:39:43,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:43,728][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 1.3330472707748413, acc: 0.6170212626457214)
[2024-12-12 02:39:43,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:44,123][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 1.1175154447555542, acc: 0.717391312122345)
[2024-12-12 02:39:44,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:44,460][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.15140531957149506, acc: 0.9599999785423279)
[2024-12-12 02:39:44,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:44,753][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.050657425075769424, acc: 1.0)
[2024-12-12 02:39:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:45,078][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.3135364055633545, acc: 0.9259259104728699)
[2024-12-12 02:39:45,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:45,447][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.10643014311790466, acc: 0.9629629850387573)
[2024-12-12 02:39:45,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:45,746][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.39019903540611267, acc: 0.8867924809455872)
[2024-12-12 02:39:45,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:46,085][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.45217809081077576, acc: 0.8965517282485962)
[2024-12-12 02:39:46,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:46,688][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 1.0991648435592651, acc: 0.684684693813324)
[2024-12-12 02:39:46,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:47,137][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.6300371289253235, acc: 0.8591549396514893)
[2024-12-12 02:39:47,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:47,501][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.2077803909778595, acc: 0.949999988079071)
[2024-12-12 02:39:47,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:47,842][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.228836789727211, acc: 0.8999999761581421)
[2024-12-12 02:39:47,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:48,198][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.16263239085674286, acc: 0.9230769276618958)
[2024-12-12 02:39:49,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:50,925][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 1.058633804321289, acc: 0.6857143044471741)
[2024-12-12 02:39:51,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:51,683][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.5273244976997375, acc: 0.8095238208770752)
[2024-12-12 02:39:51,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:51,996][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.34867745637893677, acc: 0.8571428656578064)
[2024-12-12 02:39:52,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:52,337][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.18161247670650482, acc: 0.9333333373069763)
[2024-12-12 02:39:52,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:53,026][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.5781649947166443, acc: 0.8888888955116272)
[2024-12-12 02:39:53,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:53,389][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.019526027143001556, acc: 1.0)
[2024-12-12 02:39:53,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:53,728][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.2127586156129837, acc: 0.9354838728904724)
[2024-12-12 02:39:53,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:54,037][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.14285556972026825, acc: 0.949999988079071)
[2024-12-12 02:39:54,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:54,399][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.14620091021060944, acc: 0.9629629850387573)
[2024-12-12 02:39:54,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:55,433][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.8490355610847473, acc: 0.7669491767883301)
[2024-12-12 02:39:55,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:55,779][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.38383081555366516, acc: 0.8656716346740723)
[2024-12-12 02:39:55,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:56,157][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.46116361021995544, acc: 0.8686131238937378)
[2024-12-12 02:39:56,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:56,721][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.7232718467712402, acc: 0.8050000071525574)
[2024-12-12 02:39:56,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:57,039][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.0769871398806572, acc: 0.9814814925193787)
[2024-12-12 02:39:57,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:57,363][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.2442483901977539, acc: 0.9230769276618958)
[2024-12-12 02:39:57,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:57,733][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.12947413325309753, acc: 0.9523809552192688)
[2024-12-12 02:39:57,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:58,128][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 1.2315226793289185, acc: 0.6721311211585999)
[2024-12-12 02:39:58,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:58,496][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.3317446708679199, acc: 0.8813559412956238)
[2024-12-12 02:39:58,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:58,874][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 1.249255657196045, acc: 0.6511628031730652)
[2024-12-12 02:39:58,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:59,204][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.9236631989479065, acc: 0.7727272510528564)
[2024-12-12 02:39:59,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:59,536][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 0.950925350189209, acc: 0.8113207817077637)
[2024-12-12 02:39:59,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:59,858][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.6521669030189514, acc: 0.8181818127632141)
[2024-12-12 02:39:59,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:00,233][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.4217217266559601, acc: 0.8799999952316284)
[2024-12-12 02:40:00,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:00,660][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.2682754397392273, acc: 0.949999988079071)
[2024-12-12 02:40:00,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:01,067][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.2389596402645111, acc: 0.9090909361839294)
[2024-12-12 02:40:01,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:01,454][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.4996039867401123, acc: 0.8307692408561707)
[2024-12-12 02:40:01,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:01,852][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.6102186441421509, acc: 0.828125)
[2024-12-12 02:40:02,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:02,290][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.4651426672935486, acc: 0.84375)
[2024-12-12 02:40:02,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:02,658][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.5248732566833496, acc: 0.7575757503509521)
[2024-12-12 02:40:02,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:02,972][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.24238523840904236, acc: 0.875)
[2024-12-12 02:40:03,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:03,273][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.05180922523140907, acc: 1.0)
[2024-12-12 02:40:03,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:03,568][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.01191769726574421, acc: 1.0)
[2024-12-12 02:40:03,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:03,908][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.19962982833385468, acc: 0.8999999761581421)
[2024-12-12 02:40:04,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:04,296][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.11940947920084, acc: 0.9756097793579102)
[2024-12-12 02:40:04,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:04,649][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.023927703499794006, acc: 1.0)
[2024-12-12 02:40:04,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:05,019][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.13177470862865448, acc: 0.9473684430122375)
[2024-12-12 02:40:05,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:05,356][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.1468278467655182, acc: 0.9354838728904724)
[2024-12-12 02:40:05,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:05,673][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.03741562366485596, acc: 1.0)
[2024-12-12 02:40:05,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:06,020][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.19672241806983948, acc: 0.939393937587738)
[2024-12-12 02:40:06,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:06,371][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.10440795123577118, acc: 0.949999988079071)
[2024-12-12 02:40:06,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:06,699][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.28175145387649536, acc: 0.9285714030265808)
[2024-12-12 02:40:06,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:07,018][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.6139123439788818, acc: 0.8175182342529297)
[2024-12-12 02:40:07,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:07,375][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.3576638400554657, acc: 0.8620689511299133)
[2024-12-12 02:40:07,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:07,780][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.5111508965492249, acc: 0.8714285492897034)
[2024-12-12 02:40:07,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:08,182][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.45266368985176086, acc: 0.8675496578216553)
[2024-12-12 02:40:08,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:08,545][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.16207657754421234, acc: 0.9316239356994629)
[2024-12-12 02:40:08,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:08,955][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.1915370374917984, acc: 0.9599999785423279)
[2024-12-12 02:40:09,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:09,315][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.2222929745912552, acc: 0.9230769276618958)
[2024-12-12 02:40:09,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:09,663][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.12854944169521332, acc: 0.9615384340286255)
[2024-12-12 02:40:09,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:10,049][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.23002232611179352, acc: 0.9230769276618958)
[2024-12-12 02:40:10,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:10,396][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.6018338203430176, acc: 0.8111110925674438)
[2024-12-12 02:40:10,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:10,731][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.3641730546951294, acc: 0.8961039185523987)
[2024-12-12 02:40:10,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:11,041][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.27110883593559265, acc: 0.8958333134651184)
[2024-12-12 02:40:11,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:11,369][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.3637111783027649, acc: 0.8793103694915771)
[2024-12-12 02:40:12,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:12,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:12,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:13,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:13,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:13,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:14,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:14,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:14,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:15,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:15,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:15,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:16,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:16,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:16,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:17,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:17,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:17,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:18,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:18,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:19,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:19,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:19,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:20,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:20,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:20,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:21,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:21,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:22,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:22,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:22,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:23,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:23,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:23,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:24,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:24,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:24,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:25,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:25,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:25,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:26,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:26,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:26,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:27,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:27,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:28,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:28,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:28,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:29,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:29,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:29,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:30,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:30,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:30,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:31,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:31,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:31,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:32,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:32,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:32,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:33,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:33,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:34,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:34,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:34,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:35,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:35,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:35,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:36,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:36,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:37,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:37,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:37,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:37,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:38,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:38,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:38,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:39,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:39,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:39,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:40,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:40,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:40,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:41,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:41,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:42,187][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4882, device='cuda:0') eval_epoch_loss=tensor(0.9115, device='cuda:0') eval_epoch_acc=tensor(0.7683, device='cuda:0')
[2024-12-12 02:40:42,188][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:40:42,189][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:40:42,446][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_4_step_566_loss_0.9115465879440308/model.pt
[2024-12-12 02:40:42,456][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:40:42,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:42,815][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.30359014868736267, acc: 0.9166666865348816)
[2024-12-12 02:40:42,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:43,217][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.04100576043128967, acc: 1.0)
[2024-12-12 02:40:43,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:43,547][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.011210990138351917, acc: 1.0)
[2024-12-12 02:40:43,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:43,925][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.3611755669116974, acc: 0.893048107624054)
[2024-12-12 02:40:44,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:44,190][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.10810693353414536, acc: 0.9677419066429138)
[2024-12-12 02:40:44,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:44,551][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.2937336564064026, acc: 0.9059829115867615)
[2024-12-12 02:40:44,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:44,907][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.6993871927261353, acc: 0.7908163070678711)
[2024-12-12 02:40:45,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:45,269][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.6614721417427063, acc: 0.8176100850105286)
[2024-12-12 02:40:45,674][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.6928, train_epoch_loss=0.5264, epoch time 362.3844491764903s
[2024-12-12 02:40:45,674][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:40:45,674][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:40:45,674][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:40:45,674][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2024-12-12 02:40:45,675][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:40:46,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:46,560][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.12106628715991974, acc: 0.9629629850387573)
[2024-12-12 02:40:46,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:46,892][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.050931233912706375, acc: 1.0)
[2024-12-12 02:40:47,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:47,296][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.3360919654369354, acc: 0.8918918967247009)
[2024-12-12 02:40:47,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:47,695][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.26007279753685, acc: 0.8947368264198303)
[2024-12-12 02:40:47,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:48,069][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.37872517108917236, acc: 0.9189189076423645)
[2024-12-12 02:40:48,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:48,439][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.11925984174013138, acc: 0.9642857313156128)
[2024-12-12 02:40:48,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:48,846][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.6657519936561584, acc: 0.7755101919174194)
[2024-12-12 02:40:48,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:49,221][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.22687523066997528, acc: 0.9333333373069763)
[2024-12-12 02:40:49,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:49,658][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.09119567275047302, acc: 0.9545454382896423)
[2024-12-12 02:40:49,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:49,977][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.19019757211208344, acc: 0.9230769276618958)
[2024-12-12 02:40:50,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:50,363][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.09187383949756622, acc: 0.9259259104728699)
[2024-12-12 02:40:50,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:50,716][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.10449928790330887, acc: 0.9743589758872986)
[2024-12-12 02:40:50,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:51,109][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.03246782720088959, acc: 1.0)
[2024-12-12 02:40:51,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:51,561][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.09138236194849014, acc: 0.95652174949646)
[2024-12-12 02:40:51,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:51,942][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.12148669362068176, acc: 0.9607843160629272)
[2024-12-12 02:40:52,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:52,344][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.42456361651420593, acc: 0.8775510191917419)
[2024-12-12 02:40:52,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:52,748][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.06989157944917679, acc: 1.0)
[2024-12-12 02:40:52,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:53,123][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.15482980012893677, acc: 0.9166666865348816)
[2024-12-12 02:40:53,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:53,416][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.8024209141731262, acc: 0.8055555820465088)
[2024-12-12 02:40:53,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:53,716][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.350374311208725, acc: 0.9473684430122375)
[2024-12-12 02:40:53,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:54,066][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.17468135058879852, acc: 0.9615384340286255)
[2024-12-12 02:40:54,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:54,430][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.14263209700584412, acc: 0.931034505367279)
[2024-12-12 02:40:54,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:54,756][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.5563991069793701, acc: 0.8799999952316284)
[2024-12-12 02:40:54,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:55,063][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.14929750561714172, acc: 0.9523809552192688)
[2024-12-12 02:40:55,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:55,425][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.09022440016269684, acc: 1.0)
[2024-12-12 02:40:55,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:55,847][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.5459666848182678, acc: 0.8679245114326477)
[2024-12-12 02:40:55,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:56,243][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.767159104347229, acc: 0.8082191944122314)
[2024-12-12 02:40:56,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:57,533][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 1.123993158340454, acc: 0.687747061252594)
[2024-12-12 02:40:57,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:57,874][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.608620285987854, acc: 0.8604651093482971)
[2024-12-12 02:40:57,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:58,233][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.6432449817657471, acc: 0.8072289228439331)
[2024-12-12 02:40:58,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:58,586][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.7095388770103455, acc: 0.8024691343307495)
[2024-12-12 02:40:58,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:58,863][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.26164835691452026, acc: 0.9285714030265808)
[2024-12-12 02:40:58,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:59,183][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.4489777982234955, acc: 0.8518518805503845)
[2024-12-12 02:40:59,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:59,591][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.039235860109329224, acc: 1.0)
[2024-12-12 02:40:59,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:59,996][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.64329594373703, acc: 0.8571428656578064)
[2024-12-12 02:41:00,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:00,312][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.41522759199142456, acc: 0.8032786846160889)
[2024-12-12 02:41:00,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:00,683][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.5795069932937622, acc: 0.841269850730896)
[2024-12-12 02:41:00,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:01,044][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.37090256810188293, acc: 0.9152542352676392)
[2024-12-12 02:41:01,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:01,436][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.26517632603645325, acc: 0.9080459475517273)
[2024-12-12 02:41:01,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:01,818][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.22937601804733276, acc: 0.9047619104385376)
[2024-12-12 02:41:01,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:02,239][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.7441887259483337, acc: 0.807692289352417)
[2024-12-12 02:41:02,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:02,664][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.34650057554244995, acc: 0.8918918967247009)
[2024-12-12 02:41:02,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:03,052][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.6680629849433899, acc: 0.8307692408561707)
[2024-12-12 02:41:03,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:03,468][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.3830713629722595, acc: 0.868686854839325)
[2024-12-12 02:41:03,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:03,853][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.49492689967155457, acc: 0.8659793734550476)
[2024-12-12 02:41:03,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:04,277][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.6081299781799316, acc: 0.8308823704719543)
[2024-12-12 02:41:04,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:04,607][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.255215585231781, acc: 0.9615384340286255)
[2024-12-12 02:41:04,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:04,976][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.12729093432426453, acc: 0.9629629850387573)
[2024-12-12 02:41:05,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:05,389][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.31121447682380676, acc: 0.9285714030265808)
[2024-12-12 02:41:05,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:05,784][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.07392079383134842, acc: 0.9722222089767456)
[2024-12-12 02:41:05,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:06,157][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.4813864529132843, acc: 0.8245614171028137)
[2024-12-12 02:41:06,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:06,598][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.3483746349811554, acc: 0.8888888955116272)
[2024-12-12 02:41:06,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:07,007][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.8305674195289612, acc: 0.7464788556098938)
[2024-12-12 02:41:07,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:07,503][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 1.4098979234695435, acc: 0.6066666841506958)
[2024-12-12 02:41:07,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:07,900][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.5581115484237671, acc: 0.8108108043670654)
[2024-12-12 02:41:08,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:08,332][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.04839338734745979, acc: 1.0)
[2024-12-12 02:41:09,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:11,455][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 1.236804485321045, acc: 0.6655290126800537)
[2024-12-12 02:41:11,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:12,753][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 1.2841507196426392, acc: 0.6405228972434998)
[2024-12-12 02:41:12,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:13,399][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.8882471323013306, acc: 0.75)
[2024-12-12 02:41:13,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:13,979][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.5185542106628418, acc: 0.8235294222831726)
[2024-12-12 02:41:14,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:14,537][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.9324043393135071, acc: 0.739130437374115)
[2024-12-12 02:41:14,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:14,939][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.7015154957771301, acc: 0.800000011920929)
[2024-12-12 02:41:15,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:15,313][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.1204303428530693, acc: 0.970588207244873)
[2024-12-12 02:41:15,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:15,674][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.44755083322525024, acc: 0.8333333134651184)
[2024-12-12 02:41:15,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:15,973][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.26422685384750366, acc: 0.921875)
[2024-12-12 02:41:16,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:16,280][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.16339971125125885, acc: 0.931034505367279)
[2024-12-12 02:41:16,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:16,685][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.6238788366317749, acc: 0.8214285969734192)
[2024-12-12 02:41:16,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:17,072][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.5751173496246338, acc: 0.8500000238418579)
[2024-12-12 02:41:17,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:17,472][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.037938546389341354, acc: 1.0)
[2024-12-12 02:41:17,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:17,842][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.40964362025260925, acc: 0.8611111044883728)
[2024-12-12 02:41:17,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:18,206][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.4913490116596222, acc: 0.9090909361839294)
[2024-12-12 02:41:18,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:18,619][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 1.1731700897216797, acc: 0.654411792755127)
[2024-12-12 02:41:18,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:19,015][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.8902575969696045, acc: 0.7539682388305664)
[2024-12-12 02:41:19,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:19,439][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 1.5301684141159058, acc: 0.6000000238418579)
[2024-12-12 02:41:19,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:19,836][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.9399451613426208, acc: 0.7244898080825806)
[2024-12-12 02:41:19,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:20,193][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 1.1591511964797974, acc: 0.6865671873092651)
[2024-12-12 02:41:20,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:20,596][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 1.4327936172485352, acc: 0.6094890236854553)
[2024-12-12 02:41:20,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:20,969][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.0175153911113739, acc: 1.0)
[2024-12-12 02:41:21,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:21,306][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.06015349552035332, acc: 1.0)
[2024-12-12 02:41:21,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:21,684][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.1491381824016571, acc: 0.9090909361839294)
[2024-12-12 02:41:21,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:22,082][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.032353632152080536, acc: 1.0)
[2024-12-12 02:41:22,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:22,465][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.3116515874862671, acc: 0.8846153616905212)
[2024-12-12 02:41:22,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:22,828][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.7605201005935669, acc: 0.807692289352417)
[2024-12-12 02:41:22,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:23,186][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.19678696990013123, acc: 0.96875)
[2024-12-12 02:41:23,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:23,510][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.4338030517101288, acc: 0.8695651888847351)
[2024-12-12 02:41:23,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:23,848][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.32047900557518005, acc: 0.8399999737739563)
[2024-12-12 02:41:23,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:24,228][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.22785578668117523, acc: 0.95652174949646)
[2024-12-12 02:41:24,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:24,700][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.8015483021736145, acc: 0.7599999904632568)
[2024-12-12 02:41:24,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:25,119][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.9045342803001404, acc: 0.7475728392601013)
[2024-12-12 02:41:25,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:26,239][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.8927239179611206, acc: 0.7718446850776672)
[2024-12-12 02:41:26,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:27,087][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 1.1979153156280518, acc: 0.6881720423698425)
[2024-12-12 02:41:27,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:27,884][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.8385195136070251, acc: 0.7586206793785095)
[2024-12-12 02:41:28,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:28,623][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.8008706569671631, acc: 0.7789473533630371)
[2024-12-12 02:41:28,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:29,607][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 1.3576637506484985, acc: 0.594059407711029)
[2024-12-12 02:41:29,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:29,994][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.7852476835250854, acc: 0.7903226017951965)
[2024-12-12 02:41:30,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:30,385][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.6336439847946167, acc: 0.782608687877655)
[2024-12-12 02:41:30,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:30,745][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 1.1356059312820435, acc: 0.6470588445663452)
[2024-12-12 02:41:30,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:31,096][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 1.1279569864273071, acc: 0.7019230723381042)
[2024-12-12 02:41:31,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:31,467][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 1.2053890228271484, acc: 0.6861313581466675)
[2024-12-12 02:41:31,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:31,772][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.979845404624939, acc: 0.6865671873092651)
[2024-12-12 02:41:31,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:32,155][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.18654069304466248, acc: 0.949999988079071)
[2024-12-12 02:41:32,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:32,491][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.03607622906565666, acc: 1.0)
[2024-12-12 02:41:32,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:32,810][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.017649304121732712, acc: 1.0)
[2024-12-12 02:41:32,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:33,115][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.03595433011651039, acc: 1.0)
[2024-12-12 02:41:33,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:33,473][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.31842461228370667, acc: 0.8620689511299133)
[2024-12-12 02:41:33,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:33,847][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.11686325818300247, acc: 0.930232584476471)
[2024-12-12 02:41:33,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:34,246][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.2834697663784027, acc: 0.9200000166893005)
[2024-12-12 02:41:34,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:34,594][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.011728347279131413, acc: 1.0)
[2024-12-12 02:41:34,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:34,916][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.03473583236336708, acc: 1.0)
[2024-12-12 02:41:35,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:35,290][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.12991587817668915, acc: 0.9285714030265808)
[2024-12-12 02:41:35,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:35,649][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.240566685795784, acc: 0.9076923131942749)
[2024-12-12 02:41:35,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:36,044][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.40419796109199524, acc: 0.8947368264198303)
[2024-12-12 02:41:36,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:36,430][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.6947420835494995, acc: 0.8070175647735596)
[2024-12-12 02:41:36,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:36,777][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.36920100450515747, acc: 0.8974359035491943)
[2024-12-12 02:41:36,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:37,174][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.319643497467041, acc: 0.8979591727256775)
[2024-12-12 02:41:37,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:37,568][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.13150663673877716, acc: 0.9545454382896423)
[2024-12-12 02:41:37,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:37,959][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.3913881182670593, acc: 0.920634925365448)
[2024-12-12 02:41:38,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:38,309][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.4855358302593231, acc: 0.8617886304855347)
[2024-12-12 02:41:38,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:38,725][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.1761380136013031, acc: 0.9516128897666931)
[2024-12-12 02:41:38,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:39,560][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.859614372253418, acc: 0.730038046836853)
[2024-12-12 02:41:39,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:39,884][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.256835013628006, acc: 0.9200000166893005)
[2024-12-12 02:41:39,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:40,281][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.24930578470230103, acc: 0.9230769276618958)
[2024-12-12 02:41:40,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:40,691][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.07465099543333054, acc: 1.0)
[2024-12-12 02:41:40,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:41,076][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.11751984059810638, acc: 0.9473684430122375)
[2024-12-12 02:41:41,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:41,491][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 1.1190330982208252, acc: 0.6871165633201599)
[2024-12-12 02:41:41,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:41,919][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 1.1538214683532715, acc: 0.6736111044883728)
[2024-12-12 02:41:42,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:42,359][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 1.1235278844833374, acc: 0.6583333611488342)
[2024-12-12 02:41:42,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:42,791][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 1.1009011268615723, acc: 0.6904761791229248)
[2024-12-12 02:41:42,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:43,150][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.9195583462715149, acc: 0.7333333492279053)
[2024-12-12 02:41:43,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:43,587][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.9068777561187744, acc: 0.7279411554336548)
[2024-12-12 02:41:43,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:43,950][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.0715242475271225, acc: 1.0)
[2024-12-12 02:41:44,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:44,285][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.36537572741508484, acc: 0.8695651888847351)
[2024-12-12 02:41:44,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:44,594][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.4122636020183563, acc: 0.84375)
[2024-12-12 02:41:44,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:44,908][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.36324217915534973, acc: 0.8695651888847351)
[2024-12-12 02:41:45,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:45,254][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.22534385323524475, acc: 0.9142857193946838)
[2024-12-12 02:41:46,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:46,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:46,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:47,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:47,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:47,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:48,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:48,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:48,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:49,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:49,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:50,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:50,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:50,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:51,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:51,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:51,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:52,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:52,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:53,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:53,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:53,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:54,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:54,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:54,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:55,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:55,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:55,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:56,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:56,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:56,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:57,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:57,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:57,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:58,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:58,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:59,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:59,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:59,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:00,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:00,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:00,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:01,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:01,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:01,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:02,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:02,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:03,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:03,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:03,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:04,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:04,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:04,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:05,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:05,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:06,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:06,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:06,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:07,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:07,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:08,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:08,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:08,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:09,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:09,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:09,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:10,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:10,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:11,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:11,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:11,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:12,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:12,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:12,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:13,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:13,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:13,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:14,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:14,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:14,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:15,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:15,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:15,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:16,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:16,881][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2630, device='cuda:0') eval_epoch_loss=tensor(0.8167, device='cuda:0') eval_epoch_acc=tensor(0.7866, device='cuda:0')
[2024-12-12 02:42:16,882][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:42:16,882][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:42:17,160][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_5_step_135_loss_0.8166968822479248/model.pt
[2024-12-12 02:42:17,168][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:42:17,169][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.7866180539131165
[2024-12-12 02:42:17,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:17,568][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.08251167088747025, acc: 1.0)
[2024-12-12 02:42:17,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:17,920][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.37754595279693604, acc: 0.9047619104385376)
[2024-12-12 02:42:18,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:18,249][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.6134361624717712, acc: 0.800000011920929)
[2024-12-12 02:42:18,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:18,600][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.20698797702789307, acc: 0.9130434989929199)
[2024-12-12 02:42:18,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:18,947][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.27825045585632324, acc: 0.9047619104385376)
[2024-12-12 02:42:19,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:19,320][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.12067585438489914, acc: 1.0)
[2024-12-12 02:42:19,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:19,706][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.24886760115623474, acc: 0.9354838728904724)
[2024-12-12 02:42:19,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:20,099][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.20231784880161285, acc: 0.9459459185600281)
[2024-12-12 02:42:20,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:20,659][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.8960410952568054, acc: 0.7280701994895935)
[2024-12-12 02:42:20,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:21,056][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.8637793064117432, acc: 0.746268630027771)
[2024-12-12 02:42:21,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:21,446][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.7873224020004272, acc: 0.795918345451355)
[2024-12-12 02:42:21,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:21,883][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 1.037111759185791, acc: 0.6595744490623474)
[2024-12-12 02:42:21,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:22,215][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.5788319706916809, acc: 0.7857142686843872)
[2024-12-12 02:42:22,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:22,572][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.31574034690856934, acc: 0.8928571343421936)
[2024-12-12 02:42:22,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:22,959][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.19876836240291595, acc: 0.95652174949646)
[2024-12-12 02:42:23,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:23,326][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.2602856457233429, acc: 0.931034505367279)
[2024-12-12 02:42:23,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:23,696][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.9495609402656555, acc: 0.739130437374115)
[2024-12-12 02:42:23,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:24,093][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.6979479789733887, acc: 0.8135592937469482)
[2024-12-12 02:42:24,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:24,429][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.643703043460846, acc: 0.8421052694320679)
[2024-12-12 02:42:24,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:24,755][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.6316723823547363, acc: 0.7702702879905701)
[2024-12-12 02:42:24,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:25,109][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.18036743998527527, acc: 0.9642857313156128)
[2024-12-12 02:42:25,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:25,475][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.34086158871650696, acc: 0.9130434989929199)
[2024-12-12 02:42:25,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:25,807][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 0.8479605317115784, acc: 0.7894737124443054)
[2024-12-12 02:42:26,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:27,509][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.901517391204834, acc: 0.7432432174682617)
[2024-12-12 02:42:27,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:27,872][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 1.0609383583068848, acc: 0.7407407164573669)
[2024-12-12 02:42:27,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:28,239][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 6.459369659423828, acc: 0.06976744532585144)
[2024-12-12 02:42:28,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:28,830][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 6.381202697753906, acc: 0.12941177189350128)
[2024-12-12 02:42:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:29,383][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 4.074254989624023, acc: 0.2584269642829895)
[2024-12-12 02:42:29,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:29,765][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 1.7674517631530762, acc: 0.6363636255264282)
[2024-12-12 02:42:29,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:30,135][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.2191990613937378, acc: 0.9047619104385376)
[2024-12-12 02:42:30,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:30,514][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.6832319498062134, acc: 0.7931034564971924)
[2024-12-12 02:42:30,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:30,907][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.5007866024971008, acc: 0.8163265585899353)
[2024-12-12 02:42:31,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:31,256][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.6565916538238525, acc: 0.8399999737739563)
[2024-12-12 02:42:31,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:31,657][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 1.2453160285949707, acc: 0.6527777910232544)
[2024-12-12 02:42:31,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:32,104][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 1.500191330909729, acc: 0.6372548937797546)
[2024-12-12 02:42:32,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:33,130][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 2.081186056137085, acc: 0.48630136251449585)
[2024-12-12 02:42:33,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:33,470][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.13181336224079132, acc: 0.9583333134651184)
[2024-12-12 02:42:33,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:33,803][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.4631458818912506, acc: 0.8888888955116272)
[2024-12-12 02:42:33,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:34,194][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.307291716337204, acc: 0.8214285969734192)
[2024-12-12 02:42:34,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:34,732][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 1.6964514255523682, acc: 0.5840708017349243)
[2024-12-12 02:42:34,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:35,096][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.7879635691642761, acc: 0.7681159377098083)
[2024-12-12 02:42:35,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:35,479][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.46101778745651245, acc: 0.8636363744735718)
[2024-12-12 02:42:35,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:36,382][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 1.7409578561782837, acc: 0.5343511700630188)
[2024-12-12 02:42:36,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:37,046][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 1.3924484252929688, acc: 0.6296296119689941)
[2024-12-12 02:42:37,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:37,381][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.44954463839530945, acc: 0.8032786846160889)
[2024-12-12 02:42:37,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:37,728][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.02978864312171936, acc: 1.0)
[2024-12-12 02:42:37,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:38,088][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.1346145123243332, acc: 0.9200000166893005)
[2024-12-12 02:42:38,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:38,383][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.05081188306212425, acc: 1.0)
[2024-12-12 02:42:38,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:38,774][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.3047810196876526, acc: 0.8902438879013062)
[2024-12-12 02:42:38,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:39,235][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.6800397038459778, acc: 0.8308157324790955)
[2024-12-12 02:42:39,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:39,663][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.9021763801574707, acc: 0.7348703145980835)
[2024-12-12 02:42:39,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:40,158][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.7250694036483765, acc: 0.7906249761581421)
[2024-12-12 02:42:40,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:40,692][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 1.036189079284668, acc: 0.7035647034645081)
[2024-12-12 02:42:40,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:41,138][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.7429087162017822, acc: 0.77224200963974)
[2024-12-12 02:42:41,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:41,524][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.08006611466407776, acc: 1.0)
[2024-12-12 02:42:41,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:42,088][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.9405105710029602, acc: 0.7325581312179565)
[2024-12-12 02:42:42,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:42,881][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 1.5414810180664062, acc: 0.6269841194152832)
[2024-12-12 02:42:43,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:43,790][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 1.2062172889709473, acc: 0.6287878751754761)
[2024-12-12 02:42:43,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:44,530][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.6513081789016724, acc: 0.8352941274642944)
[2024-12-12 02:42:44,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:45,601][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.9655901193618774, acc: 0.7037037014961243)
[2024-12-12 02:42:45,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:46,547][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.47002100944519043, acc: 0.8709677457809448)
[2024-12-12 02:42:46,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:46,895][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.0477629229426384, acc: 1.0)
[2024-12-12 02:42:47,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:47,261][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.46568912267684937, acc: 0.824999988079071)
[2024-12-12 02:42:47,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:47,657][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.6840607523918152, acc: 0.8088235259056091)
[2024-12-12 02:42:47,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:48,064][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.9622548222541809, acc: 0.75)
[2024-12-12 02:42:48,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:48,445][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.7398895621299744, acc: 0.805084764957428)
[2024-12-12 02:42:48,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:48,784][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.9551116824150085, acc: 0.7388059496879578)
[2024-12-12 02:42:48,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:49,143][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.8534793257713318, acc: 0.7475728392601013)
[2024-12-12 02:42:49,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:49,529][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.5912015438079834, acc: 0.8253968358039856)
[2024-12-12 02:42:49,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:49,864][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.26219213008880615, acc: 0.9560439586639404)
[2024-12-12 02:42:49,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:50,249][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.48738962411880493, acc: 0.8609865307807922)
[2024-12-12 02:42:50,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:50,633][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.6253041625022888, acc: 0.8149606585502625)
[2024-12-12 02:42:50,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:51,021][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.5662735104560852, acc: 0.8534482717514038)
[2024-12-12 02:42:51,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:51,450][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.6453598737716675, acc: 0.8333333134651184)
[2024-12-12 02:42:51,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:51,813][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.5003234148025513, acc: 0.8326848149299622)
[2024-12-12 02:42:51,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:52,181][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.4310065507888794, acc: 0.8369565010070801)
[2024-12-12 02:42:52,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:52,521][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.02845519781112671, acc: 1.0)
[2024-12-12 02:42:52,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:52,886][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.11976233869791031, acc: 0.9285714030265808)
[2024-12-12 02:42:52,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:53,232][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.18147557973861694, acc: 0.957446813583374)
[2024-12-12 02:42:53,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:53,904][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.2202010154724121, acc: 0.9461538195610046)
[2024-12-12 02:42:53,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:54,291][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.1649346798658371, acc: 0.9459459185600281)
[2024-12-12 02:42:54,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:54,694][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.1621946096420288, acc: 0.9418604373931885)
[2024-12-12 02:42:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:55,228][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.26674684882164, acc: 0.9279279112815857)
[2024-12-12 02:42:55,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:55,625][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.11485538631677628, acc: 0.9888888597488403)
[2024-12-12 02:42:55,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:55,971][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.08313014358282089, acc: 0.9696969985961914)
[2024-12-12 02:42:56,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:56,314][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.04132556915283203, acc: 1.0)
[2024-12-12 02:42:56,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:56,639][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.028624199330806732, acc: 1.0)
[2024-12-12 02:42:56,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:56,981][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.4690456986427307, acc: 0.8269230723381042)
[2024-12-12 02:42:57,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:57,747][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.4028197228908539, acc: 0.8804348111152649)
[2024-12-12 02:42:57,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:58,340][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.6342629790306091, acc: 0.8181818127632141)
[2024-12-12 02:42:58,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:58,779][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.673302412033081, acc: 0.7659574747085571)
[2024-12-12 02:42:58,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:59,124][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.44378194212913513, acc: 0.849056601524353)
[2024-12-12 02:42:59,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:59,537][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.3335472643375397, acc: 0.8999999761581421)
[2024-12-12 02:42:59,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:59,904][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.17362311482429504, acc: 0.9534883499145508)
[2024-12-12 02:42:59,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:00,239][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.5920001268386841, acc: 0.8333333134651184)
[2024-12-12 02:43:00,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:00,663][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 1.8936028480529785, acc: 0.4842105209827423)
[2024-12-12 02:43:00,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:01,057][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 1.4310451745986938, acc: 0.6000000238418579)
[2024-12-12 02:43:01,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:01,467][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 1.560068964958191, acc: 0.5777778029441833)
[2024-12-12 02:43:01,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:01,952][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 2.055056571960449, acc: 0.49082568287849426)
[2024-12-12 02:43:02,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:02,408][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 1.5825995206832886, acc: 0.5615384578704834)
[2024-12-12 02:43:02,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:02,760][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.11865069717168808, acc: 0.9473684430122375)
[2024-12-12 02:43:02,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:03,142][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.1564527153968811, acc: 0.9583333134651184)
[2024-12-12 02:43:03,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:03,530][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.3336614668369293, acc: 0.9090909361839294)
[2024-12-12 02:43:03,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:03,885][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.5623903274536133, acc: 0.8888888955116272)
[2024-12-12 02:43:04,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:04,221][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.27633559703826904, acc: 0.9142857193946838)
[2024-12-12 02:43:04,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:04,556][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.7700442671775818, acc: 0.75)
[2024-12-12 02:43:04,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:04,918][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.47718217968940735, acc: 0.8409090638160706)
[2024-12-12 02:43:05,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:05,494][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 1.3239998817443848, acc: 0.6451612710952759)
[2024-12-12 02:43:05,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:06,023][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.7634589672088623, acc: 0.7954545617103577)
[2024-12-12 02:43:06,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:06,339][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.027423478662967682, acc: 1.0)
[2024-12-12 02:43:06,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:06,688][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.16243857145309448, acc: 0.9615384340286255)
[2024-12-12 02:43:06,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:07,044][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.05173297971487045, acc: 1.0)
[2024-12-12 02:43:07,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:07,417][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.016324348747730255, acc: 1.0)
[2024-12-12 02:43:07,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:07,765][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.30939945578575134, acc: 0.9459459185600281)
[2024-12-12 02:43:07,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:08,121][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.4007199704647064, acc: 0.8648648858070374)
[2024-12-12 02:43:08,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:08,515][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.10688361525535583, acc: 0.9729729890823364)
[2024-12-12 02:43:08,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:08,904][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.24938496947288513, acc: 0.9117646813392639)
[2024-12-12 02:43:09,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:09,284][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.09815269708633423, acc: 0.9512194991111755)
[2024-12-12 02:43:09,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:09,637][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.03055477887392044, acc: 1.0)
[2024-12-12 02:43:09,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:09,946][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.009940731339156628, acc: 1.0)
[2024-12-12 02:43:10,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:10,299][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.03789665177464485, acc: 1.0)
[2024-12-12 02:43:10,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:10,712][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.09059533476829529, acc: 0.9649122953414917)
[2024-12-12 02:43:10,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:11,091][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.10081197321414948, acc: 0.9571428298950195)
[2024-12-12 02:43:11,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:11,501][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.2321905791759491, acc: 0.9605262875556946)
[2024-12-12 02:43:11,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:12,061][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.4782335162162781, acc: 0.8962264060974121)
[2024-12-12 02:43:12,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:12,638][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.38931819796562195, acc: 0.8833333253860474)
[2024-12-12 02:43:12,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:12,908][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.08055238425731659, acc: 0.9722222089767456)
[2024-12-12 02:43:12,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:13,262][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.12568794190883636, acc: 0.9677419066429138)
[2024-12-12 02:43:13,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:13,642][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.8585498332977295, acc: 0.7733333110809326)
[2024-12-12 02:43:13,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:13,985][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.5922157168388367, acc: 0.7708333134651184)
[2024-12-12 02:43:14,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:14,835][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 1.1869326829910278, acc: 0.6880000233650208)
[2024-12-12 02:43:14,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:15,179][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.8791220188140869, acc: 0.7303370833396912)
[2024-12-12 02:43:15,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:15,611][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.7771267294883728, acc: 0.7972972989082336)
[2024-12-12 02:43:15,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:16,073][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.3852195143699646, acc: 0.8793103694915771)
[2024-12-12 02:43:16,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:16,405][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.19210782647132874, acc: 0.9545454382896423)
[2024-12-12 02:43:16,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:16,797][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.020555613562464714, acc: 1.0)
[2024-12-12 02:43:16,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:17,152][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.014218912459909916, acc: 1.0)
[2024-12-12 02:43:17,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:17,495][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.16746269166469574, acc: 0.9666666388511658)
[2024-12-12 02:43:17,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:17,865][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.6472493410110474, acc: 0.8666666746139526)
[2024-12-12 02:43:17,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:18,168][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.23451974987983704, acc: 0.9375)
[2024-12-12 02:43:18,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:18,559][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.2221483439207077, acc: 0.9333333373069763)
[2024-12-12 02:43:18,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:18,904][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.3569381833076477, acc: 0.931034505367279)
[2024-12-12 02:43:18,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:19,218][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.049029625952243805, acc: 1.0)
[2024-12-12 02:43:19,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:20,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:20,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:20,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:21,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:21,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:21,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:22,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:22,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:23,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:23,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:24,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:24,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:24,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:25,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:25,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:25,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:25,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:26,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:26,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:26,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:27,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:27,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:27,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:28,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:28,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:28,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:29,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:29,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:29,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:30,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:30,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:30,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:31,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:31,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:31,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:32,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:32,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:32,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:33,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:33,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:34,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:34,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:34,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:35,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:35,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:35,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:36,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:36,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:36,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:37,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:37,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:37,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:38,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:38,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:38,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:39,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:39,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:39,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:40,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:40,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:41,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:41,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:41,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:42,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:42,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:42,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:43,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:43,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:44,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:44,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:44,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:45,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:45,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:45,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:46,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:46,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:46,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:47,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:47,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:47,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:48,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:48,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:48,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:49,419][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3908, device='cuda:0') eval_epoch_loss=tensor(0.8716, device='cuda:0') eval_epoch_acc=tensor(0.7901, device='cuda:0')
[2024-12-12 02:43:49,420][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:43:49,420][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:43:49,673][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_5_step_278_loss_0.8716414570808411/model.pt
[2024-12-12 02:43:49,682][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:43:49,683][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.7901425957679749
[2024-12-12 02:43:49,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:50,119][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.2994692325592041, acc: 0.8723404407501221)
[2024-12-12 02:43:50,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:50,490][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.27536165714263916, acc: 0.875)
[2024-12-12 02:43:50,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:50,836][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.05230436101555824, acc: 0.9772727489471436)
[2024-12-12 02:43:50,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:51,253][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.9229626655578613, acc: 0.7710843086242676)
[2024-12-12 02:43:51,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:51,569][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.8531575798988342, acc: 0.7685185074806213)
[2024-12-12 02:43:51,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:51,879][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.14985805749893188, acc: 0.9473684430122375)
[2024-12-12 02:43:51,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:52,230][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.23857815563678741, acc: 0.9117646813392639)
[2024-12-12 02:43:52,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:52,647][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.1502726823091507, acc: 0.925000011920929)
[2024-12-12 02:43:52,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:53,046][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.48352333903312683, acc: 0.859375)
[2024-12-12 02:43:53,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:53,422][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.6588855981826782, acc: 0.8560000061988831)
[2024-12-12 02:43:53,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:53,834][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.414053738117218, acc: 0.8901098966598511)
[2024-12-12 02:43:53,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:54,188][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.5389621257781982, acc: 0.8447204828262329)
[2024-12-12 02:43:54,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:54,531][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.6790243983268738, acc: 0.7938144207000732)
[2024-12-12 02:43:54,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:54,870][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.019415197893977165, acc: 1.0)
[2024-12-12 02:43:54,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:55,240][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.4413439929485321, acc: 0.8095238208770752)
[2024-12-12 02:43:55,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:55,638][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.3106969892978668, acc: 0.8965517282485962)
[2024-12-12 02:43:55,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:56,119][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.3380587697029114, acc: 0.9090909361839294)
[2024-12-12 02:43:56,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:56,673][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.7284524440765381, acc: 0.7835051417350769)
[2024-12-12 02:43:56,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:57,038][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.5728327035903931, acc: 0.7931034564971924)
[2024-12-12 02:43:57,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:57,425][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.24358484148979187, acc: 0.8888888955116272)
[2024-12-12 02:43:57,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:57,817][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.31406643986701965, acc: 0.8947368264198303)
[2024-12-12 02:43:57,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:58,169][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.182895690202713, acc: 0.9285714030265808)
[2024-12-12 02:43:58,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:58,533][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.17128057777881622, acc: 0.9375)
[2024-12-12 02:43:58,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:58,852][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.3317777216434479, acc: 0.9245283007621765)
[2024-12-12 02:43:58,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:59,185][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.059095144271850586, acc: 0.9811320900917053)
[2024-12-12 02:43:59,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:59,545][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.05157463252544403, acc: 1.0)
[2024-12-12 02:43:59,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:59,883][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.03579065948724747, acc: 1.0)
[2024-12-12 02:44:00,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:00,252][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.26930123567581177, acc: 0.9016393423080444)
[2024-12-12 02:44:00,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:00,575][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.03600398451089859, acc: 1.0)
[2024-12-12 02:44:00,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:00,920][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.006081221159547567, acc: 1.0)
[2024-12-12 02:44:01,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:01,303][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.19919973611831665, acc: 0.9420289993286133)
[2024-12-12 02:44:01,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:01,732][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.2077186107635498, acc: 0.9444444179534912)
[2024-12-12 02:44:01,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:02,125][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.1709262728691101, acc: 0.9518072009086609)
[2024-12-12 02:44:02,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:02,444][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.33373770117759705, acc: 0.8846153616905212)
[2024-12-12 02:44:02,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:02,818][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.21306291222572327, acc: 0.9387755393981934)
[2024-12-12 02:44:02,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:03,150][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.11205565184354782, acc: 0.9583333134651184)
[2024-12-12 02:44:03,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:03,490][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.013053208589553833, acc: 1.0)
[2024-12-12 02:44:03,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:03,864][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.28523701429367065, acc: 0.8709677457809448)
[2024-12-12 02:44:03,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:04,142][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.3412708044052124, acc: 0.9032257795333862)
[2024-12-12 02:44:04,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:04,503][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.1345178633928299, acc: 0.9402984976768494)
[2024-12-12 02:44:04,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:04,855][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.20612232387065887, acc: 0.9519230723381042)
[2024-12-12 02:44:04,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:05,250][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.16738197207450867, acc: 0.9111111164093018)
[2024-12-12 02:44:05,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:05,616][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.09314168244600296, acc: 0.9677419066429138)
[2024-12-12 02:44:05,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:05,931][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.02888929843902588, acc: 0.9800000190734863)
[2024-12-12 02:44:06,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:06,283][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.3518402576446533, acc: 0.8518518805503845)
[2024-12-12 02:44:06,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:06,646][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.8215929269790649, acc: 0.7428571581840515)
[2024-12-12 02:44:06,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:07,006][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.7017995119094849, acc: 0.7948718070983887)
[2024-12-12 02:44:07,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:07,389][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 1.209870457649231, acc: 0.6097561120986938)
[2024-12-12 02:44:07,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:07,751][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.8575457334518433, acc: 0.7368420958518982)
[2024-12-12 02:44:07,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:08,149][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.06929489225149155, acc: 1.0)
[2024-12-12 02:44:08,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:08,538][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.03811782971024513, acc: 1.0)
[2024-12-12 02:44:08,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:08,946][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.07415420562028885, acc: 1.0)
[2024-12-12 02:44:09,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:09,304][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.007841970771551132, acc: 1.0)
[2024-12-12 02:44:09,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:09,686][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.2787844240665436, acc: 0.8870967626571655)
[2024-12-12 02:44:09,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:10,060][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.09303939342498779, acc: 0.9824561476707458)
[2024-12-12 02:44:10,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:10,418][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.5291785001754761, acc: 0.84375)
[2024-12-12 02:44:10,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:10,794][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.06382814049720764, acc: 1.0)
[2024-12-12 02:44:10,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:11,193][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.16362471878528595, acc: 0.9473684430122375)
[2024-12-12 02:44:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:11,577][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.8259820342063904, acc: 0.7799999713897705)
[2024-12-12 02:44:11,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:11,937][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 1.3043209314346313, acc: 0.6666666865348816)
[2024-12-12 02:44:12,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:12,304][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 1.2570736408233643, acc: 0.6702127456665039)
[2024-12-12 02:44:12,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:12,611][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 1.145442008972168, acc: 0.6385542154312134)
[2024-12-12 02:44:12,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:12,899][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.007137608248740435, acc: 1.0)
[2024-12-12 02:44:12,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:13,192][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.43888428807258606, acc: 0.8974359035491943)
[2024-12-12 02:44:13,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:13,554][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.24522002041339874, acc: 0.9036144614219666)
[2024-12-12 02:44:13,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:13,899][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.26158323884010315, acc: 0.9245283007621765)
[2024-12-12 02:44:13,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:14,194][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.12193411588668823, acc: 0.949367105960846)
[2024-12-12 02:44:14,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:14,514][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.060274410992860794, acc: 1.0)
[2024-12-12 02:44:14,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:14,852][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.2471485733985901, acc: 0.9402984976768494)
[2024-12-12 02:44:14,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:15,217][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.009910596534609795, acc: 1.0)
[2024-12-12 02:44:15,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:15,590][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.12325841933488846, acc: 0.9200000166893005)
[2024-12-12 02:44:15,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:16,010][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.5285094976425171, acc: 0.8611111044883728)
[2024-12-12 02:44:16,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:16,380][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.41009846329689026, acc: 0.8372092843055725)
[2024-12-12 02:44:16,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:16,719][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.11244865506887436, acc: 0.9743589758872986)
[2024-12-12 02:44:16,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:17,083][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.7283769249916077, acc: 0.7333333492279053)
[2024-12-12 02:44:17,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:17,351][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.2506299316883087, acc: 0.95652174949646)
[2024-12-12 02:44:17,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:17,650][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.3199794590473175, acc: 0.8461538553237915)
[2024-12-12 02:44:17,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:18,008][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.8203094005584717, acc: 0.791208803653717)
[2024-12-12 02:44:18,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:18,511][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.633840024471283, acc: 0.7739130258560181)
[2024-12-12 02:44:18,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:18,856][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.4052973687648773, acc: 0.8152173757553101)
[2024-12-12 02:44:18,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:19,241][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.48368775844573975, acc: 0.8367347121238708)
[2024-12-12 02:44:19,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:19,610][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.04323076084256172, acc: 0.9583333134651184)
[2024-12-12 02:44:19,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:19,945][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.05652175843715668, acc: 1.0)
[2024-12-12 02:44:20,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:20,256][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.3905223608016968, acc: 0.8536585569381714)
[2024-12-12 02:44:20,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:20,573][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.33253014087677, acc: 0.9333333373069763)
[2024-12-12 02:44:20,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:20,949][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.1494358777999878, acc: 0.9473684430122375)
[2024-12-12 02:44:21,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:21,332][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.10638655722141266, acc: 0.9756097793579102)
[2024-12-12 02:44:21,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:21,687][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.08219923079013824, acc: 0.9696969985961914)
[2024-12-12 02:44:21,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:22,055][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.005756707862019539, acc: 1.0)
[2024-12-12 02:44:22,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:22,390][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.09881459176540375, acc: 0.95652174949646)
[2024-12-12 02:44:22,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:22,731][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.12700028717517853, acc: 0.9285714030265808)
[2024-12-12 02:44:22,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:23,104][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.3032415807247162, acc: 0.9375)
[2024-12-12 02:44:23,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:23,711][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.6207495331764221, acc: 0.8121212124824524)
[2024-12-12 02:44:24,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:24,606][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.3025989830493927, acc: 0.8962264060974121)
[2024-12-12 02:44:24,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:24,947][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.33712339401245117, acc: 0.8888888955116272)
[2024-12-12 02:44:25,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:25,271][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.09623507410287857, acc: 0.9821428656578064)
[2024-12-12 02:44:25,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:25,598][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.04981531575322151, acc: 1.0)
[2024-12-12 02:44:25,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:25,986][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.02430499717593193, acc: 1.0)
[2024-12-12 02:44:26,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:26,384][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.12506048381328583, acc: 0.95652174949646)
[2024-12-12 02:44:26,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:26,733][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.1738802194595337, acc: 0.9375)
[2024-12-12 02:44:26,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:27,060][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.10049530863761902, acc: 0.9473684430122375)
[2024-12-12 02:44:27,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:27,630][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.3106105625629425, acc: 0.9341317415237427)
[2024-12-12 02:44:27,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:28,058][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.32520854473114014, acc: 0.9172932505607605)
[2024-12-12 02:44:28,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:29,332][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.7007784247398376, acc: 0.8235294222831726)
[2024-12-12 02:44:29,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:29,897][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.09669717401266098, acc: 0.9819819927215576)
[2024-12-12 02:44:29,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:30,216][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.14519253373146057, acc: 0.9285714030265808)
[2024-12-12 02:44:30,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:30,578][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.06679662317037582, acc: 1.0)
[2024-12-12 02:44:30,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:30,928][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.03556737303733826, acc: 1.0)
[2024-12-12 02:44:31,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:31,235][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.03980160132050514, acc: 0.9722222089767456)
[2024-12-12 02:44:31,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:31,579][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.015917787328362465, acc: 1.0)
[2024-12-12 02:44:31,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:31,939][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.0060759661719202995, acc: 1.0)
[2024-12-12 02:44:32,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:32,246][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.041643667966127396, acc: 1.0)
[2024-12-12 02:44:32,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:32,575][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.2525262236595154, acc: 0.9047619104385376)
[2024-12-12 02:44:32,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:32,959][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.7530478239059448, acc: 0.8148148059844971)
[2024-12-12 02:44:33,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:33,307][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.9112145900726318, acc: 0.7475728392601013)
[2024-12-12 02:44:33,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:33,820][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.8845445513725281, acc: 0.7647058963775635)
[2024-12-12 02:44:33,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:34,208][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.8817974925041199, acc: 0.753333330154419)
[2024-12-12 02:44:34,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:34,583][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.6647612452507019, acc: 0.8194444179534912)
[2024-12-12 02:44:34,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:34,934][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.23847344517707825, acc: 0.9069767594337463)
[2024-12-12 02:44:35,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:35,264][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.06073572114109993, acc: 1.0)
[2024-12-12 02:44:35,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:35,635][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.12376335263252258, acc: 0.9767441749572754)
[2024-12-12 02:44:35,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:36,025][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.04036189988255501, acc: 0.9599999785423279)
[2024-12-12 02:44:36,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:36,553][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.2396184206008911, acc: 0.9558823704719543)
[2024-12-12 02:44:36,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:36,871][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.5129871368408203, acc: 0.8799999952316284)
[2024-12-12 02:44:36,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:37,181][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.16048991680145264, acc: 0.9696969985961914)
[2024-12-12 02:44:37,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:37,481][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.19872185587882996, acc: 0.9090909361839294)
[2024-12-12 02:44:37,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:37,828][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.22423680126667023, acc: 0.9354838728904724)
[2024-12-12 02:44:37,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:38,200][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.04245317727327347, acc: 1.0)
[2024-12-12 02:44:38,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:38,556][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.05999235808849335, acc: 0.9599999785423279)
[2024-12-12 02:44:38,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:38,915][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.04017395153641701, acc: 0.9722222089767456)
[2024-12-12 02:44:39,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:39,292][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.049571819603443146, acc: 1.0)
[2024-12-12 02:44:39,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:39,649][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.025755178183317184, acc: 1.0)
[2024-12-12 02:44:39,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:40,006][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.1861579567193985, acc: 0.9137930870056152)
[2024-12-12 02:44:40,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:40,363][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.04772711172699928, acc: 1.0)
[2024-12-12 02:44:40,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:40,672][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.044278014451265335, acc: 0.9666666388511658)
[2024-12-12 02:44:40,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:41,001][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.06858719885349274, acc: 0.9696969985961914)
[2024-12-12 02:44:41,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:41,334][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.009727117605507374, acc: 1.0)
[2024-12-12 02:44:41,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:41,687][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.2403499335050583, acc: 0.9215686321258545)
[2024-12-12 02:44:41,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:42,031][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.10127965360879898, acc: 0.9615384340286255)
[2024-12-12 02:44:42,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:42,428][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.12551337480545044, acc: 0.9444444179534912)
[2024-12-12 02:44:42,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:42,803][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.20461352169513702, acc: 0.8999999761581421)
[2024-12-12 02:44:42,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:43,121][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.14189092814922333, acc: 0.949999988079071)
[2024-12-12 02:44:43,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:43,474][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.06729225069284439, acc: 0.9523809552192688)
[2024-12-12 02:44:44,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:44,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:44,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:45,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:45,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:46,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:46,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:46,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:47,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:47,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:47,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:48,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:48,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:49,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:49,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:49,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:50,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:50,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:50,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:51,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:51,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:51,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:52,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:52,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:52,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:53,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:53,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:53,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:54,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:54,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:55,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:55,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:56,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:56,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:56,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:57,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:57,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:58,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:58,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:58,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:59,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:59,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:59,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:00,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:00,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:00,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:01,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:01,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:01,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:02,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:02,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:02,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:03,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:03,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:03,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:04,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:04,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:04,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:05,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:05,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:06,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:06,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:06,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:07,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:07,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:08,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:08,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:09,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:09,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:09,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:10,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:10,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:10,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:11,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:11,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:11,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:12,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:12,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:12,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:13,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:13,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:13,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:14,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:14,879][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4637, device='cuda:0') eval_epoch_loss=tensor(0.9016, device='cuda:0') eval_epoch_acc=tensor(0.7843, device='cuda:0')
[2024-12-12 02:45:14,880][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:45:14,881][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:45:15,133][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_5_step_421_loss_0.9016447067260742/model.pt
[2024-12-12 02:45:15,137][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:45:15,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:15,542][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.11615575850009918, acc: 0.9666666388511658)
[2024-12-12 02:45:15,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:15,900][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.2261105179786682, acc: 0.9375)
[2024-12-12 02:45:15,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:16,291][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.05884242802858353, acc: 1.0)
[2024-12-12 02:45:16,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:16,651][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.06621876358985901, acc: 0.9629629850387573)
[2024-12-12 02:45:16,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:17,010][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.05288492143154144, acc: 1.0)
[2024-12-12 02:45:17,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:17,391][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 0.5401285290718079, acc: 0.95652174949646)
[2024-12-12 02:45:17,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:17,741][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.13728365302085876, acc: 0.9459459185600281)
[2024-12-12 02:45:17,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:18,077][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.03216163441538811, acc: 0.9629629850387573)
[2024-12-12 02:45:18,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:18,459][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.014608425088226795, acc: 1.0)
[2024-12-12 02:45:18,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:18,823][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.003303749952465296, acc: 1.0)
[2024-12-12 02:45:18,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:19,193][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.0018843916477635503, acc: 1.0)
[2024-12-12 02:45:19,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:19,552][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.01121519785374403, acc: 1.0)
[2024-12-12 02:45:19,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:19,912][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.10669184476137161, acc: 0.9722222089767456)
[2024-12-12 02:45:20,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:20,274][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.03596792742609978, acc: 1.0)
[2024-12-12 02:45:20,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:20,631][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.02766023762524128, acc: 1.0)
[2024-12-12 02:45:20,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:20,973][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.11766734719276428, acc: 0.9444444179534912)
[2024-12-12 02:45:21,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:21,337][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.13635636866092682, acc: 0.9545454382896423)
[2024-12-12 02:45:21,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:21,700][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.002691475674510002, acc: 1.0)
[2024-12-12 02:45:21,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:22,062][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.15973953902721405, acc: 0.9487179517745972)
[2024-12-12 02:45:22,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:22,542][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.22588913142681122, acc: 0.939393937587738)
[2024-12-12 02:45:22,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:23,250][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.6787084341049194, acc: 0.7919999957084656)
[2024-12-12 02:45:23,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:23,680][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.8220999836921692, acc: 0.774193525314331)
[2024-12-12 02:45:23,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:24,330][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.6752902865409851, acc: 0.8557214140892029)
[2024-12-12 02:45:24,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:24,669][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.08057346940040588, acc: 0.9811320900917053)
[2024-12-12 02:45:24,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:25,121][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.03594628721475601, acc: 1.0)
[2024-12-12 02:45:25,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:25,463][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.10322742164134979, acc: 0.95652174949646)
[2024-12-12 02:45:25,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:25,754][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.04625198617577553, acc: 1.0)
[2024-12-12 02:45:25,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:26,076][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.04494468495249748, acc: 1.0)
[2024-12-12 02:45:26,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:26,451][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.20975106954574585, acc: 0.9402984976768494)
[2024-12-12 02:45:26,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:26,796][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.05684686452150345, acc: 1.0)
[2024-12-12 02:45:26,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:27,156][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.09655732661485672, acc: 0.97826087474823)
[2024-12-12 02:45:27,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:27,560][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.2331644892692566, acc: 0.9487179517745972)
[2024-12-12 02:45:27,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:27,919][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.18990635871887207, acc: 0.9078947305679321)
[2024-12-12 02:45:28,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:28,242][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.2109214961528778, acc: 0.918367326259613)
[2024-12-12 02:45:28,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:28,524][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.15468059480190277, acc: 0.939393937587738)
[2024-12-12 02:45:28,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:28,887][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.755675196647644, acc: 0.8247422575950623)
[2024-12-12 02:45:28,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:29,248][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.11544638872146606, acc: 0.9571428298950195)
[2024-12-12 02:45:29,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:29,650][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.5533567667007446, acc: 0.8372092843055725)
[2024-12-12 02:45:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:29,987][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.12016022205352783, acc: 0.9821428656578064)
[2024-12-12 02:45:30,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:30,371][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.2067076861858368, acc: 0.9382715821266174)
[2024-12-12 02:45:30,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:30,688][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.1785154640674591, acc: 0.9444444179534912)
[2024-12-12 02:45:30,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:31,021][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.04488607123494148, acc: 1.0)
[2024-12-12 02:45:31,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:31,311][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.04792909324169159, acc: 1.0)
[2024-12-12 02:45:31,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:31,673][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.10847169160842896, acc: 0.95652174949646)
[2024-12-12 02:45:31,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:32,022][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.288609117269516, acc: 0.9047619104385376)
[2024-12-12 02:45:32,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:32,344][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.5144146680831909, acc: 0.891566276550293)
[2024-12-12 02:45:32,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:32,687][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.1998741626739502, acc: 0.9369369149208069)
[2024-12-12 02:45:32,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:33,079][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.5941758155822754, acc: 0.844660222530365)
[2024-12-12 02:45:33,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:33,472][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.47219181060791016, acc: 0.8780487775802612)
[2024-12-12 02:45:33,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:33,848][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.06769362837076187, acc: 1.0)
[2024-12-12 02:45:33,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:34,227][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.2191770374774933, acc: 0.9285714030265808)
[2024-12-12 02:45:34,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:34,682][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.5229089856147766, acc: 0.8039215803146362)
[2024-12-12 02:45:34,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:35,087][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.8524549007415771, acc: 0.7379912734031677)
[2024-12-12 02:45:35,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:35,411][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.5384035706520081, acc: 0.8541666865348816)
[2024-12-12 02:45:35,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:35,766][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.38895198702812195, acc: 0.89570552110672)
[2024-12-12 02:45:35,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:36,117][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.2912623882293701, acc: 0.935251772403717)
[2024-12-12 02:45:36,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:36,508][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.719912052154541, acc: 0.7989949584007263)
[2024-12-12 02:45:36,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:36,886][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.2197968065738678, acc: 0.9444444179534912)
[2024-12-12 02:45:36,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:37,225][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.1499769687652588, acc: 0.9696969985961914)
[2024-12-12 02:45:37,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:37,533][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.2004752904176712, acc: 0.9629629850387573)
[2024-12-12 02:45:37,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:37,920][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.0314611941576004, acc: 1.0)
[2024-12-12 02:45:38,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:38,273][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.563652515411377, acc: 0.800000011920929)
[2024-12-12 02:45:38,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:38,623][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.5471580028533936, acc: 0.8448275923728943)
[2024-12-12 02:45:38,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:38,945][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.051418665796518326, acc: 0.9677419066429138)
[2024-12-12 02:45:39,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:39,306][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.1271062195301056, acc: 0.9473684430122375)
[2024-12-12 02:45:39,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:39,667][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.354074627161026, acc: 0.8518518805503845)
[2024-12-12 02:45:39,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:40,040][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.1783459186553955, acc: 0.9523809552192688)
[2024-12-12 02:45:40,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:40,370][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.0834578424692154, acc: 1.0)
[2024-12-12 02:45:40,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:40,723][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.6934475302696228, acc: 0.7846153974533081)
[2024-12-12 02:45:40,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:41,076][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.20385217666625977, acc: 0.9333333373069763)
[2024-12-12 02:45:41,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:41,404][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.1976204216480255, acc: 0.931034505367279)
[2024-12-12 02:45:41,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:41,779][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.28902700543403625, acc: 0.9215686321258545)
[2024-12-12 02:45:41,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:42,121][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.16501106321811676, acc: 0.8965517282485962)
[2024-12-12 02:45:42,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:42,462][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.19433967769145966, acc: 0.8947368264198303)
[2024-12-12 02:45:42,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:42,843][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.1930188685655594, acc: 0.9473684430122375)
[2024-12-12 02:45:42,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:43,208][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.7004679441452026, acc: 0.7857142686843872)
[2024-12-12 02:45:43,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:43,588][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.37630710005760193, acc: 0.8876404762268066)
[2024-12-12 02:45:43,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:43,885][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.6913543939590454, acc: 0.7865168452262878)
[2024-12-12 02:45:43,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:44,248][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 1.1401164531707764, acc: 0.652482271194458)
[2024-12-12 02:45:44,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:44,594][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.6655235886573792, acc: 0.8260869383811951)
[2024-12-12 02:45:44,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:44,953][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.008415666408836842, acc: 1.0)
[2024-12-12 02:45:45,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:45,284][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.03984791040420532, acc: 1.0)
[2024-12-12 02:45:45,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:45,603][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.03770547732710838, acc: 1.0)
[2024-12-12 02:45:45,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:45,964][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.12561526894569397, acc: 0.9629629850387573)
[2024-12-12 02:45:46,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:46,327][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.38269907236099243, acc: 0.8679245114326477)
[2024-12-12 02:45:46,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:46,728][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.6836678981781006, acc: 0.8620689511299133)
[2024-12-12 02:45:46,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:47,325][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 0.9346141815185547, acc: 0.7477477192878723)
[2024-12-12 02:45:47,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:47,775][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.5437894463539124, acc: 0.8873239159584045)
[2024-12-12 02:45:47,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:48,141][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.12178432941436768, acc: 0.949999988079071)
[2024-12-12 02:45:48,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:48,448][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.2675381898880005, acc: 0.9333333373069763)
[2024-12-12 02:45:48,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:48,801][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.31907886266708374, acc: 0.9615384340286255)
[2024-12-12 02:45:50,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:51,515][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 1.004348635673523, acc: 0.7571428418159485)
[2024-12-12 02:45:51,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:52,271][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.2718830108642578, acc: 0.9365079402923584)
[2024-12-12 02:45:52,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:52,650][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.344941645860672, acc: 0.8928571343421936)
[2024-12-12 02:45:52,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:53,012][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.12617254257202148, acc: 0.949999988079071)
[2024-12-12 02:45:53,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:53,703][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.6820346713066101, acc: 0.8194444179534912)
[2024-12-12 02:45:53,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:54,085][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.03126377984881401, acc: 1.0)
[2024-12-12 02:45:54,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:54,437][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.11869803816080093, acc: 0.9354838728904724)
[2024-12-12 02:45:54,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:54,758][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.22370067238807678, acc: 0.949999988079071)
[2024-12-12 02:45:54,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:55,076][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.20466363430023193, acc: 0.8888888955116272)
[2024-12-12 02:45:55,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:56,082][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.6776670813560486, acc: 0.8135592937469482)
[2024-12-12 02:45:56,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:56,444][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.2912914454936981, acc: 0.9179104566574097)
[2024-12-12 02:45:56,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:56,843][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.31197619438171387, acc: 0.9124087691307068)
[2024-12-12 02:45:57,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:57,402][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.694061279296875, acc: 0.8100000023841858)
[2024-12-12 02:45:57,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:57,763][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.09163640439510345, acc: 0.9629629850387573)
[2024-12-12 02:45:57,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:58,137][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.19826611876487732, acc: 0.9807692170143127)
[2024-12-12 02:45:58,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:58,471][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.15697258710861206, acc: 0.9047619104385376)
[2024-12-12 02:45:58,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:58,805][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 1.1275650262832642, acc: 0.7049180269241333)
[2024-12-12 02:45:58,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:59,158][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.17030251026153564, acc: 0.9830508232116699)
[2024-12-12 02:45:59,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:59,541][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.7563791275024414, acc: 0.7209302186965942)
[2024-12-12 02:45:59,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:59,880][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.5321347713470459, acc: 0.8181818127632141)
[2024-12-12 02:45:59,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:00,273][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.6284381747245789, acc: 0.849056601524353)
[2024-12-12 02:46:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:00,604][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.5059554576873779, acc: 0.7954545617103577)
[2024-12-12 02:46:00,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:00,919][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.4472336173057556, acc: 0.9200000166893005)
[2024-12-12 02:46:01,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:01,296][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.5636130571365356, acc: 0.800000011920929)
[2024-12-12 02:46:01,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:01,633][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.05377072095870972, acc: 1.0)
[2024-12-12 02:46:01,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:02,017][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.4077191650867462, acc: 0.8769230842590332)
[2024-12-12 02:46:02,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:02,396][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.3645765483379364, acc: 0.890625)
[2024-12-12 02:46:02,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:02,777][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.25037476420402527, acc: 0.96875)
[2024-12-12 02:46:02,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:03,135][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.5961663126945496, acc: 0.7878788113594055)
[2024-12-12 02:46:03,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:03,453][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.18184785544872284, acc: 0.9375)
[2024-12-12 02:46:03,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:03,827][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.034967340528964996, acc: 1.0)
[2024-12-12 02:46:03,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:04,228][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.02907670848071575, acc: 1.0)
[2024-12-12 02:46:04,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:04,594][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.14415855705738068, acc: 0.9666666388511658)
[2024-12-12 02:46:04,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:05,002][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.02891918271780014, acc: 1.0)
[2024-12-12 02:46:05,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:05,363][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.01228640042245388, acc: 1.0)
[2024-12-12 02:46:05,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:05,725][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.11171311140060425, acc: 0.9736841917037964)
[2024-12-12 02:46:05,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:06,085][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.06367120146751404, acc: 1.0)
[2024-12-12 02:46:06,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:06,433][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.017720498144626617, acc: 1.0)
[2024-12-12 02:46:06,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:06,789][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.14774684607982635, acc: 0.9090909361839294)
[2024-12-12 02:46:06,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:07,161][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.12890422344207764, acc: 0.925000011920929)
[2024-12-12 02:46:07,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:07,477][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.07781434804201126, acc: 0.9714285731315613)
[2024-12-12 02:46:07,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:07,762][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.5262467861175537, acc: 0.8686131238937378)
[2024-12-12 02:46:07,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:08,172][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.34045904874801636, acc: 0.8758620619773865)
[2024-12-12 02:46:08,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:08,582][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.37183263897895813, acc: 0.8785714507102966)
[2024-12-12 02:46:08,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:08,904][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.3880211412906647, acc: 0.9072847962379456)
[2024-12-12 02:46:09,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:09,289][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.11984094977378845, acc: 0.9572649598121643)
[2024-12-12 02:46:09,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:09,647][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.3874027132987976, acc: 0.9200000166893005)
[2024-12-12 02:46:09,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:09,990][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.1234530583024025, acc: 0.9615384340286255)
[2024-12-12 02:46:10,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:10,342][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.017429690808057785, acc: 1.0)
[2024-12-12 02:46:10,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:10,727][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.11078296601772308, acc: 0.9487179517745972)
[2024-12-12 02:46:10,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:11,068][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.32731056213378906, acc: 0.9111111164093018)
[2024-12-12 02:46:11,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:11,388][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.28559640049934387, acc: 0.9350649118423462)
[2024-12-12 02:46:12,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:12,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:12,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:13,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:13,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:13,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:14,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:14,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:15,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:15,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:16,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:16,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:16,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:17,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:17,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:18,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:18,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:18,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:19,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:19,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:19,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:20,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:20,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:21,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:21,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:21,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:22,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:22,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:22,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:23,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:23,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:23,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:24,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:24,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:24,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:25,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:25,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:26,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:26,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:26,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:27,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:27,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:27,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:28,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:28,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:28,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:29,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:29,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:29,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:30,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:30,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:30,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:31,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:31,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:32,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:32,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:32,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:33,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:33,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:33,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:34,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:34,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:35,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:35,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:35,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:36,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:36,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:37,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:37,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:37,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:38,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:38,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:38,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:39,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:39,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:39,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:39,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:40,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:40,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:41,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:41,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:41,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:42,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:42,679][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5015, device='cuda:0') eval_epoch_loss=tensor(0.9169, device='cuda:0') eval_epoch_acc=tensor(0.7807, device='cuda:0')
[2024-12-12 02:46:42,681][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:46:42,681][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:46:42,934][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_5_step_564_loss_0.9168794751167297/model.pt
[2024-12-12 02:46:42,938][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:46:43,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:43,270][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.2874948978424072, acc: 0.9375)
[2024-12-12 02:46:43,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:43,617][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.29865705966949463, acc: 0.8793103694915771)
[2024-12-12 02:46:43,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:43,942][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.10115785896778107, acc: 0.9404761791229248)
[2024-12-12 02:46:44,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:44,277][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.08392979949712753, acc: 0.9210526347160339)
[2024-12-12 02:46:44,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:44,582][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.05057127773761749, acc: 1.0)
[2024-12-12 02:46:44,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:45,005][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.34740594029426575, acc: 0.9197860956192017)
[2024-12-12 02:46:45,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:45,379][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.021465718746185303, acc: 1.0)
[2024-12-12 02:46:45,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:45,766][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.168700709939003, acc: 0.9743589758872986)
[2024-12-12 02:46:45,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:46,097][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.4814051687717438, acc: 0.8724489808082581)
[2024-12-12 02:46:46,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:46,430][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.4565774202346802, acc: 0.8679245114326477)
[2024-12-12 02:46:46,838][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.5292, train_epoch_loss=0.4248, epoch time 361.16273244842887s
[2024-12-12 02:46:46,839][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:46:46,839][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:46:46,839][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:46:46,839][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 15
[2024-12-12 02:46:46,839][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:46:47,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:47,726][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.054134219884872437, acc: 1.0)
[2024-12-12 02:46:47,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:48,042][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.026580382138490677, acc: 1.0)
[2024-12-12 02:46:48,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:48,341][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.2980334758758545, acc: 0.8918918967247009)
[2024-12-12 02:46:48,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:48,681][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.3048209249973297, acc: 0.9473684430122375)
[2024-12-12 02:46:48,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:49,076][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.2033400982618332, acc: 0.9189189076423645)
[2024-12-12 02:46:49,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:49,421][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.0444166325032711, acc: 1.0)
[2024-12-12 02:46:49,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:49,787][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.4257201850414276, acc: 0.8571428656578064)
[2024-12-12 02:46:49,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:50,123][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.2786846458911896, acc: 0.9333333373069763)
[2024-12-12 02:46:50,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:50,449][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.0713578537106514, acc: 0.9545454382896423)
[2024-12-12 02:46:50,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:50,820][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.004380191210657358, acc: 1.0)
[2024-12-12 02:46:50,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:51,200][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.04256807267665863, acc: 0.9629629850387573)
[2024-12-12 02:46:51,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:51,576][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.0820976197719574, acc: 0.9743589758872986)
[2024-12-12 02:46:51,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:51,954][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.015315279364585876, acc: 1.0)
[2024-12-12 02:46:52,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:52,345][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.09098738431930542, acc: 0.97826087474823)
[2024-12-12 02:46:52,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:52,723][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.10601738095283508, acc: 0.9215686321258545)
[2024-12-12 02:46:52,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:53,061][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.17387396097183228, acc: 0.9387755393981934)
[2024-12-12 02:46:53,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:53,460][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.2707992196083069, acc: 0.9473684430122375)
[2024-12-12 02:46:53,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:53,829][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.042198773473501205, acc: 1.0)
[2024-12-12 02:46:53,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:54,186][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.1884291172027588, acc: 0.8888888955116272)
[2024-12-12 02:46:54,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:54,556][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.05732477456331253, acc: 1.0)
[2024-12-12 02:46:54,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:54,892][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.34878382086753845, acc: 0.9615384340286255)
[2024-12-12 02:46:55,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:55,268][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.2889098525047302, acc: 0.8965517282485962)
[2024-12-12 02:46:55,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:55,635][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.26866647601127625, acc: 0.9200000166893005)
[2024-12-12 02:46:55,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:56,015][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.46414923667907715, acc: 0.8571428656578064)
[2024-12-12 02:46:56,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:56,387][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.05585256591439247, acc: 1.0)
[2024-12-12 02:46:56,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:56,774][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.5393478274345398, acc: 0.8679245114326477)
[2024-12-12 02:46:56,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:57,178][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.5319860577583313, acc: 0.8493150472640991)
[2024-12-12 02:46:57,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:58,493][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 1.1311534643173218, acc: 0.7035573124885559)
[2024-12-12 02:46:58,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:58,870][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.37615764141082764, acc: 0.8837209343910217)
[2024-12-12 02:46:58,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:59,305][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.4371547996997833, acc: 0.8674699068069458)
[2024-12-12 02:46:59,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:59,707][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.4711371660232544, acc: 0.8641975522041321)
[2024-12-12 02:46:59,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:00,065][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.14177152514457703, acc: 0.9642857313156128)
[2024-12-12 02:47:00,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:00,465][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.2810201942920685, acc: 0.8888888955116272)
[2024-12-12 02:47:00,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:00,842][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.3206251263618469, acc: 0.8695651888847351)
[2024-12-12 02:47:00,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:01,256][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.4291015565395355, acc: 0.8907563090324402)
[2024-12-12 02:47:01,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:01,608][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.1493922621011734, acc: 0.9508196711540222)
[2024-12-12 02:47:01,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:01,933][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.17214170098304749, acc: 0.9682539701461792)
[2024-12-12 02:47:02,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:02,327][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.33439284563064575, acc: 0.8813559412956238)
[2024-12-12 02:47:02,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:02,668][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.25287747383117676, acc: 0.8850574493408203)
[2024-12-12 02:47:02,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:03,015][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.12085391581058502, acc: 0.9523809552192688)
[2024-12-12 02:47:03,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:03,297][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.1142977848649025, acc: 0.9615384340286255)
[2024-12-12 02:47:03,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:03,700][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.11688697338104248, acc: 0.9729729890823364)
[2024-12-12 02:47:03,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:04,098][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.4744851291179657, acc: 0.892307698726654)
[2024-12-12 02:47:04,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:04,512][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.5068296790122986, acc: 0.8484848737716675)
[2024-12-12 02:47:04,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:04,914][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.4099035859107971, acc: 0.8556700944900513)
[2024-12-12 02:47:05,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:05,301][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.3706510365009308, acc: 0.8823529481887817)
[2024-12-12 02:47:05,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:05,617][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.1954074651002884, acc: 0.8846153616905212)
[2024-12-12 02:47:05,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:05,946][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.04154203087091446, acc: 1.0)
[2024-12-12 02:47:06,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:06,269][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.0790485218167305, acc: 0.9642857313156128)
[2024-12-12 02:47:06,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:06,535][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.04530559852719307, acc: 1.0)
[2024-12-12 02:47:06,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:06,929][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.4641561806201935, acc: 0.859649121761322)
[2024-12-12 02:47:07,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:07,278][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.4136894941329956, acc: 0.8730158805847168)
[2024-12-12 02:47:07,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:07,648][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.5473580956459045, acc: 0.8450704216957092)
[2024-12-12 02:47:07,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:08,125][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 1.2199962139129639, acc: 0.6333333253860474)
[2024-12-12 02:47:08,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:08,467][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.5049093961715698, acc: 0.8918918967247009)
[2024-12-12 02:47:08,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:08,851][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.03388521075248718, acc: 1.0)
[2024-12-12 02:47:10,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:11,910][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 1.0051567554473877, acc: 0.7098976373672485)
[2024-12-12 02:47:12,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:13,183][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 1.3793349266052246, acc: 0.6078431606292725)
[2024-12-12 02:47:13,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:13,813][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.8798506259918213, acc: 0.7386363744735718)
[2024-12-12 02:47:13,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:14,391][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.36343225836753845, acc: 0.8970588445663452)
[2024-12-12 02:47:14,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:14,950][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.6391196250915527, acc: 0.7971014380455017)
[2024-12-12 02:47:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:15,383][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.5399930477142334, acc: 0.862500011920929)
[2024-12-12 02:47:15,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:15,727][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.10581880807876587, acc: 0.970588207244873)
[2024-12-12 02:47:15,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:16,087][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.05859655141830444, acc: 1.0)
[2024-12-12 02:47:16,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:16,546][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.2721787095069885, acc: 0.953125)
[2024-12-12 02:47:16,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:16,930][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.03165445104241371, acc: 1.0)
[2024-12-12 02:47:17,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:17,300][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.4366641342639923, acc: 0.8928571343421936)
[2024-12-12 02:47:17,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:17,686][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.3655431568622589, acc: 0.9166666865348816)
[2024-12-12 02:47:17,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:18,082][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.09727068245410919, acc: 0.9599999785423279)
[2024-12-12 02:47:18,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:18,470][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.28473183512687683, acc: 0.9166666865348816)
[2024-12-12 02:47:18,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:18,877][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.24001002311706543, acc: 0.939393937587738)
[2024-12-12 02:47:18,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:19,256][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.893250584602356, acc: 0.7279411554336548)
[2024-12-12 02:47:19,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:19,595][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.7459643483161926, acc: 0.7698412537574768)
[2024-12-12 02:47:19,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:19,938][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 1.182637333869934, acc: 0.6974359154701233)
[2024-12-12 02:47:20,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:20,266][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.6798347234725952, acc: 0.795918345451355)
[2024-12-12 02:47:20,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:20,613][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.8109698295593262, acc: 0.7313432693481445)
[2024-12-12 02:47:20,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:21,018][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 1.3615108728408813, acc: 0.6131386756896973)
[2024-12-12 02:47:21,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:21,354][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.024204108864068985, acc: 1.0)
[2024-12-12 02:47:21,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:21,676][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.05943809077143669, acc: 1.0)
[2024-12-12 02:47:21,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:22,056][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.0856190174818039, acc: 0.9696969985961914)
[2024-12-12 02:47:22,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:22,427][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.03288641571998596, acc: 1.0)
[2024-12-12 02:47:22,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:22,805][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.3809157907962799, acc: 0.8269230723381042)
[2024-12-12 02:47:22,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:23,201][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.4048466682434082, acc: 0.8269230723381042)
[2024-12-12 02:47:23,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:23,543][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.14317981898784637, acc: 0.9375)
[2024-12-12 02:47:23,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:23,910][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.3617623746395111, acc: 0.8695651888847351)
[2024-12-12 02:47:24,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:24,303][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.4221530854701996, acc: 0.8600000143051147)
[2024-12-12 02:47:24,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:24,688][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.05631992220878601, acc: 1.0)
[2024-12-12 02:47:24,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:25,184][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.600364089012146, acc: 0.800000011920929)
[2024-12-12 02:47:25,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:25,535][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.6341232061386108, acc: 0.8155339956283569)
[2024-12-12 02:47:25,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:26,673][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.7278977036476135, acc: 0.7815533876419067)
[2024-12-12 02:47:26,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:27,488][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 1.0518146753311157, acc: 0.698924720287323)
[2024-12-12 02:47:27,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:28,298][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.8364543914794922, acc: 0.767241358757019)
[2024-12-12 02:47:28,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:29,048][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.8221129775047302, acc: 0.7789473533630371)
[2024-12-12 02:47:29,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:30,043][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 1.1097108125686646, acc: 0.6831682920455933)
[2024-12-12 02:47:30,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:30,466][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.4678858518600464, acc: 0.8870967626571655)
[2024-12-12 02:47:30,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:30,892][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.5147579908370972, acc: 0.8405796885490417)
[2024-12-12 02:47:31,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:31,289][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.9388325810432434, acc: 0.7058823704719543)
[2024-12-12 02:47:31,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:31,628][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.9068238139152527, acc: 0.7115384340286255)
[2024-12-12 02:47:31,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:31,996][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 1.007515788078308, acc: 0.7080292105674744)
[2024-12-12 02:47:32,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:32,333][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.8254772424697876, acc: 0.7313432693481445)
[2024-12-12 02:47:32,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:32,671][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.0980956181883812, acc: 1.0)
[2024-12-12 02:47:32,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:33,026][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.02833707258105278, acc: 1.0)
[2024-12-12 02:47:33,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:33,349][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.016604306176304817, acc: 1.0)
[2024-12-12 02:47:33,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:33,713][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.03188091889023781, acc: 1.0)
[2024-12-12 02:47:33,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:34,110][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.2052455097436905, acc: 0.9137930870056152)
[2024-12-12 02:47:34,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:34,442][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.050742316991090775, acc: 1.0)
[2024-12-12 02:47:34,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:34,775][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.22135688364505768, acc: 0.9599999785423279)
[2024-12-12 02:47:34,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:35,154][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.00366736832074821, acc: 1.0)
[2024-12-12 02:47:35,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:35,519][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.003812628798186779, acc: 1.0)
[2024-12-12 02:47:35,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:35,891][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.03424305468797684, acc: 1.0)
[2024-12-12 02:47:36,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:36,289][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.14732934534549713, acc: 0.9538461565971375)
[2024-12-12 02:47:36,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:36,733][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.3288722038269043, acc: 0.9298245906829834)
[2024-12-12 02:47:36,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:37,083][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.7832292318344116, acc: 0.8245614171028137)
[2024-12-12 02:47:37,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:37,445][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.3634071946144104, acc: 0.9230769276618958)
[2024-12-12 02:47:37,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:37,825][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.19071659445762634, acc: 0.9591836929321289)
[2024-12-12 02:47:37,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:38,144][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.015627676621079445, acc: 1.0)
[2024-12-12 02:47:38,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:38,498][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.2797411382198334, acc: 0.9523809552192688)
[2024-12-12 02:47:38,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:38,867][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.5042528510093689, acc: 0.8130081295967102)
[2024-12-12 02:47:38,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:39,226][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.1650884598493576, acc: 0.9354838728904724)
[2024-12-12 02:47:39,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:40,123][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.7878231406211853, acc: 0.7832699418067932)
[2024-12-12 02:47:40,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:40,465][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.2666870951652527, acc: 0.9333333373069763)
[2024-12-12 02:47:40,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:40,863][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.21524447202682495, acc: 0.942307710647583)
[2024-12-12 02:47:40,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:41,154][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.025386517867445946, acc: 1.0)
[2024-12-12 02:47:41,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:41,512][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.13260363042354584, acc: 0.9473684430122375)
[2024-12-12 02:47:41,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:41,915][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.944342851638794, acc: 0.7300613522529602)
[2024-12-12 02:47:42,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:42,310][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 0.9735438823699951, acc: 0.7361111044883728)
[2024-12-12 02:47:42,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:42,649][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.9474684000015259, acc: 0.7083333134651184)
[2024-12-12 02:47:42,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:43,076][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.8749868273735046, acc: 0.726190447807312)
[2024-12-12 02:47:43,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:43,461][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.6969261765480042, acc: 0.7948718070983887)
[2024-12-12 02:47:43,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:43,860][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.6577070355415344, acc: 0.8161764740943909)
[2024-12-12 02:47:43,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:44,192][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.4913133382797241, acc: 0.9230769276618958)
[2024-12-12 02:47:44,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:44,530][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.19336169958114624, acc: 0.9130434989929199)
[2024-12-12 02:47:44,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:44,862][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.15467846393585205, acc: 0.96875)
[2024-12-12 02:47:45,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:45,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:46,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:46,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:46,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:47,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:47,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:47,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:48,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:48,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:48,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:49,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:49,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:50,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:50,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:50,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:51,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:51,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:51,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:52,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:52,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:52,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:53,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:53,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:53,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:54,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:54,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:54,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:55,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:55,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:55,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:56,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:56,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:56,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:57,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:57,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:57,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:58,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:58,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:58,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:59,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:59,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:59,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:00,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:00,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:01,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:01,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:01,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:02,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:02,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:03,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:03,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:03,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:04,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:04,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:04,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:04,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:05,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:05,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:06,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:06,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:06,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:07,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:07,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:07,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:08,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:08,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:08,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:09,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:09,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:10,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:10,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:10,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:11,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:11,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:11,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:12,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:12,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:12,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:13,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:13,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:13,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:14,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:14,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:14,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:15,384][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3006, device='cuda:0') eval_epoch_loss=tensor(0.8332, device='cuda:0') eval_epoch_acc=tensor(0.7862, device='cuda:0')
[2024-12-12 02:48:15,385][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:48:15,386][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:48:15,646][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_6_step_133_loss_0.8331514596939087/model.pt
[2024-12-12 02:48:15,655][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:48:15,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:16,005][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.11273037642240524, acc: 0.95652174949646)
[2024-12-12 02:48:16,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:16,410][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.24750271439552307, acc: 0.8857142925262451)
[2024-12-12 02:48:16,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:16,784][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.24839971959590912, acc: 0.9230769276618958)
[2024-12-12 02:48:16,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:17,157][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.1779516041278839, acc: 0.9523809552192688)
[2024-12-12 02:48:17,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:17,527][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.6324366331100464, acc: 0.8333333134651184)
[2024-12-12 02:48:17,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:17,897][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.646257758140564, acc: 0.9130434989929199)
[2024-12-12 02:48:18,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:18,243][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.10603508353233337, acc: 0.9523809552192688)
[2024-12-12 02:48:18,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:18,641][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.29950469732284546, acc: 0.9230769276618958)
[2024-12-12 02:48:18,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:18,962][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.3010355830192566, acc: 0.9677419066429138)
[2024-12-12 02:48:19,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:19,353][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.1824885606765747, acc: 0.9729729890823364)
[2024-12-12 02:48:19,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:19,891][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.5853028297424316, acc: 0.7894737124443054)
[2024-12-12 02:48:19,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:20,257][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.5943926572799683, acc: 0.8283582329750061)
[2024-12-12 02:48:20,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:20,648][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.5040343403816223, acc: 0.8265306353569031)
[2024-12-12 02:48:20,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:21,080][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.82659512758255, acc: 0.7340425252914429)
[2024-12-12 02:48:21,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:21,446][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.36533069610595703, acc: 0.8857142925262451)
[2024-12-12 02:48:21,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:21,821][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.23074690997600555, acc: 0.9642857313156128)
[2024-12-12 02:48:21,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:22,229][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.11294746398925781, acc: 0.95652174949646)
[2024-12-12 02:48:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:22,589][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.23390737175941467, acc: 0.931034505367279)
[2024-12-12 02:48:22,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:22,970][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.35129448771476746, acc: 0.8695651888847351)
[2024-12-12 02:48:23,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:23,345][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.5710269212722778, acc: 0.8135592937469482)
[2024-12-12 02:48:23,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:23,717][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.44372114539146423, acc: 0.8421052694320679)
[2024-12-12 02:48:23,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:24,059][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.39254823327064514, acc: 0.837837815284729)
[2024-12-12 02:48:24,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:24,407][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.027958491817116737, acc: 1.0)
[2024-12-12 02:48:24,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:24,803][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.170543372631073, acc: 0.9130434989929199)
[2024-12-12 02:48:24,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:25,165][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.3758034408092499, acc: 0.7894737124443054)
[2024-12-12 02:48:25,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:26,899][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.8715497851371765, acc: 0.7162162065505981)
[2024-12-12 02:48:26,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:27,196][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.8041778206825256, acc: 0.7592592835426331)
[2024-12-12 02:48:27,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:27,590][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 1.2570369243621826, acc: 0.6627907156944275)
[2024-12-12 02:48:27,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:28,172][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 1.4135793447494507, acc: 0.6235294342041016)
[2024-12-12 02:48:28,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:28,728][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 1.3797399997711182, acc: 0.6292135119438171)
[2024-12-12 02:48:28,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:29,071][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.22961455583572388, acc: 0.9318181872367859)
[2024-12-12 02:48:29,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:29,428][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.023834798485040665, acc: 1.0)
[2024-12-12 02:48:29,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:29,803][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.3648294508457184, acc: 0.8965517282485962)
[2024-12-12 02:48:29,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:30,198][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.10260925441980362, acc: 0.9795918464660645)
[2024-12-12 02:48:30,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:30,513][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.2408769577741623, acc: 0.9599999785423279)
[2024-12-12 02:48:30,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:30,952][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.4536662697792053, acc: 0.8888888955116272)
[2024-12-12 02:48:31,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:31,287][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.8937650918960571, acc: 0.7549019455909729)
[2024-12-12 02:48:31,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:32,313][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.8336825370788574, acc: 0.7739726305007935)
[2024-12-12 02:48:32,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:32,626][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.0793415978550911, acc: 0.9583333134651184)
[2024-12-12 02:48:32,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:33,002][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.32908833026885986, acc: 0.9259259104728699)
[2024-12-12 02:48:33,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:33,337][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.21948124468326569, acc: 0.8928571343421936)
[2024-12-12 02:48:33,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:33,870][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 1.0297753810882568, acc: 0.7345132827758789)
[2024-12-12 02:48:33,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:34,252][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.8415930867195129, acc: 0.7971014380455017)
[2024-12-12 02:48:34,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:34,643][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.39887702465057373, acc: 0.8295454382896423)
[2024-12-12 02:48:34,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:35,547][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 1.2564353942871094, acc: 0.6717557311058044)
[2024-12-12 02:48:35,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:36,215][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.9097354412078857, acc: 0.7259259223937988)
[2024-12-12 02:48:36,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:36,533][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.3148799240589142, acc: 0.8852459192276001)
[2024-12-12 02:48:36,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:36,895][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.010679583065211773, acc: 1.0)
[2024-12-12 02:48:37,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:37,242][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.019061708822846413, acc: 1.0)
[2024-12-12 02:48:37,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:37,561][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.04473438858985901, acc: 1.0)
[2024-12-12 02:48:37,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:37,859][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.34707799553871155, acc: 0.8658536672592163)
[2024-12-12 02:48:37,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:38,219][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.5705721378326416, acc: 0.8549848794937134)
[2024-12-12 02:48:38,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:38,665][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.8644374012947083, acc: 0.7463976740837097)
[2024-12-12 02:48:38,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:39,140][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.6996081471443176, acc: 0.8031250238418579)
[2024-12-12 02:48:39,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:39,662][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.9421824812889099, acc: 0.7448405027389526)
[2024-12-12 02:48:39,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:40,054][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.6777718663215637, acc: 0.8042704463005066)
[2024-12-12 02:48:40,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:40,344][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.25189724564552307, acc: 0.9599999785423279)
[2024-12-12 02:48:40,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:40,889][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.7404070496559143, acc: 0.7209302186965942)
[2024-12-12 02:48:41,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:41,682][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 1.0933929681777954, acc: 0.6269841194152832)
[2024-12-12 02:48:41,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:42,595][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.9588440656661987, acc: 0.7121211886405945)
[2024-12-12 02:48:42,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:43,333][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.4628649652004242, acc: 0.8470588326454163)
[2024-12-12 02:48:43,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:44,405][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.8631424903869629, acc: 0.7407407164573669)
[2024-12-12 02:48:44,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:45,354][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.33626002073287964, acc: 0.9032257795333862)
[2024-12-12 02:48:45,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:45,723][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.013958156108856201, acc: 1.0)
[2024-12-12 02:48:45,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:46,118][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.28206127882003784, acc: 0.8999999761581421)
[2024-12-12 02:48:46,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:46,506][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.45073604583740234, acc: 0.8970588445663452)
[2024-12-12 02:48:46,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:46,902][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.8112272024154663, acc: 0.7647058963775635)
[2024-12-12 02:48:47,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:47,313][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.5778399705886841, acc: 0.8135592937469482)
[2024-12-12 02:48:47,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:47,697][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.6885854005813599, acc: 0.7985074520111084)
[2024-12-12 02:48:47,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:48,043][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.7337944507598877, acc: 0.7669903039932251)
[2024-12-12 02:48:48,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:48,421][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.5622235536575317, acc: 0.8095238208770752)
[2024-12-12 02:48:48,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:48,824][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.11472354084253311, acc: 0.9780219793319702)
[2024-12-12 02:48:48,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:49,202][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.34350329637527466, acc: 0.8834080696105957)
[2024-12-12 02:48:49,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:49,599][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.5037980079650879, acc: 0.8622047305107117)
[2024-12-12 02:48:49,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:49,954][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.41163310408592224, acc: 0.892241358757019)
[2024-12-12 02:48:50,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:50,307][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.4494374096393585, acc: 0.8804348111152649)
[2024-12-12 02:48:50,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:50,709][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.346152663230896, acc: 0.8988326787948608)
[2024-12-12 02:48:50,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:51,053][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.24249716103076935, acc: 0.9130434989929199)
[2024-12-12 02:48:51,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:51,336][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.1030893623828888, acc: 0.95652174949646)
[2024-12-12 02:48:51,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:51,621][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.12059221416711807, acc: 0.9642857313156128)
[2024-12-12 02:48:51,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:51,981][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.14822204411029816, acc: 0.978723406791687)
[2024-12-12 02:48:52,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:52,664][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.20030811429023743, acc: 0.9538461565971375)
[2024-12-12 02:48:52,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:52,984][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.17034189403057098, acc: 0.9189189076423645)
[2024-12-12 02:48:53,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:53,382][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.19929704070091248, acc: 0.9418604373931885)
[2024-12-12 02:48:53,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:53,910][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.21142162382602692, acc: 0.9189189076423645)
[2024-12-12 02:48:54,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:54,287][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.13155728578567505, acc: 0.9666666388511658)
[2024-12-12 02:48:54,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:54,652][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.10044838488101959, acc: 0.9696969985961914)
[2024-12-12 02:48:54,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:54,969][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.014085225760936737, acc: 1.0)
[2024-12-12 02:48:55,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:55,328][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.01671987771987915, acc: 1.0)
[2024-12-12 02:48:55,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:55,684][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.37490761280059814, acc: 0.8653846383094788)
[2024-12-12 02:48:55,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:56,436][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.3395504057407379, acc: 0.907608687877655)
[2024-12-12 02:48:56,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:56,973][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.4996402859687805, acc: 0.8409090638160706)
[2024-12-12 02:48:57,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:57,401][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.5891693234443665, acc: 0.8404255509376526)
[2024-12-12 02:48:57,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:57,733][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.1869449019432068, acc: 0.9433962106704712)
[2024-12-12 02:48:57,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:58,066][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.2632502019405365, acc: 0.9166666865348816)
[2024-12-12 02:48:58,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:58,428][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.16103950142860413, acc: 0.9534883499145508)
[2024-12-12 02:48:58,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:58,802][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.45928624272346497, acc: 0.8666666746139526)
[2024-12-12 02:48:58,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:59,163][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 1.6017359495162964, acc: 0.5789473652839661)
[2024-12-12 02:48:59,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:59,469][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 1.2482471466064453, acc: 0.699999988079071)
[2024-12-12 02:48:59,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:59,888][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 1.2045708894729614, acc: 0.6666666865348816)
[2024-12-12 02:49:00,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:00,372][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 1.6234525442123413, acc: 0.5871559381484985)
[2024-12-12 02:49:00,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:00,833][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 1.3617186546325684, acc: 0.6153846383094788)
[2024-12-12 02:49:00,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:01,128][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.08906247466802597, acc: 0.9473684430122375)
[2024-12-12 02:49:01,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:01,415][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.021526949480175972, acc: 1.0)
[2024-12-12 02:49:01,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:01,701][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.2541762590408325, acc: 0.9090909361839294)
[2024-12-12 02:49:01,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:02,012][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.45268556475639343, acc: 0.8148148059844971)
[2024-12-12 02:49:02,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:02,363][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.20915605127811432, acc: 0.9142857193946838)
[2024-12-12 02:49:02,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:02,765][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.5184307098388672, acc: 0.8863636255264282)
[2024-12-12 02:49:02,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:03,115][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.3315558135509491, acc: 0.8636363744735718)
[2024-12-12 02:49:03,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:03,691][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.9383614659309387, acc: 0.6935483813285828)
[2024-12-12 02:49:03,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:04,217][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.5856372117996216, acc: 0.8409090638160706)
[2024-12-12 02:49:04,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:04,546][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.009827373549342155, acc: 1.0)
[2024-12-12 02:49:04,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:04,852][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.1356610655784607, acc: 0.9615384340286255)
[2024-12-12 02:49:04,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:05,227][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.017801251262426376, acc: 1.0)
[2024-12-12 02:49:05,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:05,625][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.2041107714176178, acc: 0.8999999761581421)
[2024-12-12 02:49:05,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:06,038][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.1099623590707779, acc: 0.9729729890823364)
[2024-12-12 02:49:06,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:06,429][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.12129198759794235, acc: 0.9729729890823364)
[2024-12-12 02:49:06,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:06,835][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.020351547747850418, acc: 1.0)
[2024-12-12 02:49:06,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:07,224][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.22139890491962433, acc: 0.9264705777168274)
[2024-12-12 02:49:07,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:07,624][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.08370154350996017, acc: 0.9756097793579102)
[2024-12-12 02:49:07,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:07,952][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.03535374999046326, acc: 1.0)
[2024-12-12 02:49:08,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:08,257][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.05210129916667938, acc: 0.9599999785423279)
[2024-12-12 02:49:08,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:08,615][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.16878749430179596, acc: 0.9354838728904724)
[2024-12-12 02:49:08,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:09,002][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.1656636744737625, acc: 0.9649122953414917)
[2024-12-12 02:49:09,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:09,328][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.3636251986026764, acc: 0.8999999761581421)
[2024-12-12 02:49:09,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:09,741][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.24214714765548706, acc: 0.9473684430122375)
[2024-12-12 02:49:09,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:10,302][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.44484037160873413, acc: 0.8773584961891174)
[2024-12-12 02:49:10,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:10,889][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.3535519540309906, acc: 0.8999999761581421)
[2024-12-12 02:49:10,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:11,186][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.07555605471134186, acc: 0.9722222089767456)
[2024-12-12 02:49:11,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:11,481][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.17074403166770935, acc: 0.9032257795333862)
[2024-12-12 02:49:11,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:11,902][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.607255756855011, acc: 0.8133333325386047)
[2024-12-12 02:49:12,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:12,282][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.38593289256095886, acc: 0.8333333134651184)
[2024-12-12 02:49:12,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:13,183][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.9108384847640991, acc: 0.7279999852180481)
[2024-12-12 02:49:13,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:13,525][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.672150731086731, acc: 0.8089887499809265)
[2024-12-12 02:49:13,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:13,865][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.5323088765144348, acc: 0.8243243098258972)
[2024-12-12 02:49:13,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:14,310][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.3725123703479767, acc: 0.8965517282485962)
[2024-12-12 02:49:14,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:14,618][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.010125833563506603, acc: 1.0)
[2024-12-12 02:49:14,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:14,986][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.06271737068891525, acc: 1.0)
[2024-12-12 02:49:15,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:15,350][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.0201667919754982, acc: 1.0)
[2024-12-12 02:49:15,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:15,721][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.030881935730576515, acc: 1.0)
[2024-12-12 02:49:15,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:16,121][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.3362988829612732, acc: 0.9166666865348816)
[2024-12-12 02:49:16,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:16,520][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.0858086422085762, acc: 0.96875)
[2024-12-12 02:49:16,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:16,919][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.04049205407500267, acc: 1.0)
[2024-12-12 02:49:17,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:18,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:18,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:18,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:19,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:19,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:20,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:20,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:20,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:21,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:21,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:21,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:22,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:23,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:23,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:23,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:23,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:24,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:24,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:24,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:25,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:25,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:25,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:26,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:26,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:26,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:27,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:27,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:27,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:28,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:28,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:28,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:29,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:29,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:30,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:30,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:30,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:31,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:31,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:32,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:32,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:32,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:33,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:33,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:33,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:34,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:34,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:35,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:35,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:35,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:36,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:36,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:37,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:37,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:37,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:38,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:38,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:38,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:39,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:39,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:39,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:40,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:40,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:40,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:41,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:41,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:42,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:42,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:42,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:43,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:43,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:43,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:44,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:44,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:44,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:45,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:45,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:45,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:46,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:46,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:47,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:47,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:47,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:48,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:48,615][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4728, device='cuda:0') eval_epoch_loss=tensor(0.9054, device='cuda:0') eval_epoch_acc=tensor(0.7916, device='cuda:0')
[2024-12-12 02:49:48,616][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:49:48,617][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:49:48,890][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_6_step_276_loss_0.9053713083267212/model.pt
[2024-12-12 02:49:48,897][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:49:48,898][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 6 is 0.7915514707565308
[2024-12-12 02:49:48,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:49,246][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.2936288118362427, acc: 0.9655172228813171)
[2024-12-12 02:49:49,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:49,604][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.1079944297671318, acc: 0.9599999785423279)
[2024-12-12 02:49:49,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:49,940][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.50319904088974, acc: 0.8723404407501221)
[2024-12-12 02:49:50,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:50,268][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.22553294897079468, acc: 0.8958333134651184)
[2024-12-12 02:49:50,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:50,598][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.05970476567745209, acc: 0.9545454382896423)
[2024-12-12 02:49:50,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:51,021][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.5037640929222107, acc: 0.8674699068069458)
[2024-12-12 02:49:51,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:51,375][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.766170084476471, acc: 0.75)
[2024-12-12 02:49:51,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:51,700][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.18644870817661285, acc: 0.9736841917037964)
[2024-12-12 02:49:51,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:52,073][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.1961647868156433, acc: 0.9411764740943909)
[2024-12-12 02:49:52,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:52,424][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.10638417303562164, acc: 0.9750000238418579)
[2024-12-12 02:49:52,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:52,839][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.3566839098930359, acc: 0.90625)
[2024-12-12 02:49:52,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:53,273][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.4725359082221985, acc: 0.8640000224113464)
[2024-12-12 02:49:53,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:53,726][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.4476664662361145, acc: 0.8681318759918213)
[2024-12-12 02:49:53,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:54,067][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.3729270100593567, acc: 0.9006211161613464)
[2024-12-12 02:49:54,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:54,475][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.5491538643836975, acc: 0.8453608155250549)
[2024-12-12 02:49:54,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:54,843][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.04554969072341919, acc: 0.9545454382896423)
[2024-12-12 02:49:54,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:55,194][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.18861718475818634, acc: 0.976190447807312)
[2024-12-12 02:49:55,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:55,583][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.2605687975883484, acc: 0.9137930870056152)
[2024-12-12 02:49:55,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:56,037][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.290617436170578, acc: 0.9272727370262146)
[2024-12-12 02:49:56,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:56,592][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.6649897694587708, acc: 0.7886598110198975)
[2024-12-12 02:49:56,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:56,918][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.2794417440891266, acc: 0.9137930870056152)
[2024-12-12 02:49:57,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:57,305][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.07799534499645233, acc: 0.9629629850387573)
[2024-12-12 02:49:57,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:57,663][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.21531717479228973, acc: 0.8947368264198303)
[2024-12-12 02:49:57,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:58,013][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.11244983226060867, acc: 0.9464285969734192)
[2024-12-12 02:49:58,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:58,370][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.032861799001693726, acc: 1.0)
[2024-12-12 02:49:58,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:58,725][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.2513020932674408, acc: 0.9433962106704712)
[2024-12-12 02:49:58,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:59,106][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.1615142822265625, acc: 0.9622641801834106)
[2024-12-12 02:49:59,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:59,439][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.07149053364992142, acc: 0.970588207244873)
[2024-12-12 02:49:59,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:59,737][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.05227344483137131, acc: 1.0)
[2024-12-12 02:49:59,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:00,102][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.1871720403432846, acc: 0.9180327653884888)
[2024-12-12 02:50:00,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:00,435][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.025233816355466843, acc: 1.0)
[2024-12-12 02:50:00,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:00,739][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.006805642507970333, acc: 1.0)
[2024-12-12 02:50:00,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:01,098][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.2994064688682556, acc: 0.9420289993286133)
[2024-12-12 02:50:01,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:01,528][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.18875139951705933, acc: 0.9722222089767456)
[2024-12-12 02:50:01,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:01,898][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.21990220248699188, acc: 0.9277108311653137)
[2024-12-12 02:50:02,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:02,255][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.31539052724838257, acc: 0.8974359035491943)
[2024-12-12 02:50:02,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:02,642][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.2526635229587555, acc: 0.9387755393981934)
[2024-12-12 02:50:02,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:02,971][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.12403228133916855, acc: 0.9583333134651184)
[2024-12-12 02:50:03,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:03,323][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.039658840745687485, acc: 1.0)
[2024-12-12 02:50:03,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:03,672][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.03327350690960884, acc: 1.0)
[2024-12-12 02:50:03,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:04,080][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.22100107371807098, acc: 0.9032257795333862)
[2024-12-12 02:50:04,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:04,442][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.10530320554971695, acc: 0.9552238583564758)
[2024-12-12 02:50:04,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:04,809][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.10693169385194778, acc: 0.9807692170143127)
[2024-12-12 02:50:04,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:05,169][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.06693194061517715, acc: 0.9777777791023254)
[2024-12-12 02:50:05,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:05,483][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.11479921638965607, acc: 0.9677419066429138)
[2024-12-12 02:50:05,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:05,850][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.027995359152555466, acc: 0.9800000190734863)
[2024-12-12 02:50:05,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:06,180][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.4866674840450287, acc: 0.8148148059844971)
[2024-12-12 02:50:06,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:06,481][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.5744145512580872, acc: 0.8571428656578064)
[2024-12-12 02:50:06,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:06,860][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.2839481234550476, acc: 0.9230769276618958)
[2024-12-12 02:50:06,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:07,220][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.6376860737800598, acc: 0.7560975551605225)
[2024-12-12 02:50:07,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:07,587][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.392719030380249, acc: 0.8421052694320679)
[2024-12-12 02:50:07,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:07,902][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.054823845624923706, acc: 1.0)
[2024-12-12 02:50:07,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:08,217][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.020378101617097855, acc: 1.0)
[2024-12-12 02:50:08,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:08,521][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.007380042225122452, acc: 1.0)
[2024-12-12 02:50:08,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:08,879][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.009350940585136414, acc: 1.0)
[2024-12-12 02:50:08,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:09,241][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.12624989449977875, acc: 0.9516128897666931)
[2024-12-12 02:50:09,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:09,610][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.0406530499458313, acc: 1.0)
[2024-12-12 02:50:09,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:09,991][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.14975011348724365, acc: 0.96875)
[2024-12-12 02:50:10,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:10,394][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.043011970818042755, acc: 0.9666666388511658)
[2024-12-12 02:50:10,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:10,770][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.08513990044593811, acc: 0.9473684430122375)
[2024-12-12 02:50:10,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:11,190][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.6188724040985107, acc: 0.800000011920929)
[2024-12-12 02:50:11,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:11,593][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.8163309097290039, acc: 0.8045976758003235)
[2024-12-12 02:50:11,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:11,985][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.9359102249145508, acc: 0.7127659320831299)
[2024-12-12 02:50:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:12,332][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.8901354074478149, acc: 0.7349397540092468)
[2024-12-12 02:50:12,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:12,670][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.018460560590028763, acc: 1.0)
[2024-12-12 02:50:12,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:13,045][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.1706984043121338, acc: 0.9230769276618958)
[2024-12-12 02:50:13,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:13,414][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.16096049547195435, acc: 0.9397590160369873)
[2024-12-12 02:50:13,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:13,754][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.2922564148902893, acc: 0.9056603908538818)
[2024-12-12 02:50:13,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:14,127][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.1452612429857254, acc: 0.9367088675498962)
[2024-12-12 02:50:14,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:14,499][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.05065911263227463, acc: 0.9803921580314636)
[2024-12-12 02:50:14,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:14,862][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.2890583574771881, acc: 0.9104477763175964)
[2024-12-12 02:50:14,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:15,187][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.14023315906524658, acc: 0.949999988079071)
[2024-12-12 02:50:15,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:15,537][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.01692325249314308, acc: 1.0)
[2024-12-12 02:50:15,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:15,966][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.5977258086204529, acc: 0.8611111044883728)
[2024-12-12 02:50:16,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:16,358][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.33066675066947937, acc: 0.9069767594337463)
[2024-12-12 02:50:16,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:16,743][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.09659414738416672, acc: 0.9743589758872986)
[2024-12-12 02:50:16,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:17,143][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.3002575933933258, acc: 0.9333333373069763)
[2024-12-12 02:50:17,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:17,517][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.03455329313874245, acc: 1.0)
[2024-12-12 02:50:17,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:17,912][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.026776330545544624, acc: 1.0)
[2024-12-12 02:50:18,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:18,310][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.5926510691642761, acc: 0.8241758346557617)
[2024-12-12 02:50:18,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:18,829][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.4974905848503113, acc: 0.8260869383811951)
[2024-12-12 02:50:18,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:19,155][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.3274206221103668, acc: 0.8804348111152649)
[2024-12-12 02:50:19,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:19,538][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.2925501763820648, acc: 0.9387755393981934)
[2024-12-12 02:50:19,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:19,872][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.001602628850378096, acc: 1.0)
[2024-12-12 02:50:19,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:20,165][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.027038654312491417, acc: 1.0)
[2024-12-12 02:50:20,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:20,526][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.180833637714386, acc: 0.9024389982223511)
[2024-12-12 02:50:20,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:20,867][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.1771049052476883, acc: 0.9333333373069763)
[2024-12-12 02:50:20,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:21,234][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.08256767690181732, acc: 0.9736841917037964)
[2024-12-12 02:50:21,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:21,568][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.05648729205131531, acc: 0.9756097793579102)
[2024-12-12 02:50:21,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:21,932][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.12054530531167984, acc: 0.9696969985961914)
[2024-12-12 02:50:22,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:22,290][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.010145925916731358, acc: 1.0)
[2024-12-12 02:50:22,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:22,654][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.017717896029353142, acc: 1.0)
[2024-12-12 02:50:22,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:22,998][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.06508278101682663, acc: 0.9642857313156128)
[2024-12-12 02:50:23,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:23,329][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.25553613901138306, acc: 0.875)
[2024-12-12 02:50:23,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:23,921][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.4059850871562958, acc: 0.8666666746139526)
[2024-12-12 02:50:24,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:24,788][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.2678240239620209, acc: 0.9245283007621765)
[2024-12-12 02:50:24,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:25,181][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.14906756579875946, acc: 0.9555555582046509)
[2024-12-12 02:50:25,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:25,532][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.05308670923113823, acc: 1.0)
[2024-12-12 02:50:25,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:25,859][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.032582953572273254, acc: 1.0)
[2024-12-12 02:50:25,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:26,242][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 0.008961879648268223, acc: 1.0)
[2024-12-12 02:50:26,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:26,575][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.01646987348794937, acc: 1.0)
[2024-12-12 02:50:26,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:26,876][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.022244827821850777, acc: 1.0)
[2024-12-12 02:50:26,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:27,220][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.0401582233607769, acc: 0.9789473414421082)
[2024-12-12 02:50:27,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:27,790][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.18449415266513824, acc: 0.9520958065986633)
[2024-12-12 02:50:27,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:28,194][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.24360553920269012, acc: 0.9172932505607605)
[2024-12-12 02:50:28,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:29,440][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.5467525124549866, acc: 0.8770053386688232)
[2024-12-12 02:50:29,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:30,000][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.1341320127248764, acc: 0.9459459185600281)
[2024-12-12 02:50:30,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:30,327][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.019802818074822426, acc: 1.0)
[2024-12-12 02:50:30,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:30,686][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.07843102514743805, acc: 0.9642857313156128)
[2024-12-12 02:50:30,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:31,084][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.017505111172795296, acc: 1.0)
[2024-12-12 02:50:31,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:31,456][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.029503682628273964, acc: 1.0)
[2024-12-12 02:50:31,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:31,775][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.011137081310153008, acc: 1.0)
[2024-12-12 02:50:31,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:32,078][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.015462330542504787, acc: 1.0)
[2024-12-12 02:50:32,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:32,412][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.02256912551820278, acc: 1.0)
[2024-12-12 02:50:32,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:32,750][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.06930211931467056, acc: 0.9523809552192688)
[2024-12-12 02:50:32,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:33,087][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.5671745538711548, acc: 0.8333333134651184)
[2024-12-12 02:50:33,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:33,460][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.7195973992347717, acc: 0.8155339956283569)
[2024-12-12 02:50:33,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:33,983][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.6951306462287903, acc: 0.8308823704719543)
[2024-12-12 02:50:34,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:34,385][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.7127835750579834, acc: 0.8066666722297668)
[2024-12-12 02:50:34,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:34,768][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.5968902707099915, acc: 0.8541666865348816)
[2024-12-12 02:50:34,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:35,099][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.07027018815279007, acc: 0.9767441749572754)
[2024-12-12 02:50:35,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:35,402][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.03651231899857521, acc: 0.9583333134651184)
[2024-12-12 02:50:35,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:35,743][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.15416494011878967, acc: 0.9534883499145508)
[2024-12-12 02:50:35,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:36,081][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.01376635767519474, acc: 1.0)
[2024-12-12 02:50:36,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:36,607][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.2181183248758316, acc: 0.9558823704719543)
[2024-12-12 02:50:36,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:36,991][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.42207762598991394, acc: 0.8666666746139526)
[2024-12-12 02:50:37,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:37,325][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.16854281723499298, acc: 0.9090909361839294)
[2024-12-12 02:50:37,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:37,685][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.20145085453987122, acc: 0.939393937587738)
[2024-12-12 02:50:37,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:38,070][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.025064365938305855, acc: 1.0)
[2024-12-12 02:50:38,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:38,462][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.10184495896100998, acc: 0.9629629850387573)
[2024-12-12 02:50:38,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:38,822][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.00671806326135993, acc: 1.0)
[2024-12-12 02:50:38,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:39,167][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.0108347712084651, acc: 1.0)
[2024-12-12 02:50:39,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:39,529][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.023113353177905083, acc: 1.0)
[2024-12-12 02:50:39,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:39,905][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.034171417355537415, acc: 1.0)
[2024-12-12 02:50:40,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:40,289][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.0401880107820034, acc: 1.0)
[2024-12-12 02:50:40,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:40,584][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.03554411232471466, acc: 1.0)
[2024-12-12 02:50:40,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:40,881][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.05196196585893631, acc: 0.9666666388511658)
[2024-12-12 02:50:40,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:41,245][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.2368450164794922, acc: 0.9696969985961914)
[2024-12-12 02:50:41,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:41,577][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.036983273923397064, acc: 1.0)
[2024-12-12 02:50:41,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:41,941][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.19603261351585388, acc: 0.9411764740943909)
[2024-12-12 02:50:42,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:42,240][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.14214663207530975, acc: 0.9230769276618958)
[2024-12-12 02:50:42,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:42,604][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.0425417535007, acc: 1.0)
[2024-12-12 02:50:42,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:42,953][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.11533151566982269, acc: 0.949999988079071)
[2024-12-12 02:50:43,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:44,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:44,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:44,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:45,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:45,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:46,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:46,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:46,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:46,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:47,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:47,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:47,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:48,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:48,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:48,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:49,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:49,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:49,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:50,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:50,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:50,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:51,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:51,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:51,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:52,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:52,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:53,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:53,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:53,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:54,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:54,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:55,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:55,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:55,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:56,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:56,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:56,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:57,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:57,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:58,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:58,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:58,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:59,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:59,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:59,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:00,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:00,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:01,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:01,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:01,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:01,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:02,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:02,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:02,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:03,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:03,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:03,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:04,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:04,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:05,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:05,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:05,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:06,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:06,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:06,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:07,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:07,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:08,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:08,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:08,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:09,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:09,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:09,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:10,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:10,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:10,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:11,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:11,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:11,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:12,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:12,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:13,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:13,871][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5777, device='cuda:0') eval_epoch_loss=tensor(0.9469, device='cuda:0') eval_epoch_acc=tensor(0.7834, device='cuda:0')
[2024-12-12 02:51:13,872][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:51:13,873][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:51:14,125][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_6_step_419_loss_0.946904182434082/model.pt
[2024-12-12 02:51:14,128][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:51:14,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:14,527][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.032904624938964844, acc: 1.0)
[2024-12-12 02:51:14,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:14,871][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.007532688323408365, acc: 1.0)
[2024-12-12 02:51:14,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:15,223][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.11653288453817368, acc: 0.9666666388511658)
[2024-12-12 02:51:15,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:15,539][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.09839361906051636, acc: 0.9375)
[2024-12-12 02:51:15,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:15,870][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.10173794627189636, acc: 0.9722222089767456)
[2024-12-12 02:51:16,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:16,249][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.29145684838294983, acc: 0.9259259104728699)
[2024-12-12 02:51:16,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:16,620][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.033039391040802, acc: 1.0)
[2024-12-12 02:51:16,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:16,964][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.009991936385631561, acc: 1.0)
[2024-12-12 02:51:17,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:17,343][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.10539335757493973, acc: 0.9459459185600281)
[2024-12-12 02:51:17,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:17,704][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.02680359035730362, acc: 1.0)
[2024-12-12 02:51:17,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:18,074][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.024522921070456505, acc: 1.0)
[2024-12-12 02:51:18,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:18,498][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.0012957127764821053, acc: 1.0)
[2024-12-12 02:51:18,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:18,899][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.005455570761114359, acc: 1.0)
[2024-12-12 02:51:19,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:19,253][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.010572487488389015, acc: 1.0)
[2024-12-12 02:51:19,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:19,607][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.13786348700523376, acc: 0.9444444179534912)
[2024-12-12 02:51:19,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:19,950][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.0031749713234603405, acc: 1.0)
[2024-12-12 02:51:20,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:20,380][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.012280121445655823, acc: 1.0)
[2024-12-12 02:51:20,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:20,736][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.10633207857608795, acc: 0.9722222089767456)
[2024-12-12 02:51:20,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:21,105][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.06627266854047775, acc: 0.9545454382896423)
[2024-12-12 02:51:21,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:21,458][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.0024437918327748775, acc: 1.0)
[2024-12-12 02:51:21,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:21,787][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.0639636367559433, acc: 0.9743589758872986)
[2024-12-12 02:51:21,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:22,248][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.38198021054267883, acc: 0.9090909361839294)
[2024-12-12 02:51:22,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:22,962][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.6036115884780884, acc: 0.7760000228881836)
[2024-12-12 02:51:23,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:23,369][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.7119357585906982, acc: 0.7822580933570862)
[2024-12-12 02:51:23,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:24,028][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.5807411074638367, acc: 0.8308457732200623)
[2024-12-12 02:51:24,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:24,398][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.1921733170747757, acc: 0.9433962106704712)
[2024-12-12 02:51:24,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:24,811][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.03689242899417877, acc: 0.9772727489471436)
[2024-12-12 02:51:24,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:25,117][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.06289640814065933, acc: 0.95652174949646)
[2024-12-12 02:51:25,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:25,508][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.07443907111883163, acc: 0.9615384340286255)
[2024-12-12 02:51:25,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:25,865][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.014717824757099152, acc: 1.0)
[2024-12-12 02:51:25,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:26,209][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.08332133293151855, acc: 0.9701492786407471)
[2024-12-12 02:51:26,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:26,621][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.10885623842477798, acc: 0.9722222089767456)
[2024-12-12 02:51:26,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:27,007][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.10164890438318253, acc: 0.967391312122345)
[2024-12-12 02:51:27,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:27,336][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.14718976616859436, acc: 0.9487179517745972)
[2024-12-12 02:51:27,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:27,708][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.32405364513397217, acc: 0.9078947305679321)
[2024-12-12 02:51:27,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:28,116][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.045396167784929276, acc: 1.0)
[2024-12-12 02:51:28,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:28,500][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.0712357759475708, acc: 0.9696969985961914)
[2024-12-12 02:51:28,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:28,876][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.5495633482933044, acc: 0.8556700944900513)
[2024-12-12 02:51:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:29,264][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.0917491689324379, acc: 0.9714285731315613)
[2024-12-12 02:51:29,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:29,645][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.3066115379333496, acc: 0.9186046719551086)
[2024-12-12 02:51:29,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:29,982][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.04402404651045799, acc: 1.0)
[2024-12-12 02:51:30,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:30,353][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.1733906865119934, acc: 0.9259259104728699)
[2024-12-12 02:51:30,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:30,701][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.1501268744468689, acc: 0.9444444179534912)
[2024-12-12 02:51:30,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:31,032][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.05666179582476616, acc: 0.96875)
[2024-12-12 02:51:31,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:31,367][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.017907168716192245, acc: 1.0)
[2024-12-12 02:51:31,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:31,738][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.04426557570695877, acc: 0.97826087474823)
[2024-12-12 02:51:31,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:32,127][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.18589581549167633, acc: 0.9404761791229248)
[2024-12-12 02:51:32,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:32,494][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.4048730432987213, acc: 0.8674699068069458)
[2024-12-12 02:51:32,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:32,891][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.1947685033082962, acc: 0.9369369149208069)
[2024-12-12 02:51:32,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:33,227][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.3948765993118286, acc: 0.893203854560852)
[2024-12-12 02:51:33,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:33,542][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.40700599551200867, acc: 0.8943089246749878)
[2024-12-12 02:51:33,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:33,887][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.02400367148220539, acc: 1.0)
[2024-12-12 02:51:33,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:34,245][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.050011374056339264, acc: 0.9642857313156128)
[2024-12-12 02:51:34,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:34,662][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.3478323221206665, acc: 0.8627451062202454)
[2024-12-12 02:51:34,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:35,062][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.6683580279350281, acc: 0.7772925496101379)
[2024-12-12 02:51:35,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:35,382][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.37761107087135315, acc: 0.8541666865348816)
[2024-12-12 02:51:35,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:35,764][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.2359645515680313, acc: 0.9325153231620789)
[2024-12-12 02:51:35,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:36,116][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.22938871383666992, acc: 0.935251772403717)
[2024-12-12 02:51:36,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:36,466][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.4339132010936737, acc: 0.8844221234321594)
[2024-12-12 02:51:36,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:36,814][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.09410323202610016, acc: 1.0)
[2024-12-12 02:51:36,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:37,190][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.06793760508298874, acc: 0.9696969985961914)
[2024-12-12 02:51:37,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:37,534][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.12552210688591003, acc: 0.9259259104728699)
[2024-12-12 02:51:37,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:37,893][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.5481341481208801, acc: 0.8500000238418579)
[2024-12-12 02:51:37,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:38,222][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.4872799515724182, acc: 0.800000011920929)
[2024-12-12 02:51:38,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:38,584][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.3218584954738617, acc: 0.8620689511299133)
[2024-12-12 02:51:38,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:38,878][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.042382773011922836, acc: 0.9677419066429138)
[2024-12-12 02:51:38,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:39,193][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.008628450334072113, acc: 1.0)
[2024-12-12 02:51:39,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:39,517][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.03157105669379234, acc: 1.0)
[2024-12-12 02:51:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:39,896][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.3287016749382019, acc: 0.9047619104385376)
[2024-12-12 02:51:40,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:40,249][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.07671812176704407, acc: 0.9545454382896423)
[2024-12-12 02:51:40,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:40,594][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.7403988242149353, acc: 0.7692307829856873)
[2024-12-12 02:51:40,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:40,891][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.229715958237648, acc: 0.8999999761581421)
[2024-12-12 02:51:40,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:41,216][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.1750449538230896, acc: 0.931034505367279)
[2024-12-12 02:51:41,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:41,551][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.1748082935810089, acc: 0.9215686321258545)
[2024-12-12 02:51:41,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:41,924][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.33949005603790283, acc: 0.8965517282485962)
[2024-12-12 02:51:42,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:42,260][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.15429361164569855, acc: 0.9473684430122375)
[2024-12-12 02:51:42,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:42,620][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.26338639855384827, acc: 0.8947368264198303)
[2024-12-12 02:51:42,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:43,041][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.43997225165367126, acc: 0.8928571343421936)
[2024-12-12 02:51:43,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:43,462][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.2262437790632248, acc: 0.9438202381134033)
[2024-12-12 02:51:43,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:43,861][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.5754879713058472, acc: 0.8426966071128845)
[2024-12-12 02:51:43,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:44,245][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.8543969988822937, acc: 0.7446808218955994)
[2024-12-12 02:51:44,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:44,594][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.41282910108566284, acc: 0.8478260636329651)
[2024-12-12 02:51:44,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:44,941][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.24499136209487915, acc: 0.8799999952316284)
[2024-12-12 02:51:45,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:45,268][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.005091969855129719, acc: 1.0)
[2024-12-12 02:51:45,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:45,604][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.10381938517093658, acc: 0.9629629850387573)
[2024-12-12 02:51:45,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:45,942][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.11642154306173325, acc: 0.9629629850387573)
[2024-12-12 02:51:46,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:46,321][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.19835001230239868, acc: 0.9056603908538818)
[2024-12-12 02:51:46,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:46,696][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.8340726494789124, acc: 0.7931034564971924)
[2024-12-12 02:51:46,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:47,277][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 1.1597182750701904, acc: 0.7117117047309875)
[2024-12-12 02:51:47,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:47,756][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.38371291756629944, acc: 0.9295774698257446)
[2024-12-12 02:51:47,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:48,101][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.012458473443984985, acc: 1.0)
[2024-12-12 02:51:48,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:48,431][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.04345574602484703, acc: 0.9666666388511658)
[2024-12-12 02:51:48,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:48,740][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.06312759220600128, acc: 0.9615384340286255)
[2024-12-12 02:51:50,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:51,561][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.9809988737106323, acc: 0.7357142567634583)
[2024-12-12 02:51:51,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:52,317][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.3468452990055084, acc: 0.8650793433189392)
[2024-12-12 02:51:52,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:52,586][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.3313957154750824, acc: 0.8928571343421936)
[2024-12-12 02:51:52,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:52,979][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.08946622163057327, acc: 0.949999988079071)
[2024-12-12 02:51:53,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:53,679][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.33636942505836487, acc: 0.9027777910232544)
[2024-12-12 02:51:53,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:53,983][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.003018720541149378, acc: 1.0)
[2024-12-12 02:51:54,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:54,283][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.03753183037042618, acc: 1.0)
[2024-12-12 02:51:54,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:54,596][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.08590491861104965, acc: 0.949999988079071)
[2024-12-12 02:51:54,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:54,959][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.43732163310050964, acc: 0.8888888955116272)
[2024-12-12 02:51:55,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:55,997][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.7697697877883911, acc: 0.7923728823661804)
[2024-12-12 02:51:56,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:56,317][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.2160402089357376, acc: 0.9552238583564758)
[2024-12-12 02:51:56,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:56,675][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.3216066360473633, acc: 0.8905109763145447)
[2024-12-12 02:51:56,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:57,231][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.6098048090934753, acc: 0.8199999928474426)
[2024-12-12 02:51:57,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:57,585][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.04031222686171532, acc: 1.0)
[2024-12-12 02:51:57,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:57,975][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.13215039670467377, acc: 0.9615384340286255)
[2024-12-12 02:51:58,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:58,353][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.10862164199352264, acc: 0.9523809552192688)
[2024-12-12 02:51:58,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:58,707][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.8457116484642029, acc: 0.7704917788505554)
[2024-12-12 02:51:58,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:59,027][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.09488102793693542, acc: 0.9830508232116699)
[2024-12-12 02:51:59,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:59,321][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.4476015865802765, acc: 0.8604651093482971)
[2024-12-12 02:51:59,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:59,592][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.3249908983707428, acc: 0.9090909361839294)
[2024-12-12 02:51:59,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:59,940][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.5410935282707214, acc: 0.8679245114326477)
[2024-12-12 02:52:00,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:00,316][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.24670611321926117, acc: 0.9318181872367859)
[2024-12-12 02:52:00,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:00,694][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.07150014489889145, acc: 1.0)
[2024-12-12 02:52:00,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:01,072][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.07273723185062408, acc: 0.949999988079071)
[2024-12-12 02:52:01,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:01,435][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.022408025339245796, acc: 1.0)
[2024-12-12 02:52:01,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:01,824][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.22576549649238586, acc: 0.892307698726654)
[2024-12-12 02:52:01,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:02,177][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.35395440459251404, acc: 0.890625)
[2024-12-12 02:52:02,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:02,542][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.26584362983703613, acc: 0.875)
[2024-12-12 02:52:02,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:02,862][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.23129040002822876, acc: 0.939393937587738)
[2024-12-12 02:52:02,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:03,220][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.029400458559393883, acc: 1.0)
[2024-12-12 02:52:03,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:03,589][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.09203103929758072, acc: 0.9677419066429138)
[2024-12-12 02:52:03,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:03,964][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.0018238772172480822, acc: 1.0)
[2024-12-12 02:52:04,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:04,367][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.057174816727638245, acc: 0.9666666388511658)
[2024-12-12 02:52:04,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:04,771][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.04433779790997505, acc: 0.9756097793579102)
[2024-12-12 02:52:04,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:05,123][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.0076528387144207954, acc: 1.0)
[2024-12-12 02:52:05,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:05,437][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.007079029455780983, acc: 1.0)
[2024-12-12 02:52:05,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:05,827][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.01714473031461239, acc: 1.0)
[2024-12-12 02:52:05,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:06,189][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.027968905866146088, acc: 1.0)
[2024-12-12 02:52:06,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:06,521][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.18992915749549866, acc: 0.9696969985961914)
[2024-12-12 02:52:06,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:06,898][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.09089673310518265, acc: 0.9750000238418579)
[2024-12-12 02:52:07,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:07,293][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.09426013380289078, acc: 0.9428571462631226)
[2024-12-12 02:52:07,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:07,642][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.3598312735557556, acc: 0.9051094651222229)
[2024-12-12 02:52:07,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:07,960][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.233876571059227, acc: 0.931034505367279)
[2024-12-12 02:52:08,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:08,275][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.307143896818161, acc: 0.9071428775787354)
[2024-12-12 02:52:08,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:08,628][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.3551889657974243, acc: 0.8807947039604187)
[2024-12-12 02:52:08,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:08,968][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.08863197267055511, acc: 0.9829059839248657)
[2024-12-12 02:52:09,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:09,306][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.029556045308709145, acc: 1.0)
[2024-12-12 02:52:09,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:09,638][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.15429289638996124, acc: 0.9230769276618958)
[2024-12-12 02:52:09,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:09,950][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.013105830177664757, acc: 1.0)
[2024-12-12 02:52:10,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:10,279][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.03176402300596237, acc: 1.0)
[2024-12-12 02:52:10,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:11,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:11,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:12,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:12,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:12,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:13,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:13,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:14,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:14,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:14,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:15,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:15,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:15,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:16,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:16,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:16,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:17,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:17,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:17,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:18,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:18,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:18,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:19,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:19,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:20,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:20,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:20,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:21,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:21,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:21,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:21,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:22,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:22,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:22,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:23,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:23,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:24,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:24,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:24,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:25,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:25,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:25,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:26,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:26,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:27,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:27,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:27,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:28,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:28,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:28,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:29,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:29,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:29,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:30,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:30,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:31,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:31,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:31,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:32,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:32,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:32,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:33,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:33,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:34,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:34,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:34,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:35,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:35,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:36,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:36,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:36,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:36,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:37,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:37,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:37,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:38,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:38,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:38,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:39,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:39,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:39,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:40,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:40,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:41,276][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5953, device='cuda:0') eval_epoch_loss=tensor(0.9537, device='cuda:0') eval_epoch_acc=tensor(0.7856, device='cuda:0')
[2024-12-12 02:52:41,277][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:52:41,277][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:52:41,537][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_6_step_562_loss_0.9537206292152405/model.pt
[2024-12-12 02:52:41,541][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:52:41,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:41,931][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.228070467710495, acc: 0.9222221970558167)
[2024-12-12 02:52:42,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:42,296][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.14834445714950562, acc: 0.9350649118423462)
[2024-12-12 02:52:42,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:42,666][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.06356430798768997, acc: 1.0)
[2024-12-12 02:52:42,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:42,987][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.13634297251701355, acc: 0.931034505367279)
[2024-12-12 02:52:43,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:43,324][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.11576712876558304, acc: 0.9642857313156128)
[2024-12-12 02:52:43,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:43,676][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.03600616380572319, acc: 1.0)
[2024-12-12 02:52:43,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:44,062][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.10955837368965149, acc: 0.9629629850387573)
[2024-12-12 02:52:44,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:44,436][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.25381824374198914, acc: 0.9197860956192017)
[2024-12-12 02:52:44,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:44,826][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.1039958968758583, acc: 0.9516128897666931)
[2024-12-12 02:52:44,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:45,195][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.20487891137599945, acc: 0.9316239356994629)
[2024-12-12 02:52:45,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:45,514][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.4679860770702362, acc: 0.8571428656578064)
[2024-12-12 02:52:45,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:45,854][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.3123827874660492, acc: 0.9119496941566467)
[2024-12-12 02:52:46,253][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.3544, train_epoch_loss=0.3033, epoch time 359.4133889749646s
[2024-12-12 02:52:46,254][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:52:46,254][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2024-12-12 02:52:46,254][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:52:46,254][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 18
[2024-12-12 02:52:46,254][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:52:46,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:47,140][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.07639007270336151, acc: 0.9629629850387573)
[2024-12-12 02:52:47,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:47,473][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.17424072325229645, acc: 0.9599999785423279)
[2024-12-12 02:52:47,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:47,817][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.15418019890785217, acc: 0.9729729890823364)
[2024-12-12 02:52:47,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:48,201][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.17585448920726776, acc: 0.9473684430122375)
[2024-12-12 02:52:48,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:48,592][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.10447663813829422, acc: 0.9729729890823364)
[2024-12-12 02:52:48,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:48,946][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.17167271673679352, acc: 0.9285714030265808)
[2024-12-12 02:52:49,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:49,264][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.15816354751586914, acc: 0.9795918464660645)
[2024-12-12 02:52:49,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:49,585][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.05462479218840599, acc: 1.0)
[2024-12-12 02:52:49,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:49,975][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.00469863461330533, acc: 1.0)
[2024-12-12 02:52:50,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:50,343][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.0701097920536995, acc: 0.9615384340286255)
[2024-12-12 02:52:50,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:50,710][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.08973764628171921, acc: 0.9629629850387573)
[2024-12-12 02:52:50,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:51,116][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.17939387261867523, acc: 0.8974359035491943)
[2024-12-12 02:52:51,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:51,487][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.04411713406443596, acc: 1.0)
[2024-12-12 02:52:51,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:51,877][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.061035167425870895, acc: 0.97826087474823)
[2024-12-12 02:52:51,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:52,277][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.24108055233955383, acc: 0.9411764740943909)
[2024-12-12 02:52:52,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:52,660][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.12375279515981674, acc: 0.9591836929321289)
[2024-12-12 02:52:52,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:52,998][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.18420319259166718, acc: 0.9473684430122375)
[2024-12-12 02:52:53,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:53,356][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.15544718503952026, acc: 0.9166666865348816)
[2024-12-12 02:52:53,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:53,728][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.15095281600952148, acc: 0.9722222089767456)
[2024-12-12 02:52:53,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:54,083][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.2036605328321457, acc: 0.9473684430122375)
[2024-12-12 02:52:54,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:54,448][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.04880691319704056, acc: 0.9615384340286255)
[2024-12-12 02:52:54,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:54,777][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.2948043942451477, acc: 0.8965517282485962)
[2024-12-12 02:52:54,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:55,143][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.014726389199495316, acc: 1.0)
[2024-12-12 02:52:55,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:55,525][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.09409958124160767, acc: 0.9523809552192688)
[2024-12-12 02:52:55,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:55,885][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.48749682307243347, acc: 0.9375)
[2024-12-12 02:52:56,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:56,255][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.1343960165977478, acc: 0.9811320900917053)
[2024-12-12 02:52:56,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:56,607][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.4008433222770691, acc: 0.8904109597206116)
[2024-12-12 02:52:57,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:57,871][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.8881169557571411, acc: 0.7588932514190674)
[2024-12-12 02:52:57,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:58,189][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.390956848859787, acc: 0.930232584476471)
[2024-12-12 02:52:58,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:58,547][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.24934837222099304, acc: 0.8674699068069458)
[2024-12-12 02:52:58,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:58,937][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.4205431044101715, acc: 0.8641975522041321)
[2024-12-12 02:52:59,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:59,333][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.05611201375722885, acc: 0.9642857313156128)
[2024-12-12 02:52:59,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:59,752][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.08602256327867508, acc: 0.9629629850387573)
[2024-12-12 02:52:59,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:00,121][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.24746444821357727, acc: 0.95652174949646)
[2024-12-12 02:53:00,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:00,468][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.3174774646759033, acc: 0.9075630307197571)
[2024-12-12 02:53:00,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:00,841][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.23573355376720428, acc: 0.9180327653884888)
[2024-12-12 02:53:00,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:01,223][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.2711544930934906, acc: 0.9365079402923584)
[2024-12-12 02:53:01,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:01,577][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.3164931535720825, acc: 0.8813559412956238)
[2024-12-12 02:53:01,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:01,964][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.2087346613407135, acc: 0.954023003578186)
[2024-12-12 02:53:02,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:02,325][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.06881258636713028, acc: 1.0)
[2024-12-12 02:53:02,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:02,691][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.07769445329904556, acc: 0.9615384340286255)
[2024-12-12 02:53:02,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:03,087][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.20490393042564392, acc: 0.9189189076423645)
[2024-12-12 02:53:03,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:03,429][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.4568272829055786, acc: 0.8769230842590332)
[2024-12-12 02:53:03,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:03,841][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.3053220212459564, acc: 0.9090909361839294)
[2024-12-12 02:53:03,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:04,247][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.303340882062912, acc: 0.8969072103500366)
[2024-12-12 02:53:04,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:04,672][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.32935500144958496, acc: 0.8823529481887817)
[2024-12-12 02:53:04,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:05,043][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.21757367253303528, acc: 0.9230769276618958)
[2024-12-12 02:53:05,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:05,421][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.008547463454306126, acc: 1.0)
[2024-12-12 02:53:05,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:05,801][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.24234600365161896, acc: 0.9642857313156128)
[2024-12-12 02:53:05,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:06,192][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.033346664160490036, acc: 1.0)
[2024-12-12 02:53:06,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:06,583][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.3201039433479309, acc: 0.859649121761322)
[2024-12-12 02:53:06,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:06,980][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.25668615102767944, acc: 0.920634925365448)
[2024-12-12 02:53:07,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:07,356][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.3720186948776245, acc: 0.8591549396514893)
[2024-12-12 02:53:07,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:07,813][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 1.0309689044952393, acc: 0.6933333277702332)
[2024-12-12 02:53:07,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:08,187][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.18831008672714233, acc: 0.9729729890823364)
[2024-12-12 02:53:08,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:08,584][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.010011120699346066, acc: 1.0)
[2024-12-12 02:53:10,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:11,585][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.8621393442153931, acc: 0.7679181098937988)
[2024-12-12 02:53:12,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:12,881][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 1.2282311916351318, acc: 0.655773401260376)
[2024-12-12 02:53:13,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:13,503][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.7048683762550354, acc: 0.8295454382896423)
[2024-12-12 02:53:13,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:14,068][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.25503525137901306, acc: 0.9117646813392639)
[2024-12-12 02:53:14,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:14,625][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.6142063140869141, acc: 0.782608687877655)
[2024-12-12 02:53:14,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:15,029][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.4474089741706848, acc: 0.887499988079071)
[2024-12-12 02:53:15,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:15,435][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.2991720139980316, acc: 0.9411764740943909)
[2024-12-12 02:53:15,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:15,843][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.11594678461551666, acc: 0.9444444179534912)
[2024-12-12 02:53:15,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:16,204][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.3145407736301422, acc: 0.953125)
[2024-12-12 02:53:16,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:16,519][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.06549163907766342, acc: 1.0)
[2024-12-12 02:53:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:16,915][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.38632017374038696, acc: 0.9107142686843872)
[2024-12-12 02:53:17,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:17,315][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.3151428997516632, acc: 0.9333333373069763)
[2024-12-12 02:53:17,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:17,717][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.05972666293382645, acc: 0.9599999785423279)
[2024-12-12 02:53:17,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:18,119][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.2872124910354614, acc: 0.9166666865348816)
[2024-12-12 02:53:18,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:18,542][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.12613749504089355, acc: 0.9696969985961914)
[2024-12-12 02:53:18,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:18,975][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.7088433504104614, acc: 0.7352941036224365)
[2024-12-12 02:53:19,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:19,366][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.6671561598777771, acc: 0.8253968358039856)
[2024-12-12 02:53:19,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:19,750][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 1.0594526529312134, acc: 0.692307710647583)
[2024-12-12 02:53:19,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:20,124][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.6705492734909058, acc: 0.8163265585899353)
[2024-12-12 02:53:20,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:20,455][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.7057167887687683, acc: 0.8134328126907349)
[2024-12-12 02:53:20,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:20,842][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 1.220978021621704, acc: 0.6751824617385864)
[2024-12-12 02:53:20,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:21,204][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.019894635304808617, acc: 1.0)
[2024-12-12 02:53:21,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:21,581][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.11981263756752014, acc: 0.9166666865348816)
[2024-12-12 02:53:21,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:21,980][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.054508138447999954, acc: 1.0)
[2024-12-12 02:53:22,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:22,321][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.09135958552360535, acc: 0.9615384340286255)
[2024-12-12 02:53:22,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:22,646][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.15805020928382874, acc: 0.9615384340286255)
[2024-12-12 02:53:22,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:22,982][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.15027469396591187, acc: 0.9807692170143127)
[2024-12-12 02:53:23,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:23,367][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.0938214510679245, acc: 0.96875)
[2024-12-12 02:53:23,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:23,723][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.2168111354112625, acc: 0.9420289993286133)
[2024-12-12 02:53:23,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:24,104][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.2597196698188782, acc: 0.8999999761581421)
[2024-12-12 02:53:24,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:24,503][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.05514577776193619, acc: 1.0)
[2024-12-12 02:53:24,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:24,996][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.3969804346561432, acc: 0.8399999737739563)
[2024-12-12 02:53:25,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:25,419][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.5053015351295471, acc: 0.8252426981925964)
[2024-12-12 02:53:25,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:26,560][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.5604658126831055, acc: 0.8155339956283569)
[2024-12-12 02:53:26,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:27,382][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.9693921804428101, acc: 0.7311828136444092)
[2024-12-12 02:53:27,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:28,181][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.7131773829460144, acc: 0.806034505367279)
[2024-12-12 02:53:28,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:28,921][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.5149956345558167, acc: 0.8315789699554443)
[2024-12-12 02:53:29,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:29,911][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.7422857284545898, acc: 0.7920792102813721)
[2024-12-12 02:53:29,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:30,274][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.48360976576805115, acc: 0.8548387289047241)
[2024-12-12 02:53:30,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:30,676][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.4401606321334839, acc: 0.8550724387168884)
[2024-12-12 02:53:30,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:31,065][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.734618067741394, acc: 0.756302535533905)
[2024-12-12 02:53:31,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:31,484][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.6684086918830872, acc: 0.7884615659713745)
[2024-12-12 02:53:31,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:31,886][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.7933638095855713, acc: 0.7226277589797974)
[2024-12-12 02:53:31,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:32,269][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.3681963086128235, acc: 0.89552241563797)
[2024-12-12 02:53:32,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:32,654][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.15708445012569427, acc: 0.8999999761581421)
[2024-12-12 02:53:32,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:33,041][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.05444570630788803, acc: 1.0)
[2024-12-12 02:53:33,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:33,441][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.017229249700903893, acc: 1.0)
[2024-12-12 02:53:33,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:33,806][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.04296763613820076, acc: 0.9772727489471436)
[2024-12-12 02:53:33,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:34,195][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.1698443591594696, acc: 0.931034505367279)
[2024-12-12 02:53:34,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:34,539][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.043884534388780594, acc: 0.9767441749572754)
[2024-12-12 02:53:34,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:34,857][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.04841236770153046, acc: 1.0)
[2024-12-12 02:53:34,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:35,157][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.02577805519104004, acc: 1.0)
[2024-12-12 02:53:35,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:35,521][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.009452604688704014, acc: 1.0)
[2024-12-12 02:53:35,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:35,926][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.06812223047018051, acc: 0.9523809552192688)
[2024-12-12 02:53:36,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:36,269][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.08846665918827057, acc: 0.9846153855323792)
[2024-12-12 02:53:36,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:36,661][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.16769717633724213, acc: 0.9649122953414917)
[2024-12-12 02:53:36,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:37,058][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.41623207926750183, acc: 0.8947368264198303)
[2024-12-12 02:53:37,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:37,407][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.1976262629032135, acc: 0.9743589758872986)
[2024-12-12 02:53:37,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:37,770][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.19884994626045227, acc: 0.9387755393981934)
[2024-12-12 02:53:37,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:38,049][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.0019019346218556166, acc: 1.0)
[2024-12-12 02:53:38,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:38,434][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.3039613664150238, acc: 0.8888888955116272)
[2024-12-12 02:53:38,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:38,766][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.3066103160381317, acc: 0.9186992049217224)
[2024-12-12 02:53:38,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:39,125][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.08155660331249237, acc: 0.9838709831237793)
[2024-12-12 02:53:39,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:40,023][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.6689763069152832, acc: 0.802281379699707)
[2024-12-12 02:53:40,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:40,382][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.13734599947929382, acc: 0.9599999785423279)
[2024-12-12 02:53:40,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:40,782][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.3417982757091522, acc: 0.8846153616905212)
[2024-12-12 02:53:40,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:41,149][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.10074421763420105, acc: 0.9583333134651184)
[2024-12-12 02:53:41,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:41,554][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.045932479202747345, acc: 1.0)
[2024-12-12 02:53:41,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:41,940][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.6593760251998901, acc: 0.7852760553359985)
[2024-12-12 02:53:42,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:42,286][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.6749206185340881, acc: 0.7847222089767456)
[2024-12-12 02:53:42,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:42,635][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.6985545754432678, acc: 0.7583333253860474)
[2024-12-12 02:53:42,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:43,037][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.701799750328064, acc: 0.8154761791229248)
[2024-12-12 02:53:43,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:43,371][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.5314536094665527, acc: 0.8461538553237915)
[2024-12-12 02:53:43,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:43,768][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.5591943264007568, acc: 0.845588207244873)
[2024-12-12 02:53:43,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:44,120][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.114397794008255, acc: 0.9615384340286255)
[2024-12-12 02:53:44,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:45,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:45,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:45,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:46,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:46,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:47,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:47,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:47,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:48,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:48,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:48,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:49,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:49,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:49,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:50,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:50,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:50,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:51,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:51,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:52,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:52,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:52,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:53,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:53,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:54,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:54,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:54,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:55,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:55,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:55,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:56,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:56,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:56,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:57,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:57,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:57,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:58,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:58,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:58,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:59,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:59,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:59,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:00,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:00,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:01,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:01,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:01,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:02,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:02,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:02,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:03,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:03,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:03,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:04,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:04,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:04,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:05,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:05,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:05,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:06,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:06,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:06,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:07,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:07,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:07,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:08,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:08,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:09,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:09,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:09,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:10,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:10,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:10,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:11,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:11,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:11,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:12,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:12,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:12,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:13,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:13,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:13,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:14,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:14,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:15,112][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3213, device='cuda:0') eval_epoch_loss=tensor(0.8421, device='cuda:0') eval_epoch_acc=tensor(0.8076, device='cuda:0')
[2024-12-12 02:54:15,113][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:54:15,114][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:54:15,383][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_7_step_131_loss_0.842126190662384/model.pt
[2024-12-12 02:54:15,387][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:54:15,387][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 7 is 0.8075922727584839
[2024-12-12 02:54:15,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:15,743][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.1274540275335312, acc: 0.95652174949646)
[2024-12-12 02:54:15,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:16,119][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.07233130931854248, acc: 0.96875)
[2024-12-12 02:54:16,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:16,448][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.2018231749534607, acc: 0.9130434989929199)
[2024-12-12 02:54:16,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:16,803][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.13520127534866333, acc: 0.9428571462631226)
[2024-12-12 02:54:16,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:17,166][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.043304041028022766, acc: 1.0)
[2024-12-12 02:54:17,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:17,500][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.10098570585250854, acc: 0.976190447807312)
[2024-12-12 02:54:17,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:17,826][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.31679728627204895, acc: 0.8999999761581421)
[2024-12-12 02:54:17,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:18,158][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.054794907569885254, acc: 1.0)
[2024-12-12 02:54:18,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:18,523][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.024289162829518318, acc: 1.0)
[2024-12-12 02:54:18,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:18,883][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.1637042909860611, acc: 0.9615384340286255)
[2024-12-12 02:54:18,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:19,237][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.14223437011241913, acc: 0.9677419066429138)
[2024-12-12 02:54:19,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:19,569][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.09611097723245621, acc: 0.9729729890823364)
[2024-12-12 02:54:19,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:20,125][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.6684560179710388, acc: 0.7631579041481018)
[2024-12-12 02:54:20,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:20,494][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.5805611610412598, acc: 0.8283582329750061)
[2024-12-12 02:54:20,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:20,869][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.19620849192142487, acc: 0.9387755393981934)
[2024-12-12 02:54:20,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:21,303][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.6536498665809631, acc: 0.7553191781044006)
[2024-12-12 02:54:21,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:21,648][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.2353793829679489, acc: 0.9142857193946838)
[2024-12-12 02:54:21,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:22,000][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.33503347635269165, acc: 0.8214285969734192)
[2024-12-12 02:54:22,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:22,373][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.18573173880577087, acc: 0.9130434989929199)
[2024-12-12 02:54:22,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:22,756][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.07696464657783508, acc: 0.9655172228813171)
[2024-12-12 02:54:22,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:23,071][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.43753781914711, acc: 0.8913043737411499)
[2024-12-12 02:54:23,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:23,364][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.3978165090084076, acc: 0.8983050584793091)
[2024-12-12 02:54:23,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:23,691][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.42530325055122375, acc: 0.8947368264198303)
[2024-12-12 02:54:23,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:24,045][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.3889320492744446, acc: 0.8648648858070374)
[2024-12-12 02:54:24,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:24,433][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.08975517004728317, acc: 0.9642857313156128)
[2024-12-12 02:54:24,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:24,801][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.10474710166454315, acc: 0.95652174949646)
[2024-12-12 02:54:24,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:25,149][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.12631230056285858, acc: 1.0)
[2024-12-12 02:54:25,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:26,896][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.6213183403015137, acc: 0.7972972989082336)
[2024-12-12 02:54:26,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:27,214][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.7283198833465576, acc: 0.7777777910232544)
[2024-12-12 02:54:27,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:27,635][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.7579983472824097, acc: 0.7209302186965942)
[2024-12-12 02:54:27,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:28,221][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.8379324674606323, acc: 0.7529411911964417)
[2024-12-12 02:54:28,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:28,778][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.9212644696235657, acc: 0.7640449404716492)
[2024-12-12 02:54:28,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:29,169][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.21804828941822052, acc: 0.9318181872367859)
[2024-12-12 02:54:29,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:29,490][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.2479443997144699, acc: 0.9523809552192688)
[2024-12-12 02:54:29,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:29,786][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.23832528293132782, acc: 0.8965517282485962)
[2024-12-12 02:54:29,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:30,128][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.11816245317459106, acc: 0.9795918464660645)
[2024-12-12 02:54:30,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:30,472][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.22995871305465698, acc: 0.9399999976158142)
[2024-12-12 02:54:30,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:30,875][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.309627890586853, acc: 0.9444444179534912)
[2024-12-12 02:54:30,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:31,229][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.9934645891189575, acc: 0.7941176295280457)
[2024-12-12 02:54:31,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:32,321][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.8033686280250549, acc: 0.7465753555297852)
[2024-12-12 02:54:32,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:32,690][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.018909817561507225, acc: 1.0)
[2024-12-12 02:54:32,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:33,061][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.20782974362373352, acc: 0.9259259104728699)
[2024-12-12 02:54:33,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:33,422][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.15154306590557098, acc: 0.9642857313156128)
[2024-12-12 02:54:33,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:33,959][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.8563323616981506, acc: 0.76106196641922)
[2024-12-12 02:54:34,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:34,341][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.5367817282676697, acc: 0.8260869383811951)
[2024-12-12 02:54:34,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:34,750][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.2254016399383545, acc: 0.9318181872367859)
[2024-12-12 02:54:35,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:35,669][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.948228120803833, acc: 0.732824444770813)
[2024-12-12 02:54:35,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:36,335][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.8116322755813599, acc: 0.7851851582527161)
[2024-12-12 02:54:36,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:36,651][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.21175764501094818, acc: 0.9180327653884888)
[2024-12-12 02:54:36,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:36,993][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.0056542460806667805, acc: 1.0)
[2024-12-12 02:54:37,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:37,311][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.08620533347129822, acc: 0.9599999785423279)
[2024-12-12 02:54:37,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:37,667][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.02098160609602928, acc: 1.0)
[2024-12-12 02:54:37,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:38,052][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.20637840032577515, acc: 0.9634146094322205)
[2024-12-12 02:54:38,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:38,451][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.43916624784469604, acc: 0.8912386894226074)
[2024-12-12 02:54:38,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:38,780][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.6360906958580017, acc: 0.818443775177002)
[2024-12-12 02:54:38,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:39,265][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.5189318656921387, acc: 0.8218749761581421)
[2024-12-12 02:54:39,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:39,789][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.7724494338035583, acc: 0.7879924774169922)
[2024-12-12 02:54:39,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:40,177][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.5119831562042236, acc: 0.8576512336730957)
[2024-12-12 02:54:40,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:40,459][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.07681366801261902, acc: 0.9599999785423279)
[2024-12-12 02:54:40,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:41,002][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.6013849973678589, acc: 0.8372092843055725)
[2024-12-12 02:54:41,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:41,796][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.9725770950317383, acc: 0.682539701461792)
[2024-12-12 02:54:42,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:42,714][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.885878324508667, acc: 0.7348484992980957)
[2024-12-12 02:54:42,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:43,452][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.430255651473999, acc: 0.8705882430076599)
[2024-12-12 02:54:43,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:44,523][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.8853684663772583, acc: 0.7345678806304932)
[2024-12-12 02:54:44,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:45,472][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.25506874918937683, acc: 0.9193548560142517)
[2024-12-12 02:54:45,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:45,835][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.015744974836707115, acc: 1.0)
[2024-12-12 02:54:45,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:46,147][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.2519304156303406, acc: 0.8999999761581421)
[2024-12-12 02:54:46,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:46,521][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.2046506702899933, acc: 0.9264705777168274)
[2024-12-12 02:54:46,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:46,914][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.5627917647361755, acc: 0.8676470518112183)
[2024-12-12 02:54:47,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:47,289][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.40523073077201843, acc: 0.8898305296897888)
[2024-12-12 02:54:47,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:47,655][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.5323905348777771, acc: 0.8358209133148193)
[2024-12-12 02:54:47,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:48,004][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.5278975367546082, acc: 0.844660222530365)
[2024-12-12 02:54:48,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:48,348][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.24833807349205017, acc: 0.9365079402923584)
[2024-12-12 02:54:48,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:48,711][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.09568589925765991, acc: 0.9670329689979553)
[2024-12-12 02:54:48,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:49,114][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.2728497385978699, acc: 0.9058296084403992)
[2024-12-12 02:54:49,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:49,532][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.3975972533226013, acc: 0.874015748500824)
[2024-12-12 02:54:49,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:49,926][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.32586896419525146, acc: 0.9008620977401733)
[2024-12-12 02:54:50,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:50,338][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.30485624074935913, acc: 0.9239130616188049)
[2024-12-12 02:54:50,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:50,744][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.3439219892024994, acc: 0.8949416279792786)
[2024-12-12 02:54:50,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:51,159][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.2281123399734497, acc: 0.9347826242446899)
[2024-12-12 02:54:51,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:51,521][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.07187070697546005, acc: 0.95652174949646)
[2024-12-12 02:54:51,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:51,826][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.03271424397826195, acc: 1.0)
[2024-12-12 02:54:51,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:52,152][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.06079942733049393, acc: 0.978723406791687)
[2024-12-12 02:54:52,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:52,823][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.1556781381368637, acc: 0.9692307710647583)
[2024-12-12 02:54:52,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:53,147][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.06409452110528946, acc: 0.9594594836235046)
[2024-12-12 02:54:53,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:53,509][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.0824592188000679, acc: 0.9883720874786377)
[2024-12-12 02:54:53,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:54,038][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.15129253268241882, acc: 0.9459459185600281)
[2024-12-12 02:54:54,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:54,443][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.14032641053199768, acc: 0.9444444179534912)
[2024-12-12 02:54:54,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:54,802][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.09862213581800461, acc: 0.9696969985961914)
[2024-12-12 02:54:54,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:55,146][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.07046719640493393, acc: 0.9629629850387573)
[2024-12-12 02:54:55,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:55,481][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.305408239364624, acc: 0.9599999785423279)
[2024-12-12 02:54:55,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:55,838][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.2363673746585846, acc: 0.942307710647583)
[2024-12-12 02:54:56,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:56,614][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.42740198969841003, acc: 0.8695651888847351)
[2024-12-12 02:54:56,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:57,151][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.6436734199523926, acc: 0.8181818127632141)
[2024-12-12 02:54:57,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:57,581][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.5685734152793884, acc: 0.8510638475418091)
[2024-12-12 02:54:57,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:57,956][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.17699189484119415, acc: 0.9622641801834106)
[2024-12-12 02:54:58,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:58,290][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.18604400753974915, acc: 0.8833333253860474)
[2024-12-12 02:54:58,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:58,697][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.3147205412387848, acc: 0.9069767594337463)
[2024-12-12 02:54:58,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:59,107][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.3151771128177643, acc: 0.8999999761581421)
[2024-12-12 02:54:59,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:59,522][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 1.3566025495529175, acc: 0.6105263233184814)
[2024-12-12 02:54:59,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:59,863][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.8374693393707275, acc: 0.7333333492279053)
[2024-12-12 02:55:00,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:00,288][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 1.293462872505188, acc: 0.6388888955116272)
[2024-12-12 02:55:00,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:00,784][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 1.6833828687667847, acc: 0.5550458431243896)
[2024-12-12 02:55:00,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:01,250][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 1.2193727493286133, acc: 0.6384615302085876)
[2024-12-12 02:55:01,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:01,627][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.09040182828903198, acc: 0.9473684430122375)
[2024-12-12 02:55:01,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:02,001][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.07272060215473175, acc: 1.0)
[2024-12-12 02:55:02,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:02,345][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.24940313398838043, acc: 0.9545454382896423)
[2024-12-12 02:55:02,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:02,702][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.18351879715919495, acc: 1.0)
[2024-12-12 02:55:02,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:03,041][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.3050765097141266, acc: 0.8571428656578064)
[2024-12-12 02:55:03,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:03,375][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.5699066519737244, acc: 0.8181818127632141)
[2024-12-12 02:55:03,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:03,714][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.19305528700351715, acc: 0.9545454382896423)
[2024-12-12 02:55:03,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:04,289][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.782029926776886, acc: 0.7419354915618896)
[2024-12-12 02:55:04,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:04,813][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.7040446996688843, acc: 0.7954545617103577)
[2024-12-12 02:55:04,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:05,096][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.0041013723239302635, acc: 1.0)
[2024-12-12 02:55:05,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:05,438][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.07461045682430267, acc: 1.0)
[2024-12-12 02:55:05,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:05,772][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.026372961699962616, acc: 1.0)
[2024-12-12 02:55:05,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:06,082][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.04058174416422844, acc: 1.0)
[2024-12-12 02:55:06,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:06,430][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.0890955999493599, acc: 0.9459459185600281)
[2024-12-12 02:55:06,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:06,773][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.10012791305780411, acc: 0.9459459185600281)
[2024-12-12 02:55:06,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:07,160][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.020711177960038185, acc: 1.0)
[2024-12-12 02:55:07,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:07,546][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.13589230179786682, acc: 0.9558823704719543)
[2024-12-12 02:55:07,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:07,904][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.017355989664793015, acc: 1.0)
[2024-12-12 02:55:07,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:08,213][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.006027338095009327, acc: 1.0)
[2024-12-12 02:55:08,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:08,538][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.0022252481430768967, acc: 1.0)
[2024-12-12 02:55:08,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:08,894][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.1692170798778534, acc: 0.9677419066429138)
[2024-12-12 02:55:09,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:09,277][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.03547348082065582, acc: 1.0)
[2024-12-12 02:55:09,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:09,655][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.09527398645877838, acc: 0.9714285731315613)
[2024-12-12 02:55:09,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:10,035][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.18554063141345978, acc: 0.9605262875556946)
[2024-12-12 02:55:10,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:10,598][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.3974897265434265, acc: 0.9056603908538818)
[2024-12-12 02:55:10,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:11,186][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.3015355169773102, acc: 0.949999988079071)
[2024-12-12 02:55:11,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:11,557][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.11860035359859467, acc: 0.9166666865348816)
[2024-12-12 02:55:11,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:11,937][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.0968063473701477, acc: 0.9354838728904724)
[2024-12-12 02:55:12,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:12,345][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.4344283938407898, acc: 0.8666666746139526)
[2024-12-12 02:55:12,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:12,753][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.29258760809898376, acc: 0.9375)
[2024-12-12 02:55:13,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:13,629][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.8173493146896362, acc: 0.7360000014305115)
[2024-12-12 02:55:13,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:13,999][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.5170690417289734, acc: 0.8202247023582458)
[2024-12-12 02:55:14,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:14,347][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.26291701197624207, acc: 0.9054054021835327)
[2024-12-12 02:55:14,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:14,810][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.17263108491897583, acc: 0.9482758641242981)
[2024-12-12 02:55:14,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:15,108][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.00882241316139698, acc: 1.0)
[2024-12-12 02:55:15,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:15,448][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.12368381023406982, acc: 0.9545454382896423)
[2024-12-12 02:55:15,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:15,790][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.010211871936917305, acc: 1.0)
[2024-12-12 02:55:15,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:16,159][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.05075111240148544, acc: 0.9666666388511658)
[2024-12-12 02:55:16,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:16,568][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.1460217386484146, acc: 0.9666666388511658)
[2024-12-12 02:55:17,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:17,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:17,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:18,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:18,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:18,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:19,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:19,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:20,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:20,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:20,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:21,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:21,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:21,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:22,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:22,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:22,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:23,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:23,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:24,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:24,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:24,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:25,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:25,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:25,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:26,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:26,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:27,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:27,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:27,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:28,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:28,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:29,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:29,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:29,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:30,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:30,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:30,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:31,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:31,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:31,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:32,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:32,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:32,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:33,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:33,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:33,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:34,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:34,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:34,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:35,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:35,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:35,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:36,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:36,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:37,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:37,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:37,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:37,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:38,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:38,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:38,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:39,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:39,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:40,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:40,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:40,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:41,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:41,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:41,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:42,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:42,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:42,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:43,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:43,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:43,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:44,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:44,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:44,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:45,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:45,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:46,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:46,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:46,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:47,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:47,803][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4801, device='cuda:0') eval_epoch_loss=tensor(0.9083, device='cuda:0') eval_epoch_acc=tensor(0.7989, device='cuda:0')
[2024-12-12 02:55:47,805][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:55:47,805][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:55:48,046][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_7_step_274_loss_0.908292293548584/model.pt
[2024-12-12 02:55:48,049][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:55:48,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:48,398][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.0403544045984745, acc: 1.0)
[2024-12-12 02:55:48,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:48,766][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.031604792922735214, acc: 1.0)
[2024-12-12 02:55:48,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:49,126][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.15874701738357544, acc: 0.931034505367279)
[2024-12-12 02:55:49,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:49,530][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.021170401945710182, acc: 1.0)
[2024-12-12 02:55:49,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:49,890][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.4501290023326874, acc: 0.8723404407501221)
[2024-12-12 02:55:50,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:50,240][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.0363859198987484, acc: 1.0)
[2024-12-12 02:55:50,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:50,625][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.009155883453786373, acc: 1.0)
[2024-12-12 02:55:50,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:51,054][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.2572730779647827, acc: 0.9156626462936401)
[2024-12-12 02:55:51,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:51,438][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.7292934656143188, acc: 0.7870370149612427)
[2024-12-12 02:55:51,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:51,840][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.08172203600406647, acc: 0.9736841917037964)
[2024-12-12 02:55:51,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:52,236][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.025503870099782944, acc: 1.0)
[2024-12-12 02:55:52,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:52,603][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.06341834366321564, acc: 0.9750000238418579)
[2024-12-12 02:55:52,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:52,913][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.31539061665534973, acc: 0.90625)
[2024-12-12 02:55:53,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:53,284][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.26448479294776917, acc: 0.9359999895095825)
[2024-12-12 02:55:53,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:53,683][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.23462070524692535, acc: 0.9340659379959106)
[2024-12-12 02:55:53,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:54,020][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.3000269830226898, acc: 0.9130434989929199)
[2024-12-12 02:55:54,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:54,395][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.4523465633392334, acc: 0.8814433217048645)
[2024-12-12 02:55:54,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:54,757][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.01893521100282669, acc: 1.0)
[2024-12-12 02:55:54,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:55,099][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.273288369178772, acc: 0.9047619104385376)
[2024-12-12 02:55:55,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:55,428][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.25082817673683167, acc: 0.931034505367279)
[2024-12-12 02:55:55,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:55,879][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.16111969947814941, acc: 0.9454545378684998)
[2024-12-12 02:55:56,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:56,420][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.540805995464325, acc: 0.8092783689498901)
[2024-12-12 02:55:56,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:56,733][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.1822924017906189, acc: 0.9482758641242981)
[2024-12-12 02:55:56,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:57,109][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.07256311923265457, acc: 0.9629629850387573)
[2024-12-12 02:55:57,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:57,430][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.08714383840560913, acc: 0.9736841917037964)
[2024-12-12 02:55:57,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:57,734][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.048543237149715424, acc: 0.9821428656578064)
[2024-12-12 02:55:57,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:58,095][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.006900400388985872, acc: 1.0)
[2024-12-12 02:55:58,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:58,443][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.2672554552555084, acc: 0.9622641801834106)
[2024-12-12 02:55:58,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:58,825][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.019423261284828186, acc: 0.9811320900917053)
[2024-12-12 02:55:58,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:59,180][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.03656294569373131, acc: 0.970588207244873)
[2024-12-12 02:55:59,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:59,578][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.2618895173072815, acc: 0.90625)
[2024-12-12 02:55:59,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:59,979][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.1253746896982193, acc: 0.9508196711540222)
[2024-12-12 02:56:00,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:00,391][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.07235953956842422, acc: 0.9666666388511658)
[2024-12-12 02:56:00,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:00,783][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.032788608223199844, acc: 1.0)
[2024-12-12 02:56:00,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:01,161][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.14526529610157013, acc: 0.9420289993286133)
[2024-12-12 02:56:01,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:01,563][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.22785189747810364, acc: 0.8888888955116272)
[2024-12-12 02:56:01,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:01,947][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.14464963972568512, acc: 0.9759036302566528)
[2024-12-12 02:56:02,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:02,324][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.11940091848373413, acc: 0.9743589758872986)
[2024-12-12 02:56:02,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:02,718][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.23602008819580078, acc: 0.9081632494926453)
[2024-12-12 02:56:02,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:03,039][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.010482355952262878, acc: 1.0)
[2024-12-12 02:56:03,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:03,340][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.03167329728603363, acc: 1.0)
[2024-12-12 02:56:03,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:03,641][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.13092149794101715, acc: 0.9354838728904724)
[2024-12-12 02:56:03,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:03,929][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.269226998090744, acc: 0.9354838728904724)
[2024-12-12 02:56:04,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:04,244][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.13577653467655182, acc: 0.9402984976768494)
[2024-12-12 02:56:04,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:04,554][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.11450771987438202, acc: 0.9615384340286255)
[2024-12-12 02:56:04,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:04,848][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.16747444868087769, acc: 0.9333333373069763)
[2024-12-12 02:56:04,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:05,193][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.05971423536539078, acc: 0.9838709831237793)
[2024-12-12 02:56:05,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:05,585][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.012720719911158085, acc: 1.0)
[2024-12-12 02:56:05,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:05,926][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.31601640582084656, acc: 0.9259259104728699)
[2024-12-12 02:56:06,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:06,264][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.37427619099617004, acc: 0.8571428656578064)
[2024-12-12 02:56:06,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:06,580][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.4998478591442108, acc: 0.8461538553237915)
[2024-12-12 02:56:06,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:06,946][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.8606960773468018, acc: 0.707317054271698)
[2024-12-12 02:56:07,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:07,286][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.15379337966442108, acc: 0.9736841917037964)
[2024-12-12 02:56:07,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:07,635][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.03776026889681816, acc: 1.0)
[2024-12-12 02:56:07,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:07,960][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.01418010052293539, acc: 1.0)
[2024-12-12 02:56:08,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:08,285][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.005903228651732206, acc: 1.0)
[2024-12-12 02:56:08,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:08,659][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.020050300285220146, acc: 1.0)
[2024-12-12 02:56:08,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:09,025][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.11774275451898575, acc: 0.9516128897666931)
[2024-12-12 02:56:09,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:09,375][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.026207707822322845, acc: 1.0)
[2024-12-12 02:56:09,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:09,686][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.3871779143810272, acc: 0.875)
[2024-12-12 02:56:09,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:10,063][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.022793259471654892, acc: 1.0)
[2024-12-12 02:56:10,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:10,440][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.2221360206604004, acc: 0.8947368264198303)
[2024-12-12 02:56:10,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:10,813][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.3233913779258728, acc: 0.9399999976158142)
[2024-12-12 02:56:10,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:11,166][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.7511609196662903, acc: 0.7241379022598267)
[2024-12-12 02:56:11,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:11,536][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.9465379118919373, acc: 0.7340425252914429)
[2024-12-12 02:56:11,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:11,914][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.6510378122329712, acc: 0.759036123752594)
[2024-12-12 02:56:12,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:12,249][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.019366465508937836, acc: 1.0)
[2024-12-12 02:56:12,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:12,656][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.020762333646416664, acc: 1.0)
[2024-12-12 02:56:12,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:13,027][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.3167945146560669, acc: 0.9277108311653137)
[2024-12-12 02:56:13,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:13,359][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.5297638773918152, acc: 0.849056601524353)
[2024-12-12 02:56:13,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:13,695][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.08670239895582199, acc: 0.9746835231781006)
[2024-12-12 02:56:13,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:14,039][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.03543504327535629, acc: 0.9803921580314636)
[2024-12-12 02:56:14,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:14,356][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.15101802349090576, acc: 0.9701492786407471)
[2024-12-12 02:56:14,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:14,732][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.07189380377531052, acc: 0.949999988079071)
[2024-12-12 02:56:14,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:15,088][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.022054286673665047, acc: 1.0)
[2024-12-12 02:56:15,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:15,486][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.31029942631721497, acc: 0.8611111044883728)
[2024-12-12 02:56:15,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:15,888][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.2541320025920868, acc: 0.9069767594337463)
[2024-12-12 02:56:16,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:16,256][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.18663768470287323, acc: 0.9487179517745972)
[2024-12-12 02:56:16,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:16,621][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.3502138555049896, acc: 0.8666666746139526)
[2024-12-12 02:56:16,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:17,014][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.01435787882655859, acc: 1.0)
[2024-12-12 02:56:17,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:17,398][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.16222889721393585, acc: 0.9615384340286255)
[2024-12-12 02:56:17,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:17,752][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.29376664757728577, acc: 0.8791208863258362)
[2024-12-12 02:56:17,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:18,243][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.31974121928215027, acc: 0.895652174949646)
[2024-12-12 02:56:18,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:18,612][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.24726523458957672, acc: 0.8695651888847351)
[2024-12-12 02:56:18,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:18,997][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.09895233064889908, acc: 0.9795918464660645)
[2024-12-12 02:56:19,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:19,356][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.0019773610401898623, acc: 1.0)
[2024-12-12 02:56:19,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:19,680][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.023074278607964516, acc: 1.0)
[2024-12-12 02:56:19,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:20,042][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.20261038839817047, acc: 0.9024389982223511)
[2024-12-12 02:56:20,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:20,384][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.055270031094551086, acc: 0.9777777791023254)
[2024-12-12 02:56:20,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:20,690][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.11126859486103058, acc: 0.9605262875556946)
[2024-12-12 02:56:20,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:21,037][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.030526673421263695, acc: 1.0)
[2024-12-12 02:56:21,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:21,420][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.01812364161014557, acc: 1.0)
[2024-12-12 02:56:21,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:21,758][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.0037962242495268583, acc: 1.0)
[2024-12-12 02:56:21,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:22,129][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.024856355041265488, acc: 1.0)
[2024-12-12 02:56:22,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:22,534][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.07102470099925995, acc: 0.9642857313156128)
[2024-12-12 02:56:22,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:22,919][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.08099491894245148, acc: 0.96875)
[2024-12-12 02:56:23,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:23,527][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.5360413789749146, acc: 0.8060606122016907)
[2024-12-12 02:56:23,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:24,455][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.35777419805526733, acc: 0.8867924809455872)
[2024-12-12 02:56:24,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:24,834][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.17343445122241974, acc: 0.9666666388511658)
[2024-12-12 02:56:24,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:25,172][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.15923786163330078, acc: 0.9285714030265808)
[2024-12-12 02:56:25,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:25,557][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.037805184721946716, acc: 1.0)
[2024-12-12 02:56:25,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:25,892][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.011198521591722965, acc: 1.0)
[2024-12-12 02:56:25,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:26,225][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.13797935843467712, acc: 0.95652174949646)
[2024-12-12 02:56:26,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:26,634][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.03288945555686951, acc: 0.9791666865348816)
[2024-12-12 02:56:26,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:27,042][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.03527023643255234, acc: 0.9894737005233765)
[2024-12-12 02:56:27,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:27,633][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.2556527554988861, acc: 0.9341317415237427)
[2024-12-12 02:56:27,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:28,070][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.2810479998588562, acc: 0.932330846786499)
[2024-12-12 02:56:28,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:29,161][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.4355994760990143, acc: 0.8716577291488647)
[2024-12-12 02:56:29,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:29,720][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.0660359337925911, acc: 0.9819819927215576)
[2024-12-12 02:56:29,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:30,005][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.04029809311032295, acc: 1.0)
[2024-12-12 02:56:30,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:30,312][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.06373865902423859, acc: 0.9642857313156128)
[2024-12-12 02:56:30,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:30,621][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.019893676042556763, acc: 1.0)
[2024-12-12 02:56:30,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:30,970][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.012396630831062794, acc: 1.0)
[2024-12-12 02:56:31,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:31,336][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.050587788224220276, acc: 0.9736841917037964)
[2024-12-12 02:56:31,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:31,692][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.014221343211829662, acc: 1.0)
[2024-12-12 02:56:31,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:32,067][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.0034431989770382643, acc: 1.0)
[2024-12-12 02:56:32,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:32,440][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.07493442296981812, acc: 0.9523809552192688)
[2024-12-12 02:56:32,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:32,766][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.4085768163204193, acc: 0.8518518805503845)
[2024-12-12 02:56:32,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:33,144][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.5174762010574341, acc: 0.8640776872634888)
[2024-12-12 02:56:33,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:33,668][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.4596399664878845, acc: 0.845588207244873)
[2024-12-12 02:56:33,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:34,054][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.5115166306495667, acc: 0.8199999928474426)
[2024-12-12 02:56:34,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:34,445][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.4187512695789337, acc: 0.875)
[2024-12-12 02:56:34,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:34,829][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.02296740934252739, acc: 1.0)
[2024-12-12 02:56:34,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:35,205][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.01738211326301098, acc: 1.0)
[2024-12-12 02:56:35,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:35,595][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.11485254019498825, acc: 0.930232584476471)
[2024-12-12 02:56:35,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:35,973][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.0342746265232563, acc: 0.9599999785423279)
[2024-12-12 02:56:36,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:36,501][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.11820998042821884, acc: 0.9852941036224365)
[2024-12-12 02:56:36,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:36,814][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.13879023492336273, acc: 0.9599999785423279)
[2024-12-12 02:56:36,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:37,171][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.41060909628868103, acc: 0.939393937587738)
[2024-12-12 02:56:37,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:37,535][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.11959077417850494, acc: 0.939393937587738)
[2024-12-12 02:56:37,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:37,910][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.012319905683398247, acc: 1.0)
[2024-12-12 02:56:38,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:38,268][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.013370276428759098, acc: 1.0)
[2024-12-12 02:56:38,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:38,613][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.21574804186820984, acc: 0.9200000166893005)
[2024-12-12 02:56:38,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:38,896][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.0475907102227211, acc: 1.0)
[2024-12-12 02:56:38,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:39,163][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.029191864654421806, acc: 1.0)
[2024-12-12 02:56:39,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:39,449][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.006191595457494259, acc: 1.0)
[2024-12-12 02:56:39,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:39,818][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.1713191568851471, acc: 0.9655172228813171)
[2024-12-12 02:56:39,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:40,191][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.011637953110039234, acc: 1.0)
[2024-12-12 02:56:40,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:40,539][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.1259385049343109, acc: 0.9666666388511658)
[2024-12-12 02:56:40,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:40,905][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.017393097281455994, acc: 1.0)
[2024-12-12 02:56:40,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:41,271][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.0185345858335495, acc: 1.0)
[2024-12-12 02:56:41,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:41,656][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.17975600063800812, acc: 0.9411764740943909)
[2024-12-12 02:56:41,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:42,051][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.14291413128376007, acc: 0.9615384340286255)
[2024-12-12 02:56:42,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:43,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:43,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:43,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:44,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:44,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:44,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:45,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:45,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:45,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:46,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:46,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:47,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:47,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:47,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:48,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:48,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:48,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:49,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:49,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:49,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:50,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:50,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:50,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:51,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:51,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:52,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:52,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:52,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:52,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:53,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:53,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:54,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:54,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:54,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:55,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:55,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:55,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:56,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:56,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:56,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:57,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:57,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:57,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:58,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:58,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:58,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:59,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:59,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:59,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:00,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:00,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:00,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:01,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:01,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:01,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:02,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:02,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:02,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:03,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:03,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:03,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:04,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:04,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:05,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:05,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:05,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:06,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:06,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:06,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:07,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:07,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:07,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:08,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:08,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:09,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:09,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:09,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:10,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:10,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:10,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:11,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:11,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:11,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:12,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:12,906][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5415, device='cuda:0') eval_epoch_loss=tensor(0.9328, device='cuda:0') eval_epoch_acc=tensor(0.7884, device='cuda:0')
[2024-12-12 02:57:12,908][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:57:12,909][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:57:13,209][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_7_step_417_loss_0.9327538013458252/model.pt
[2024-12-12 02:57:13,214][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:57:13,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:13,645][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.04003826528787613, acc: 1.0)
[2024-12-12 02:57:13,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:14,030][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.029534125700592995, acc: 1.0)
[2024-12-12 02:57:14,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:14,426][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.13477925956249237, acc: 0.949999988079071)
[2024-12-12 02:57:14,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:14,775][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.00975237600505352, acc: 1.0)
[2024-12-12 02:57:14,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:15,110][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.0950976088643074, acc: 0.9666666388511658)
[2024-12-12 02:57:15,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:15,448][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.04453280195593834, acc: 1.0)
[2024-12-12 02:57:15,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:15,817][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.043453555554151535, acc: 1.0)
[2024-12-12 02:57:15,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:16,139][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.09099000692367554, acc: 0.9629629850387573)
[2024-12-12 02:57:16,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:16,446][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.0463726744055748, acc: 1.0)
[2024-12-12 02:57:16,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:16,800][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.0016927218530327082, acc: 1.0)
[2024-12-12 02:57:16,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:17,159][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.13320542871952057, acc: 0.9459459185600281)
[2024-12-12 02:57:17,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:17,507][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.05706598609685898, acc: 0.9629629850387573)
[2024-12-12 02:57:17,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:17,863][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.07709723711013794, acc: 0.95652174949646)
[2024-12-12 02:57:17,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:18,225][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.035163991153240204, acc: 0.9629629850387573)
[2024-12-12 02:57:18,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:18,533][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.03794805705547333, acc: 1.0)
[2024-12-12 02:57:18,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:18,893][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.04114186763763428, acc: 1.0)
[2024-12-12 02:57:19,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:19,250][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.035070184618234634, acc: 0.9722222089767456)
[2024-12-12 02:57:19,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:19,554][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.0043123080395162106, acc: 1.0)
[2024-12-12 02:57:19,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:19,890][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.07887902110815048, acc: 0.9696969985961914)
[2024-12-12 02:57:20,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:20,274][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.3273550868034363, acc: 0.9166666865348816)
[2024-12-12 02:57:20,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:20,623][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.08730212599039078, acc: 0.9772727489471436)
[2024-12-12 02:57:20,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:20,994][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.002958215307444334, acc: 1.0)
[2024-12-12 02:57:21,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:21,399][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.27356868982315063, acc: 0.9743589758872986)
[2024-12-12 02:57:21,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:21,881][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.25965073704719543, acc: 0.9090909361839294)
[2024-12-12 02:57:22,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:22,624][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.557771623134613, acc: 0.800000011920929)
[2024-12-12 02:57:22,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:23,020][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.5505216121673584, acc: 0.8387096524238586)
[2024-12-12 02:57:23,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:23,667][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.4656260013580322, acc: 0.8855721354484558)
[2024-12-12 02:57:23,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:23,976][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.17230960726737976, acc: 0.9056603908538818)
[2024-12-12 02:57:24,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:24,390][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.054449960589408875, acc: 1.0)
[2024-12-12 02:57:24,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:24,688][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.03288903087377548, acc: 1.0)
[2024-12-12 02:57:24,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:25,037][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.03481215611100197, acc: 1.0)
[2024-12-12 02:57:25,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:25,402][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.08840427547693253, acc: 0.9642857313156128)
[2024-12-12 02:57:25,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:25,768][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.1071840226650238, acc: 0.9552238583564758)
[2024-12-12 02:57:25,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:26,114][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.056130096316337585, acc: 0.9722222089767456)
[2024-12-12 02:57:26,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:26,509][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.09057677537202835, acc: 0.967391312122345)
[2024-12-12 02:57:26,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:26,885][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.23107868432998657, acc: 0.9230769276618958)
[2024-12-12 02:57:26,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:27,222][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.06574006378650665, acc: 0.9868420958518982)
[2024-12-12 02:57:27,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:27,593][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.03956812620162964, acc: 1.0)
[2024-12-12 02:57:27,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:27,975][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.07143453508615494, acc: 0.9696969985961914)
[2024-12-12 02:57:28,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:28,360][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.3386574685573578, acc: 0.8659793734550476)
[2024-12-12 02:57:28,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:28,714][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.09373782575130463, acc: 0.9571428298950195)
[2024-12-12 02:57:28,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:29,089][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.2955979108810425, acc: 0.9127907156944275)
[2024-12-12 02:57:29,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:29,451][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.1614687591791153, acc: 0.9464285969734192)
[2024-12-12 02:57:29,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:29,799][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.08584200590848923, acc: 0.9629629850387573)
[2024-12-12 02:57:29,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:30,170][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.1260429322719574, acc: 0.9722222089767456)
[2024-12-12 02:57:30,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:30,545][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.06978454440832138, acc: 0.96875)
[2024-12-12 02:57:30,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:30,943][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.07139658182859421, acc: 0.9615384340286255)
[2024-12-12 02:57:31,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:31,345][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.13560831546783447, acc: 0.9347826242446899)
[2024-12-12 02:57:31,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:31,743][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.18661685287952423, acc: 0.9404761791229248)
[2024-12-12 02:57:31,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:32,070][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.4954243302345276, acc: 0.8554216623306274)
[2024-12-12 02:57:32,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:32,470][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.12544070184230804, acc: 0.9639639854431152)
[2024-12-12 02:57:32,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:32,866][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.42315495014190674, acc: 0.893203854560852)
[2024-12-12 02:57:32,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:33,207][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.34610530734062195, acc: 0.9024389982223511)
[2024-12-12 02:57:33,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:33,588][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.011076666414737701, acc: 1.0)
[2024-12-12 02:57:33,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:33,954][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.04304850101470947, acc: 0.9642857313156128)
[2024-12-12 02:57:34,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:34,351][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.24764342606067657, acc: 0.9117646813392639)
[2024-12-12 02:57:34,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:34,725][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.5169571042060852, acc: 0.8253275156021118)
[2024-12-12 02:57:34,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:35,105][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.16276530921459198, acc: 0.9375)
[2024-12-12 02:57:35,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:35,482][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.2780628204345703, acc: 0.9018405079841614)
[2024-12-12 02:57:35,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:35,863][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.2181810885667801, acc: 0.9280575513839722)
[2024-12-12 02:57:35,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:36,267][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.3941326141357422, acc: 0.8994975090026855)
[2024-12-12 02:57:36,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:36,602][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.06789091229438782, acc: 0.9722222089767456)
[2024-12-12 02:57:36,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:36,985][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.08247337490320206, acc: 0.9696969985961914)
[2024-12-12 02:57:37,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:37,333][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.09879239648580551, acc: 0.9629629850387573)
[2024-12-12 02:57:37,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:37,689][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.13605546951293945, acc: 0.949999988079071)
[2024-12-12 02:57:37,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:38,068][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.20349952578544617, acc: 0.949999988079071)
[2024-12-12 02:57:38,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:38,434][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.21083050966262817, acc: 0.9655172228813171)
[2024-12-12 02:57:38,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:38,795][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.13507886230945587, acc: 0.9354838728904724)
[2024-12-12 02:57:38,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:39,139][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.2558983862400055, acc: 0.9473684430122375)
[2024-12-12 02:57:39,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:39,465][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.25999686121940613, acc: 0.8888888955116272)
[2024-12-12 02:57:39,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:39,830][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.6316061615943909, acc: 0.8571428656578064)
[2024-12-12 02:57:39,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:40,184][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.3384251892566681, acc: 0.9090909361839294)
[2024-12-12 02:57:40,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:40,592][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.40241366624832153, acc: 0.8461538553237915)
[2024-12-12 02:57:40,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:40,994][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.08261919021606445, acc: 0.9666666388511658)
[2024-12-12 02:57:41,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:41,376][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.0793047547340393, acc: 1.0)
[2024-12-12 02:57:41,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:41,713][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.11319242417812347, acc: 0.9607843160629272)
[2024-12-12 02:57:41,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:42,078][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.327710896730423, acc: 0.8965517282485962)
[2024-12-12 02:57:42,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:42,393][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.1197289451956749, acc: 0.9473684430122375)
[2024-12-12 02:57:42,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:42,699][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.05889229476451874, acc: 1.0)
[2024-12-12 02:57:42,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:43,086][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.4879206120967865, acc: 0.8660714030265808)
[2024-12-12 02:57:43,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:43,513][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.3407920002937317, acc: 0.8651685118675232)
[2024-12-12 02:57:43,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:43,898][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.3359769880771637, acc: 0.9101123809814453)
[2024-12-12 02:57:44,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:44,285][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.9235039949417114, acc: 0.695035457611084)
[2024-12-12 02:57:44,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:44,646][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.28415700793266296, acc: 0.9021739363670349)
[2024-12-12 02:57:44,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:44,987][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.060582295060157776, acc: 0.9599999785423279)
[2024-12-12 02:57:45,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:45,319][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.20006126165390015, acc: 0.9230769276618958)
[2024-12-12 02:57:45,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:45,658][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.05373021587729454, acc: 0.9629629850387573)
[2024-12-12 02:57:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:46,027][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.01473658811300993, acc: 1.0)
[2024-12-12 02:57:46,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:46,324][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.31512966752052307, acc: 0.9056603908538818)
[2024-12-12 02:57:46,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:46,691][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.5400211215019226, acc: 0.8275862336158752)
[2024-12-12 02:57:46,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:47,306][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.7401320934295654, acc: 0.8018018007278442)
[2024-12-12 02:57:47,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:47,749][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.2958126664161682, acc: 0.9295774698257446)
[2024-12-12 02:57:47,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:48,094][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.011937422677874565, acc: 1.0)
[2024-12-12 02:57:48,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:48,431][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.06111880764365196, acc: 0.9666666388511658)
[2024-12-12 02:57:48,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:48,769][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.06124674901366234, acc: 0.9615384340286255)
[2024-12-12 02:57:50,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:51,835][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.7352372407913208, acc: 0.8071428537368774)
[2024-12-12 02:57:52,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:52,619][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.21407616138458252, acc: 0.9285714030265808)
[2024-12-12 02:57:52,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:52,970][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.065799780189991, acc: 1.0)
[2024-12-12 02:57:53,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:53,341][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.059627778828144073, acc: 0.9833333492279053)
[2024-12-12 02:57:53,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:54,082][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.17490524053573608, acc: 0.9444444179534912)
[2024-12-12 02:57:54,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:54,466][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.0018652355065569282, acc: 1.0)
[2024-12-12 02:57:54,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:54,817][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.03875787556171417, acc: 1.0)
[2024-12-12 02:57:54,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:55,157][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.050910770893096924, acc: 1.0)
[2024-12-12 02:57:55,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:55,483][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.20067176222801208, acc: 0.9629629850387573)
[2024-12-12 02:57:55,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:56,547][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.6906790137290955, acc: 0.8093220591545105)
[2024-12-12 02:57:56,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:56,952][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.19552618265151978, acc: 0.9253731369972229)
[2024-12-12 02:57:57,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:57,369][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.20239228010177612, acc: 0.9270073175430298)
[2024-12-12 02:57:57,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:57,926][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.4115261733531952, acc: 0.8799999952316284)
[2024-12-12 02:57:57,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:58,236][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.016098197549581528, acc: 1.0)
[2024-12-12 02:57:58,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:58,539][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.07133021205663681, acc: 1.0)
[2024-12-12 02:57:58,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:58,907][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.03104044497013092, acc: 1.0)
[2024-12-12 02:57:59,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:59,287][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.7835103869438171, acc: 0.7540983557701111)
[2024-12-12 02:57:59,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:59,648][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.2052040547132492, acc: 0.9322034120559692)
[2024-12-12 02:57:59,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:59,981][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.42088109254837036, acc: 0.8604651093482971)
[2024-12-12 02:58:00,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:00,293][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.25630104541778564, acc: 0.8863636255264282)
[2024-12-12 02:58:00,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:00,623][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.24527069926261902, acc: 0.9245283007621765)
[2024-12-12 02:58:00,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:00,987][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.08590810745954514, acc: 0.9772727489471436)
[2024-12-12 02:58:01,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:01,341][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.045046366751194, acc: 1.0)
[2024-12-12 02:58:01,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:01,710][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.017562733963131905, acc: 1.0)
[2024-12-12 02:58:01,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:02,053][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.027895087376236916, acc: 1.0)
[2024-12-12 02:58:02,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:02,454][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.2708442807197571, acc: 0.9384615421295166)
[2024-12-12 02:58:02,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:02,842][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.2440403699874878, acc: 0.953125)
[2024-12-12 02:58:02,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:03,260][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.12298835813999176, acc: 0.96875)
[2024-12-12 02:58:03,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:03,648][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.3501260280609131, acc: 0.9090909361839294)
[2024-12-12 02:58:03,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:04,020][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.03274676203727722, acc: 1.0)
[2024-12-12 02:58:04,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:04,346][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.006646579597145319, acc: 1.0)
[2024-12-12 02:58:04,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:04,717][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.001415593666024506, acc: 1.0)
[2024-12-12 02:58:04,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:05,077][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.08582668006420135, acc: 0.9666666388511658)
[2024-12-12 02:58:05,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:05,418][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.1987169086933136, acc: 0.9024389982223511)
[2024-12-12 02:58:05,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:05,700][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.014898805879056454, acc: 1.0)
[2024-12-12 02:58:05,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:06,016][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.004615905694663525, acc: 1.0)
[2024-12-12 02:58:06,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:06,343][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.01503992173820734, acc: 1.0)
[2024-12-12 02:58:06,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:06,689][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 0.005041997414082289, acc: 1.0)
[2024-12-12 02:58:06,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:07,067][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.13019844889640808, acc: 0.9696969985961914)
[2024-12-12 02:58:07,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:07,445][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.045076772570610046, acc: 0.9750000238418579)
[2024-12-12 02:58:07,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:07,804][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.10161200910806656, acc: 0.9571428298950195)
[2024-12-12 02:58:07,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:08,159][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.29415667057037354, acc: 0.9197080135345459)
[2024-12-12 02:58:08,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:08,536][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.11693829298019409, acc: 0.951724112033844)
[2024-12-12 02:58:08,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:08,932][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.12816067039966583, acc: 0.9571428298950195)
[2024-12-12 02:58:09,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:09,283][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.1669820249080658, acc: 0.9470198750495911)
[2024-12-12 02:58:09,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:09,618][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.08154603838920593, acc: 0.9743589758872986)
[2024-12-12 02:58:09,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:09,954][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.08350498229265213, acc: 0.9599999785423279)
[2024-12-12 02:58:10,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:10,272][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.025767358019948006, acc: 1.0)
[2024-12-12 02:58:11,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:11,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:11,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:12,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:12,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:12,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:13,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:13,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:13,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:14,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:14,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:14,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:15,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:15,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:16,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:16,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:16,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:17,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:17,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:17,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:18,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:18,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:18,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:19,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:20,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:20,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:20,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:21,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:21,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:22,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:22,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:22,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:23,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:23,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:23,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:24,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:24,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:24,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:25,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:25,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:25,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:26,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:26,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:26,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:27,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:27,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:28,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:28,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:28,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:29,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:29,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:29,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:30,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:30,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:30,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:31,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:31,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:32,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:32,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:32,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:33,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:33,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:34,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:34,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:34,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:35,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:35,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:36,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:36,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:36,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:37,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:37,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:37,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:38,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:38,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:39,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:39,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:39,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:40,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:40,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:40,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:41,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:41,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:41,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:42,591][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6265, device='cuda:0') eval_epoch_loss=tensor(0.9656, device='cuda:0') eval_epoch_acc=tensor(0.7956, device='cuda:0')
[2024-12-12 02:58:42,592][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:58:42,593][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:58:43,016][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_7_step_560_loss_0.9656424522399902/model.pt
[2024-12-12 02:58:43,024][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 02:58:43,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:43,388][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.021604491397738457, acc: 1.0)
[2024-12-12 02:58:43,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:43,701][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.033413249999284744, acc: 1.0)
[2024-12-12 02:58:43,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:44,087][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.14731600880622864, acc: 0.9444444179534912)
[2024-12-12 02:58:44,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:44,415][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.10355304926633835, acc: 0.9870129823684692)
[2024-12-12 02:58:44,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:44,755][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.0756903663277626, acc: 0.9791666865348816)
[2024-12-12 02:58:44,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:45,125][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.22021161019802094, acc: 0.9137930870056152)
[2024-12-12 02:58:45,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:45,466][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.027834750711917877, acc: 1.0)
[2024-12-12 02:58:45,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:45,762][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.012690359726548195, acc: 1.0)
[2024-12-12 02:58:45,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:46,098][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.015412579290568829, acc: 1.0)
[2024-12-12 02:58:46,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:46,562][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.1984502226114273, acc: 0.9465240836143494)
[2024-12-12 02:58:46,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:46,949][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.07149169594049454, acc: 0.9677419066429138)
[2024-12-12 02:58:47,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:47,281][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.09689416736364365, acc: 0.9487179517745972)
[2024-12-12 02:58:47,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:47,584][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.4632796049118042, acc: 0.8418367505073547)
[2024-12-12 02:58:47,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:47,910][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.230233296751976, acc: 0.9119496941566467)
[2024-12-12 02:58:48,355][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.2756, train_epoch_loss=0.2435, epoch time 362.09965544193983s
[2024-12-12 02:58:48,355][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:58:48,355][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2024-12-12 02:58:48,355][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:58:48,356][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 20
[2024-12-12 02:58:48,356][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:58:48,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:49,259][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.011162210255861282, acc: 1.0)
[2024-12-12 02:58:49,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:49,576][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.2491285502910614, acc: 0.9200000166893005)
[2024-12-12 02:58:49,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:49,958][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.06266043335199356, acc: 0.9729729890823364)
[2024-12-12 02:58:50,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:50,364][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.01757342554628849, acc: 1.0)
[2024-12-12 02:58:50,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:50,758][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.2550990581512451, acc: 0.9729729890823364)
[2024-12-12 02:58:50,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:51,155][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.08285356312990189, acc: 0.9642857313156128)
[2024-12-12 02:58:51,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:51,560][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.08432842046022415, acc: 1.0)
[2024-12-12 02:58:51,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:51,916][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.47228047251701355, acc: 0.8999999761581421)
[2024-12-12 02:58:52,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:52,346][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.00197990657761693, acc: 1.0)
[2024-12-12 02:58:52,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:52,686][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.03221162036061287, acc: 0.9615384340286255)
[2024-12-12 02:58:52,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:53,085][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.004194220528006554, acc: 1.0)
[2024-12-12 02:58:53,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:53,485][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.06714154779911041, acc: 0.9743589758872986)
[2024-12-12 02:58:53,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:53,903][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.16449236869812012, acc: 0.939393937587738)
[2024-12-12 02:58:54,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:54,327][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.12528181076049805, acc: 0.9347826242446899)
[2024-12-12 02:58:54,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:54,678][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.03306879475712776, acc: 1.0)
[2024-12-12 02:58:54,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:55,077][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.13013102114200592, acc: 0.9591836929321289)
[2024-12-12 02:58:55,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:55,406][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.00449365796521306, acc: 1.0)
[2024-12-12 02:58:55,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:55,802][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.0685848817229271, acc: 0.9583333134651184)
[2024-12-12 02:58:55,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:56,221][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.2714526951313019, acc: 0.9166666865348816)
[2024-12-12 02:58:56,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:56,636][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.054657768458127975, acc: 1.0)
[2024-12-12 02:58:56,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:57,016][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.16671183705329895, acc: 0.9615384340286255)
[2024-12-12 02:58:57,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:57,376][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.029135826975107193, acc: 1.0)
[2024-12-12 02:58:57,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:57,678][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.08261459320783615, acc: 0.9599999785423279)
[2024-12-12 02:58:57,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:58,051][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.07278812676668167, acc: 0.9523809552192688)
[2024-12-12 02:58:58,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:58,464][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.03172492980957031, acc: 1.0)
[2024-12-12 02:58:58,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:58,900][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.3406609296798706, acc: 0.9056603908538818)
[2024-12-12 02:58:59,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:59,321][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.3185741603374481, acc: 0.8904109597206116)
[2024-12-12 02:58:59,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:00,633][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.7709982991218567, acc: 0.782608687877655)
[2024-12-12 02:59:00,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:01,029][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.1253851354122162, acc: 0.9767441749572754)
[2024-12-12 02:59:01,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:01,390][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.14438462257385254, acc: 0.9397590160369873)
[2024-12-12 02:59:01,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:01,807][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.21972933411598206, acc: 0.9382715821266174)
[2024-12-12 02:59:01,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:02,170][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.0652255266904831, acc: 1.0)
[2024-12-12 02:59:02,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:02,514][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.06860233098268509, acc: 0.9629629850387573)
[2024-12-12 02:59:02,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:02,906][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.005422850139439106, acc: 1.0)
[2024-12-12 02:59:03,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:03,322][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.34393444657325745, acc: 0.9075630307197571)
[2024-12-12 02:59:03,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:03,675][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.11177266389131546, acc: 0.9672130942344666)
[2024-12-12 02:59:03,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:04,067][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.09698466956615448, acc: 0.9682539701461792)
[2024-12-12 02:59:04,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:04,457][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.1746712028980255, acc: 0.9661017060279846)
[2024-12-12 02:59:04,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:04,836][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.059307947754859924, acc: 0.9885057210922241)
[2024-12-12 02:59:04,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:05,225][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.015348166227340698, acc: 1.0)
[2024-12-12 02:59:05,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:05,564][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.05200783163309097, acc: 1.0)
[2024-12-12 02:59:05,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:05,929][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.20723973214626312, acc: 0.9594594836235046)
[2024-12-12 02:59:06,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:06,290][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.30508968234062195, acc: 0.9230769276618958)
[2024-12-12 02:59:06,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:06,715][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.29055312275886536, acc: 0.8888888955116272)
[2024-12-12 02:59:06,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:07,128][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.23610587418079376, acc: 0.938144326210022)
[2024-12-12 02:59:07,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:07,527][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.268771231174469, acc: 0.9191176295280457)
[2024-12-12 02:59:07,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:07,879][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.021963991224765778, acc: 1.0)
[2024-12-12 02:59:07,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:08,227][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.1455077975988388, acc: 0.9259259104728699)
[2024-12-12 02:59:08,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:08,621][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.16003766655921936, acc: 0.9285714030265808)
[2024-12-12 02:59:08,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:08,986][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.038525450974702835, acc: 0.9722222089767456)
[2024-12-12 02:59:09,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:09,390][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.2329442948102951, acc: 0.9298245906829834)
[2024-12-12 02:59:09,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:09,823][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.3604232370853424, acc: 0.841269850730896)
[2024-12-12 02:59:09,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:10,260][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.6116408109664917, acc: 0.8169013857841492)
[2024-12-12 02:59:10,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:10,753][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 0.8359607458114624, acc: 0.7400000095367432)
[2024-12-12 02:59:10,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:11,127][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.27353769540786743, acc: 0.9459459185600281)
[2024-12-12 02:59:11,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:11,419][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.14893072843551636, acc: 0.9230769276618958)
[2024-12-12 02:59:12,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:14,408][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.7904825210571289, acc: 0.7610921263694763)
[2024-12-12 02:59:14,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:15,673][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 1.159499168395996, acc: 0.6688452959060669)
[2024-12-12 02:59:15,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:16,290][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.41915395855903625, acc: 0.8579545617103577)
[2024-12-12 02:59:16,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:16,856][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.25028377771377563, acc: 0.9338235259056091)
[2024-12-12 02:59:16,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:17,415][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.4549625813961029, acc: 0.8913043737411499)
[2024-12-12 02:59:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:17,842][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.4008597433567047, acc: 0.8500000238418579)
[2024-12-12 02:59:17,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:18,188][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.13937175273895264, acc: 0.9411764740943909)
[2024-12-12 02:59:18,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:18,586][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.1198655217885971, acc: 0.9444444179534912)
[2024-12-12 02:59:18,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:18,935][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.2333754301071167, acc: 0.96875)
[2024-12-12 02:59:19,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:19,322][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.003829628461971879, acc: 1.0)
[2024-12-12 02:59:19,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:19,715][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.19181351363658905, acc: 0.9464285969734192)
[2024-12-12 02:59:19,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:20,076][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.1965615153312683, acc: 0.949999988079071)
[2024-12-12 02:59:20,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:20,422][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.009625660255551338, acc: 1.0)
[2024-12-12 02:59:20,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:20,809][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.03757445886731148, acc: 1.0)
[2024-12-12 02:59:20,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:21,135][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.04323318600654602, acc: 1.0)
[2024-12-12 02:59:21,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:21,506][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.703650951385498, acc: 0.7573529481887817)
[2024-12-12 02:59:21,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:21,877][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.4332713186740875, acc: 0.8888888955116272)
[2024-12-12 02:59:22,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:22,253][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 1.0096064805984497, acc: 0.728205144405365)
[2024-12-12 02:59:22,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:22,671][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.5796213150024414, acc: 0.8673469424247742)
[2024-12-12 02:59:22,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:23,084][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.6761001944541931, acc: 0.8208954930305481)
[2024-12-12 02:59:23,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:23,500][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 1.115272879600525, acc: 0.6861313581466675)
[2024-12-12 02:59:23,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:23,847][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.12353911995887756, acc: 0.9047619104385376)
[2024-12-12 02:59:23,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:24,235][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.015575787983834743, acc: 1.0)
[2024-12-12 02:59:24,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:24,610][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.03161764517426491, acc: 0.9696969985961914)
[2024-12-12 02:59:24,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:24,962][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.011933892965316772, acc: 1.0)
[2024-12-12 02:59:25,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:25,322][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.06932716816663742, acc: 0.9807692170143127)
[2024-12-12 02:59:25,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:25,718][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.3054805099964142, acc: 0.8653846383094788)
[2024-12-12 02:59:25,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:26,134][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.03509833291172981, acc: 1.0)
[2024-12-12 02:59:26,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:26,520][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.21678943932056427, acc: 0.95652174949646)
[2024-12-12 02:59:26,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:26,867][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.2040085792541504, acc: 0.9599999785423279)
[2024-12-12 02:59:26,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:27,184][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.07034100592136383, acc: 0.95652174949646)
[2024-12-12 02:59:27,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:27,679][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.33714839816093445, acc: 0.8799999952316284)
[2024-12-12 02:59:27,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:28,053][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.43821072578430176, acc: 0.893203854560852)
[2024-12-12 02:59:28,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:29,189][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.5585469007492065, acc: 0.8009708523750305)
[2024-12-12 02:59:29,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:30,000][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.706384539604187, acc: 0.7903226017951965)
[2024-12-12 02:59:30,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:30,796][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.5470904111862183, acc: 0.8620689511299133)
[2024-12-12 02:59:30,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:31,534][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.42737695574760437, acc: 0.8736842274665833)
[2024-12-12 02:59:31,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:32,528][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.7449209094047546, acc: 0.801980197429657)
[2024-12-12 02:59:32,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:32,863][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.349459707736969, acc: 0.8870967626571655)
[2024-12-12 02:59:32,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:33,266][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.3237236440181732, acc: 0.8840579986572266)
[2024-12-12 02:59:33,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:33,659][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.6496820449829102, acc: 0.8403361439704895)
[2024-12-12 02:59:33,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:34,093][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.6435317397117615, acc: 0.7884615659713745)
[2024-12-12 02:59:34,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:34,498][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.6876534223556519, acc: 0.7883211970329285)
[2024-12-12 02:59:34,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:34,893][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.24189092218875885, acc: 0.9253731369972229)
[2024-12-12 02:59:34,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:35,233][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.13462141156196594, acc: 0.8999999761581421)
[2024-12-12 02:59:35,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:35,636][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.0033487998880445957, acc: 1.0)
[2024-12-12 02:59:35,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:35,914][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.009081577882170677, acc: 1.0)
[2024-12-12 02:59:36,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:36,272][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.02605438232421875, acc: 1.0)
[2024-12-12 02:59:36,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:36,655][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.08117415010929108, acc: 0.9482758641242981)
[2024-12-12 02:59:36,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:36,965][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.033499039709568024, acc: 1.0)
[2024-12-12 02:59:37,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:37,298][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.049599483609199524, acc: 1.0)
[2024-12-12 02:59:37,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:37,679][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.01338227465748787, acc: 1.0)
[2024-12-12 02:59:37,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:38,060][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.010299728251993656, acc: 1.0)
[2024-12-12 02:59:38,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:38,396][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.11356134712696075, acc: 0.9523809552192688)
[2024-12-12 02:59:38,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:38,716][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.0801631286740303, acc: 0.9846153855323792)
[2024-12-12 02:59:38,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:39,120][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.2422894835472107, acc: 0.9122806787490845)
[2024-12-12 02:59:39,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:39,466][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.33022019267082214, acc: 0.8947368264198303)
[2024-12-12 02:59:39,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:39,845][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.13090695440769196, acc: 0.9743589758872986)
[2024-12-12 02:59:39,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:40,216][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.20496319234371185, acc: 0.918367326259613)
[2024-12-12 02:59:40,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:40,512][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.0051080454140901566, acc: 1.0)
[2024-12-12 02:59:40,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:40,875][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.140208899974823, acc: 0.9682539701461792)
[2024-12-12 02:59:40,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:41,200][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.29041895270347595, acc: 0.9105691313743591)
[2024-12-12 02:59:41,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:41,536][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.04241795837879181, acc: 1.0)
[2024-12-12 02:59:41,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:42,403][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.5819913744926453, acc: 0.8250950574874878)
[2024-12-12 02:59:42,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:42,796][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.11270740628242493, acc: 0.9599999785423279)
[2024-12-12 02:59:42,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:43,220][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.19922088086605072, acc: 0.942307710647583)
[2024-12-12 02:59:43,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:43,614][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.19926588237285614, acc: 0.9583333134651184)
[2024-12-12 02:59:43,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:44,000][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.01010973285883665, acc: 1.0)
[2024-12-12 02:59:44,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:44,379][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.536693811416626, acc: 0.8650306463241577)
[2024-12-12 02:59:44,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:44,813][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.4565095901489258, acc: 0.8611111044883728)
[2024-12-12 02:59:44,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:45,233][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.6144136786460876, acc: 0.8416666388511658)
[2024-12-12 02:59:45,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:45,628][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.5700621604919434, acc: 0.8273809552192688)
[2024-12-12 02:59:45,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:46,006][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.45937925577163696, acc: 0.8666666746139526)
[2024-12-12 02:59:46,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:47,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:47,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:47,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:48,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:48,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:48,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:49,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:49,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:50,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:50,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:50,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:51,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:51,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:52,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:52,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:52,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:53,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:53,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:53,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:54,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:54,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:54,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:55,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:55,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:55,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:56,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:56,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:56,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:57,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:57,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:57,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:58,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:58,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:58,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:59,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:59,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:59,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:00,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:00,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:01,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:01,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:01,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:02,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:02,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:02,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:03,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:03,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:03,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:04,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:04,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:04,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:05,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:05,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:05,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:06,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:06,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:06,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:07,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:07,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:07,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:08,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:08,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:09,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:09,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:09,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:10,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:10,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:10,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:11,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:11,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:11,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:12,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:12,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:12,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:13,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:13,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:13,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:14,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:14,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:15,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:15,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:15,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:16,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:16,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:17,207][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4347, device='cuda:0') eval_epoch_loss=tensor(0.8898, device='cuda:0') eval_epoch_acc=tensor(0.8036, device='cuda:0')
[2024-12-12 03:00:17,209][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:00:17,209][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:00:17,532][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_8_step_129_loss_0.8898410797119141/model.pt
[2024-12-12 03:00:17,543][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:00:17,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:18,018][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.5291104316711426, acc: 0.8235294222831726)
[2024-12-12 03:00:18,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:18,384][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.0937262699007988, acc: 0.9615384340286255)
[2024-12-12 03:00:18,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:18,771][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.1019824966788292, acc: 0.95652174949646)
[2024-12-12 03:00:18,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:19,115][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.08792434632778168, acc: 0.96875)
[2024-12-12 03:00:19,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:19,480][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.09821049869060516, acc: 0.95652174949646)
[2024-12-12 03:00:19,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:19,869][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.12383730709552765, acc: 0.9714285731315613)
[2024-12-12 03:00:19,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:20,256][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.15234053134918213, acc: 0.9615384340286255)
[2024-12-12 03:00:20,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:20,573][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.08955876529216766, acc: 0.9523809552192688)
[2024-12-12 03:00:20,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:20,926][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.1172419860959053, acc: 0.9666666388511658)
[2024-12-12 03:00:21,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:21,291][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.23239867389202118, acc: 0.95652174949646)
[2024-12-12 03:00:21,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:21,645][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.030729880556464195, acc: 1.0)
[2024-12-12 03:00:21,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:21,980][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.07016813009977341, acc: 1.0)
[2024-12-12 03:00:22,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:22,358][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.1060779020190239, acc: 0.9677419066429138)
[2024-12-12 03:00:22,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:22,698][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.07425195723772049, acc: 1.0)
[2024-12-12 03:00:22,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:23,223][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.326896607875824, acc: 0.8947368264198303)
[2024-12-12 03:00:23,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:23,591][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.508805513381958, acc: 0.8358209133148193)
[2024-12-12 03:00:23,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:23,959][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.3222423195838928, acc: 0.8571428656578064)
[2024-12-12 03:00:24,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:24,415][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.4275570213794708, acc: 0.8617021441459656)
[2024-12-12 03:00:24,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:24,799][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.1956407129764557, acc: 0.9428571462631226)
[2024-12-12 03:00:24,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:25,166][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.061118584126234055, acc: 0.9642857313156128)
[2024-12-12 03:00:25,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:25,499][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.3938281834125519, acc: 0.8695651888847351)
[2024-12-12 03:00:25,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:25,877][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.04919920861721039, acc: 1.0)
[2024-12-12 03:00:26,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:26,284][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.12392892688512802, acc: 0.95652174949646)
[2024-12-12 03:00:26,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:26,665][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.2073086053133011, acc: 0.9491525292396545)
[2024-12-12 03:00:26,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:27,023][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.1752062439918518, acc: 0.9649122953414917)
[2024-12-12 03:00:27,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:27,369][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.12199154496192932, acc: 0.9594594836235046)
[2024-12-12 03:00:27,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:27,719][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.04263421893119812, acc: 1.0)
[2024-12-12 03:00:27,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:28,086][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.17467664182186127, acc: 0.95652174949646)
[2024-12-12 03:00:28,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:28,429][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.19213536381721497, acc: 0.8947368264198303)
[2024-12-12 03:00:29,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:30,132][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.530945360660553, acc: 0.8783783912658691)
[2024-12-12 03:00:30,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:30,475][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.5673532485961914, acc: 0.8148148059844971)
[2024-12-12 03:00:30,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:30,921][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.602678120136261, acc: 0.8372092843055725)
[2024-12-12 03:00:31,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:31,506][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.6473495364189148, acc: 0.8235294222831726)
[2024-12-12 03:00:31,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:32,055][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.7637748122215271, acc: 0.7865168452262878)
[2024-12-12 03:00:32,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:32,366][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.10220573097467422, acc: 0.9772727489471436)
[2024-12-12 03:00:32,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:32,675][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.12168984115123749, acc: 0.9523809552192688)
[2024-12-12 03:00:32,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:33,076][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.19842417538166046, acc: 0.931034505367279)
[2024-12-12 03:00:33,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:33,418][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.10292069613933563, acc: 0.9591836929321289)
[2024-12-12 03:00:33,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:33,796][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.1692107915878296, acc: 0.9599999785423279)
[2024-12-12 03:00:33,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:34,236][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.1784014105796814, acc: 0.9305555820465088)
[2024-12-12 03:00:34,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:34,635][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.6693824529647827, acc: 0.8333333134651184)
[2024-12-12 03:00:34,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:35,662][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.45352858304977417, acc: 0.8767123222351074)
[2024-12-12 03:00:35,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:35,985][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.17143785953521729, acc: 0.9583333134651184)
[2024-12-12 03:00:36,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:36,351][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.23331224918365479, acc: 0.8888888955116272)
[2024-12-12 03:00:36,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:36,739][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.10923147946596146, acc: 0.9285714030265808)
[2024-12-12 03:00:36,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:37,274][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.5792255401611328, acc: 0.8495575189590454)
[2024-12-12 03:00:37,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:37,540][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.327223002910614, acc: 0.8985507488250732)
[2024-12-12 03:00:37,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:37,920][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.10632956773042679, acc: 0.9659090638160706)
[2024-12-12 03:00:38,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:38,834][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.7123258113861084, acc: 0.7786259651184082)
[2024-12-12 03:00:39,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:39,499][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.5972639322280884, acc: 0.8148148059844971)
[2024-12-12 03:00:39,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:39,891][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.1411314606666565, acc: 0.9508196711540222)
[2024-12-12 03:00:39,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:40,228][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.009020469151437283, acc: 1.0)
[2024-12-12 03:00:40,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:40,567][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.04220170900225639, acc: 0.9599999785423279)
[2024-12-12 03:00:40,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:40,959][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.03050811029970646, acc: 1.0)
[2024-12-12 03:00:41,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:41,361][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.08366027474403381, acc: 0.9756097793579102)
[2024-12-12 03:00:41,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:41,718][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.44759809970855713, acc: 0.8851963877677917)
[2024-12-12 03:00:41,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:42,030][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.5090814232826233, acc: 0.8414985537528992)
[2024-12-12 03:00:42,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:42,515][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.5053446292877197, acc: 0.846875011920929)
[2024-12-12 03:00:42,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:43,047][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.610359787940979, acc: 0.8198874592781067)
[2024-12-12 03:00:43,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:43,487][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.46995440125465393, acc: 0.871886134147644)
[2024-12-12 03:00:43,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:43,915][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.08485815674066544, acc: 0.9599999785423279)
[2024-12-12 03:00:44,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:44,460][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.36859723925590515, acc: 0.895348846912384)
[2024-12-12 03:00:44,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:45,257][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.6511109471321106, acc: 0.7857142686843872)
[2024-12-12 03:00:45,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:46,167][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.7178292870521545, acc: 0.7954545617103577)
[2024-12-12 03:00:46,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:46,910][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.317745178937912, acc: 0.9529411792755127)
[2024-12-12 03:00:47,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:47,985][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.6208062171936035, acc: 0.8395061492919922)
[2024-12-12 03:00:48,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:48,948][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.16080062091350555, acc: 0.9677419066429138)
[2024-12-12 03:00:49,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:49,228][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.08407114446163177, acc: 0.9642857313156128)
[2024-12-12 03:00:49,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:49,579][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.06244897097349167, acc: 0.9750000238418579)
[2024-12-12 03:00:49,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:49,940][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.31052613258361816, acc: 0.9117646813392639)
[2024-12-12 03:00:50,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:50,286][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.42470526695251465, acc: 0.904411792755127)
[2024-12-12 03:00:50,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:50,619][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.34601664543151855, acc: 0.8728813529014587)
[2024-12-12 03:00:50,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:50,987][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.3645709753036499, acc: 0.9029850959777832)
[2024-12-12 03:00:51,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:51,409][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.3156692385673523, acc: 0.8737863898277283)
[2024-12-12 03:00:51,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:51,779][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.22413870692253113, acc: 0.9365079402923584)
[2024-12-12 03:00:51,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:52,196][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.023211872205138206, acc: 1.0)
[2024-12-12 03:00:52,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:52,602][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.1897328794002533, acc: 0.9192824959754944)
[2024-12-12 03:00:52,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:52,999][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.27426499128341675, acc: 0.9173228144645691)
[2024-12-12 03:00:53,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:53,372][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.2640282213687897, acc: 0.9137930870056152)
[2024-12-12 03:00:53,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:53,776][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.30885013937950134, acc: 0.9021739363670349)
[2024-12-12 03:00:53,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:54,141][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.2418963462114334, acc: 0.9143968820571899)
[2024-12-12 03:00:54,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:54,501][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.16081315279006958, acc: 0.967391312122345)
[2024-12-12 03:00:54,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:54,894][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.013222870416939259, acc: 1.0)
[2024-12-12 03:00:55,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:55,247][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.024290146306157112, acc: 1.0)
[2024-12-12 03:00:55,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:55,661][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.0890451967716217, acc: 0.978723406791687)
[2024-12-12 03:00:55,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:56,399][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.10498044639825821, acc: 0.9692307710647583)
[2024-12-12 03:00:56,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:56,789][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.015122368931770325, acc: 1.0)
[2024-12-12 03:00:56,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:57,183][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.039384111762046814, acc: 0.9883720874786377)
[2024-12-12 03:00:57,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:57,725][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.10021258145570755, acc: 0.9729729890823364)
[2024-12-12 03:00:57,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:58,121][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.0904461145401001, acc: 0.9777777791023254)
[2024-12-12 03:00:58,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:58,457][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.11733567714691162, acc: 0.939393937587738)
[2024-12-12 03:00:58,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:58,845][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.007141212001442909, acc: 1.0)
[2024-12-12 03:00:58,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:59,252][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.008849567733705044, acc: 1.0)
[2024-12-12 03:00:59,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:59,638][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.25134411454200745, acc: 0.9230769276618958)
[2024-12-12 03:00:59,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:00,393][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.24087880551815033, acc: 0.929347813129425)
[2024-12-12 03:01:00,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:00,937][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.3856033384799957, acc: 0.8806818127632141)
[2024-12-12 03:01:01,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:01,386][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.3777490258216858, acc: 0.8936170339584351)
[2024-12-12 03:01:01,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:01,747][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.19128632545471191, acc: 0.9433962106704712)
[2024-12-12 03:01:01,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:02,101][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.11075884848833084, acc: 0.9666666388511658)
[2024-12-12 03:01:02,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:02,489][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.10813798755407333, acc: 0.9534883499145508)
[2024-12-12 03:01:02,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:02,812][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.1988234519958496, acc: 0.9333333373069763)
[2024-12-12 03:01:02,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:03,179][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.9586023688316345, acc: 0.7263157963752747)
[2024-12-12 03:01:03,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:03,590][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.6871576309204102, acc: 0.7888888716697693)
[2024-12-12 03:01:03,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:04,007][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.9538525342941284, acc: 0.699999988079071)
[2024-12-12 03:01:04,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:04,504][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 1.4767987728118896, acc: 0.5917431116104126)
[2024-12-12 03:01:04,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:04,975][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 1.0310968160629272, acc: 0.699999988079071)
[2024-12-12 03:01:05,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:05,313][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.07364950329065323, acc: 0.9473684430122375)
[2024-12-12 03:01:05,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:05,688][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.027078060433268547, acc: 1.0)
[2024-12-12 03:01:05,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:05,988][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.2655058801174164, acc: 0.9545454382896423)
[2024-12-12 03:01:06,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:06,356][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.32293301820755005, acc: 0.8888888955116272)
[2024-12-12 03:01:06,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:06,722][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.11942064762115479, acc: 0.9428571462631226)
[2024-12-12 03:01:06,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:07,090][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.275522917509079, acc: 0.8863636255264282)
[2024-12-12 03:01:07,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:07,409][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.13441342115402222, acc: 0.9545454382896423)
[2024-12-12 03:01:07,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:07,989][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.6177228093147278, acc: 0.8709677457809448)
[2024-12-12 03:01:08,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:08,515][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.3905601501464844, acc: 0.8636363744735718)
[2024-12-12 03:01:08,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:08,873][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 0.0010744204046204686, acc: 1.0)
[2024-12-12 03:01:08,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:09,219][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.40107786655426025, acc: 0.9615384340286255)
[2024-12-12 03:01:09,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:09,575][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.12014609575271606, acc: 0.9677419066429138)
[2024-12-12 03:01:09,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:09,898][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.03794868662953377, acc: 1.0)
[2024-12-12 03:01:10,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:10,272][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.24670350551605225, acc: 0.9459459185600281)
[2024-12-12 03:01:10,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:10,605][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.1613144874572754, acc: 0.9189189076423645)
[2024-12-12 03:01:10,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:10,978][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.009468108415603638, acc: 1.0)
[2024-12-12 03:01:11,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:11,314][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.1225874125957489, acc: 0.970588207244873)
[2024-12-12 03:01:11,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:11,643][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.12771660089492798, acc: 0.9512194991111755)
[2024-12-12 03:01:11,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:11,989][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.005801234394311905, acc: 1.0)
[2024-12-12 03:01:12,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:12,325][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 0.02908526360988617, acc: 1.0)
[2024-12-12 03:01:12,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:12,701][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.010912352241575718, acc: 1.0)
[2024-12-12 03:01:12,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:13,056][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.021491700783371925, acc: 1.0)
[2024-12-12 03:01:13,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:13,424][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.16624124348163605, acc: 0.9571428298950195)
[2024-12-12 03:01:13,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:13,765][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.2283322960138321, acc: 0.9473684430122375)
[2024-12-12 03:01:13,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:14,328][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.18967477977275848, acc: 0.9433962106704712)
[2024-12-12 03:01:14,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:14,917][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.2173687368631363, acc: 0.925000011920929)
[2024-12-12 03:01:15,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:15,247][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.059065137058496475, acc: 0.9722222089767456)
[2024-12-12 03:01:15,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:15,615][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.060829851776361465, acc: 1.0)
[2024-12-12 03:01:15,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:16,034][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.2985268533229828, acc: 0.9066666960716248)
[2024-12-12 03:01:16,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:16,434][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.2671404182910919, acc: 0.9375)
[2024-12-12 03:01:16,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:17,308][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.693474531173706, acc: 0.7919999957084656)
[2024-12-12 03:01:17,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:17,645][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.4772838354110718, acc: 0.8764045238494873)
[2024-12-12 03:01:17,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:18,031][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.3573320209980011, acc: 0.8918918967247009)
[2024-12-12 03:01:18,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:18,478][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.2112453281879425, acc: 0.931034505367279)
[2024-12-12 03:01:18,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:18,878][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.019027937203645706, acc: 1.0)
[2024-12-12 03:01:18,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:19,254][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.04877937212586403, acc: 1.0)
[2024-12-12 03:01:19,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:19,597][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.015689576044678688, acc: 1.0)
[2024-12-12 03:01:20,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:20,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:20,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:21,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:21,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:22,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:22,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:22,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:23,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:23,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:23,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:24,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:24,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:24,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:25,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:25,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:26,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:26,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:26,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:27,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:27,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:27,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:28,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:28,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:28,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:29,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:29,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:30,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:30,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:30,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:31,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:31,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:31,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:32,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:32,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:32,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:33,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:33,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:33,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:34,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:34,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:34,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:35,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:35,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:36,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:36,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:36,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:37,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:37,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:37,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:38,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:38,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:38,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:39,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:39,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:39,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:40,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:40,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:40,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:41,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:41,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:42,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:42,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:42,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:43,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:43,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:43,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:44,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:44,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:45,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:45,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:45,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:46,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:46,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:46,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:47,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:47,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:48,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:48,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:48,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:49,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:49,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:50,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:50,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:50,990][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4850, device='cuda:0') eval_epoch_loss=tensor(0.9103, device='cuda:0') eval_epoch_acc=tensor(0.8029, device='cuda:0')
[2024-12-12 03:01:50,991][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:01:50,992][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:01:51,528][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_8_step_272_loss_0.9102875590324402/model.pt
[2024-12-12 03:01:51,542][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:01:51,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:51,989][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.024235200136899948, acc: 1.0)
[2024-12-12 03:01:52,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:52,407][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.13726262748241425, acc: 0.949999988079071)
[2024-12-12 03:01:52,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:52,778][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.017305931076407433, acc: 1.0)
[2024-12-12 03:01:52,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:53,165][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.18357986211776733, acc: 0.9666666388511658)
[2024-12-12 03:01:53,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:53,528][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.027377968654036522, acc: 1.0)
[2024-12-12 03:01:53,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:53,912][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.29391470551490784, acc: 0.9599999785423279)
[2024-12-12 03:01:54,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:54,226][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.21758824586868286, acc: 0.936170220375061)
[2024-12-12 03:01:54,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:54,589][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.04781519994139671, acc: 0.9791666865348816)
[2024-12-12 03:01:54,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:54,985][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.0038295031990855932, acc: 1.0)
[2024-12-12 03:01:55,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:55,440][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.3508564829826355, acc: 0.9277108311653137)
[2024-12-12 03:01:55,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:55,791][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.436078280210495, acc: 0.8888888955116272)
[2024-12-12 03:01:55,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:56,079][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.13629020750522614, acc: 0.9473684430122375)
[2024-12-12 03:01:56,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:56,365][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.1787656843662262, acc: 0.970588207244873)
[2024-12-12 03:01:56,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:56,670][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.11389505863189697, acc: 0.925000011920929)
[2024-12-12 03:01:56,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:57,021][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.2514248490333557, acc: 0.9375)
[2024-12-12 03:01:57,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:57,349][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.2172556072473526, acc: 0.9440000057220459)
[2024-12-12 03:01:57,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:57,710][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.09127843379974365, acc: 0.9670329689979553)
[2024-12-12 03:01:57,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:58,055][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.23020140826702118, acc: 0.9254658222198486)
[2024-12-12 03:01:58,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:58,451][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.30971401929855347, acc: 0.9020618796348572)
[2024-12-12 03:01:58,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:58,781][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.011664304882287979, acc: 1.0)
[2024-12-12 03:01:58,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:59,098][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.24302110075950623, acc: 0.9285714030265808)
[2024-12-12 03:01:59,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:59,484][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.09836147725582123, acc: 0.982758641242981)
[2024-12-12 03:01:59,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:00,008][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.19564789533615112, acc: 0.8909090757369995)
[2024-12-12 03:02:00,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:00,566][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.47408151626586914, acc: 0.8659793734550476)
[2024-12-12 03:02:00,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:00,975][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.15447573363780975, acc: 0.9482758641242981)
[2024-12-12 03:02:01,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:01,407][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.0427447147667408, acc: 1.0)
[2024-12-12 03:02:01,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:01,804][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.07467694580554962, acc: 1.0)
[2024-12-12 03:02:01,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:02,149][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.15750065445899963, acc: 0.9285714030265808)
[2024-12-12 03:02:02,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:02,579][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.008635111153125763, acc: 1.0)
[2024-12-12 03:02:02,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:02,952][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.10991886258125305, acc: 0.9622641801834106)
[2024-12-12 03:02:03,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:03,354][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.03375031054019928, acc: 0.9811320900917053)
[2024-12-12 03:02:03,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:03,705][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.02160106599330902, acc: 1.0)
[2024-12-12 03:02:03,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:04,076][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.07563173025846481, acc: 0.96875)
[2024-12-12 03:02:04,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:04,442][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.1218801736831665, acc: 0.9672130942344666)
[2024-12-12 03:02:04,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:04,778][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.02934342622756958, acc: 1.0)
[2024-12-12 03:02:04,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:05,174][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.0030552316457033157, acc: 1.0)
[2024-12-12 03:02:05,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:05,542][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.14490965008735657, acc: 0.95652174949646)
[2024-12-12 03:02:05,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:05,986][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.03599589318037033, acc: 1.0)
[2024-12-12 03:02:06,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:06,311][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.10709349066019058, acc: 0.9638554453849792)
[2024-12-12 03:02:06,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:06,695][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.12049566209316254, acc: 0.9487179517745972)
[2024-12-12 03:02:06,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:07,101][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.07025524228811264, acc: 0.9795918464660645)
[2024-12-12 03:02:07,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:07,436][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.005971803795546293, acc: 1.0)
[2024-12-12 03:02:07,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:07,767][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.08539827913045883, acc: 0.9583333134651184)
[2024-12-12 03:02:07,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:08,114][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.006102684885263443, acc: 1.0)
[2024-12-12 03:02:08,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:08,384][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.06793733686208725, acc: 0.9677419066429138)
[2024-12-12 03:02:08,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:08,778][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.032386261969804764, acc: 0.9850746393203735)
[2024-12-12 03:02:08,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:09,162][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.02447037771344185, acc: 0.9903846383094788)
[2024-12-12 03:02:09,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:09,484][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.025215230882167816, acc: 1.0)
[2024-12-12 03:02:09,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:09,894][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.01615380123257637, acc: 1.0)
[2024-12-12 03:02:09,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:10,267][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.014912497252225876, acc: 1.0)
[2024-12-12 03:02:10,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:10,593][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.15055027604103088, acc: 0.9259259104728699)
[2024-12-12 03:02:10,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:10,887][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.16328935325145721, acc: 0.9428571462631226)
[2024-12-12 03:02:10,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:11,182][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.06995900720357895, acc: 1.0)
[2024-12-12 03:02:11,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:11,507][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.41338565945625305, acc: 0.8780487775802612)
[2024-12-12 03:02:11,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:11,865][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.10730308294296265, acc: 0.9736841917037964)
[2024-12-12 03:02:11,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:12,270][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.03063286654651165, acc: 1.0)
[2024-12-12 03:02:12,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:12,656][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.017571130767464638, acc: 1.0)
[2024-12-12 03:02:12,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:13,065][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.024070261046290398, acc: 1.0)
[2024-12-12 03:02:13,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:13,425][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.0028113876469433308, acc: 1.0)
[2024-12-12 03:02:13,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:13,776][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.08818790316581726, acc: 0.9677419066429138)
[2024-12-12 03:02:13,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:14,131][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.11080672591924667, acc: 0.9649122953414917)
[2024-12-12 03:02:14,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:14,475][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.06195273622870445, acc: 0.96875)
[2024-12-12 03:02:14,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:14,860][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.08752436190843582, acc: 0.9333333373069763)
[2024-12-12 03:02:14,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:15,276][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.019499003887176514, acc: 1.0)
[2024-12-12 03:02:15,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:15,684][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.27638253569602966, acc: 0.8600000143051147)
[2024-12-12 03:02:15,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:16,091][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.5063319206237793, acc: 0.8275862336158752)
[2024-12-12 03:02:16,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:16,486][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.5577149987220764, acc: 0.8191489577293396)
[2024-12-12 03:02:16,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:16,834][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.5116402506828308, acc: 0.8313252925872803)
[2024-12-12 03:02:16,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:17,188][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.18398210406303406, acc: 0.95652174949646)
[2024-12-12 03:02:17,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:17,583][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.08754677325487137, acc: 0.9743589758872986)
[2024-12-12 03:02:17,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:17,960][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.15777625143527985, acc: 0.9277108311653137)
[2024-12-12 03:02:18,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:18,365][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.1521957367658615, acc: 0.9245283007621765)
[2024-12-12 03:02:18,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:18,752][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.01845618151128292, acc: 1.0)
[2024-12-12 03:02:18,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:19,139][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.050414182245731354, acc: 0.9803921580314636)
[2024-12-12 03:02:19,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:19,478][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.139495849609375, acc: 0.9402984976768494)
[2024-12-12 03:02:19,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:19,797][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.0010584727860987186, acc: 1.0)
[2024-12-12 03:02:19,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:20,122][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.047772299498319626, acc: 0.9599999785423279)
[2024-12-12 03:02:20,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:20,498][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.10673093795776367, acc: 1.0)
[2024-12-12 03:02:20,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:20,821][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.27197808027267456, acc: 0.8837209343910217)
[2024-12-12 03:02:20,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:21,176][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.2096891701221466, acc: 0.9487179517745972)
[2024-12-12 03:02:21,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:21,547][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.20423147082328796, acc: 0.9555555582046509)
[2024-12-12 03:02:21,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:21,838][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.0018746803980320692, acc: 1.0)
[2024-12-12 03:02:21,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:22,159][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.1794508546590805, acc: 0.9615384340286255)
[2024-12-12 03:02:22,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:22,540][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.34594112634658813, acc: 0.901098906993866)
[2024-12-12 03:02:22,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:23,073][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.23348359763622284, acc: 0.9130434989929199)
[2024-12-12 03:02:23,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:23,402][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.29613015055656433, acc: 0.8913043737411499)
[2024-12-12 03:02:23,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:23,798][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.10914577543735504, acc: 0.9591836929321289)
[2024-12-12 03:02:23,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:24,132][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.000573033350519836, acc: 1.0)
[2024-12-12 03:02:24,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:24,466][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.010092518292367458, acc: 1.0)
[2024-12-12 03:02:24,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:24,848][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.08545111119747162, acc: 0.9512194991111755)
[2024-12-12 03:02:24,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:25,215][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.09188627451658249, acc: 0.9777777791023254)
[2024-12-12 03:02:25,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:25,598][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.039844904094934464, acc: 0.9868420958518982)
[2024-12-12 03:02:25,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:25,974][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.028424352407455444, acc: 1.0)
[2024-12-12 03:02:26,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:26,352][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.1291741579771042, acc: 0.9696969985961914)
[2024-12-12 03:02:26,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:26,710][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.0017940764082595706, acc: 1.0)
[2024-12-12 03:02:26,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:27,018][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.1187395229935646, acc: 0.95652174949646)
[2024-12-12 03:02:27,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:27,294][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.014234049245715141, acc: 1.0)
[2024-12-12 03:02:27,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:27,581][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.020186638459563255, acc: 1.0)
[2024-12-12 03:02:27,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:28,187][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.39652955532073975, acc: 0.8606060743331909)
[2024-12-12 03:02:28,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:29,056][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.16293735802173615, acc: 0.9528301954269409)
[2024-12-12 03:02:29,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:29,419][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.09716159105300903, acc: 0.9888888597488403)
[2024-12-12 03:02:29,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:29,802][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.2044813483953476, acc: 0.9285714030265808)
[2024-12-12 03:02:29,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:30,238][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.04754423350095749, acc: 0.9714285731315613)
[2024-12-12 03:02:30,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:30,587][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 0.0006551807164214551, acc: 1.0)
[2024-12-12 03:02:30,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:30,924][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.003830861998721957, acc: 1.0)
[2024-12-12 03:02:31,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:31,318][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.09611815959215164, acc: 0.9791666865348816)
[2024-12-12 03:02:31,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:31,652][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.015730183571577072, acc: 0.9894737005233765)
[2024-12-12 03:02:31,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:32,251][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.1680201292037964, acc: 0.9520958065986633)
[2024-12-12 03:02:32,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:32,657][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.11293817311525345, acc: 0.9624060392379761)
[2024-12-12 03:02:33,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:33,991][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.45025601983070374, acc: 0.8823529481887817)
[2024-12-12 03:02:34,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:34,549][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.031381282955408096, acc: 0.9909909963607788)
[2024-12-12 03:02:34,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:34,923][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.14056222140789032, acc: 0.9642857313156128)
[2024-12-12 03:02:35,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:35,319][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.0032085557468235493, acc: 1.0)
[2024-12-12 03:02:35,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:35,684][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.007113877683877945, acc: 1.0)
[2024-12-12 03:02:35,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:36,059][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.0020729657262563705, acc: 1.0)
[2024-12-12 03:02:36,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:36,437][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.011261815205216408, acc: 1.0)
[2024-12-12 03:02:36,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:36,799][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.003081162227317691, acc: 1.0)
[2024-12-12 03:02:36,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:37,136][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 0.0031484984792768955, acc: 1.0)
[2024-12-12 03:02:37,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:37,466][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.033086925745010376, acc: 1.0)
[2024-12-12 03:02:37,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:37,792][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.232656791806221, acc: 0.9259259104728699)
[2024-12-12 03:02:37,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:38,166][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.5680188536643982, acc: 0.8349514603614807)
[2024-12-12 03:02:38,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:38,706][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.5781047344207764, acc: 0.8382353186607361)
[2024-12-12 03:02:38,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:39,133][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.35284122824668884, acc: 0.8866666555404663)
[2024-12-12 03:02:39,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:39,513][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.261380136013031, acc: 0.9236111044883728)
[2024-12-12 03:02:39,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:39,890][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.096964992582798, acc: 0.9534883499145508)
[2024-12-12 03:02:40,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:40,253][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.009845026768743992, acc: 1.0)
[2024-12-12 03:02:40,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:40,592][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.0686478540301323, acc: 0.9767441749572754)
[2024-12-12 03:02:40,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:40,943][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.011396284215152264, acc: 1.0)
[2024-12-12 03:02:41,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:41,516][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.13906146585941315, acc: 0.970588207244873)
[2024-12-12 03:02:41,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:41,903][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.21597902476787567, acc: 0.9733333587646484)
[2024-12-12 03:02:42,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:42,234][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.13221049308776855, acc: 0.9696969985961914)
[2024-12-12 03:02:42,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:42,575][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.12569661438465118, acc: 0.939393937587738)
[2024-12-12 03:02:42,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:42,970][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.00882148277014494, acc: 1.0)
[2024-12-12 03:02:43,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:43,315][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.03919800743460655, acc: 0.9629629850387573)
[2024-12-12 03:02:43,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:43,662][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.035052694380283356, acc: 1.0)
[2024-12-12 03:02:43,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:44,016][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.013668803498148918, acc: 1.0)
[2024-12-12 03:02:44,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:44,372][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.03780477121472359, acc: 1.0)
[2024-12-12 03:02:44,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:44,732][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.006962487939745188, acc: 1.0)
[2024-12-12 03:02:44,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:45,132][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.020796751603484154, acc: 1.0)
[2024-12-12 03:02:45,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:45,474][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.003252920461818576, acc: 1.0)
[2024-12-12 03:02:45,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:45,848][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.23841428756713867, acc: 0.9333333373069763)
[2024-12-12 03:02:45,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:46,229][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.3014734089374542, acc: 0.9696969985961914)
[2024-12-12 03:02:46,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:46,559][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.003387442324310541, acc: 1.0)
[2024-12-12 03:02:47,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:47,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:47,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:48,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:48,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:49,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:49,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:49,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:50,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:50,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:51,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:51,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:51,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:52,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:52,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:52,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:53,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:53,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:54,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:54,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:54,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:55,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:55,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:55,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:56,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:56,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:56,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:57,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:57,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:57,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:58,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:58,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:58,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:59,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:59,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:59,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:00,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:00,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:01,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:01,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:01,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:02,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:02,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:03,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:03,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:03,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:03,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:04,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:04,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:04,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:05,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:05,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:06,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:06,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:06,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:07,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:07,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:07,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:08,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:08,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:09,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:09,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:09,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:10,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:10,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:10,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:11,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:11,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:11,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:12,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:12,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:13,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:13,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:13,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:14,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:14,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:14,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:14,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:15,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:15,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:15,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:16,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:16,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:17,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:17,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:18,053][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5260, device='cuda:0') eval_epoch_loss=tensor(0.9267, device='cuda:0') eval_epoch_acc=tensor(0.7908, device='cuda:0')
[2024-12-12 03:03:18,054][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:03:18,055][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:03:18,377][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_8_step_415_loss_0.9266557693481445/model.pt
[2024-12-12 03:03:18,381][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:03:18,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:18,870][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.1544695645570755, acc: 0.9411764740943909)
[2024-12-12 03:03:18,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:19,250][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.020576000213623047, acc: 1.0)
[2024-12-12 03:03:19,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:19,582][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.2637398838996887, acc: 0.9444444179534912)
[2024-12-12 03:03:19,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:19,934][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.011957756243646145, acc: 1.0)
[2024-12-12 03:03:20,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:20,260][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.014734117314219475, acc: 1.0)
[2024-12-12 03:03:20,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:20,559][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.08122000098228455, acc: 0.9523809552192688)
[2024-12-12 03:03:20,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:20,870][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.16391485929489136, acc: 0.9333333373069763)
[2024-12-12 03:03:20,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:21,164][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.06590750813484192, acc: 0.96875)
[2024-12-12 03:03:21,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:21,562][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.05578206107020378, acc: 1.0)
[2024-12-12 03:03:21,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:21,898][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.07372049242258072, acc: 0.9629629850387573)
[2024-12-12 03:03:21,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:22,272][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.015860699117183685, acc: 1.0)
[2024-12-12 03:03:22,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:22,646][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.0014933691127225757, acc: 1.0)
[2024-12-12 03:03:22,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:23,036][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.012135673314332962, acc: 1.0)
[2024-12-12 03:03:23,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:23,371][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.002793193096294999, acc: 1.0)
[2024-12-12 03:03:23,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:23,716][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.00830391701310873, acc: 1.0)
[2024-12-12 03:03:23,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:24,060][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.0015183924697339535, acc: 1.0)
[2024-12-12 03:03:24,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:24,436][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.012842617928981781, acc: 1.0)
[2024-12-12 03:03:24,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:24,800][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.0027285199612379074, acc: 1.0)
[2024-12-12 03:03:24,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:25,171][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.05091603845357895, acc: 0.9722222089767456)
[2024-12-12 03:03:25,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:25,483][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.0022503011859953403, acc: 1.0)
[2024-12-12 03:03:25,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:25,838][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.1848977506160736, acc: 0.939393937587738)
[2024-12-12 03:03:25,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:26,189][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.4442998766899109, acc: 0.8611111044883728)
[2024-12-12 03:03:26,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:26,563][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.14620763063430786, acc: 0.9772727489471436)
[2024-12-12 03:03:26,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:26,907][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.0456111840903759, acc: 0.9523809552192688)
[2024-12-12 03:03:27,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:27,298][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.03202873468399048, acc: 1.0)
[2024-12-12 03:03:27,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:27,773][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.16835135221481323, acc: 0.9545454382896423)
[2024-12-12 03:03:28,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:28,526][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.4902185797691345, acc: 0.8159999847412109)
[2024-12-12 03:03:28,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:28,966][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.36723583936691284, acc: 0.8951612710952759)
[2024-12-12 03:03:29,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:29,648][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.51658695936203, acc: 0.8756219148635864)
[2024-12-12 03:03:29,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:29,957][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.12428132444620132, acc: 0.9622641801834106)
[2024-12-12 03:03:30,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:30,385][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.03266580402851105, acc: 1.0)
[2024-12-12 03:03:30,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:30,681][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.05389838665723801, acc: 0.95652174949646)
[2024-12-12 03:03:30,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:30,987][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.0538901761174202, acc: 0.9615384340286255)
[2024-12-12 03:03:31,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:31,323][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.02830275520682335, acc: 1.0)
[2024-12-12 03:03:31,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:31,705][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.038179777562618256, acc: 0.9850746393203735)
[2024-12-12 03:03:31,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:32,082][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.10400880873203278, acc: 0.9722222089767456)
[2024-12-12 03:03:32,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:32,413][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.09305103123188019, acc: 0.97826087474823)
[2024-12-12 03:03:32,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:32,794][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.07239380478858948, acc: 0.9743589758872986)
[2024-12-12 03:03:32,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:33,202][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.20089559257030487, acc: 0.9605262875556946)
[2024-12-12 03:03:33,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:33,589][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.037041209638118744, acc: 1.0)
[2024-12-12 03:03:33,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:33,940][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.08205468952655792, acc: 0.939393937587738)
[2024-12-12 03:03:34,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:34,300][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.15819914638996124, acc: 0.9484536051750183)
[2024-12-12 03:03:34,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:34,702][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.03892165794968605, acc: 0.9857142567634583)
[2024-12-12 03:03:34,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:35,092][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.2536998689174652, acc: 0.930232584476471)
[2024-12-12 03:03:35,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:35,448][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.1626075804233551, acc: 0.9642857313156128)
[2024-12-12 03:03:35,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:35,803][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.15914949774742126, acc: 0.9753086566925049)
[2024-12-12 03:03:35,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:36,172][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.3015833795070648, acc: 0.9166666865348816)
[2024-12-12 03:03:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:36,498][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.08378089219331741, acc: 0.96875)
[2024-12-12 03:03:36,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:36,849][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.0066948989406228065, acc: 1.0)
[2024-12-12 03:03:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:37,255][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.013564305379986763, acc: 1.0)
[2024-12-12 03:03:37,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:37,576][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.0823894590139389, acc: 0.976190447807312)
[2024-12-12 03:03:37,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:37,980][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.09757841378450394, acc: 0.9638554453849792)
[2024-12-12 03:03:38,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:38,392][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.07458394765853882, acc: 0.9729729890823364)
[2024-12-12 03:03:38,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:38,742][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.3027254343032837, acc: 0.9417475461959839)
[2024-12-12 03:03:38,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:39,060][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.22662445902824402, acc: 0.934959352016449)
[2024-12-12 03:03:39,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:39,343][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.09264374524354935, acc: 1.0)
[2024-12-12 03:03:39,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:39,734][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.22946295142173767, acc: 0.9285714030265808)
[2024-12-12 03:03:39,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:40,145][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.14853926002979279, acc: 0.9509803652763367)
[2024-12-12 03:03:40,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:40,549][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.5259588360786438, acc: 0.8340611457824707)
[2024-12-12 03:03:40,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:40,926][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.11416473239660263, acc: 0.9583333134651184)
[2024-12-12 03:03:41,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:41,307][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.14167600870132446, acc: 0.9447852969169617)
[2024-12-12 03:03:41,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:41,687][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.1543324738740921, acc: 0.9280575513839722)
[2024-12-12 03:03:41,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:42,048][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.34330201148986816, acc: 0.8894472122192383)
[2024-12-12 03:03:42,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:42,423][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.019859690219163895, acc: 1.0)
[2024-12-12 03:03:42,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:42,794][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.15115661919116974, acc: 0.939393937587738)
[2024-12-12 03:03:42,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:43,122][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.06110158935189247, acc: 1.0)
[2024-12-12 03:03:43,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:43,509][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.010831530205905437, acc: 1.0)
[2024-12-12 03:03:43,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:43,824][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.18228387832641602, acc: 0.8999999761581421)
[2024-12-12 03:03:43,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:44,246][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.21778695285320282, acc: 0.931034505367279)
[2024-12-12 03:03:44,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:44,647][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.006154980044811964, acc: 1.0)
[2024-12-12 03:03:44,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:45,035][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.018914734944701195, acc: 1.0)
[2024-12-12 03:03:45,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:45,401][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.07376240938901901, acc: 1.0)
[2024-12-12 03:03:45,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:45,767][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.1083136722445488, acc: 0.9523809552192688)
[2024-12-12 03:03:45,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:46,091][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.08061610907316208, acc: 0.9545454382896423)
[2024-12-12 03:03:46,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:46,457][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.2031809687614441, acc: 0.892307698726654)
[2024-12-12 03:03:46,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:46,835][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.028007889166474342, acc: 1.0)
[2024-12-12 03:03:46,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:47,226][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.05396755784749985, acc: 0.9655172228813171)
[2024-12-12 03:03:47,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:47,572][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.06749191135168076, acc: 1.0)
[2024-12-12 03:03:47,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:47,936][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.04561344534158707, acc: 0.9655172228813171)
[2024-12-12 03:03:48,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:48,340][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.30688872933387756, acc: 0.8421052694320679)
[2024-12-12 03:03:48,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:48,685][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.1932772397994995, acc: 0.9473684430122375)
[2024-12-12 03:03:48,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:49,024][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.46723154187202454, acc: 0.8660714030265808)
[2024-12-12 03:03:49,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:49,409][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.12072257697582245, acc: 0.9438202381134033)
[2024-12-12 03:03:49,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:49,735][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.3115527629852295, acc: 0.9101123809814453)
[2024-12-12 03:03:49,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:50,066][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.5030782222747803, acc: 0.8368794322013855)
[2024-12-12 03:03:50,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:50,438][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.24566707015037537, acc: 0.9239130616188049)
[2024-12-12 03:03:50,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:50,811][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.009234631434082985, acc: 1.0)
[2024-12-12 03:03:50,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:51,176][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.0018134081037715077, acc: 1.0)
[2024-12-12 03:03:51,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:51,559][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.06326285749673843, acc: 0.9629629850387573)
[2024-12-12 03:03:51,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:51,887][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.0062630134634673595, acc: 1.0)
[2024-12-12 03:03:51,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:52,185][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.16064420342445374, acc: 0.9056603908538818)
[2024-12-12 03:03:52,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:52,465][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.23971980810165405, acc: 0.8965517282485962)
[2024-12-12 03:03:52,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:53,062][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.7225908637046814, acc: 0.8018018007278442)
[2024-12-12 03:03:53,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:53,494][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.4806082248687744, acc: 0.8591549396514893)
[2024-12-12 03:03:53,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:53,799][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.0041000498458743095, acc: 1.0)
[2024-12-12 03:03:53,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:54,114][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.001720606698654592, acc: 1.0)
[2024-12-12 03:03:54,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:54,459][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.011666004545986652, acc: 1.0)
[2024-12-12 03:03:56,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:57,435][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.647883415222168, acc: 0.800000011920929)
[2024-12-12 03:03:57,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:58,192][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.09737419337034225, acc: 0.976190447807312)
[2024-12-12 03:03:58,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:58,479][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.016234133392572403, acc: 1.0)
[2024-12-12 03:03:58,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:58,831][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.06834903359413147, acc: 0.9833333492279053)
[2024-12-12 03:03:59,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:59,547][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.17545241117477417, acc: 0.9583333134651184)
[2024-12-12 03:03:59,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:59,892][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.0007339542498812079, acc: 1.0)
[2024-12-12 03:04:00,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:00,233][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.05377527326345444, acc: 0.9677419066429138)
[2024-12-12 03:04:00,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:00,596][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.005392329767346382, acc: 1.0)
[2024-12-12 03:04:00,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:00,959][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.04965384304523468, acc: 0.9629629850387573)
[2024-12-12 03:04:01,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:02,054][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.5871633887290955, acc: 0.8220338821411133)
[2024-12-12 03:04:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:02,433][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.19207565486431122, acc: 0.9477611780166626)
[2024-12-12 03:04:02,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:02,830][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.18234409391880035, acc: 0.9197080135345459)
[2024-12-12 03:04:03,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:03,399][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.35263508558273315, acc: 0.8999999761581421)
[2024-12-12 03:04:03,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:03,772][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.013956297188997269, acc: 1.0)
[2024-12-12 03:04:03,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:04,134][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.02927745133638382, acc: 1.0)
[2024-12-12 03:04:04,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:04,448][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.14958593249320984, acc: 0.9523809552192688)
[2024-12-12 03:04:04,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:04,810][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.2311171591281891, acc: 0.9344262480735779)
[2024-12-12 03:04:04,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:05,148][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.025028521195054054, acc: 1.0)
[2024-12-12 03:04:05,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:05,513][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.3196638822555542, acc: 0.9069767594337463)
[2024-12-12 03:04:05,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:05,830][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.04332645982503891, acc: 1.0)
[2024-12-12 03:04:05,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:06,127][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.22114422917366028, acc: 0.9245283007621765)
[2024-12-12 03:04:06,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:06,408][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.11012575775384903, acc: 0.9545454382896423)
[2024-12-12 03:04:06,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:06,771][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.18915851414203644, acc: 0.9599999785423279)
[2024-12-12 03:04:06,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:07,075][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.029540399089455605, acc: 1.0)
[2024-12-12 03:04:07,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:07,438][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.0609632283449173, acc: 0.9545454382896423)
[2024-12-12 03:04:07,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:07,836][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.11702851951122284, acc: 0.9692307710647583)
[2024-12-12 03:04:07,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:08,201][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.1320209503173828, acc: 0.953125)
[2024-12-12 03:04:08,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:08,613][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.1048731803894043, acc: 0.96875)
[2024-12-12 03:04:08,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:09,014][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.2163246124982834, acc: 0.9090909361839294)
[2024-12-12 03:04:09,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:09,387][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.008167097344994545, acc: 1.0)
[2024-12-12 03:04:09,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:09,735][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.005562265403568745, acc: 1.0)
[2024-12-12 03:04:09,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:10,105][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.020330440253019333, acc: 1.0)
[2024-12-12 03:04:10,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:10,478][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.00971045158803463, acc: 1.0)
[2024-12-12 03:04:10,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:10,874][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.19611191749572754, acc: 0.9756097793579102)
[2024-12-12 03:04:10,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:11,213][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.005670086946338415, acc: 1.0)
[2024-12-12 03:04:11,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:11,581][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.01645970530807972, acc: 1.0)
[2024-12-12 03:04:11,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:11,915][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.07126250118017197, acc: 0.9677419066429138)
[2024-12-12 03:04:11,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:12,243][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.6486698389053345, acc: 0.8799999952316284)
[2024-12-12 03:04:12,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:12,630][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.005504506174474955, acc: 1.0)
[2024-12-12 03:04:12,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:13,007][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.27981680631637573, acc: 0.9750000238418579)
[2024-12-12 03:04:13,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:13,372][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.010883606038987637, acc: 1.0)
[2024-12-12 03:04:13,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:13,706][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.20368628203868866, acc: 0.9197080135345459)
[2024-12-12 03:04:13,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:14,075][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.09884627908468246, acc: 0.9862068891525269)
[2024-12-12 03:04:14,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:14,468][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.17707090079784393, acc: 0.9642857313156128)
[2024-12-12 03:04:14,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:14,859][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.2889363467693329, acc: 0.9139072895050049)
[2024-12-12 03:04:14,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:15,252][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.04642157629132271, acc: 0.9829059839248657)
[2024-12-12 03:04:15,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:16,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:16,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:17,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:17,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:17,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:18,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:18,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:18,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:19,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:19,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:19,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:20,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:20,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:20,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:21,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:21,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:22,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:22,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:22,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:23,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:23,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:23,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:24,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:24,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:24,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:25,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:25,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:26,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:26,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:27,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:27,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:27,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:28,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:28,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:28,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:29,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:29,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:29,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:30,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:30,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:31,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:31,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:31,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:32,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:32,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:32,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:33,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:33,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:33,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:34,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:34,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:34,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:35,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:35,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:35,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:36,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:36,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:36,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:37,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:37,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:38,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:38,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:38,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:39,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:39,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:39,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:40,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:40,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:41,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:41,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:41,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:42,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:42,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:42,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:43,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:43,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:44,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:44,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:44,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:45,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:45,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:45,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:46,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:46,901][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.7357, device='cuda:0') eval_epoch_loss=tensor(1.0064, device='cuda:0') eval_epoch_acc=tensor(0.7965, device='cuda:0')
[2024-12-12 03:04:46,903][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:04:46,903][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:04:47,528][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_8_step_558_loss_1.0063797235488892/model.pt
[2024-12-12 03:04:47,540][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:04:47,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:47,935][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.17537769675254822, acc: 0.9599999785423279)
[2024-12-12 03:04:48,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:48,318][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.026067107915878296, acc: 1.0)
[2024-12-12 03:04:48,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:48,688][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.012780534103512764, acc: 1.0)
[2024-12-12 03:04:48,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:49,043][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.03452542796730995, acc: 1.0)
[2024-12-12 03:04:49,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:49,439][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.14869412779808044, acc: 0.9555555582046509)
[2024-12-12 03:04:49,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:49,744][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.11265924572944641, acc: 0.9740259647369385)
[2024-12-12 03:04:49,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:50,124][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.07706958800554276, acc: 0.9791666865348816)
[2024-12-12 03:04:50,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:50,423][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.16015072166919708, acc: 0.9655172228813171)
[2024-12-12 03:04:50,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:50,750][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.12453416734933853, acc: 0.9523809552192688)
[2024-12-12 03:04:50,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:51,138][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.10101280361413956, acc: 0.9473684430122375)
[2024-12-12 03:04:51,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:51,447][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.033642254769802094, acc: 1.0)
[2024-12-12 03:04:51,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:51,823][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.17599128186702728, acc: 0.9572192430496216)
[2024-12-12 03:04:51,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:52,174][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.0650186613202095, acc: 0.9838709831237793)
[2024-12-12 03:04:52,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:52,589][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.11059977114200592, acc: 0.9658119678497314)
[2024-12-12 03:04:52,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:52,911][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.30775102972984314, acc: 0.9030612111091614)
[2024-12-12 03:04:53,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:53,238][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.1494666337966919, acc: 0.955974817276001)
[2024-12-12 03:04:53,674][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.2079, train_epoch_loss=0.1889, epoch time 365.3173970095813s
[2024-12-12 03:04:53,674][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 03:04:53,674][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 03:04:53,675][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 03:04:53,675][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 23
[2024-12-12 03:04:53,675][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 03:04:54,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:54,596][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.059394218027591705, acc: 1.0)
[2024-12-12 03:04:54,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:54,989][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.058840490877628326, acc: 0.9599999785423279)
[2024-12-12 03:04:55,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:55,391][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.1362255960702896, acc: 0.9459459185600281)
[2024-12-12 03:04:55,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:55,815][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.01495517510920763, acc: 1.0)
[2024-12-12 03:04:55,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:56,219][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.05158726125955582, acc: 1.0)
[2024-12-12 03:04:56,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:56,598][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.02189505659043789, acc: 1.0)
[2024-12-12 03:04:56,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:56,973][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.15382561087608337, acc: 0.9387755393981934)
[2024-12-12 03:04:57,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:57,286][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.4060928523540497, acc: 0.9333333373069763)
[2024-12-12 03:04:57,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:57,682][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.008086130954325199, acc: 1.0)
[2024-12-12 03:04:57,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:58,060][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.015534652397036552, acc: 1.0)
[2024-12-12 03:04:58,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:58,408][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.0879017636179924, acc: 0.9629629850387573)
[2024-12-12 03:04:58,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:58,738][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.04518500715494156, acc: 1.0)
[2024-12-12 03:04:58,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:59,131][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.005479393061250448, acc: 1.0)
[2024-12-12 03:04:59,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:59,497][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.16596469283103943, acc: 0.9130434989929199)
[2024-12-12 03:04:59,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:59,894][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.03811032325029373, acc: 1.0)
[2024-12-12 03:05:00,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:00,284][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.049749407917261124, acc: 0.9795918464660645)
[2024-12-12 03:05:00,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:00,667][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.0022958284243941307, acc: 1.0)
[2024-12-12 03:05:00,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:01,054][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.0034921104088425636, acc: 1.0)
[2024-12-12 03:05:01,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:01,426][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.03896109759807587, acc: 0.9722222089767456)
[2024-12-12 03:05:01,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:01,763][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.12074577063322067, acc: 0.9473684430122375)
[2024-12-12 03:05:01,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:02,143][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.029472945258021355, acc: 1.0)
[2024-12-12 03:05:02,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:02,485][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.05737735331058502, acc: 0.9655172228813171)
[2024-12-12 03:05:02,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:02,826][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.07896801829338074, acc: 0.9599999785423279)
[2024-12-12 03:05:02,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:03,146][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.09685806930065155, acc: 0.9523809552192688)
[2024-12-12 03:05:03,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:03,430][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.04792744666337967, acc: 1.0)
[2024-12-12 03:05:03,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:03,793][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.17063592374324799, acc: 0.9245283007621765)
[2024-12-12 03:05:03,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:04,184][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.2597807049751282, acc: 0.9452054500579834)
[2024-12-12 03:05:04,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:05,563][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.5700516700744629, acc: 0.8063241243362427)
[2024-12-12 03:05:05,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:05,858][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.06768914312124252, acc: 0.9767441749572754)
[2024-12-12 03:05:05,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:06,238][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.10115863382816315, acc: 0.9638554453849792)
[2024-12-12 03:05:06,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:06,590][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.13903923332691193, acc: 0.9506173133850098)
[2024-12-12 03:05:06,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:06,974][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.05563199892640114, acc: 0.9642857313156128)
[2024-12-12 03:05:07,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:07,346][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.07160989195108414, acc: 0.9629629850387573)
[2024-12-12 03:05:07,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:07,714][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.0019575245678424835, acc: 1.0)
[2024-12-12 03:05:07,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:08,102][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.2585269808769226, acc: 0.9411764740943909)
[2024-12-12 03:05:08,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:08,508][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.28374865651130676, acc: 0.9180327653884888)
[2024-12-12 03:05:08,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:08,842][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.11525555700063705, acc: 0.9682539701461792)
[2024-12-12 03:05:08,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:09,206][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.0792011171579361, acc: 0.9830508232116699)
[2024-12-12 03:05:09,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:09,595][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.08146560937166214, acc: 0.977011501789093)
[2024-12-12 03:05:09,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:09,918][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.2804694175720215, acc: 0.9523809552192688)
[2024-12-12 03:05:10,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:10,327][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.040427424013614655, acc: 1.0)
[2024-12-12 03:05:10,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:10,772][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.14498093724250793, acc: 0.9594594836235046)
[2024-12-12 03:05:10,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:11,134][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.14398708939552307, acc: 0.9538461565971375)
[2024-12-12 03:05:11,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:11,538][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.18903207778930664, acc: 0.9696969985961914)
[2024-12-12 03:05:11,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:11,965][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.163426473736763, acc: 0.9484536051750183)
[2024-12-12 03:05:12,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:12,372][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.22735807299613953, acc: 0.9338235259056091)
[2024-12-12 03:05:12,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:12,736][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.04566814377903938, acc: 1.0)
[2024-12-12 03:05:12,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:13,066][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.003309589345008135, acc: 1.0)
[2024-12-12 03:05:13,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:13,439][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.04277896136045456, acc: 0.9642857313156128)
[2024-12-12 03:05:13,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:13,776][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.009086161851882935, acc: 1.0)
[2024-12-12 03:05:13,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:14,171][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.2318505197763443, acc: 0.9122806787490845)
[2024-12-12 03:05:14,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:14,531][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.07095489650964737, acc: 0.9682539701461792)
[2024-12-12 03:05:14,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:14,863][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.15250656008720398, acc: 0.9436619877815247)
[2024-12-12 03:05:14,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:15,322][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.8047139644622803, acc: 0.746666669845581)
[2024-12-12 03:05:15,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:15,627][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.18116183578968048, acc: 0.9459459185600281)
[2024-12-12 03:05:15,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:16,006][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.0011355491587892175, acc: 1.0)
[2024-12-12 03:05:17,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:19,072][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.6849448680877686, acc: 0.7713310718536377)
[2024-12-12 03:05:19,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:20,348][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 1.1376527547836304, acc: 0.6775599122047424)
[2024-12-12 03:05:20,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:20,965][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.45546844601631165, acc: 0.8579545617103577)
[2024-12-12 03:05:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:21,532][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.16723303496837616, acc: 0.9485294222831726)
[2024-12-12 03:05:21,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:22,092][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.3786134123802185, acc: 0.8695651888847351)
[2024-12-12 03:05:22,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:22,506][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.25248274207115173, acc: 0.925000011920929)
[2024-12-12 03:05:22,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:22,903][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.005224259570240974, acc: 1.0)
[2024-12-12 03:05:23,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:23,241][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.09492263942956924, acc: 0.9722222089767456)
[2024-12-12 03:05:23,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:23,680][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.11952638626098633, acc: 0.984375)
[2024-12-12 03:05:23,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:24,111][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.00509240897372365, acc: 1.0)
[2024-12-12 03:05:24,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:24,511][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.12961526215076447, acc: 0.9821428656578064)
[2024-12-12 03:05:24,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:24,895][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.11375612765550613, acc: 0.9666666388511658)
[2024-12-12 03:05:25,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:25,292][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.0041796499863266945, acc: 1.0)
[2024-12-12 03:05:25,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:25,697][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.05820601433515549, acc: 0.9722222089767456)
[2024-12-12 03:05:25,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:26,080][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.3225567638874054, acc: 0.8484848737716675)
[2024-12-12 03:05:26,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:26,510][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.5006340146064758, acc: 0.845588207244873)
[2024-12-12 03:05:26,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:26,929][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.3070005774497986, acc: 0.89682537317276)
[2024-12-12 03:05:27,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:27,348][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.7371309995651245, acc: 0.7846153974533081)
[2024-12-12 03:05:27,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:27,728][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.3889100253582001, acc: 0.8673469424247742)
[2024-12-12 03:05:27,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:28,130][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.42439156770706177, acc: 0.8731343150138855)
[2024-12-12 03:05:28,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:28,518][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 1.1442654132843018, acc: 0.7189781069755554)
[2024-12-12 03:05:28,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:28,823][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.0009444244205951691, acc: 1.0)
[2024-12-12 03:05:28,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:29,183][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.12436925619840622, acc: 0.9583333134651184)
[2024-12-12 03:05:29,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:29,585][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.016737179830670357, acc: 1.0)
[2024-12-12 03:05:29,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:29,978][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.010882189497351646, acc: 1.0)
[2024-12-12 03:05:30,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:30,353][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.03739313408732414, acc: 1.0)
[2024-12-12 03:05:30,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:30,749][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.048583127558231354, acc: 1.0)
[2024-12-12 03:05:30,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:31,113][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.02604810521006584, acc: 1.0)
[2024-12-12 03:05:31,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:31,491][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.12408852577209473, acc: 0.95652174949646)
[2024-12-12 03:05:31,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:31,920][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.2058919370174408, acc: 0.9399999976158142)
[2024-12-12 03:05:32,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:32,309][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.06652417033910751, acc: 1.0)
[2024-12-12 03:05:32,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:32,786][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.19336816668510437, acc: 0.8999999761581421)
[2024-12-12 03:05:32,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:33,125][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.43730926513671875, acc: 0.8834951519966125)
[2024-12-12 03:05:33,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:34,237][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.42390844225883484, acc: 0.8786407709121704)
[2024-12-12 03:05:34,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:35,050][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.7334665060043335, acc: 0.774193525314331)
[2024-12-12 03:05:35,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:35,844][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.45721447467803955, acc: 0.8793103694915771)
[2024-12-12 03:05:36,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:36,580][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.3697386384010315, acc: 0.8526315689086914)
[2024-12-12 03:05:36,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:37,565][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.5323008298873901, acc: 0.8514851331710815)
[2024-12-12 03:05:37,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:37,947][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.31935790181159973, acc: 0.9032257795333862)
[2024-12-12 03:05:38,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:38,375][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.18908703327178955, acc: 0.9855072498321533)
[2024-12-12 03:05:38,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:38,778][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.45853057503700256, acc: 0.848739504814148)
[2024-12-12 03:05:38,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:39,144][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.3266771733760834, acc: 0.875)
[2024-12-12 03:05:39,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:39,543][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.5165473222732544, acc: 0.8394160866737366)
[2024-12-12 03:05:39,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:39,925][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.30366238951683044, acc: 0.9104477763175964)
[2024-12-12 03:05:40,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:40,259][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.013310949318110943, acc: 1.0)
[2024-12-12 03:05:40,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:40,637][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.020781245082616806, acc: 1.0)
[2024-12-12 03:05:40,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:40,960][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.09434355050325394, acc: 0.95652174949646)
[2024-12-12 03:05:41,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:41,329][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.03173325955867767, acc: 1.0)
[2024-12-12 03:05:41,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:41,661][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.047209132462739944, acc: 1.0)
[2024-12-12 03:05:41,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:42,015][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.00529678026214242, acc: 1.0)
[2024-12-12 03:05:42,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:42,353][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.04237302392721176, acc: 1.0)
[2024-12-12 03:05:42,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:42,660][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.0023064042907208204, acc: 1.0)
[2024-12-12 03:05:42,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:43,020][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.002051182324066758, acc: 1.0)
[2024-12-12 03:05:43,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:43,398][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.011007199063897133, acc: 1.0)
[2024-12-12 03:05:43,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:43,752][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.038619231432676315, acc: 0.9846153855323792)
[2024-12-12 03:05:43,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:44,147][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.09797433018684387, acc: 0.9649122953414917)
[2024-12-12 03:05:44,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:44,497][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.14460772275924683, acc: 0.9473684430122375)
[2024-12-12 03:05:44,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:44,884][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.02416880987584591, acc: 1.0)
[2024-12-12 03:05:45,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:45,271][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.08538904041051865, acc: 0.9795918464660645)
[2024-12-12 03:05:45,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:45,633][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.09135837107896805, acc: 0.9545454382896423)
[2024-12-12 03:05:45,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:46,031][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.22575058043003082, acc: 0.920634925365448)
[2024-12-12 03:05:46,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:46,443][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.3011375963687897, acc: 0.9024389982223511)
[2024-12-12 03:05:46,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:46,809][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.06017954275012016, acc: 0.9838709831237793)
[2024-12-12 03:05:47,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:47,661][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.5647076368331909, acc: 0.8288973569869995)
[2024-12-12 03:05:47,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:47,990][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.0958315059542656, acc: 0.9733333587646484)
[2024-12-12 03:05:48,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:48,383][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.06797091662883759, acc: 0.9807692170143127)
[2024-12-12 03:05:48,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:48,685][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.016561292111873627, acc: 1.0)
[2024-12-12 03:05:48,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:49,018][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.0371989905834198, acc: 1.0)
[2024-12-12 03:05:49,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:49,464][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.5361758470535278, acc: 0.8466257452964783)
[2024-12-12 03:05:49,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:49,905][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.6779869794845581, acc: 0.7569444179534912)
[2024-12-12 03:05:49,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:50,279][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.30641576647758484, acc: 0.8999999761581421)
[2024-12-12 03:05:51,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:51,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:51,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:52,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:52,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:52,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:53,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:53,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:53,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:54,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:54,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:54,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:55,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:55,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:55,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:56,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:56,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:57,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:57,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:57,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:58,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:58,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:58,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:59,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:59,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:59,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:00,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:00,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:00,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:01,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:01,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:01,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:02,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:02,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:03,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:03,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:03,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:04,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:04,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:04,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:04,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:05,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:05,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:05,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:06,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:06,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:07,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:07,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:07,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:08,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:08,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:08,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:09,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:09,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:10,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:10,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:10,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:11,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:11,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:11,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:12,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:12,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:13,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:13,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:13,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:14,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:14,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:15,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:15,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:16,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:16,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:16,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:17,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:17,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:18,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:18,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:18,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:19,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:19,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:19,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:20,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:20,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:20,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:21,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:21,879][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4898, device='cuda:0') eval_epoch_loss=tensor(0.9122, device='cuda:0') eval_epoch_acc=tensor(0.8119, device='cuda:0')
[2024-12-12 03:06:21,880][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:06:21,881][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:06:22,180][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_9_step_127_loss_0.9121833443641663/model.pt
[2024-12-12 03:06:22,191][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:06:22,199][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 9 is 0.8118541240692139
[2024-12-12 03:06:22,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:22,667][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.34956708550453186, acc: 0.8690476417541504)
[2024-12-12 03:06:22,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:23,059][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.3783019185066223, acc: 0.892307698726654)
[2024-12-12 03:06:23,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:23,456][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.37568995356559753, acc: 0.8823529481887817)
[2024-12-12 03:06:23,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:23,776][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.017689861357212067, acc: 1.0)
[2024-12-12 03:06:23,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:24,042][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.0101348040625453, acc: 1.0)
[2024-12-12 03:06:24,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:24,344][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.04277653992176056, acc: 0.96875)
[2024-12-12 03:06:24,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:24,688][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.1578429639339447, acc: 0.95652174949646)
[2024-12-12 03:06:24,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:24,993][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.24351932108402252, acc: 0.9714285731315613)
[2024-12-12 03:06:25,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:25,294][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.07883629202842712, acc: 0.9615384340286255)
[2024-12-12 03:06:25,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:25,640][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.15980839729309082, acc: 0.9523809552192688)
[2024-12-12 03:06:25,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:25,940][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.13305456936359406, acc: 0.9666666388511658)
[2024-12-12 03:06:26,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:26,242][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.17418637871742249, acc: 0.9130434989929199)
[2024-12-12 03:06:26,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:26,623][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.011884110048413277, acc: 1.0)
[2024-12-12 03:06:26,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:27,008][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.5327479839324951, acc: 0.9230769276618958)
[2024-12-12 03:06:27,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:27,373][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.029811348766088486, acc: 1.0)
[2024-12-12 03:06:27,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:27,764][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.161343052983284, acc: 0.9729729890823364)
[2024-12-12 03:06:27,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:28,321][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.5006792545318604, acc: 0.8421052694320679)
[2024-12-12 03:06:28,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:28,719][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.4918590486049652, acc: 0.8208954930305481)
[2024-12-12 03:06:28,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:29,099][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.34007149934768677, acc: 0.8877550959587097)
[2024-12-12 03:06:29,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:29,532][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.4430907666683197, acc: 0.8404255509376526)
[2024-12-12 03:06:29,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:29,898][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.29488641023635864, acc: 0.8857142925262451)
[2024-12-12 03:06:30,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:30,259][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.1030958741903305, acc: 0.9642857313156128)
[2024-12-12 03:06:30,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:30,629][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.20125415921211243, acc: 0.95652174949646)
[2024-12-12 03:06:30,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:30,998][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.06347410380840302, acc: 0.9655172228813171)
[2024-12-12 03:06:31,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:31,399][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.1344209611415863, acc: 0.97826087474823)
[2024-12-12 03:06:31,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:31,740][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.04981880262494087, acc: 1.0)
[2024-12-12 03:06:31,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:32,079][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.13796913623809814, acc: 0.9649122953414917)
[2024-12-12 03:06:32,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:32,419][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.23854461312294006, acc: 0.8783783912658691)
[2024-12-12 03:06:32,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:32,789][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.07344970852136612, acc: 0.9642857313156128)
[2024-12-12 03:06:32,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:33,126][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.043708883225917816, acc: 1.0)
[2024-12-12 03:06:33,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:33,449][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.3743434250354767, acc: 0.7894737124443054)
[2024-12-12 03:06:34,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:35,173][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.3612876534461975, acc: 0.9054054021835327)
[2024-12-12 03:06:35,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:35,462][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 0.3867923617362976, acc: 0.9074074029922485)
[2024-12-12 03:06:35,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:35,853][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.5186048746109009, acc: 0.8837209343910217)
[2024-12-12 03:06:35,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:36,435][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.43567362427711487, acc: 0.8941176533699036)
[2024-12-12 03:06:36,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:36,986][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.6506811380386353, acc: 0.7752808928489685)
[2024-12-12 03:06:37,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:37,372][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.05589478090405464, acc: 0.9772727489471436)
[2024-12-12 03:06:37,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:37,715][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.17725154757499695, acc: 0.9047619104385376)
[2024-12-12 03:06:37,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:38,100][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.2615398168563843, acc: 0.9655172228813171)
[2024-12-12 03:06:38,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:38,431][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.058339186012744904, acc: 0.9591836929321289)
[2024-12-12 03:06:38,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:38,780][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.034757569432258606, acc: 0.9800000190734863)
[2024-12-12 03:06:38,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:39,186][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.11589598655700684, acc: 0.9722222089767456)
[2024-12-12 03:06:39,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:39,575][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.6412635445594788, acc: 0.8039215803146362)
[2024-12-12 03:06:39,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:40,643][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.38785678148269653, acc: 0.9041095972061157)
[2024-12-12 03:06:40,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:41,022][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.06813906878232956, acc: 0.9583333134651184)
[2024-12-12 03:06:41,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:41,397][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.09175887703895569, acc: 0.9629629850387573)
[2024-12-12 03:06:41,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:41,708][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.3335004448890686, acc: 0.9642857313156128)
[2024-12-12 03:06:41,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:42,247][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.6034796237945557, acc: 0.8230088353157043)
[2024-12-12 03:06:42,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:42,539][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.3555186092853546, acc: 0.8695651888847351)
[2024-12-12 03:06:42,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:42,963][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.1557881385087967, acc: 0.9318181872367859)
[2024-12-12 03:06:43,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:43,871][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.6433932185173035, acc: 0.7862595319747925)
[2024-12-12 03:06:44,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:44,536][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.44322359561920166, acc: 0.8592592477798462)
[2024-12-12 03:06:44,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:44,909][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.04630979150533676, acc: 0.9836065769195557)
[2024-12-12 03:06:45,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:45,284][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.4313206672668457, acc: 0.9166666865348816)
[2024-12-12 03:06:45,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:45,620][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.009678900241851807, acc: 1.0)
[2024-12-12 03:06:45,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:45,920][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.02602352201938629, acc: 1.0)
[2024-12-12 03:06:46,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:46,296][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.03576985001564026, acc: 1.0)
[2024-12-12 03:06:46,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:46,646][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.3401758372783661, acc: 0.9003021121025085)
[2024-12-12 03:06:46,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:46,951][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.45041853189468384, acc: 0.8645533323287964)
[2024-12-12 03:06:47,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:47,432][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.3347965180873871, acc: 0.8999999761581421)
[2024-12-12 03:06:47,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:47,954][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.5779435634613037, acc: 0.8273921012878418)
[2024-12-12 03:06:48,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:48,346][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.3488166034221649, acc: 0.8932384252548218)
[2024-12-12 03:06:48,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:48,638][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.18987222015857697, acc: 0.9599999785423279)
[2024-12-12 03:06:48,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:49,189][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.4983575940132141, acc: 0.8372092843055725)
[2024-12-12 03:06:49,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:49,986][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.8589540123939514, acc: 0.7142857313156128)
[2024-12-12 03:06:50,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:50,897][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.6271246671676636, acc: 0.8409090638160706)
[2024-12-12 03:06:51,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:51,633][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.20597293972969055, acc: 0.9411764740943909)
[2024-12-12 03:06:51,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:52,701][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.5253338813781738, acc: 0.8333333134651184)
[2024-12-12 03:06:52,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:53,650][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.12425247579813004, acc: 0.9677419066429138)
[2024-12-12 03:06:53,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:53,990][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.020014595240354538, acc: 1.0)
[2024-12-12 03:06:54,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:54,332][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.08917570859193802, acc: 1.0)
[2024-12-12 03:06:54,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:54,733][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.1783936768770218, acc: 0.9411764740943909)
[2024-12-12 03:06:54,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:55,050][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.4559589922428131, acc: 0.8529411554336548)
[2024-12-12 03:06:55,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:55,464][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.2873719334602356, acc: 0.9152542352676392)
[2024-12-12 03:06:55,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:55,821][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.35311293601989746, acc: 0.8805969953536987)
[2024-12-12 03:06:55,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:56,187][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.2896858751773834, acc: 0.8834951519966125)
[2024-12-12 03:06:56,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:56,589][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.15853329002857208, acc: 0.9523809552192688)
[2024-12-12 03:06:56,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:56,942][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.05170242488384247, acc: 0.9780219793319702)
[2024-12-12 03:06:57,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:57,333][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.13393069803714752, acc: 0.9506726264953613)
[2024-12-12 03:06:57,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:57,732][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.24084240198135376, acc: 0.9173228144645691)
[2024-12-12 03:06:57,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:58,057][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.20327338576316833, acc: 0.9353448152542114)
[2024-12-12 03:06:58,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:58,470][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.2385297268629074, acc: 0.9347826242446899)
[2024-12-12 03:06:58,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:58,839][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.17799469828605652, acc: 0.9533073902130127)
[2024-12-12 03:06:58,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:59,190][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.06730445474386215, acc: 0.97826087474823)
[2024-12-12 03:06:59,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:59,540][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.0045813219621777534, acc: 1.0)
[2024-12-12 03:06:59,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:59,864][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.012699316255748272, acc: 1.0)
[2024-12-12 03:06:59,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:00,234][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.1004035547375679, acc: 0.978723406791687)
[2024-12-12 03:07:00,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:00,916][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.11847300827503204, acc: 0.9692307710647583)
[2024-12-12 03:07:01,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:01,308][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.04429497569799423, acc: 0.9864864945411682)
[2024-12-12 03:07:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:01,700][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.04745269939303398, acc: 0.9767441749572754)
[2024-12-12 03:07:01,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:02,232][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.07885075360536575, acc: 0.9639639854431152)
[2024-12-12 03:07:02,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:02,610][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.06070111691951752, acc: 0.9777777791023254)
[2024-12-12 03:07:02,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:02,973][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.052204228937625885, acc: 1.0)
[2024-12-12 03:07:03,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:03,323][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.0013902479549869895, acc: 1.0)
[2024-12-12 03:07:03,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:03,664][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.0014329352416098118, acc: 1.0)
[2024-12-12 03:07:03,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:04,070][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.45496925711631775, acc: 0.942307710647583)
[2024-12-12 03:07:04,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:04,836][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.20270349085330963, acc: 0.9402173757553101)
[2024-12-12 03:07:04,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:05,377][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.19651557505130768, acc: 0.9318181872367859)
[2024-12-12 03:07:05,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:05,809][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.4046638309955597, acc: 0.8617021441459656)
[2024-12-12 03:07:05,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:06,211][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.06351155042648315, acc: 1.0)
[2024-12-12 03:07:06,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:06,597][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.07834085077047348, acc: 0.9833333492279053)
[2024-12-12 03:07:06,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:06,878][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.028618257492780685, acc: 1.0)
[2024-12-12 03:07:06,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:07,219][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.19416986405849457, acc: 0.9333333373069763)
[2024-12-12 03:07:07,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:07,600][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.8410364985466003, acc: 0.7473683953285217)
[2024-12-12 03:07:07,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:07,946][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.5351065993309021, acc: 0.8333333134651184)
[2024-12-12 03:07:08,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:08,391][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.7252965569496155, acc: 0.7722222208976746)
[2024-12-12 03:07:08,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:08,883][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 1.1465861797332764, acc: 0.6743119359016418)
[2024-12-12 03:07:09,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:09,354][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.7210126519203186, acc: 0.7769230604171753)
[2024-12-12 03:07:09,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:09,684][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.011499925516545773, acc: 1.0)
[2024-12-12 03:07:09,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:10,057][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.14367137849330902, acc: 0.9583333134651184)
[2024-12-12 03:07:10,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:10,424][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.19671817123889923, acc: 0.9090909361839294)
[2024-12-12 03:07:10,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:10,800][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.48359254002571106, acc: 0.8518518805503845)
[2024-12-12 03:07:10,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:11,133][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.20656552910804749, acc: 0.9428571462631226)
[2024-12-12 03:07:11,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:11,472][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.07674016058444977, acc: 0.9772727489471436)
[2024-12-12 03:07:11,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:11,810][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.1609412282705307, acc: 0.9772727489471436)
[2024-12-12 03:07:11,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:12,397][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.47118544578552246, acc: 0.8387096524238586)
[2024-12-12 03:07:12,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:12,919][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.2323572188615799, acc: 0.9090909361839294)
[2024-12-12 03:07:13,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:13,274][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.00185921392403543, acc: 1.0)
[2024-12-12 03:07:13,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:13,628][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.05967208370566368, acc: 1.0)
[2024-12-12 03:07:13,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:13,983][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.11226633936166763, acc: 0.9354838728904724)
[2024-12-12 03:07:14,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:14,316][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.09879046678543091, acc: 0.949999988079071)
[2024-12-12 03:07:14,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:14,662][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.07764918357133865, acc: 1.0)
[2024-12-12 03:07:14,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:14,891][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.037465326488018036, acc: 1.0)
[2024-12-12 03:07:14,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:15,213][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.022837333381175995, acc: 1.0)
[2024-12-12 03:07:15,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:15,551][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.18589618802070618, acc: 0.9558823704719543)
[2024-12-12 03:07:15,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:15,934][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.035663142800331116, acc: 1.0)
[2024-12-12 03:07:16,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:16,288][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.01056909654289484, acc: 1.0)
[2024-12-12 03:07:16,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:16,576][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.05763932317495346, acc: 0.9599999785423279)
[2024-12-12 03:07:16,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:16,913][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.05233941599726677, acc: 1.0)
[2024-12-12 03:07:17,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:17,303][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.16536083817481995, acc: 0.9649122953414917)
[2024-12-12 03:07:17,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:17,717][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.14065642654895782, acc: 0.9285714030265808)
[2024-12-12 03:07:17,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:18,076][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.08428672701120377, acc: 0.9736841917037964)
[2024-12-12 03:07:18,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:18,643][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.17795389890670776, acc: 0.9433962106704712)
[2024-12-12 03:07:18,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:19,233][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.1786872297525406, acc: 0.9416666626930237)
[2024-12-12 03:07:19,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:19,616][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.026423387229442596, acc: 1.0)
[2024-12-12 03:07:19,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:19,932][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.04181887209415436, acc: 1.0)
[2024-12-12 03:07:20,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:20,270][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.40597838163375854, acc: 0.8533333539962769)
[2024-12-12 03:07:20,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:20,662][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.31448185443878174, acc: 0.9375)
[2024-12-12 03:07:20,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:21,545][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.6871442794799805, acc: 0.8240000009536743)
[2024-12-12 03:07:21,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:21,925][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.3195522725582123, acc: 0.9101123809814453)
[2024-12-12 03:07:22,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:22,327][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.19679225981235504, acc: 0.9324324131011963)
[2024-12-12 03:07:22,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:22,784][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.10832270234823227, acc: 0.9655172228813171)
[2024-12-12 03:07:22,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:23,144][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.007509337272495031, acc: 1.0)
[2024-12-12 03:07:23,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:24,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:24,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:25,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:25,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:25,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:25,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:26,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:26,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:26,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:27,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:27,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:28,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:28,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:28,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:29,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:29,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:29,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:30,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:30,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:30,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:31,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:31,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:32,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:32,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:33,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:33,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:33,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:34,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:34,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:35,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:35,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:35,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:36,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:36,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:36,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:37,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:37,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:37,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:38,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:38,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:39,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:39,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:39,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:40,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:40,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:40,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:41,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:41,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:41,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:42,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:42,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:42,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:43,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:43,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:43,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:44,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:44,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:44,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:45,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:45,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:46,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:46,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:46,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:47,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:47,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:47,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:48,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:48,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:49,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:49,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:49,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:50,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:50,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:51,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:51,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:51,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:52,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:52,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:52,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:53,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:53,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:53,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:54,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:54,730][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4862, device='cuda:0') eval_epoch_loss=tensor(0.9107, device='cuda:0') eval_epoch_acc=tensor(0.8055, device='cuda:0')
[2024-12-12 03:07:54,731][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:07:54,732][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:07:55,042][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_9_step_270_loss_0.9107370972633362/model.pt
[2024-12-12 03:07:55,050][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:07:55,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:55,430][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.0206303708255291, acc: 1.0)
[2024-12-12 03:07:55,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:55,774][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.043738722801208496, acc: 0.96875)
[2024-12-12 03:07:55,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:56,152][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.07042572647333145, acc: 0.9666666388511658)
[2024-12-12 03:07:56,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:56,535][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.11843898147344589, acc: 0.949999988079071)
[2024-12-12 03:07:56,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:56,863][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.010121569037437439, acc: 1.0)
[2024-12-12 03:07:56,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:57,231][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.011877951212227345, acc: 1.0)
[2024-12-12 03:07:57,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:57,606][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.03935921937227249, acc: 0.9655172228813171)
[2024-12-12 03:07:57,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:57,964][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.0477617122232914, acc: 1.0)
[2024-12-12 03:07:58,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:58,303][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.179364413022995, acc: 0.957446813583374)
[2024-12-12 03:07:58,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:58,611][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.13744057714939117, acc: 0.9375)
[2024-12-12 03:07:58,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:58,905][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.06600511819124222, acc: 0.9772727489471436)
[2024-12-12 03:07:59,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:59,342][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.16538654267787933, acc: 0.9277108311653137)
[2024-12-12 03:07:59,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:59,746][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.32704049348831177, acc: 0.9166666865348816)
[2024-12-12 03:07:59,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:00,073][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.0214365404099226, acc: 1.0)
[2024-12-12 03:08:00,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:00,397][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.09720836579799652, acc: 0.970588207244873)
[2024-12-12 03:08:00,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:00,716][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.0480034202337265, acc: 0.9750000238418579)
[2024-12-12 03:08:00,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:01,050][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.1644323170185089, acc: 0.9375)
[2024-12-12 03:08:01,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:01,399][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.19734998047351837, acc: 0.9200000166893005)
[2024-12-12 03:08:01,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:01,712][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.09369631111621857, acc: 0.9670329689979553)
[2024-12-12 03:08:01,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:02,035][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.23879683017730713, acc: 0.9378882050514221)
[2024-12-12 03:08:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:02,395][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.28379079699516296, acc: 0.9123711585998535)
[2024-12-12 03:08:02,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:02,773][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.04098391905426979, acc: 1.0)
[2024-12-12 03:08:02,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:03,100][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.11628471314907074, acc: 0.976190447807312)
[2024-12-12 03:08:03,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:03,507][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.06493101269006729, acc: 0.982758641242981)
[2024-12-12 03:08:03,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:03,999][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.2337610274553299, acc: 0.9636363387107849)
[2024-12-12 03:08:04,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:04,544][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.33372384309768677, acc: 0.907216489315033)
[2024-12-12 03:08:04,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:04,841][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.05757470056414604, acc: 0.982758641242981)
[2024-12-12 03:08:04,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:05,163][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.01308420766144991, acc: 1.0)
[2024-12-12 03:08:05,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:05,491][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.14250317215919495, acc: 0.9473684430122375)
[2024-12-12 03:08:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:05,822][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.012948657386004925, acc: 1.0)
[2024-12-12 03:08:05,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:06,197][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.04923337697982788, acc: 0.96875)
[2024-12-12 03:08:06,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:06,552][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.11723410338163376, acc: 0.9811320900917053)
[2024-12-12 03:08:06,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:06,880][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.004078454338014126, acc: 1.0)
[2024-12-12 03:08:06,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:07,213][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.06637076288461685, acc: 0.970588207244873)
[2024-12-12 03:08:07,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:07,541][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.011277846060693264, acc: 1.0)
[2024-12-12 03:08:07,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:07,868][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.05281291902065277, acc: 0.9836065769195557)
[2024-12-12 03:08:07,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:08,291][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.02789701707661152, acc: 1.0)
[2024-12-12 03:08:08,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:08,661][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.000820719578769058, acc: 1.0)
[2024-12-12 03:08:08,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:09,031][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.061490315943956375, acc: 0.9855072498321533)
[2024-12-12 03:08:09,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:09,492][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.02449772320687771, acc: 1.0)
[2024-12-12 03:08:09,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:09,866][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.1456589549779892, acc: 0.9518072009086609)
[2024-12-12 03:08:09,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:10,268][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.0753295049071312, acc: 0.9743589758872986)
[2024-12-12 03:08:10,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:10,670][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.11030728369951248, acc: 0.9387755393981934)
[2024-12-12 03:08:10,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:11,030][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.03040650300681591, acc: 1.0)
[2024-12-12 03:08:11,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:11,370][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.019894421100616455, acc: 1.0)
[2024-12-12 03:08:11,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:11,691][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.07172133028507233, acc: 0.9677419066429138)
[2024-12-12 03:08:11,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:12,021][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.18315552175045013, acc: 0.9032257795333862)
[2024-12-12 03:08:12,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:12,406][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.11629442870616913, acc: 0.9850746393203735)
[2024-12-12 03:08:12,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:12,830][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.13047140836715698, acc: 0.9519230723381042)
[2024-12-12 03:08:12,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:13,162][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.05741197243332863, acc: 0.9777777791023254)
[2024-12-12 03:08:13,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:13,522][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.03843352571129799, acc: 0.9677419066429138)
[2024-12-12 03:08:13,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:13,911][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.03389614447951317, acc: 0.9800000190734863)
[2024-12-12 03:08:14,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:14,293][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.48099473118782043, acc: 0.8148148059844971)
[2024-12-12 03:08:14,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:14,630][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.16500099003314972, acc: 0.9714285731315613)
[2024-12-12 03:08:14,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:15,010][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.13405132293701172, acc: 0.9743589758872986)
[2024-12-12 03:08:15,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:15,388][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.36068159341812134, acc: 0.8780487775802612)
[2024-12-12 03:08:15,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:15,749][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.23226185142993927, acc: 0.8947368264198303)
[2024-12-12 03:08:15,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:16,158][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.011431636288762093, acc: 1.0)
[2024-12-12 03:08:16,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:16,523][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.02254009060561657, acc: 1.0)
[2024-12-12 03:08:16,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:16,875][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.0062652649357914925, acc: 1.0)
[2024-12-12 03:08:16,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:17,202][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.004424029495567083, acc: 1.0)
[2024-12-12 03:08:17,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:17,521][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.2193436622619629, acc: 0.9032257795333862)
[2024-12-12 03:08:17,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:17,893][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.10712330788373947, acc: 0.9649122953414917)
[2024-12-12 03:08:17,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:18,201][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.11143256723880768, acc: 0.90625)
[2024-12-12 03:08:18,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:18,540][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.01827981323003769, acc: 1.0)
[2024-12-12 03:08:18,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:18,880][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.01663421094417572, acc: 1.0)
[2024-12-12 03:08:19,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:19,287][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.2144923359155655, acc: 0.9200000166893005)
[2024-12-12 03:08:19,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:19,690][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.4371976852416992, acc: 0.8275862336158752)
[2024-12-12 03:08:19,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:20,082][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.4195033013820648, acc: 0.8297872543334961)
[2024-12-12 03:08:20,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:20,463][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.319233775138855, acc: 0.891566276550293)
[2024-12-12 03:08:20,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:20,783][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.004106786567717791, acc: 1.0)
[2024-12-12 03:08:20,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:21,049][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.05462365224957466, acc: 0.9743589758872986)
[2024-12-12 03:08:21,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:21,403][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.11146145313978195, acc: 0.9638554453849792)
[2024-12-12 03:08:21,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:21,794][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.10515693575143814, acc: 0.9433962106704712)
[2024-12-12 03:08:21,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:22,196][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.03457855060696602, acc: 1.0)
[2024-12-12 03:08:22,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:22,573][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.08603231608867645, acc: 0.9411764740943909)
[2024-12-12 03:08:22,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:22,974][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.07977281510829926, acc: 0.9701492786407471)
[2024-12-12 03:08:23,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:23,307][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.0020983130671083927, acc: 1.0)
[2024-12-12 03:08:23,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:23,697][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.008048612624406815, acc: 1.0)
[2024-12-12 03:08:23,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:24,120][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.18831805884838104, acc: 0.9444444179534912)
[2024-12-12 03:08:24,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:24,469][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.052321672439575195, acc: 0.9767441749572754)
[2024-12-12 03:08:24,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:24,839][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.03861191123723984, acc: 1.0)
[2024-12-12 03:08:24,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:25,232][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.1693999469280243, acc: 0.9333333373069763)
[2024-12-12 03:08:25,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:25,609][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.06420238316059113, acc: 0.95652174949646)
[2024-12-12 03:08:25,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:26,003][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.007826821878552437, acc: 1.0)
[2024-12-12 03:08:26,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:26,409][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.2099776268005371, acc: 0.9340659379959106)
[2024-12-12 03:08:26,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:26,925][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.21018482744693756, acc: 0.9304347634315491)
[2024-12-12 03:08:27,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:27,293][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.07974332571029663, acc: 0.97826087474823)
[2024-12-12 03:08:27,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:27,641][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.08333735913038254, acc: 0.9795918464660645)
[2024-12-12 03:08:27,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:28,005][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.0009922027820721269, acc: 1.0)
[2024-12-12 03:08:28,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:28,361][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.07912460714578629, acc: 0.9615384340286255)
[2024-12-12 03:08:28,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:28,734][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.037025392055511475, acc: 1.0)
[2024-12-12 03:08:28,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:29,156][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.24398371577262878, acc: 0.9333333373069763)
[2024-12-12 03:08:29,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:29,540][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.07130325585603714, acc: 0.9736841917037964)
[2024-12-12 03:08:29,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:29,923][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.032809361815452576, acc: 1.0)
[2024-12-12 03:08:30,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:30,335][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.12551623582839966, acc: 0.9696969985961914)
[2024-12-12 03:08:30,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:30,686][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.0008779880590736866, acc: 1.0)
[2024-12-12 03:08:30,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:31,033][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.00516590615734458, acc: 1.0)
[2024-12-12 03:08:31,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:31,406][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.006477347109466791, acc: 1.0)
[2024-12-12 03:08:31,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:31,748][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.020722776651382446, acc: 1.0)
[2024-12-12 03:08:31,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:32,365][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.3844517469406128, acc: 0.8848484754562378)
[2024-12-12 03:08:32,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:33,279][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.13004420697689056, acc: 0.9528301954269409)
[2024-12-12 03:08:33,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:33,674][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.02170301042497158, acc: 1.0)
[2024-12-12 03:08:33,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:34,031][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.04007149115204811, acc: 0.9821428656578064)
[2024-12-12 03:08:34,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:34,383][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.27602240443229675, acc: 0.9428571462631226)
[2024-12-12 03:08:34,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:34,740][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.00044741929741576314, acc: 1.0)
[2024-12-12 03:08:34,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:35,057][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.0021979168523103, acc: 1.0)
[2024-12-12 03:08:35,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:35,419][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.024686207994818687, acc: 1.0)
[2024-12-12 03:08:35,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:35,776][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.020882021635770798, acc: 0.9894737005233765)
[2024-12-12 03:08:35,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:36,357][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.1257173866033554, acc: 0.9520958065986633)
[2024-12-12 03:08:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:36,761][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.21628107130527496, acc: 0.9398496150970459)
[2024-12-12 03:08:37,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:37,995][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.37755683064460754, acc: 0.893048107624054)
[2024-12-12 03:08:38,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:38,555][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.11049137264490128, acc: 0.954954981803894)
[2024-12-12 03:08:38,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:38,909][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.009190445765852928, acc: 1.0)
[2024-12-12 03:08:39,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:39,243][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.0014580324059352279, acc: 1.0)
[2024-12-12 03:08:39,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:39,627][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.007291668094694614, acc: 1.0)
[2024-12-12 03:08:39,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:39,975][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.003011346096172929, acc: 1.0)
[2024-12-12 03:08:40,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:40,317][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.10487701743841171, acc: 0.9473684430122375)
[2024-12-12 03:08:40,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:40,664][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.0018431831849738955, acc: 1.0)
[2024-12-12 03:08:40,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:41,018][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.0015003705630078912, acc: 1.0)
[2024-12-12 03:08:41,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:41,417][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.057575225830078125, acc: 1.0)
[2024-12-12 03:08:41,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:41,780][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.30865418910980225, acc: 0.9259259104728699)
[2024-12-12 03:08:41,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:42,135][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.47944456338882446, acc: 0.8252426981925964)
[2024-12-12 03:08:42,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:42,683][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.5540927052497864, acc: 0.8529411554336548)
[2024-12-12 03:08:42,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:43,094][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.28173813223838806, acc: 0.9200000166893005)
[2024-12-12 03:08:43,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:43,475][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.26916658878326416, acc: 0.9305555820465088)
[2024-12-12 03:08:43,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:43,864][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.03509685769677162, acc: 0.9767441749572754)
[2024-12-12 03:08:43,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:44,222][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.003638015128672123, acc: 1.0)
[2024-12-12 03:08:44,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:44,578][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.03092540055513382, acc: 1.0)
[2024-12-12 03:08:44,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:44,936][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.005629411432892084, acc: 1.0)
[2024-12-12 03:08:45,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:45,465][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.24755775928497314, acc: 0.9264705777168274)
[2024-12-12 03:08:45,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:45,781][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.1242619976401329, acc: 0.9599999785423279)
[2024-12-12 03:08:45,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:46,188][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.04360862448811531, acc: 1.0)
[2024-12-12 03:08:46,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:46,551][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.06501457840204239, acc: 0.9696969985961914)
[2024-12-12 03:08:46,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:46,918][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.003840249264612794, acc: 1.0)
[2024-12-12 03:08:47,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:47,281][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.0029094060882925987, acc: 1.0)
[2024-12-12 03:08:47,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:47,632][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.0017043080879375339, acc: 1.0)
[2024-12-12 03:08:47,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:47,929][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.0027033379301428795, acc: 1.0)
[2024-12-12 03:08:48,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:48,277][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.0013793135294690728, acc: 1.0)
[2024-12-12 03:08:48,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:48,593][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.009749636054039001, acc: 1.0)
[2024-12-12 03:08:48,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:48,976][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.08965963125228882, acc: 0.982758641242981)
[2024-12-12 03:08:49,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:49,317][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.12768632173538208, acc: 0.9642857313156128)
[2024-12-12 03:08:49,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:49,665][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.014610969461500645, acc: 1.0)
[2024-12-12 03:08:50,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:50,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:50,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:51,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:51,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:51,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:52,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:52,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:53,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:53,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:54,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:54,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:54,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:55,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:55,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:56,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:56,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:56,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:57,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:57,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:57,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:58,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:58,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:58,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:59,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:59,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:59,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:00,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:00,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:00,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:01,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:01,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:01,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:02,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:02,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:02,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:03,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:03,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:03,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:04,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:04,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:05,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:05,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:05,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:06,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:06,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:06,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:07,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:07,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:07,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:08,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:08,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:08,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:09,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:09,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:09,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:10,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:10,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:11,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:11,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:11,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:12,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:12,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:12,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:13,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:13,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:13,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:14,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:14,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:15,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:15,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:15,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:16,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:16,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:16,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:17,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:17,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:17,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:18,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:18,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:18,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:19,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:19,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:19,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:20,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:20,852][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6908, device='cuda:0') eval_epoch_loss=tensor(0.9898, device='cuda:0') eval_epoch_acc=tensor(0.7957, device='cuda:0')
[2024-12-12 03:09:20,853][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:09:20,853][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:09:21,170][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_9_step_413_loss_0.989841878414154/model.pt
[2024-12-12 03:09:21,173][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:09:21,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:21,579][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.008540072478353977, acc: 1.0)
[2024-12-12 03:09:21,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:21,947][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.001850633299909532, acc: 1.0)
[2024-12-12 03:09:22,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:22,348][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.11978009343147278, acc: 0.9607843160629272)
[2024-12-12 03:09:22,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:22,685][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.16962864995002747, acc: 0.9615384340286255)
[2024-12-12 03:09:22,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:23,058][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.3378666341304779, acc: 0.9444444179534912)
[2024-12-12 03:09:23,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:23,462][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.06371629238128662, acc: 0.9750000238418579)
[2024-12-12 03:09:23,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:23,794][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.12972800433635712, acc: 0.949999988079071)
[2024-12-12 03:09:23,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:24,149][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.0060401177033782005, acc: 1.0)
[2024-12-12 03:09:24,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:24,495][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.019271014258265495, acc: 1.0)
[2024-12-12 03:09:24,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:24,893][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.14743730425834656, acc: 0.9375)
[2024-12-12 03:09:25,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:25,282][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.07088728994131088, acc: 0.9722222089767456)
[2024-12-12 03:09:25,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:25,625][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.018249182030558586, acc: 1.0)
[2024-12-12 03:09:25,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:25,996][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.03591281548142433, acc: 1.0)
[2024-12-12 03:09:26,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:26,357][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.0065441676415503025, acc: 1.0)
[2024-12-12 03:09:26,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:26,777][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.19260773062705994, acc: 0.9459459185600281)
[2024-12-12 03:09:26,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:27,138][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.008635102771222591, acc: 1.0)
[2024-12-12 03:09:27,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:27,506][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.1004105880856514, acc: 0.95652174949646)
[2024-12-12 03:09:27,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:27,826][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.000852592580486089, acc: 1.0)
[2024-12-12 03:09:27,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:28,209][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 0.0021278043277561665, acc: 1.0)
[2024-12-12 03:09:28,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:28,556][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.0007847831584513187, acc: 1.0)
[2024-12-12 03:09:28,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:28,975][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.06646405905485153, acc: 0.9722222089767456)
[2024-12-12 03:09:29,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:29,349][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.0010470639681443572, acc: 1.0)
[2024-12-12 03:09:29,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:29,737][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.0012059775181114674, acc: 1.0)
[2024-12-12 03:09:29,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:30,091][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.06446219980716705, acc: 1.0)
[2024-12-12 03:09:30,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:30,465][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.48696544766426086, acc: 0.9318181872367859)
[2024-12-12 03:09:30,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:30,835][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0008698016172274947, acc: 1.0)
[2024-12-12 03:09:30,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:31,153][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.13058988749980927, acc: 0.9487179517745972)
[2024-12-12 03:09:31,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:31,612][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.08013051748275757, acc: 0.9848484992980957)
[2024-12-12 03:09:31,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:32,351][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.36108413338661194, acc: 0.8799999952316284)
[2024-12-12 03:09:32,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:32,751][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.3226862847805023, acc: 0.9032257795333862)
[2024-12-12 03:09:32,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:33,400][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.495575487613678, acc: 0.8855721354484558)
[2024-12-12 03:09:33,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:33,749][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.04192229360342026, acc: 0.9811320900917053)
[2024-12-12 03:09:33,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:34,160][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.22712092101573944, acc: 0.9318181872367859)
[2024-12-12 03:09:34,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:34,452][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.05500834435224533, acc: 0.95652174949646)
[2024-12-12 03:09:34,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:34,802][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.09365757554769516, acc: 0.9615384340286255)
[2024-12-12 03:09:34,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:35,167][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.010609453544020653, acc: 1.0)
[2024-12-12 03:09:35,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:35,533][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.11325858533382416, acc: 0.9850746393203735)
[2024-12-12 03:09:35,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:35,917][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.018401997163891792, acc: 1.0)
[2024-12-12 03:09:36,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:36,249][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.039041418582201004, acc: 0.97826087474823)
[2024-12-12 03:09:36,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:36,592][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.09529817849397659, acc: 0.9615384340286255)
[2024-12-12 03:09:36,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:36,974][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.18515440821647644, acc: 0.9342105388641357)
[2024-12-12 03:09:37,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:37,354][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.07414241880178452, acc: 0.9591836929321289)
[2024-12-12 03:09:37,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:37,764][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.374481737613678, acc: 0.9696969985961914)
[2024-12-12 03:09:37,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:38,183][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.41020017862319946, acc: 0.8659793734550476)
[2024-12-12 03:09:38,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:38,607][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.011179225519299507, acc: 1.0)
[2024-12-12 03:09:38,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:39,012][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.2162124216556549, acc: 0.9360465407371521)
[2024-12-12 03:09:39,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:39,374][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.0093241510912776, acc: 1.0)
[2024-12-12 03:09:39,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:39,758][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.09174896031618118, acc: 0.9506173133850098)
[2024-12-12 03:09:39,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:40,115][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.10663294047117233, acc: 0.9444444179534912)
[2024-12-12 03:09:40,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:40,485][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.07861011475324631, acc: 0.9375)
[2024-12-12 03:09:40,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:40,802][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.018878348171710968, acc: 1.0)
[2024-12-12 03:09:40,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:41,145][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.06581573188304901, acc: 0.97826087474823)
[2024-12-12 03:09:41,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:41,493][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.05957040935754776, acc: 0.988095223903656)
[2024-12-12 03:09:41,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:41,844][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.25565865635871887, acc: 0.9036144614219666)
[2024-12-12 03:09:41,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:42,260][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.07076901197433472, acc: 0.9819819927215576)
[2024-12-12 03:09:42,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:42,616][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.2507549822330475, acc: 0.9223300814628601)
[2024-12-12 03:09:42,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:43,006][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.14936746656894684, acc: 0.9674796462059021)
[2024-12-12 03:09:43,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:43,366][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.022983483970165253, acc: 1.0)
[2024-12-12 03:09:43,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:43,686][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.12585338950157166, acc: 0.9285714030265808)
[2024-12-12 03:09:43,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:44,105][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.1978141963481903, acc: 0.9117646813392639)
[2024-12-12 03:09:44,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:44,478][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.5043995380401611, acc: 0.847161591053009)
[2024-12-12 03:09:44,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:44,845][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.17829246819019318, acc: 0.9375)
[2024-12-12 03:09:44,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:45,229][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.20489946007728577, acc: 0.9141104221343994)
[2024-12-12 03:09:45,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:45,643][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.18700823187828064, acc: 0.9640287756919861)
[2024-12-12 03:09:45,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:46,077][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.3371526896953583, acc: 0.8844221234321594)
[2024-12-12 03:09:46,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:46,446][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.09186714887619019, acc: 0.9722222089767456)
[2024-12-12 03:09:46,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:46,794][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.10321395844221115, acc: 0.9696969985961914)
[2024-12-12 03:09:46,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:47,142][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.032823529094457626, acc: 1.0)
[2024-12-12 03:09:47,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:47,467][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.07675822824239731, acc: 0.949999988079071)
[2024-12-12 03:09:47,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:47,803][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.015770765021443367, acc: 1.0)
[2024-12-12 03:09:47,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:48,190][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.252922385931015, acc: 0.9137930870056152)
[2024-12-12 03:09:48,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:48,484][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.027266371995210648, acc: 0.9677419066429138)
[2024-12-12 03:09:48,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:48,825][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.015597653575241566, acc: 1.0)
[2024-12-12 03:09:48,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:49,214][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.05438582971692085, acc: 1.0)
[2024-12-12 03:09:49,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:49,586][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.008269655518233776, acc: 1.0)
[2024-12-12 03:09:49,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:49,966][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.01364869810640812, acc: 1.0)
[2024-12-12 03:09:50,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:50,327][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.12198423594236374, acc: 0.9692307710647583)
[2024-12-12 03:09:50,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:50,650][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.006234644912183285, acc: 1.0)
[2024-12-12 03:09:50,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:50,992][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.07580344378948212, acc: 0.9655172228813171)
[2024-12-12 03:09:51,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:51,375][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.07529020309448242, acc: 0.9607843160629272)
[2024-12-12 03:09:51,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:51,768][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.02198401466012001, acc: 1.0)
[2024-12-12 03:09:51,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:52,180][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.1673394739627838, acc: 0.9473684430122375)
[2024-12-12 03:09:52,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:52,562][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.19245629012584686, acc: 0.9473684430122375)
[2024-12-12 03:09:52,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:52,947][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.24097755551338196, acc: 0.9464285969734192)
[2024-12-12 03:09:53,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:53,351][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.10770470649003983, acc: 0.9550561904907227)
[2024-12-12 03:09:53,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:53,743][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.394834965467453, acc: 0.8539325594902039)
[2024-12-12 03:09:53,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:54,084][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.3391987979412079, acc: 0.9078013896942139)
[2024-12-12 03:09:54,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:54,472][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.1866566389799118, acc: 0.945652186870575)
[2024-12-12 03:09:54,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:54,830][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.001988491741940379, acc: 1.0)
[2024-12-12 03:09:54,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:55,188][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.0024516121484339237, acc: 1.0)
[2024-12-12 03:09:55,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:55,544][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.052990082651376724, acc: 0.9629629850387573)
[2024-12-12 03:09:55,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:55,888][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.003456111066043377, acc: 1.0)
[2024-12-12 03:09:55,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:56,261][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.11506275087594986, acc: 0.9433962106704712)
[2024-12-12 03:09:56,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:56,645][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.2532068192958832, acc: 0.8965517282485962)
[2024-12-12 03:09:56,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:57,252][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.5441359877586365, acc: 0.8468468189239502)
[2024-12-12 03:09:57,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:57,705][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.27257850766181946, acc: 0.9295774698257446)
[2024-12-12 03:09:57,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:58,077][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.0070173912681639194, acc: 1.0)
[2024-12-12 03:09:58,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:58,444][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.018947668373584747, acc: 1.0)
[2024-12-12 03:09:58,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:58,826][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.007296642288565636, acc: 1.0)
[2024-12-12 03:10:00,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:01,872][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.665649950504303, acc: 0.8500000238418579)
[2024-12-12 03:10:02,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:02,641][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.1653657704591751, acc: 0.9682539701461792)
[2024-12-12 03:10:02,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:03,015][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.21551848948001862, acc: 0.9642857313156128)
[2024-12-12 03:10:03,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:03,354][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.19612351059913635, acc: 0.9833333492279053)
[2024-12-12 03:10:03,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:04,046][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.07986703515052795, acc: 0.9861111044883728)
[2024-12-12 03:10:04,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:04,409][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.01058232132345438, acc: 1.0)
[2024-12-12 03:10:04,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:04,776][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.07308923453092575, acc: 0.9677419066429138)
[2024-12-12 03:10:04,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:05,111][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.10098351538181305, acc: 0.949999988079071)
[2024-12-12 03:10:05,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:05,488][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.02179981954395771, acc: 1.0)
[2024-12-12 03:10:05,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:06,522][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.410258948802948, acc: 0.8686440587043762)
[2024-12-12 03:10:06,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:06,924][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.21743649244308472, acc: 0.9402984976768494)
[2024-12-12 03:10:07,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:07,301][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.11209715902805328, acc: 0.9635036587715149)
[2024-12-12 03:10:07,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:07,863][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.28418225049972534, acc: 0.9200000166893005)
[2024-12-12 03:10:07,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:08,187][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.009608112275600433, acc: 1.0)
[2024-12-12 03:10:08,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:08,570][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.018056634813547134, acc: 1.0)
[2024-12-12 03:10:08,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:08,937][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.11406887322664261, acc: 0.9047619104385376)
[2024-12-12 03:10:09,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:09,309][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.20898853242397308, acc: 0.9344262480735779)
[2024-12-12 03:10:09,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:09,675][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.06575928628444672, acc: 0.9830508232116699)
[2024-12-12 03:10:09,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:09,990][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.1659601777791977, acc: 0.9534883499145508)
[2024-12-12 03:10:10,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:10,369][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.18230624496936798, acc: 0.9545454382896423)
[2024-12-12 03:10:10,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:10,717][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.185512974858284, acc: 0.9622641801834106)
[2024-12-12 03:10:10,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:11,094][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.18165752291679382, acc: 0.9318181872367859)
[2024-12-12 03:10:11,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:11,473][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.048047639429569244, acc: 1.0)
[2024-12-12 03:10:11,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:11,863][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.02516820654273033, acc: 1.0)
[2024-12-12 03:10:11,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:12,248][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.09062669426202774, acc: 0.9545454382896423)
[2024-12-12 03:10:12,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:12,687][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.11333605647087097, acc: 0.9692307710647583)
[2024-12-12 03:10:12,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:13,083][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.21774570643901825, acc: 0.9375)
[2024-12-12 03:10:13,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:13,481][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.11360853165388107, acc: 0.9375)
[2024-12-12 03:10:13,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:13,855][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.18184266984462738, acc: 0.9090909361839294)
[2024-12-12 03:10:13,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:14,227][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.013476822525262833, acc: 1.0)
[2024-12-12 03:10:14,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:14,592][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.01819431036710739, acc: 1.0)
[2024-12-12 03:10:14,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:14,913][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 0.0017045371932908893, acc: 1.0)
[2024-12-12 03:10:15,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:15,277][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.023666122928261757, acc: 1.0)
[2024-12-12 03:10:15,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:15,651][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.04235442727804184, acc: 0.9756097793579102)
[2024-12-12 03:10:15,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:16,019][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.002694249851629138, acc: 1.0)
[2024-12-12 03:10:16,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:16,474][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.024963373318314552, acc: 0.9736841917037964)
[2024-12-12 03:10:16,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:16,850][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.00709539232775569, acc: 1.0)
[2024-12-12 03:10:16,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:17,232][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 0.003129072254523635, acc: 1.0)
[2024-12-12 03:10:17,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:17,598][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.023109670728445053, acc: 1.0)
[2024-12-12 03:10:17,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:17,965][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.021340493112802505, acc: 1.0)
[2024-12-12 03:10:18,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:18,299][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.029264971613883972, acc: 0.9857142567634583)
[2024-12-12 03:10:18,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:18,688][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.2084585279226303, acc: 0.9197080135345459)
[2024-12-12 03:10:18,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:19,080][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.11547229439020157, acc: 0.9724137783050537)
[2024-12-12 03:10:19,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:19,495][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.12732543051242828, acc: 0.9357143044471741)
[2024-12-12 03:10:20,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:20,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:20,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:21,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:21,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:21,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:22,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:22,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:23,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:23,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:23,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:24,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:24,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:24,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:25,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:25,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:26,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:26,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:26,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:27,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:27,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:27,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:28,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:28,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:28,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:29,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:29,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:29,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:30,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:30,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:30,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:31,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:31,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:32,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:32,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:32,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:33,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:33,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:33,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:34,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:34,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:34,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:35,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:35,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:36,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:36,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:36,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:37,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:37,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:37,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:38,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:38,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:38,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:39,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:39,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:39,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:40,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:40,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:41,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:41,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:41,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:42,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:42,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:42,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:43,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:43,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:44,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:44,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:44,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:45,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:45,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:45,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:46,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:46,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:46,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:47,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:47,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:47,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:48,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:48,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:48,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:49,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:49,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:49,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:50,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:50,877][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.7074, device='cuda:0') eval_epoch_loss=tensor(0.9960, device='cuda:0') eval_epoch_acc=tensor(0.7991, device='cuda:0')
[2024-12-12 03:10:50,879][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:10:50,879][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:10:51,177][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_9_step_556_loss_0.995971143245697/model.pt
[2024-12-12 03:10:51,181][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:10:51,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:51,547][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.14248120784759521, acc: 0.940397322177887)
[2024-12-12 03:10:51,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:51,889][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.04777329042553902, acc: 0.9914529919624329)
[2024-12-12 03:10:52,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:52,277][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.014244197867810726, acc: 1.0)
[2024-12-12 03:10:52,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:52,615][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.011206532828509808, acc: 1.0)
[2024-12-12 03:10:52,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:52,969][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.003672829130664468, acc: 1.0)
[2024-12-12 03:10:53,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:53,345][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.0191061832010746, acc: 1.0)
[2024-12-12 03:10:53,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:53,707][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.14946821331977844, acc: 0.9444444179534912)
[2024-12-12 03:10:53,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:54,010][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.11636622250080109, acc: 0.9610389471054077)
[2024-12-12 03:10:54,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:54,347][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.06173032522201538, acc: 0.9583333134651184)
[2024-12-12 03:10:54,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:54,684][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.12466677278280258, acc: 0.9482758641242981)
[2024-12-12 03:10:54,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:55,047][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.03991584852337837, acc: 0.988095223903656)
[2024-12-12 03:10:55,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:55,422][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.022661779075860977, acc: 1.0)
[2024-12-12 03:10:55,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:55,816][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.004064304754137993, acc: 1.0)
[2024-12-12 03:10:55,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:56,203][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.11671864241361618, acc: 0.9679144620895386)
[2024-12-12 03:10:56,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:56,551][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.013356974348425865, acc: 1.0)
[2024-12-12 03:10:56,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:56,950][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.06670571118593216, acc: 0.9829059839248657)
[2024-12-12 03:10:57,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:57,323][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.2774208188056946, acc: 0.9081632494926453)
[2024-12-12 03:10:57,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:57,679][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.1277017444372177, acc: 0.9685534834861755)
[2024-12-12 03:10:58,105][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.1715, train_epoch_loss=0.1583, epoch time 364.4284119606018s
[2024-12-12 03:10:58,105][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 03:10:58,105][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-12-12 03:10:58,105][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 03:10:58,105][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 26
[2024-12-12 03:10:58,105][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 03:10:58,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:59,073][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.010299621149897575, acc: 1.0)
[2024-12-12 03:10:59,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:59,411][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.017375202849507332, acc: 1.0)
[2024-12-12 03:10:59,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:59,725][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.17050933837890625, acc: 0.9459459185600281)
[2024-12-12 03:10:59,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:00,056][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.011609219014644623, acc: 1.0)
[2024-12-12 03:11:00,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:00,389][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.03677795082330704, acc: 1.0)
[2024-12-12 03:11:00,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:00,705][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.005505812354385853, acc: 1.0)
[2024-12-12 03:11:00,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:01,092][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.06918420642614365, acc: 0.9795918464660645)
[2024-12-12 03:11:01,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:01,425][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.03223586454987526, acc: 1.0)
[2024-12-12 03:11:01,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:01,766][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.0018057163106277585, acc: 1.0)
[2024-12-12 03:11:01,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:02,149][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.008286223746836185, acc: 1.0)
[2024-12-12 03:11:02,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:02,477][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.019665854051709175, acc: 1.0)
[2024-12-12 03:11:02,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:02,858][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.08139875531196594, acc: 0.9743589758872986)
[2024-12-12 03:11:02,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:03,171][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.0052653285674750805, acc: 1.0)
[2024-12-12 03:11:03,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:03,573][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.08179046958684921, acc: 0.97826087474823)
[2024-12-12 03:11:03,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:03,959][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.1285129189491272, acc: 0.9803921580314636)
[2024-12-12 03:11:04,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:04,333][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.04498669505119324, acc: 1.0)
[2024-12-12 03:11:04,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:04,682][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.008911342360079288, acc: 1.0)
[2024-12-12 03:11:04,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:05,063][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.006648802664130926, acc: 1.0)
[2024-12-12 03:11:05,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:05,432][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.09223086386919022, acc: 0.9722222089767456)
[2024-12-12 03:11:05,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:05,847][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.010754567570984364, acc: 1.0)
[2024-12-12 03:11:05,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:06,218][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.026508433744311333, acc: 1.0)
[2024-12-12 03:11:06,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:06,544][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.04436397925019264, acc: 1.0)
[2024-12-12 03:11:06,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:06,867][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.0074183642864227295, acc: 1.0)
[2024-12-12 03:11:06,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:07,198][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.010574351996183395, acc: 1.0)
[2024-12-12 03:11:07,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:07,601][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.0016917571192607284, acc: 1.0)
[2024-12-12 03:11:07,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:08,004][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.21699438989162445, acc: 0.9622641801834106)
[2024-12-12 03:11:08,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:08,402][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.14593574404716492, acc: 0.9863013625144958)
[2024-12-12 03:11:08,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:09,726][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.5205253958702087, acc: 0.8260869383811951)
[2024-12-12 03:11:09,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:10,061][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.06257472187280655, acc: 0.9767441749572754)
[2024-12-12 03:11:10,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:10,415][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.21314023435115814, acc: 0.9638554453849792)
[2024-12-12 03:11:10,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:10,786][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.09455443918704987, acc: 0.9876543283462524)
[2024-12-12 03:11:10,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:11,182][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.0227329321205616, acc: 1.0)
[2024-12-12 03:11:11,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:11,562][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.047683052718639374, acc: 0.9629629850387573)
[2024-12-12 03:11:11,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:11,904][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0007882956997491419, acc: 1.0)
[2024-12-12 03:11:12,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:12,278][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.11853503435850143, acc: 0.9663865566253662)
[2024-12-12 03:11:12,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:12,650][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.04227936640381813, acc: 0.9836065769195557)
[2024-12-12 03:11:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:13,057][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.012348711490631104, acc: 1.0)
[2024-12-12 03:11:13,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:13,465][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.012779856100678444, acc: 1.0)
[2024-12-12 03:11:13,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:13,872][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.03622450679540634, acc: 0.9885057210922241)
[2024-12-12 03:11:13,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:14,293][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.014631498605012894, acc: 1.0)
[2024-12-12 03:11:14,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:14,686][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.2808822989463806, acc: 0.9615384340286255)
[2024-12-12 03:11:14,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:15,091][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.2316887527704239, acc: 0.8918918967247009)
[2024-12-12 03:11:15,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:15,492][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.2002805471420288, acc: 0.9538461565971375)
[2024-12-12 03:11:15,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:15,907][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.20951834321022034, acc: 0.9494949579238892)
[2024-12-12 03:11:16,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:16,323][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.09118406474590302, acc: 0.9793814420700073)
[2024-12-12 03:11:16,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:16,725][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.11649531871080399, acc: 0.9632353186607361)
[2024-12-12 03:11:16,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:17,086][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.023454934358596802, acc: 1.0)
[2024-12-12 03:11:17,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:17,460][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.0014382170047610998, acc: 1.0)
[2024-12-12 03:11:17,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:17,857][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.007490572053939104, acc: 1.0)
[2024-12-12 03:11:17,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:18,250][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.0076396712101995945, acc: 1.0)
[2024-12-12 03:11:18,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:18,640][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.3488745391368866, acc: 0.9122806787490845)
[2024-12-12 03:11:18,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:18,992][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.07794872671365738, acc: 0.9682539701461792)
[2024-12-12 03:11:19,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:19,368][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.16620124876499176, acc: 0.9295774698257446)
[2024-12-12 03:11:19,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:19,841][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.5227222442626953, acc: 0.8399999737739563)
[2024-12-12 03:11:19,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:20,200][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.08576850593090057, acc: 0.9729729890823364)
[2024-12-12 03:11:20,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:20,534][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.0012418896658346057, acc: 1.0)
[2024-12-12 03:11:22,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:23,562][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.6187821626663208, acc: 0.8054607510566711)
[2024-12-12 03:11:23,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:24,847][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.936116099357605, acc: 0.7254902124404907)
[2024-12-12 03:11:25,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:25,478][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.32529011368751526, acc: 0.8636363744735718)
[2024-12-12 03:11:25,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:26,044][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.13044646382331848, acc: 0.9632353186607361)
[2024-12-12 03:11:26,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:26,602][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.3702443540096283, acc: 0.8695651888847351)
[2024-12-12 03:11:26,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:27,047][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.2941102981567383, acc: 0.8999999761581421)
[2024-12-12 03:11:27,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:27,416][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.01420197356492281, acc: 1.0)
[2024-12-12 03:11:27,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:27,751][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.1511787623167038, acc: 0.9444444179534912)
[2024-12-12 03:11:27,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:28,119][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.021357156336307526, acc: 1.0)
[2024-12-12 03:11:28,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:28,534][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.019237298518419266, acc: 1.0)
[2024-12-12 03:11:28,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:28,922][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.07484572380781174, acc: 0.9642857313156128)
[2024-12-12 03:11:29,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:29,275][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.10358013957738876, acc: 0.9666666388511658)
[2024-12-12 03:11:29,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:29,657][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.0015538029838353395, acc: 1.0)
[2024-12-12 03:11:29,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:30,058][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.11226561665534973, acc: 0.9722222089767456)
[2024-12-12 03:11:30,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:30,454][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.1539476364850998, acc: 0.939393937587738)
[2024-12-12 03:11:30,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:30,860][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.3351224362850189, acc: 0.8823529481887817)
[2024-12-12 03:11:30,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:31,210][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.3626249432563782, acc: 0.89682537317276)
[2024-12-12 03:11:31,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:31,624][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.859275758266449, acc: 0.7538461685180664)
[2024-12-12 03:11:31,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:32,009][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.19801418483257294, acc: 0.9387755393981934)
[2024-12-12 03:11:32,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:32,353][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.38339105248451233, acc: 0.8731343150138855)
[2024-12-12 03:11:32,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:32,738][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.9339602589607239, acc: 0.7189781069755554)
[2024-12-12 03:11:32,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:33,079][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.0020964425057172775, acc: 1.0)
[2024-12-12 03:11:33,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:33,407][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.012993485666811466, acc: 1.0)
[2024-12-12 03:11:33,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:33,805][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.22290955483913422, acc: 0.939393937587738)
[2024-12-12 03:11:33,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:34,149][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.0059390622191131115, acc: 1.0)
[2024-12-12 03:11:34,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:34,472][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.1352636218070984, acc: 0.9615384340286255)
[2024-12-12 03:11:34,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:34,822][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.14048482477664948, acc: 0.9038461446762085)
[2024-12-12 03:11:34,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:35,169][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.03463538736104965, acc: 0.96875)
[2024-12-12 03:11:35,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:35,504][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.12786416709423065, acc: 0.9420289993286133)
[2024-12-12 03:11:35,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:35,834][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.0524245984852314, acc: 1.0)
[2024-12-12 03:11:35,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:36,240][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.03152120113372803, acc: 1.0)
[2024-12-12 03:11:36,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:36,741][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.29318779706954956, acc: 0.9399999976158142)
[2024-12-12 03:11:36,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:37,113][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.20046228170394897, acc: 0.9417475461959839)
[2024-12-12 03:11:37,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:38,216][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.4109698235988617, acc: 0.8592233061790466)
[2024-12-12 03:11:38,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:39,037][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.5450723171234131, acc: 0.8440860509872437)
[2024-12-12 03:11:39,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:39,834][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.4084702432155609, acc: 0.892241358757019)
[2024-12-12 03:11:40,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:40,572][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.28204816579818726, acc: 0.9473684430122375)
[2024-12-12 03:11:40,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:41,590][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.4045783579349518, acc: 0.8514851331710815)
[2024-12-12 03:11:41,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:41,941][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.44899770617485046, acc: 0.9032257795333862)
[2024-12-12 03:11:42,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:42,329][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.13435092568397522, acc: 0.95652174949646)
[2024-12-12 03:11:42,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:42,686][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.34298059344291687, acc: 0.8823529481887817)
[2024-12-12 03:11:42,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:43,038][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.3297802209854126, acc: 0.9038461446762085)
[2024-12-12 03:11:43,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:43,414][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.46452897787094116, acc: 0.8540145754814148)
[2024-12-12 03:11:43,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:43,769][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.4160553514957428, acc: 0.9253731369972229)
[2024-12-12 03:11:43,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:44,194][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.20150241255760193, acc: 0.8999999761581421)
[2024-12-12 03:11:44,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:44,558][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.002212691819295287, acc: 1.0)
[2024-12-12 03:11:44,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:44,902][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.021352900192141533, acc: 1.0)
[2024-12-12 03:11:45,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:45,276][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.045684393495321274, acc: 1.0)
[2024-12-12 03:11:45,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:45,675][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.053474001586437225, acc: 0.982758641242981)
[2024-12-12 03:11:45,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:46,050][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.02500269003212452, acc: 1.0)
[2024-12-12 03:11:46,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:46,414][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.013493658043444157, acc: 1.0)
[2024-12-12 03:11:46,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:46,770][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.005129562690854073, acc: 1.0)
[2024-12-12 03:11:46,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:47,204][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.002208504592999816, acc: 1.0)
[2024-12-12 03:11:47,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:47,596][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.04611669108271599, acc: 1.0)
[2024-12-12 03:11:47,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:47,947][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.05603548139333725, acc: 0.9692307710647583)
[2024-12-12 03:11:48,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:48,343][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.0534583181142807, acc: 0.9824561476707458)
[2024-12-12 03:11:48,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:48,736][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.08428599685430527, acc: 0.9824561476707458)
[2024-12-12 03:11:48,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:49,089][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.018232159316539764, acc: 1.0)
[2024-12-12 03:11:49,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:49,449][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.09622795879840851, acc: 0.9795918464660645)
[2024-12-12 03:11:49,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:49,799][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.01840980537235737, acc: 1.0)
[2024-12-12 03:11:49,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:50,193][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.11578474193811417, acc: 0.9682539701461792)
[2024-12-12 03:11:50,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:50,560][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.1501915156841278, acc: 0.9674796462059021)
[2024-12-12 03:11:50,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:50,934][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.12317238003015518, acc: 0.9516128897666931)
[2024-12-12 03:11:51,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:51,767][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.42456889152526855, acc: 0.8821292519569397)
[2024-12-12 03:11:51,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:52,161][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.061331648379564285, acc: 0.9866666793823242)
[2024-12-12 03:11:52,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:52,578][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.09661031514406204, acc: 0.9615384340286255)
[2024-12-12 03:11:52,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:52,948][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.006539904046803713, acc: 1.0)
[2024-12-12 03:11:53,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:53,315][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.02282222919166088, acc: 1.0)
[2024-12-12 03:11:53,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:53,754][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.3067551553249359, acc: 0.89570552110672)
[2024-12-12 03:11:54,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:54,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:55,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:55,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:56,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:56,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:56,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:56,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:57,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:57,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:57,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:58,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:58,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:59,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:59,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:59,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:00,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:00,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:01,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:01,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:01,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:02,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:02,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:02,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:03,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:03,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:04,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:04,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:04,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:05,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:05,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:05,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:06,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:06,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:07,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:07,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:07,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:08,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:08,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:08,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:09,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:09,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:09,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:10,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:10,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:10,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:11,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:11,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:11,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:12,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:12,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:12,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:13,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:13,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:13,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:14,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:14,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:14,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:15,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:15,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:16,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:16,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:16,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:17,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:17,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:17,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:18,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:18,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:19,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:19,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:19,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:20,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:20,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:20,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:21,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:21,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:21,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:22,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:22,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:22,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:23,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:23,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:23,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:24,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:24,827][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6050, device='cuda:0') eval_epoch_loss=tensor(0.9574, device='cuda:0') eval_epoch_acc=tensor(0.8039, device='cuda:0')
[2024-12-12 03:12:24,828][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:12:24,829][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:12:25,101][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_10_step_125_loss_0.9574456810951233/model.pt
[2024-12-12 03:12:25,107][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:12:25,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:25,561][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.40726161003112793, acc: 0.8680555820465088)
[2024-12-12 03:12:25,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:25,914][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.5417935848236084, acc: 0.8333333134651184)
[2024-12-12 03:12:26,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:26,248][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.3597338795661926, acc: 0.875)
[2024-12-12 03:12:26,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:26,631][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.33616939187049866, acc: 0.8871794939041138)
[2024-12-12 03:12:26,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:27,029][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.2709651589393616, acc: 0.9338235259056091)
[2024-12-12 03:12:27,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:27,379][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.12182903289794922, acc: 0.9230769276618958)
[2024-12-12 03:12:27,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:27,692][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.008945515379309654, acc: 1.0)
[2024-12-12 03:12:27,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:28,022][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.03639483451843262, acc: 1.0)
[2024-12-12 03:12:28,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:28,334][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.11404291540384293, acc: 0.95652174949646)
[2024-12-12 03:12:28,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:28,697][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.017477963119745255, acc: 1.0)
[2024-12-12 03:12:28,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:29,034][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.0037236730568110943, acc: 1.0)
[2024-12-12 03:12:29,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:29,409][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.18140757083892822, acc: 0.9285714030265808)
[2024-12-12 03:12:29,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:29,770][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.173903226852417, acc: 0.9333333373069763)
[2024-12-12 03:12:29,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:30,142][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.01527032908052206, acc: 1.0)
[2024-12-12 03:12:30,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:30,495][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.014086717739701271, acc: 1.0)
[2024-12-12 03:12:30,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:30,828][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.025068147107958794, acc: 1.0)
[2024-12-12 03:12:30,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:31,221][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.07411369681358337, acc: 0.9677419066429138)
[2024-12-12 03:12:31,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:31,614][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.4700496792793274, acc: 0.9189189076423645)
[2024-12-12 03:12:31,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:32,136][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.28837859630584717, acc: 0.9122806787490845)
[2024-12-12 03:12:32,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:32,478][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.4548241198062897, acc: 0.8507462739944458)
[2024-12-12 03:12:32,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:32,921][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.3472013771533966, acc: 0.9285714030265808)
[2024-12-12 03:12:33,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:33,371][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.3803635537624359, acc: 0.9042553305625916)
[2024-12-12 03:12:33,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:33,698][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.1886521279811859, acc: 0.9428571462631226)
[2024-12-12 03:12:33,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:34,057][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.16770032048225403, acc: 0.9285714030265808)
[2024-12-12 03:12:34,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:34,435][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.017584476619958878, acc: 1.0)
[2024-12-12 03:12:34,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:34,835][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.021824251860380173, acc: 1.0)
[2024-12-12 03:12:34,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:35,201][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.2918575704097748, acc: 0.8913043737411499)
[2024-12-12 03:12:35,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:35,583][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.1712743043899536, acc: 0.9152542352676392)
[2024-12-12 03:12:35,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:35,905][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.3131483793258667, acc: 0.8771929740905762)
[2024-12-12 03:12:36,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:36,255][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.09179598838090897, acc: 0.9729729890823364)
[2024-12-12 03:12:36,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:36,619][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.018339965492486954, acc: 1.0)
[2024-12-12 03:12:36,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:37,003][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.47292473912239075, acc: 0.8695651888847351)
[2024-12-12 03:12:37,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:37,325][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.3052794933319092, acc: 0.8421052694320679)
[2024-12-12 03:12:38,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:39,053][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.2704360783100128, acc: 0.9054054021835327)
[2024-12-12 03:12:39,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:39,343][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.24461348354816437, acc: 0.9259259104728699)
[2024-12-12 03:12:39,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:39,746][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.2848093509674072, acc: 0.930232584476471)
[2024-12-12 03:12:39,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:40,331][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.28228846192359924, acc: 0.9058823585510254)
[2024-12-12 03:12:40,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:40,905][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.36493754386901855, acc: 0.8764045238494873)
[2024-12-12 03:12:41,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:41,336][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.03386824578046799, acc: 1.0)
[2024-12-12 03:12:41,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:41,754][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.00451580248773098, acc: 1.0)
[2024-12-12 03:12:41,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:42,145][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.42908188700675964, acc: 0.8965517282485962)
[2024-12-12 03:12:42,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:42,554][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.019641533493995667, acc: 1.0)
[2024-12-12 03:12:42,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:42,962][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.05102228745818138, acc: 0.9800000190734863)
[2024-12-12 03:12:43,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:43,349][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.1463910937309265, acc: 0.9444444179534912)
[2024-12-12 03:12:43,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:43,669][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.55937260389328, acc: 0.8235294222831726)
[2024-12-12 03:12:43,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:44,694][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.2969230115413666, acc: 0.8972602486610413)
[2024-12-12 03:12:44,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:45,081][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.012129061855375767, acc: 1.0)
[2024-12-12 03:12:45,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:45,435][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.20253607630729675, acc: 0.9259259104728699)
[2024-12-12 03:12:45,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:45,815][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.02959769032895565, acc: 1.0)
[2024-12-12 03:12:46,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:46,352][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.49299001693725586, acc: 0.8584070801734924)
[2024-12-12 03:12:46,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:46,663][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.3518613278865814, acc: 0.9130434989929199)
[2024-12-12 03:12:46,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:47,061][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.14138007164001465, acc: 0.9318181872367859)
[2024-12-12 03:12:47,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:47,967][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.3394312858581543, acc: 0.8396946787834167)
[2024-12-12 03:12:48,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:48,632][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.3882814049720764, acc: 0.8444444537162781)
[2024-12-12 03:12:48,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:48,998][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.06146666407585144, acc: 0.9672130942344666)
[2024-12-12 03:12:49,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:49,332][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.04401358589529991, acc: 0.9583333134651184)
[2024-12-12 03:12:49,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:49,725][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.009874788112938404, acc: 1.0)
[2024-12-12 03:12:49,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:50,102][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.09894813597202301, acc: 0.9285714030265808)
[2024-12-12 03:12:50,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:50,461][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.04190962761640549, acc: 0.9878048896789551)
[2024-12-12 03:12:50,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:50,862][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.2885875701904297, acc: 0.9274924397468567)
[2024-12-12 03:12:50,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:51,211][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.3321804404258728, acc: 0.910662829875946)
[2024-12-12 03:12:51,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:51,687][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.3990650475025177, acc: 0.8812500238418579)
[2024-12-12 03:12:51,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:52,217][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.4997270107269287, acc: 0.8499062061309814)
[2024-12-12 03:12:52,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:52,627][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.36429402232170105, acc: 0.8967971801757812)
[2024-12-12 03:12:52,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:52,922][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.010054248385131359, acc: 1.0)
[2024-12-12 03:12:53,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:53,465][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.3123702108860016, acc: 0.9069767594337463)
[2024-12-12 03:12:53,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:54,255][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.3726324737071991, acc: 0.8888888955116272)
[2024-12-12 03:12:54,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:55,165][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.3846747577190399, acc: 0.8787878751754761)
[2024-12-12 03:12:55,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:55,902][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.15730823576450348, acc: 0.9411764740943909)
[2024-12-12 03:12:56,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:56,971][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.3874276280403137, acc: 0.8827160596847534)
[2024-12-12 03:12:57,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:57,920][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.12538060545921326, acc: 0.9677419066429138)
[2024-12-12 03:12:57,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:58,309][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.00485036289319396, acc: 1.0)
[2024-12-12 03:12:58,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:58,674][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.041227418929338455, acc: 0.9750000238418579)
[2024-12-12 03:12:58,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:59,091][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.10755761712789536, acc: 0.970588207244873)
[2024-12-12 03:12:59,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:59,474][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.2582125663757324, acc: 0.9338235259056091)
[2024-12-12 03:12:59,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:59,894][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.26658275723457336, acc: 0.9406779408454895)
[2024-12-12 03:13:00,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:00,248][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.23988382518291473, acc: 0.9179104566574097)
[2024-12-12 03:13:00,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:00,607][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.2314344048500061, acc: 0.9126213788986206)
[2024-12-12 03:13:00,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:00,979][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.08565448224544525, acc: 0.9682539701461792)
[2024-12-12 03:13:01,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:01,319][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.08291749656200409, acc: 0.9780219793319702)
[2024-12-12 03:13:01,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:01,693][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.08391173183917999, acc: 0.9506726264953613)
[2024-12-12 03:13:01,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:02,080][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.18091553449630737, acc: 0.9527559280395508)
[2024-12-12 03:13:02,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:02,495][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.10983078926801682, acc: 0.9655172228813171)
[2024-12-12 03:13:02,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:02,857][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.17376449704170227, acc: 0.9420289993286133)
[2024-12-12 03:13:02,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:03,223][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.17562691867351532, acc: 0.957198441028595)
[2024-12-12 03:13:03,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:03,627][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.06396214663982391, acc: 0.989130437374115)
[2024-12-12 03:13:03,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:03,941][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.04533945396542549, acc: 1.0)
[2024-12-12 03:13:04,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:04,250][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.07288897037506104, acc: 0.9642857313156128)
[2024-12-12 03:13:04,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:04,653][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.055770084261894226, acc: 0.978723406791687)
[2024-12-12 03:13:04,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:05,390][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.07324767112731934, acc: 0.9846153855323792)
[2024-12-12 03:13:05,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:05,738][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.03252058103680611, acc: 0.9729729890823364)
[2024-12-12 03:13:05,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:06,138][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.032496578991413116, acc: 0.9883720874786377)
[2024-12-12 03:13:06,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:06,675][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.04027305543422699, acc: 0.9819819927215576)
[2024-12-12 03:13:06,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:07,087][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.06364583224058151, acc: 0.9888888597488403)
[2024-12-12 03:13:07,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:07,468][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.06433369964361191, acc: 0.9696969985961914)
[2024-12-12 03:13:07,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:07,827][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.02326708659529686, acc: 1.0)
[2024-12-12 03:13:07,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:08,165][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.1686123013496399, acc: 0.9599999785423279)
[2024-12-12 03:13:08,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:08,513][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.3584382236003876, acc: 0.8461538553237915)
[2024-12-12 03:13:08,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:09,272][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.14517220854759216, acc: 0.9402173757553101)
[2024-12-12 03:13:09,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:09,820][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.2181701809167862, acc: 0.9431818127632141)
[2024-12-12 03:13:09,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:10,250][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.24355651438236237, acc: 0.9042553305625916)
[2024-12-12 03:13:10,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:10,575][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.09708530455827713, acc: 0.9433962106704712)
[2024-12-12 03:13:10,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:10,944][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.048449981957674026, acc: 0.9833333492279053)
[2024-12-12 03:13:11,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:11,360][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.03239460289478302, acc: 1.0)
[2024-12-12 03:13:11,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:11,753][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.20754025876522064, acc: 0.8666666746139526)
[2024-12-12 03:13:11,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:12,157][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.615966260433197, acc: 0.821052610874176)
[2024-12-12 03:13:12,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:12,550][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.4675307273864746, acc: 0.8888888955116272)
[2024-12-12 03:13:12,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:13,020][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.641605019569397, acc: 0.7888888716697693)
[2024-12-12 03:13:13,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:13,527][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 1.02413809299469, acc: 0.6972476840019226)
[2024-12-12 03:13:13,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:14,023][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.5584809184074402, acc: 0.8230769038200378)
[2024-12-12 03:13:14,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:14,351][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.020161326974630356, acc: 1.0)
[2024-12-12 03:13:14,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:14,708][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.45213329792022705, acc: 0.9166666865348816)
[2024-12-12 03:13:14,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:15,084][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.06098966673016548, acc: 0.9545454382896423)
[2024-12-12 03:13:15,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:15,470][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.2274453192949295, acc: 0.9259259104728699)
[2024-12-12 03:13:15,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:15,815][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.41831550002098083, acc: 0.9428571462631226)
[2024-12-12 03:13:15,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:16,209][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.16932572424411774, acc: 0.9318181872367859)
[2024-12-12 03:13:16,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:16,542][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.18057411909103394, acc: 0.9545454382896423)
[2024-12-12 03:13:16,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:17,116][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.27002862095832825, acc: 0.9354838728904724)
[2024-12-12 03:13:17,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:17,639][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.21623072028160095, acc: 0.9318181872367859)
[2024-12-12 03:13:17,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:18,009][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.003103696508333087, acc: 1.0)
[2024-12-12 03:13:18,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:18,378][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.17778047919273376, acc: 0.9615384340286255)
[2024-12-12 03:13:18,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:18,723][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.09677793830633163, acc: 0.9677419066429138)
[2024-12-12 03:13:18,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:19,105][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.004342006053775549, acc: 1.0)
[2024-12-12 03:13:19,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:19,512][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.030435875058174133, acc: 0.9729729890823364)
[2024-12-12 03:13:19,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:19,866][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.039708126336336136, acc: 0.9729729890823364)
[2024-12-12 03:13:19,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:20,242][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.003780253930017352, acc: 1.0)
[2024-12-12 03:13:20,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:20,587][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.304335355758667, acc: 0.9411764740943909)
[2024-12-12 03:13:20,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:20,912][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.03834318742156029, acc: 0.9756097793579102)
[2024-12-12 03:13:20,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:21,211][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.3440231680870056, acc: 0.9599999785423279)
[2024-12-12 03:13:21,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:21,549][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.012535675428807735, acc: 1.0)
[2024-12-12 03:13:21,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:21,884][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.032579585909843445, acc: 0.9677419066429138)
[2024-12-12 03:13:21,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:22,250][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.006027161609381437, acc: 1.0)
[2024-12-12 03:13:22,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:22,659][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.15465369820594788, acc: 0.9428571462631226)
[2024-12-12 03:13:22,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:23,037][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.07288790494203568, acc: 0.9736841917037964)
[2024-12-12 03:13:23,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:23,619][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.07312935590744019, acc: 0.9716981053352356)
[2024-12-12 03:13:23,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:24,226][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.16557617485523224, acc: 0.9333333373069763)
[2024-12-12 03:13:24,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:24,640][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.09775160998106003, acc: 0.9722222089767456)
[2024-12-12 03:13:24,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:24,998][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.06800960004329681, acc: 0.9677419066429138)
[2024-12-12 03:13:25,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:25,395][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.17790545523166656, acc: 0.9733333587646484)
[2024-12-12 03:13:25,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:25,845][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.11518213897943497, acc: 0.9791666865348816)
[2024-12-12 03:13:26,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:26,681][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.47163891792297363, acc: 0.8640000224113464)
[2024-12-12 03:13:26,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:27,075][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.3062385320663452, acc: 0.8539325594902039)
[2024-12-12 03:13:27,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:27,446][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.16182658076286316, acc: 0.9459459185600281)
[2024-12-12 03:13:28,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:28,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:28,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:29,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:29,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:29,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:30,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:30,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:31,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:31,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:31,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:32,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:32,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:33,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:33,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:33,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:34,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:34,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:34,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:35,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:35,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:35,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:36,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:36,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:36,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:37,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:37,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:38,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:38,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:38,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:39,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:39,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:39,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:40,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:40,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:40,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:41,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:41,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:41,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:42,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:42,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:43,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:43,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:43,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:44,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:44,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:44,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:45,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:45,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:46,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:46,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:46,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:47,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:47,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:47,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:47,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:48,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:49,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:49,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:49,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:50,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:50,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:51,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:51,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:51,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:52,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:52,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:53,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:53,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:54,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:54,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:55,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:55,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:55,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:55,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:56,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:56,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:56,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:57,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:57,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:58,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:58,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:59,011][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3903, device='cuda:0') eval_epoch_loss=tensor(0.8714, device='cuda:0') eval_epoch_acc=tensor(0.8030, device='cuda:0')
[2024-12-12 03:13:59,012][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:13:59,012][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:13:59,291][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_10_step_268_loss_0.8714384436607361/model.pt
[2024-12-12 03:13:59,295][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:13:59,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:59,801][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.11955112218856812, acc: 0.9482758641242981)
[2024-12-12 03:13:59,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:00,082][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.013179055415093899, acc: 1.0)
[2024-12-12 03:14:00,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:00,386][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.004242950119078159, acc: 1.0)
[2024-12-12 03:14:00,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:00,670][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.04880204424262047, acc: 0.96875)
[2024-12-12 03:14:00,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:01,027][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.0028151737060397863, acc: 1.0)
[2024-12-12 03:14:01,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:01,425][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.1503726691007614, acc: 0.8999999761581421)
[2024-12-12 03:14:01,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:01,756][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.002598094055429101, acc: 1.0)
[2024-12-12 03:14:01,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:02,113][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.04237435385584831, acc: 1.0)
[2024-12-12 03:14:02,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:02,438][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.22921524941921234, acc: 0.9655172228813171)
[2024-12-12 03:14:02,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:02,764][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.040352411568164825, acc: 1.0)
[2024-12-12 03:14:02,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:03,102][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.04984445124864578, acc: 0.978723406791687)
[2024-12-12 03:14:03,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:03,503][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.1487630009651184, acc: 0.9583333134651184)
[2024-12-12 03:14:03,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:03,894][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.004656054545193911, acc: 1.0)
[2024-12-12 03:14:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:04,306][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.21531769633293152, acc: 0.9036144614219666)
[2024-12-12 03:14:04,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:04,668][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.5235879421234131, acc: 0.8425925970077515)
[2024-12-12 03:14:04,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:05,050][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.052981358021497726, acc: 0.9736841917037964)
[2024-12-12 03:14:05,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:05,382][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.05104522034525871, acc: 1.0)
[2024-12-12 03:14:05,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:05,742][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.0646381825208664, acc: 0.9750000238418579)
[2024-12-12 03:14:05,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:06,134][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.11888990551233292, acc: 0.96875)
[2024-12-12 03:14:06,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:06,524][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.1156260073184967, acc: 0.9679999947547913)
[2024-12-12 03:14:06,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:06,921][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.14196673035621643, acc: 0.9450549483299255)
[2024-12-12 03:14:07,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:07,277][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.12870395183563232, acc: 0.9503105878829956)
[2024-12-12 03:14:07,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:07,660][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.17919953167438507, acc: 0.9432989954948425)
[2024-12-12 03:14:07,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:08,055][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.007967718876898289, acc: 1.0)
[2024-12-12 03:14:08,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:08,400][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.024417797103524208, acc: 1.0)
[2024-12-12 03:14:08,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:08,796][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.019143955782055855, acc: 1.0)
[2024-12-12 03:14:08,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:09,267][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.09538090229034424, acc: 0.9818181991577148)
[2024-12-12 03:14:09,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:09,811][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.2988359332084656, acc: 0.8969072103500366)
[2024-12-12 03:14:09,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:10,182][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.041782133281230927, acc: 0.982758641242981)
[2024-12-12 03:14:10,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:10,569][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.01674661971628666, acc: 1.0)
[2024-12-12 03:14:10,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:10,953][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.20462866127490997, acc: 0.9473684430122375)
[2024-12-12 03:14:11,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:11,332][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.1020076796412468, acc: 0.9642857313156128)
[2024-12-12 03:14:11,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:11,713][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.006589085329324007, acc: 1.0)
[2024-12-12 03:14:11,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:12,070][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.06126630678772926, acc: 0.9622641801834106)
[2024-12-12 03:14:12,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:12,373][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.005963308271020651, acc: 1.0)
[2024-12-12 03:14:12,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:12,727][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.0018076079431921244, acc: 1.0)
[2024-12-12 03:14:12,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:13,100][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.02848227135837078, acc: 1.0)
[2024-12-12 03:14:13,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:13,423][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.07856030017137527, acc: 0.9508196711540222)
[2024-12-12 03:14:13,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:13,733][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.008358110673725605, acc: 1.0)
[2024-12-12 03:14:13,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:14,078][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.011427061632275581, acc: 1.0)
[2024-12-12 03:14:14,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:14,437][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.051959868520498276, acc: 0.9855072498321533)
[2024-12-12 03:14:14,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:14,844][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.03817228972911835, acc: 0.9861111044883728)
[2024-12-12 03:14:14,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:15,241][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.05404563620686531, acc: 0.9879518151283264)
[2024-12-12 03:14:15,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:15,614][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.09710126370191574, acc: 0.9743589758872986)
[2024-12-12 03:14:15,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:15,971][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.01460297405719757, acc: 1.0)
[2024-12-12 03:14:16,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:16,274][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.0003942920302506536, acc: 1.0)
[2024-12-12 03:14:16,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:16,631][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.015837812796235085, acc: 1.0)
[2024-12-12 03:14:16,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:16,998][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.4971374273300171, acc: 0.9354838728904724)
[2024-12-12 03:14:17,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:17,366][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.022901585325598717, acc: 1.0)
[2024-12-12 03:14:17,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:17,761][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.01856634020805359, acc: 1.0)
[2024-12-12 03:14:17,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:18,142][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.024960944429039955, acc: 0.9903846383094788)
[2024-12-12 03:14:18,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:18,501][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.011005206033587456, acc: 1.0)
[2024-12-12 03:14:18,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:18,819][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.01786019466817379, acc: 1.0)
[2024-12-12 03:14:18,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:19,166][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.003443763591349125, acc: 1.0)
[2024-12-12 03:14:19,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:19,559][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.146696537733078, acc: 0.9259259104728699)
[2024-12-12 03:14:19,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:19,941][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.106277696788311, acc: 0.9714285731315613)
[2024-12-12 03:14:20,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:20,267][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.12775754928588867, acc: 0.9487179517745972)
[2024-12-12 03:14:20,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:20,621][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.27190396189689636, acc: 0.9268292784690857)
[2024-12-12 03:14:20,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:21,014][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.2962006628513336, acc: 0.8947368264198303)
[2024-12-12 03:14:21,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:21,342][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.0052211401052773, acc: 1.0)
[2024-12-12 03:14:21,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:21,632][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.004463480319827795, acc: 1.0)
[2024-12-12 03:14:21,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:21,954][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.019885897636413574, acc: 1.0)
[2024-12-12 03:14:22,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:22,280][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.0026701062452048063, acc: 1.0)
[2024-12-12 03:14:22,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:22,639][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.044747356325387955, acc: 0.9838709831237793)
[2024-12-12 03:14:22,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:23,001][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.11273477226495743, acc: 0.9649122953414917)
[2024-12-12 03:14:23,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:23,342][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.026788586750626564, acc: 1.0)
[2024-12-12 03:14:23,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:23,650][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.013889256864786148, acc: 1.0)
[2024-12-12 03:14:23,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:24,020][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.00210515852086246, acc: 1.0)
[2024-12-12 03:14:24,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:24,421][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.0800328478217125, acc: 0.9599999785423279)
[2024-12-12 03:14:24,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:24,842][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.33274710178375244, acc: 0.9080459475517273)
[2024-12-12 03:14:24,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:25,218][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.15916632115840912, acc: 0.9680851101875305)
[2024-12-12 03:14:25,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:25,545][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.1703539341688156, acc: 0.9397590160369873)
[2024-12-12 03:14:25,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:25,845][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.004771670326590538, acc: 1.0)
[2024-12-12 03:14:25,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:26,169][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.00911586731672287, acc: 1.0)
[2024-12-12 03:14:26,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:26,529][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.03311578184366226, acc: 1.0)
[2024-12-12 03:14:26,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:26,862][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.0722026601433754, acc: 0.9811320900917053)
[2024-12-12 03:14:26,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:27,233][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.08522238582372665, acc: 0.9620253443717957)
[2024-12-12 03:14:27,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:27,569][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.16603969037532806, acc: 0.9803921580314636)
[2024-12-12 03:14:27,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:27,923][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.03229273483157158, acc: 0.9850746393203735)
[2024-12-12 03:14:28,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:28,348][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.001288250321522355, acc: 1.0)
[2024-12-12 03:14:28,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:28,738][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.025010595098137856, acc: 1.0)
[2024-12-12 03:14:28,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:29,127][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.3048078417778015, acc: 0.9166666865348816)
[2024-12-12 03:14:29,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:29,465][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.31586503982543945, acc: 0.9069767594337463)
[2024-12-12 03:14:29,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:29,824][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.12119629234075546, acc: 0.9743589758872986)
[2024-12-12 03:14:29,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:30,177][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.07257745414972305, acc: 0.9777777791023254)
[2024-12-12 03:14:30,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:30,537][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.0023521692492067814, acc: 1.0)
[2024-12-12 03:14:30,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:30,961][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.10208472609519958, acc: 0.9615384340286255)
[2024-12-12 03:14:31,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:31,351][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.44362983107566833, acc: 0.9120879173278809)
[2024-12-12 03:14:31,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:31,845][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.3686493933200836, acc: 0.939130425453186)
[2024-12-12 03:14:31,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:32,221][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.17348085343837738, acc: 0.9347826242446899)
[2024-12-12 03:14:32,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:32,566][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.21267174184322357, acc: 0.9387755393981934)
[2024-12-12 03:14:32,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:32,947][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.0034149575512856245, acc: 1.0)
[2024-12-12 03:14:33,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:33,274][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.014652087353169918, acc: 1.0)
[2024-12-12 03:14:33,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:33,689][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.09019472450017929, acc: 0.9756097793579102)
[2024-12-12 03:14:33,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:34,103][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.02008124254643917, acc: 1.0)
[2024-12-12 03:14:34,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:34,517][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.037222083657979965, acc: 1.0)
[2024-12-12 03:14:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:34,888][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.6299002766609192, acc: 0.9024389982223511)
[2024-12-12 03:14:34,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:35,210][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.013075594790279865, acc: 1.0)
[2024-12-12 03:14:35,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:35,548][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.0037449533119797707, acc: 1.0)
[2024-12-12 03:14:35,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:35,855][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.0016238443786278367, acc: 1.0)
[2024-12-12 03:14:35,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:36,201][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.15339431166648865, acc: 0.9285714030265808)
[2024-12-12 03:14:36,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:36,578][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.04205213114619255, acc: 1.0)
[2024-12-12 03:14:36,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:37,178][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.47496259212493896, acc: 0.8363636136054993)
[2024-12-12 03:14:37,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:38,059][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.30083873867988586, acc: 0.9150943160057068)
[2024-12-12 03:14:38,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:38,411][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.08100871741771698, acc: 0.9777777791023254)
[2024-12-12 03:14:38,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:38,773][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.04766589403152466, acc: 0.9821428656578064)
[2024-12-12 03:14:38,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:39,110][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.45948436856269836, acc: 0.8857142925262451)
[2024-12-12 03:14:39,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:39,471][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.0012791366316378117, acc: 1.0)
[2024-12-12 03:14:39,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:39,869][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.007803292945027351, acc: 1.0)
[2024-12-12 03:14:39,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:40,209][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.019699623808264732, acc: 1.0)
[2024-12-12 03:14:40,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:40,568][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.007270368281751871, acc: 1.0)
[2024-12-12 03:14:40,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:41,142][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.09214267879724503, acc: 0.976047933101654)
[2024-12-12 03:14:41,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:41,576][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.12486628443002701, acc: 0.9624060392379761)
[2024-12-12 03:14:42,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:42,833][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.3746378421783447, acc: 0.903743326663971)
[2024-12-12 03:14:43,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:43,397][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.06888404488563538, acc: 0.9639639854431152)
[2024-12-12 03:14:43,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:43,754][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.05796664580702782, acc: 1.0)
[2024-12-12 03:14:43,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:44,115][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.002211393788456917, acc: 1.0)
[2024-12-12 03:14:44,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:44,508][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.0630362406373024, acc: 0.96875)
[2024-12-12 03:14:44,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:44,874][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.00908091850578785, acc: 1.0)
[2024-12-12 03:14:45,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:45,247][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.003893462475389242, acc: 1.0)
[2024-12-12 03:14:45,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:45,579][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.000444319739472121, acc: 1.0)
[2024-12-12 03:14:45,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:45,996][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.0023229720536619425, acc: 1.0)
[2024-12-12 03:14:46,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:46,369][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.056620627641677856, acc: 0.9523809552192688)
[2024-12-12 03:14:46,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:46,748][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.3304770886898041, acc: 0.8518518805503845)
[2024-12-12 03:14:46,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:47,070][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.3023214340209961, acc: 0.8834951519966125)
[2024-12-12 03:14:47,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:47,615][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.2686499357223511, acc: 0.9191176295280457)
[2024-12-12 03:14:47,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:48,011][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.28164100646972656, acc: 0.9266666769981384)
[2024-12-12 03:14:48,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:48,395][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.3866764008998871, acc: 0.8819444179534912)
[2024-12-12 03:14:48,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:48,785][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.05381712317466736, acc: 0.9767441749572754)
[2024-12-12 03:14:48,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:49,140][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.049831077456474304, acc: 0.9583333134651184)
[2024-12-12 03:14:49,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:49,562][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.03805593401193619, acc: 0.9767441749572754)
[2024-12-12 03:14:49,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:49,954][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.07029654085636139, acc: 0.9599999785423279)
[2024-12-12 03:14:50,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:50,566][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.04815620183944702, acc: 1.0)
[2024-12-12 03:14:50,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:50,980][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.25582101941108704, acc: 0.9066666960716248)
[2024-12-12 03:14:51,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:51,317][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.08526697754859924, acc: 0.9696969985961914)
[2024-12-12 03:14:51,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:51,624][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.047534286975860596, acc: 0.9696969985961914)
[2024-12-12 03:14:51,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:51,958][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.0622122623026371, acc: 0.9677419066429138)
[2024-12-12 03:14:52,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:52,306][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.0025768233463168144, acc: 1.0)
[2024-12-12 03:14:52,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:52,695][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.007293574046343565, acc: 1.0)
[2024-12-12 03:14:52,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:53,043][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.001276560826227069, acc: 1.0)
[2024-12-12 03:14:53,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:53,385][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.11814364045858383, acc: 0.9629629850387573)
[2024-12-12 03:14:53,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:53,722][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.07092065364122391, acc: 0.9615384340286255)
[2024-12-12 03:14:53,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:54,050][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.014908633194863796, acc: 1.0)
[2024-12-12 03:14:54,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:55,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:55,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:55,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:56,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:56,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:56,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:57,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:57,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:58,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:58,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:58,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:59,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:59,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:59,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:00,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:00,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:01,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:01,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:01,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:01,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:02,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:02,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:02,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:03,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:03,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:04,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:04,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:04,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:05,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:05,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:06,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:06,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:06,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:07,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:07,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:07,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:08,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:08,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:08,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:09,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:09,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:09,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:10,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:10,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:10,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:11,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:11,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:11,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:12,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:12,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:12,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:13,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:13,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:14,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:14,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:14,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:14,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:15,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:15,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:16,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:16,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:16,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:17,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:17,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:17,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:18,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:18,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:19,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:19,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:19,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:20,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:20,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:20,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:21,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:21,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:21,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:22,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:22,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:22,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:23,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:23,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:23,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:24,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:24,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:25,094][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6859, device='cuda:0') eval_epoch_loss=tensor(0.9880, device='cuda:0') eval_epoch_acc=tensor(0.7934, device='cuda:0')
[2024-12-12 03:15:25,095][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:15:25,096][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:15:25,396][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_10_step_411_loss_0.9880116581916809/model.pt
[2024-12-12 03:15:25,400][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:15:25,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:25,787][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.020306741818785667, acc: 1.0)
[2024-12-12 03:15:25,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:26,111][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.003592107677832246, acc: 1.0)
[2024-12-12 03:15:26,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:26,480][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.13640789687633514, acc: 0.939393937587738)
[2024-12-12 03:15:26,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:26,889][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.001385575975291431, acc: 1.0)
[2024-12-12 03:15:27,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:27,244][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.17914043366909027, acc: 0.9215686321258545)
[2024-12-12 03:15:27,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:27,558][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.01711329072713852, acc: 1.0)
[2024-12-12 03:15:27,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:27,875][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.041659120470285416, acc: 1.0)
[2024-12-12 03:15:27,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:28,282][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.029644915834069252, acc: 1.0)
[2024-12-12 03:15:28,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:28,670][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.03723659738898277, acc: 1.0)
[2024-12-12 03:15:28,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:28,997][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.0028730067424476147, acc: 1.0)
[2024-12-12 03:15:29,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:29,300][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.07194025069475174, acc: 0.9666666388511658)
[2024-12-12 03:15:29,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:29,641][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.17658813297748566, acc: 0.9375)
[2024-12-12 03:15:29,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:30,028][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.02823886275291443, acc: 1.0)
[2024-12-12 03:15:30,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:30,413][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.02335604652762413, acc: 1.0)
[2024-12-12 03:15:30,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:30,754][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.08972184360027313, acc: 0.9696969985961914)
[2024-12-12 03:15:30,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:31,109][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.014386946335434914, acc: 1.0)
[2024-12-12 03:15:31,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:31,471][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.01052068267017603, acc: 1.0)
[2024-12-12 03:15:31,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:31,828][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.01333674043416977, acc: 1.0)
[2024-12-12 03:15:31,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:32,184][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.007802631705999374, acc: 1.0)
[2024-12-12 03:15:32,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:32,539][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.001109068631194532, acc: 1.0)
[2024-12-12 03:15:32,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:32,904][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.016599856317043304, acc: 1.0)
[2024-12-12 03:15:33,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:33,244][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.01646960899233818, acc: 1.0)
[2024-12-12 03:15:33,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:33,642][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.04571392759680748, acc: 0.9722222089767456)
[2024-12-12 03:15:33,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:34,046][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.0019111917354166508, acc: 1.0)
[2024-12-12 03:15:34,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:34,413][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.051169347018003464, acc: 0.9696969985961914)
[2024-12-12 03:15:34,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:34,773][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.0748925656080246, acc: 0.9722222089767456)
[2024-12-12 03:15:34,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:35,160][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.006526146549731493, acc: 1.0)
[2024-12-12 03:15:35,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:35,538][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.000502550567034632, acc: 1.0)
[2024-12-12 03:15:35,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:35,892][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.012536861002445221, acc: 1.0)
[2024-12-12 03:15:36,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:36,379][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.3067784011363983, acc: 0.9090909361839294)
[2024-12-12 03:15:36,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:37,098][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.396741658449173, acc: 0.871999979019165)
[2024-12-12 03:15:37,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:37,493][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.31912070512771606, acc: 0.9032257795333862)
[2024-12-12 03:15:37,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:38,138][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.3152727782726288, acc: 0.8805969953536987)
[2024-12-12 03:15:38,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:38,513][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.02816713973879814, acc: 0.9811320900917053)
[2024-12-12 03:15:38,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:38,924][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.01963322050869465, acc: 1.0)
[2024-12-12 03:15:38,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:39,258][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.004211532883346081, acc: 1.0)
[2024-12-12 03:15:39,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:39,587][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.40446943044662476, acc: 0.9615384340286255)
[2024-12-12 03:15:39,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:39,876][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.07139246165752411, acc: 0.9642857313156128)
[2024-12-12 03:15:40,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:40,286][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.01298486813902855, acc: 1.0)
[2024-12-12 03:15:40,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:40,679][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.05520239844918251, acc: 0.9722222089767456)
[2024-12-12 03:15:40,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:41,008][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.01630363054573536, acc: 1.0)
[2024-12-12 03:15:41,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:41,371][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.14588283002376556, acc: 0.9487179517745972)
[2024-12-12 03:15:41,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:41,789][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.14456681907176971, acc: 0.9605262875556946)
[2024-12-12 03:15:41,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:42,211][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.059320688247680664, acc: 1.0)
[2024-12-12 03:15:42,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:42,585][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.03510464355349541, acc: 0.9696969985961914)
[2024-12-12 03:15:42,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:42,924][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.2361350655555725, acc: 0.9175257682800293)
[2024-12-12 03:15:43,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:43,295][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.10980673134326935, acc: 0.9571428298950195)
[2024-12-12 03:15:43,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:43,653][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.2647334039211273, acc: 0.9534883499145508)
[2024-12-12 03:15:43,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:43,940][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.08681466430425644, acc: 0.9642857313156128)
[2024-12-12 03:15:44,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:44,269][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.06261607259511948, acc: 0.9876543283462524)
[2024-12-12 03:15:44,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:44,609][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.27357497811317444, acc: 0.9444444179534912)
[2024-12-12 03:15:44,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:44,976][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.08116061240434647, acc: 0.96875)
[2024-12-12 03:15:45,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:45,295][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.06814323365688324, acc: 0.9615384340286255)
[2024-12-12 03:15:45,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:45,692][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.013376748189330101, acc: 1.0)
[2024-12-12 03:15:45,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:46,046][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.11969750374555588, acc: 0.9523809552192688)
[2024-12-12 03:15:46,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:46,401][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.15491488575935364, acc: 0.9036144614219666)
[2024-12-12 03:15:46,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:46,775][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.08153674006462097, acc: 0.9729729890823364)
[2024-12-12 03:15:46,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:47,086][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.16194307804107666, acc: 0.9417475461959839)
[2024-12-12 03:15:47,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:47,433][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.13253898918628693, acc: 0.934959352016449)
[2024-12-12 03:15:47,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:47,772][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.017385520040988922, acc: 1.0)
[2024-12-12 03:15:47,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:48,091][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.03274522349238396, acc: 1.0)
[2024-12-12 03:15:48,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:48,495][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.0779872015118599, acc: 0.9901960492134094)
[2024-12-12 03:15:48,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:48,854][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.30654439330101013, acc: 0.9257642030715942)
[2024-12-12 03:15:48,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:49,213][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.08252500742673874, acc: 0.9895833134651184)
[2024-12-12 03:15:49,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:49,587][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.154281347990036, acc: 0.9509202241897583)
[2024-12-12 03:15:49,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:49,927][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.15010595321655273, acc: 0.9424460530281067)
[2024-12-12 03:15:50,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:50,286][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.31319257616996765, acc: 0.8844221234321594)
[2024-12-12 03:15:50,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:50,622][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.17108042538166046, acc: 0.9722222089767456)
[2024-12-12 03:15:50,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:50,960][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.05835067480802536, acc: 0.9696969985961914)
[2024-12-12 03:15:51,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:51,292][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.0891052857041359, acc: 0.9629629850387573)
[2024-12-12 03:15:51,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:51,625][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.02069605514407158, acc: 1.0)
[2024-12-12 03:15:51,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:51,951][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.09144968539476395, acc: 0.949999988079071)
[2024-12-12 03:15:52,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:52,337][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.1674008071422577, acc: 0.931034505367279)
[2024-12-12 03:15:52,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:52,665][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.013046023435890675, acc: 1.0)
[2024-12-12 03:15:52,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:52,978][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.1929127275943756, acc: 0.9473684430122375)
[2024-12-12 03:15:53,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:53,290][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.027020854875445366, acc: 1.0)
[2024-12-12 03:15:53,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:53,606][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.023493628948926926, acc: 1.0)
[2024-12-12 03:15:53,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:53,975][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.16019871830940247, acc: 0.9545454382896423)
[2024-12-12 03:15:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:54,360][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.23511216044425964, acc: 0.9230769276618958)
[2024-12-12 03:15:54,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:54,714][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.00825036596506834, acc: 1.0)
[2024-12-12 03:15:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:55,097][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.03053382784128189, acc: 1.0)
[2024-12-12 03:15:55,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:55,429][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.09826811403036118, acc: 0.9607843160629272)
[2024-12-12 03:15:55,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:55,776][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.012977361679077148, acc: 1.0)
[2024-12-12 03:15:55,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:56,156][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.04133308678865433, acc: 1.0)
[2024-12-12 03:15:56,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:56,534][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.022445769980549812, acc: 1.0)
[2024-12-12 03:15:56,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:56,947][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.1471072882413864, acc: 0.9464285969734192)
[2024-12-12 03:15:57,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:57,372][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.07408278435468674, acc: 0.9775280952453613)
[2024-12-12 03:15:57,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:57,753][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.32970577478408813, acc: 0.932584285736084)
[2024-12-12 03:15:57,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:58,162][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.4896678924560547, acc: 0.8368794322013855)
[2024-12-12 03:15:58,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:58,516][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.1524507701396942, acc: 0.945652186870575)
[2024-12-12 03:15:58,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:58,867][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.0027797615621238947, acc: 1.0)
[2024-12-12 03:15:58,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:59,194][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0246175117790699, acc: 1.0)
[2024-12-12 03:15:59,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:59,568][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.007566315121948719, acc: 1.0)
[2024-12-12 03:15:59,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:59,951][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.010479201562702656, acc: 1.0)
[2024-12-12 03:16:00,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:00,314][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.10643678158521652, acc: 0.9433962106704712)
[2024-12-12 03:16:00,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:00,673][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.08039959520101547, acc: 0.9655172228813171)
[2024-12-12 03:16:00,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:01,333][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.4968746602535248, acc: 0.837837815284729)
[2024-12-12 03:16:01,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:01,748][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.14062431454658508, acc: 0.9436619877815247)
[2024-12-12 03:16:01,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:01,981][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.04639241471886635, acc: 1.0)
[2024-12-12 03:16:02,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:02,213][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.03547294810414314, acc: 0.9666666388511658)
[2024-12-12 03:16:02,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:02,447][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.045041121542453766, acc: 0.9615384340286255)
[2024-12-12 03:16:04,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:06,284][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.3364734649658203, acc: 0.9071428775787354)
[2024-12-12 03:16:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:07,050][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.08173762261867523, acc: 0.976190447807312)
[2024-12-12 03:16:07,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:07,333][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.08180392533540726, acc: 0.9642857313156128)
[2024-12-12 03:16:07,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:07,645][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.03717092052102089, acc: 0.9833333492279053)
[2024-12-12 03:16:07,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:08,338][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.128862202167511, acc: 0.9305555820465088)
[2024-12-12 03:16:08,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:08,644][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 0.0013334105024114251, acc: 1.0)
[2024-12-12 03:16:08,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:08,985][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.006086533889174461, acc: 1.0)
[2024-12-12 03:16:09,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:09,293][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.005814837291836739, acc: 1.0)
[2024-12-12 03:16:09,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:09,645][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.05122930556535721, acc: 1.0)
[2024-12-12 03:16:09,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:10,687][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.37262803316116333, acc: 0.8686440587043762)
[2024-12-12 03:16:10,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:11,027][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.09420705586671829, acc: 0.9626865386962891)
[2024-12-12 03:16:11,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:11,388][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.0964585393667221, acc: 0.9635036587715149)
[2024-12-12 03:16:11,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:11,940][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.32150134444236755, acc: 0.9100000262260437)
[2024-12-12 03:16:12,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:12,203][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.003797011449933052, acc: 1.0)
[2024-12-12 03:16:12,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:12,580][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.045411255210638046, acc: 0.9807692170143127)
[2024-12-12 03:16:12,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:12,970][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.1708626002073288, acc: 0.9523809552192688)
[2024-12-12 03:16:13,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:13,317][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.24100357294082642, acc: 0.9180327653884888)
[2024-12-12 03:16:13,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:13,593][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.13034014403820038, acc: 0.9830508232116699)
[2024-12-12 03:16:13,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:13,949][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.06578180193901062, acc: 1.0)
[2024-12-12 03:16:14,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:14,274][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.2789923846721649, acc: 0.9318181872367859)
[2024-12-12 03:16:14,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:14,628][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.34743133187294006, acc: 0.9056603908538818)
[2024-12-12 03:16:14,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:14,976][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.5202683210372925, acc: 0.9090909361839294)
[2024-12-12 03:16:15,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:15,349][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.40196382999420166, acc: 0.9200000166893005)
[2024-12-12 03:16:15,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:15,686][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.06912817060947418, acc: 0.949999988079071)
[2024-12-12 03:16:15,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:16,050][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.003418869338929653, acc: 1.0)
[2024-12-12 03:16:16,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:16,438][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.1586437225341797, acc: 0.9384615421295166)
[2024-12-12 03:16:16,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:16,802][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.23643168807029724, acc: 0.9375)
[2024-12-12 03:16:16,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:17,196][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.2118391990661621, acc: 0.9375)
[2024-12-12 03:16:17,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:17,490][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.10661621391773224, acc: 0.9696969985961914)
[2024-12-12 03:16:17,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:17,794][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.060396693646907806, acc: 1.0)
[2024-12-12 03:16:17,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:18,100][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.09088405221700668, acc: 0.9677419066429138)
[2024-12-12 03:16:18,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:18,470][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.010151790454983711, acc: 1.0)
[2024-12-12 03:16:18,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:18,843][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.054928526282310486, acc: 1.0)
[2024-12-12 03:16:18,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:19,221][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.11377998441457748, acc: 0.9512194991111755)
[2024-12-12 03:16:19,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:19,581][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.013329636305570602, acc: 1.0)
[2024-12-12 03:16:19,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:19,962][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.0248460341244936, acc: 1.0)
[2024-12-12 03:16:20,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:20,335][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.014739417470991611, acc: 1.0)
[2024-12-12 03:16:20,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:20,649][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.003915607463568449, acc: 1.0)
[2024-12-12 03:16:20,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:20,993][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.08295521140098572, acc: 0.9696969985961914)
[2024-12-12 03:16:21,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:21,357][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.05827942490577698, acc: 0.9750000238418579)
[2024-12-12 03:16:21,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:21,741][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.19832949340343475, acc: 0.9285714030265808)
[2024-12-12 03:16:21,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:22,181][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.09821207076311111, acc: 0.970802903175354)
[2024-12-12 03:16:22,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:23,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:23,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:23,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:24,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:24,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:25,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:25,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:25,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:26,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:26,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:27,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:27,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:27,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:28,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:28,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:29,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:29,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:29,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:30,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:30,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:30,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:31,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:31,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:31,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:32,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:32,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:33,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:33,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:33,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:34,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:34,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:34,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:35,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:35,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:36,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:36,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:36,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:37,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:37,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:37,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:38,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:38,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:39,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:39,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:39,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:40,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:40,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:41,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:41,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:41,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:42,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:42,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:42,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:42,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:43,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:43,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:44,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:44,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:44,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:45,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:45,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:46,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:46,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:46,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:47,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:47,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:47,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:48,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:48,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:49,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:49,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:49,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:49,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:50,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:50,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:50,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:51,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:51,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:51,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:52,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:52,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:53,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:53,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:53,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:54,363][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6938, device='cuda:0') eval_epoch_loss=tensor(0.9910, device='cuda:0') eval_epoch_acc=tensor(0.7913, device='cuda:0')
[2024-12-12 03:16:54,364][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:16:54,364][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:16:54,621][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6/asr_epoch_10_step_554_loss_0.9909527897834778/model.pt
[2024-12-12 03:16:54,625][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_6 directory
[2024-12-12 03:16:54,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:55,110][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.14821836352348328, acc: 0.9448275566101074)
[2024-12-12 03:16:55,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:55,496][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.1848248690366745, acc: 0.9428571462631226)
[2024-12-12 03:16:55,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:55,863][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.15786287188529968, acc: 0.9602649211883545)
[2024-12-12 03:16:55,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:56,244][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.06324519217014313, acc: 0.9829059839248657)
[2024-12-12 03:16:56,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:56,604][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.008901620283722878, acc: 1.0)
[2024-12-12 03:16:56,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:56,952][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.1550639569759369, acc: 0.9615384340286255)
[2024-12-12 03:16:57,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:57,281][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.08280044049024582, acc: 0.9615384340286255)
[2024-12-12 03:16:57,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:57,621][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.2433892786502838, acc: 0.9743589758872986)
[2024-12-12 03:16:57,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:57,997][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.058227524161338806, acc: 0.9777777791023254)
[2024-12-12 03:16:58,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:58,389][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.04119245707988739, acc: 1.0)
[2024-12-12 03:16:58,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:58,792][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.13232412934303284, acc: 0.9583333134651184)
[2024-12-12 03:16:58,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:59,168][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.029741430655121803, acc: 1.0)
[2024-12-12 03:16:59,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:59,517][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.055509693920612335, acc: 0.9642857313156128)
[2024-12-12 03:16:59,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:16:59,875][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.018610810860991478, acc: 1.0)
[2024-12-12 03:16:59,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:17:00,243][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.05015716329216957, acc: 0.9629629850387573)
[2024-12-12 03:17:00,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:17:00,613][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.13795751333236694, acc: 0.9518716335296631)
[2024-12-12 03:17:00,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:17:00,946][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.013129719533026218, acc: 1.0)
[2024-12-12 03:17:01,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:17:01,356][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.03262155503034592, acc: 0.9914529919624329)
[2024-12-12 03:17:01,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:17:01,703][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.23325620591640472, acc: 0.9336734414100647)
[2024-12-12 03:17:01,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:17:02,042][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.1366562843322754, acc: 0.955974817276001)
[2024-12-12 03:17:02,525][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.1471, train_epoch_loss=0.1373, epoch time 364.41854065284133s
[2024-12-12 03:17:02,525][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 03:17:02,526][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 20 GB
[2024-12-12 03:17:02,526][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 03:17:02,526][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 29
[2024-12-12 03:17:02,526][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 03:17:02,531][root][INFO] - Key: avg_train_prep, Value: 3.3943328857421875
[2024-12-12 03:17:02,533][root][INFO] - Key: avg_train_loss, Value: 0.7303656935691833
[2024-12-12 03:17:02,533][root][INFO] - Key: avg_train_acc, Value: 0.8117634654045105
[2024-12-12 03:17:02,533][root][INFO] - Key: avg_eval_prep, Value: 3.7728664875030518
[2024-12-12 03:17:02,533][root][INFO] - Key: avg_eval_loss, Value: 1.1492011547088623
[2024-12-12 03:17:02,533][root][INFO] - Key: avg_eval_acc, Value: 0.720100462436676
[2024-12-12 03:17:02,534][root][INFO] - Key: avg_epoch_time, Value: 364.4002862479538
[2024-12-12 03:17:02,534][root][INFO] - Key: avg_checkpoint_time, Value: 0.32168035190552474
