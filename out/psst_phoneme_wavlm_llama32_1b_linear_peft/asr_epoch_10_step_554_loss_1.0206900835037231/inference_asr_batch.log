[2024-11-21 19:09:40,902][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-21 19:09:40,902][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-21 19:09:40,902][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-21 19:09:42,488][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-21 19:09:48,311][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-21 19:09:48,313][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-21 19:09:48,315][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-21 19:09:48,316][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-21 19:09:53,322][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-21 19:09:53,333][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-21 19:09:53,334][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-21 19:09:53,461][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-21 19:09:53,463][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-21 19:09:53,576][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-21 19:09:53,576][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-21 19:09:53,576][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2024-11-21 19:09:53,860][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-21 19:09:53,866][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-21 19:09:56,085][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-21 19:09:57,118][root][INFO] - --> Training Set Length = 652
[2024-11-21 19:09:57,119][root][INFO] - =====================================
[2024-11-21 19:09:58,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:09:59,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:09:59,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:00,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:01,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:02,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:04,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:05,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:06,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:07,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:13,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:14,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:16,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:17,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:23,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:24,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:24,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:26,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:27,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:27,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:28,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:29,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:29,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:30,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:31,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:32,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:33,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:33,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:34,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:35,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:36,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:36,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:37,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:37,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:38,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:39,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:40,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:40,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:41,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:42,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:43,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:43,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:44,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:45,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:46,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:46,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:47,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:49,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:49,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:51,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:53,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:54,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:54,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:55,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:55,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:56,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:57,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:10:58,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:00,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:01,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:01,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:03,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:04,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:06,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:11,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:14,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:15,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:15,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:16,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:17,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:17,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:18,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:20,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:21,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:22,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:22,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:23,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:25,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:27,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:29,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:35,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:37,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:38,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:44,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:45,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:45,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:46,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:46,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:52,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:53,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:56,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:11:58,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:00,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:03,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:04,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:05,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:11,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:12,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:13,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:19,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:26,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:32,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:32,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:33,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:33,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:34,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:35,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:36,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:36,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:37,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:37,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:38,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:39,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:40,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:41,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:47,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:48,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:48,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:54,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:55,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:55,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:56,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:57,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:57,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:12:58,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:00,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:06,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:06,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:07,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:07,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:08,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:08,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:14,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:15,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:16,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:17,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:18,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:18,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:19,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:20,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:21,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:21,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:22,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:23,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:24,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:24,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:25,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:26,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:32,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:32,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:33,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:34,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:35,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:43,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:49,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:55,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:56,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:56,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:57,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:58,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:59,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:13:59,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:14:07,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:16:45,533][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-21 19:16:45,534][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-21 19:16:45,534][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-21 19:16:47,128][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-21 19:16:52,809][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-21 19:16:52,812][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-21 19:16:52,814][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-21 19:16:52,816][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-21 19:16:58,427][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-21 19:16:58,428][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-21 19:16:58,429][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-21 19:16:58,555][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-21 19:16:58,557][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-21 19:16:58,671][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-21 19:16:58,672][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-21 19:16:58,672][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2024-11-21 19:16:58,781][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-21 19:16:58,785][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-21 19:17:00,944][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-21 19:17:02,194][root][INFO] - --> Training Set Length = 652
[2024-11-21 19:17:02,195][root][INFO] - =====================================
[2024-11-21 19:17:03,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:04,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:05,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:05,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:06,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:07,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:09,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:10,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:11,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:12,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:18,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:19,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:21,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:22,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:28,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:28,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:29,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:30,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:31,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:32,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:33,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:33,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:34,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:35,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:36,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:37,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:38,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:38,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:39,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:40,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:40,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:41,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:41,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:42,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:43,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:44,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:44,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:45,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:46,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:47,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:47,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:48,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:49,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:49,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:50,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:50,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:52,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:53,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:55,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:57,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:57,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:58,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:58,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:17:59,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:00,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:01,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:02,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:03,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:04,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:05,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:06,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:07,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:09,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:15,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:17,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:18,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:19,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:20,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:20,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:21,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:23,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:24,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:25,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:25,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:26,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:30,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:32,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:38,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:40,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:41,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:47,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:47,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:48,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:49,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:49,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:55,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:56,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:18:58,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:00,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:02,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:05,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:06,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:08,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:13,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:14,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:15,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:21,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:28,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:34,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:34,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:34,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:35,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:35,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:37,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:37,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:38,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:39,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:39,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:40,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:41,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:41,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:42,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:48,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:49,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:50,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:56,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:56,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:57,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:57,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:58,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:59,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:19:59,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:01,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:07,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:07,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:08,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:08,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:09,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:09,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:15,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:16,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:17,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:18,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:19,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:19,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:20,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:21,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:22,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:22,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:23,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:24,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:25,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:25,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:26,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:27,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:32,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:33,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:34,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:35,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:36,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:44,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:49,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:56,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:56,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:57,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:57,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:58,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:20:59,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:21:00,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-21 19:21:07,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:03:19,367][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-25 19:03:19,367][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-25 19:03:19,367][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-25 19:03:21,521][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-25 19:03:27,880][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-25 19:03:27,884][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-25 19:03:27,887][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-25 19:03:27,888][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-25 19:03:34,991][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-25 19:03:34,993][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-25 19:03:34,995][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-25 19:03:35,150][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-25 19:03:35,153][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-25 19:03:35,295][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-25 19:03:35,295][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-25 19:03:35,296][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2024-11-25 19:03:35,468][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-25 19:03:35,472][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-25 19:03:35,491][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-25 19:03:38,153][root][INFO] - --> Training Set Length = 652
[2024-11-25 19:03:38,153][root][INFO] - =====================================
[2024-11-25 19:03:42,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:04:22,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:05:04,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:05:48,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:06:45,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:13,461][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-25 19:50:13,461][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-25 19:50:13,461][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-25 19:50:15,540][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-25 19:50:20,966][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-25 19:50:20,968][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-25 19:50:20,970][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-25 19:50:20,971][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-25 19:50:29,931][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-25 19:50:29,932][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-25 19:50:29,933][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-25 19:50:30,056][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-25 19:50:30,058][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-25 19:50:30,167][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-25 19:50:30,168][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-25 19:50:30,168][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2024-11-25 19:50:30,356][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-25 19:50:30,360][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-25 19:50:32,408][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-25 19:50:34,271][root][INFO] - --> Training Set Length = 652
[2024-11-25 19:50:34,272][root][INFO] - =====================================
[2024-11-25 19:50:36,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:37,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:38,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:38,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:39,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:40,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:42,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:44,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:44,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:45,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:51,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:52,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:54,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:50:55,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:01,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:02,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:02,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:04,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:05,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:05,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:06,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:06,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:07,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:08,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:09,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:10,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:11,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:11,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:12,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:13,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:13,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:14,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:14,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:15,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:16,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:16,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:18,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:18,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:19,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:19,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:20,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:21,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:22,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:22,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:23,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:24,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:24,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:26,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:27,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:29,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:30,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:31,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:31,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:32,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:32,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:34,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:34,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:35,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:37,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:38,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:38,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:40,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:41,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:43,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:48,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:51,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:52,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:52,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:53,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:53,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:54,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:55,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:57,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:58,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:58,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:51:59,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:00,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:02,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:04,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:05,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:12,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:14,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:15,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:21,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:21,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:22,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:22,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:23,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:29,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:30,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:33,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:35,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:37,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:40,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:41,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:42,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:48,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:49,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:50,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:52:57,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:04,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:09,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:10,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:11,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:11,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:13,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:13,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:14,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:15,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:16,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:17,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:17,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:18,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:24,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:25,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:26,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:32,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:33,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:33,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:33,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:34,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:35,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:36,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:37,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:43,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:44,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:44,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:45,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:45,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:46,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:52,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:52,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:53,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:55,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:55,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:56,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:57,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:58,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:58,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:53:59,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:00,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:01,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:01,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:02,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:03,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:04,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:10,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:10,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:11,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:12,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:13,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:21,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:27,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:34,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:34,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:35,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:35,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:36,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:37,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:38,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-25 19:54:45,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:08,827][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-26 00:29:08,828][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-26 00:29:08,828][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-26 00:29:10,179][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-26 00:29:15,695][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-26 00:29:15,697][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-26 00:29:15,699][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-26 00:29:15,700][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-26 00:29:20,881][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-26 00:29:20,882][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-26 00:29:20,883][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-26 00:29:21,000][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-26 00:29:21,002][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-26 00:29:21,110][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-26 00:29:21,110][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-26 00:29:21,111][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2024-11-26 00:29:21,322][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-26 00:29:21,326][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-26 00:29:23,753][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-26 00:29:25,112][root][INFO] - --> Training Set Length = 652
[2024-11-26 00:29:25,112][root][INFO] - =====================================
[2024-11-26 00:29:26,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:27,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:27,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:28,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:28,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:29,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:32,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:33,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:33,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:35,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:40,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:41,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:43,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:44,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:50,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:50,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:51,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:52,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:53,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:54,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:54,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:55,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:55,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:56,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:57,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:58,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:59,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:29:59,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:01,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:01,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:02,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:03,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:03,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:04,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:05,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:06,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:06,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:07,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:07,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:08,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:09,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:10,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:10,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:11,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:11,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:12,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:14,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:15,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:16,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:18,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:19,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:19,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:19,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:20,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:21,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:22,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:23,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:25,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:26,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:26,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:28,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:29,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:30,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:36,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:38,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:39,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:39,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:40,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:41,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:41,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:42,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:44,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:45,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:46,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:46,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:47,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:49,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:51,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:53,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:30:59,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:01,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:02,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:07,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:08,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:08,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:09,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:09,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:15,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:16,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:19,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:21,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:22,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:25,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:26,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:28,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:33,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:34,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:35,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:41,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:48,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:53,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:54,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:54,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:55,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:55,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:57,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:57,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:58,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:58,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:31:59,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:00,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:01,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:01,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:02,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:08,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:09,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:09,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:15,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:16,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:16,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:17,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:17,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:18,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:19,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:20,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:26,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:26,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:27,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:27,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:28,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:28,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:34,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:34,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:35,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:37,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:37,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:38,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:39,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:40,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:40,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:41,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:41,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:42,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:43,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:44,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:44,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:45,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:51,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:51,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:52,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:53,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:32:54,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:02,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:07,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:13,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:14,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:14,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:15,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:16,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:17,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:17,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:33:24,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:42:03,320][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-26 00:42:03,320][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-26 00:42:03,321][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-26 00:42:04,963][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-26 00:42:12,247][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-26 00:42:12,266][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-26 00:42:12,289][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-26 00:42:12,305][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-26 00:42:16,344][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-26 00:42:16,354][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-26 00:42:16,365][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-26 00:42:16,752][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-26 00:42:16,792][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-26 00:42:16,927][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-26 00:42:16,927][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-26 00:42:16,928][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2024-11-26 00:42:17,074][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-26 00:42:17,133][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-26 00:42:17,204][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-26 00:42:18,222][root][INFO] - --> Training Set Length = 652
[2024-11-26 00:42:18,223][root][INFO] - =====================================
[2024-11-26 00:44:29,638][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-26 00:44:29,639][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-26 00:44:29,639][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-26 00:44:31,236][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-26 00:44:38,570][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-26 00:44:38,589][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-26 00:44:38,612][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-26 00:44:38,627][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-26 00:44:42,253][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-26 00:44:42,262][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-26 00:44:42,274][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-26 00:44:42,657][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-26 00:44:42,695][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-26 00:44:42,826][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-26 00:44:42,826][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-26 00:44:42,827][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2024-11-26 00:44:42,956][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-26 00:44:43,015][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-11-26 00:44:43,093][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-26 00:44:43,912][root][INFO] - --> Training Set Length = 652
[2024-11-26 00:44:43,914][root][INFO] - =====================================
[2024-11-26 00:47:21,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:48:37,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:49:29,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:50:23,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-26 00:51:17,948][slam_llm.models.slam_model][INFO] - modality encoder
