[2024-12-17 05:49:14,473][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2024-12-17 05:49:14,473][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-17 05:49:14,473][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-17 05:49:15,878][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-17 05:49:21,252][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-17 05:49:21,254][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-17 05:49:21,256][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-17 05:49:21,257][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-17 05:49:26,797][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-17 05:49:26,797][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-17 05:49:26,799][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-17 05:49:26,918][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-17 05:49:26,920][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-17 05:49:27,036][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-17 05:49:27,036][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-17 05:49:27,036][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_5347_loss_0.048072949051856995/model.pt
[2024-12-17 05:49:27,212][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-17 05:49:27,216][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-17 05:49:28,691][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-17 05:49:29,774][root][INFO] - --> Training Set Length = 2620
[2024-12-17 05:49:29,775][root][INFO] - =====================================
[2024-12-17 05:49:31,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:49:38,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:49:43,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:49:47,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:49:53,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:49:58,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:04,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:07,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:09,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:14,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:19,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:22,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:25,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:31,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:35,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:40,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:43,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:46,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:48,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:50,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:50:56,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:02,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:09,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:14,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:19,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:24,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:28,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:34,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:40,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:47,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:51,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:51:57,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:00,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:06,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:11,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:18,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:25,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:28,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:32,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:38,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:44,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:50,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:52:57,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:05,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:11,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:18,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:24,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:26,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:30,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:32,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:36,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:40,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:43,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:46,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:48,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:53,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:53:56,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:00,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:03,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:07,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:13,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:17,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:24,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:26,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:28,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:34,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:38,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:45,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:52,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:54:58,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:04,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:09,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:15,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:19,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:26,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:31,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:37,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:43,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:50,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:55:56,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:04,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:10,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:12,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:14,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:16,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:23,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:29,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:37,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:42,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:43,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:46,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:49,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:54,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:56:58,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:03,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:11,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:16,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:24,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:29,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:37,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:43,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:48,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:57:57,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:13,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:17,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:24,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:32,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:35,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:43,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:48,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:54,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:58:56,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:01,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:05,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:07,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:12,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:20,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:28,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:32,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:38,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:42,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:47,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:53,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 05:59:59,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:07,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:13,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:20,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:27,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:30,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:36,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:42,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:46,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:49,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:54,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:00:59,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:06,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:13,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:21,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:25,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:29,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:35,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:43,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:51,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:53,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:01:58,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:04,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:11,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:16,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:21,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:24,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:27,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:31,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:36,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:44,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:52,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:02:59,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:07,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:14,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:20,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:28,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:36,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:43,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:51,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:03:59,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:07,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:16,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:21,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:24,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:32,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:38,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:42,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:50,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:52,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:04:58,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:03,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:10,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:18,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:26,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:34,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:40,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:48,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:05:54,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:01,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:06,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:15,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:22,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:36,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:39,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:42,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:48,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:51,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:06:57,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:00,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:07,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:12,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:16,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:23,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:30,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:33,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:40,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:44,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:50,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:07:57,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:02,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:10,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:17,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:22,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:27,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:33,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:36,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:43,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:51,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:08:57,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:04,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:10,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:19,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:26,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:31,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:39,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:44,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:09:52,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:00,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:08,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:16,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:19,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:23,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:25,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:28,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:35,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:37,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:43,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:48,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:50,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:54,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:10:57,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:01,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:05,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:08,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:11,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:15,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:18,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:26,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:31,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:36,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:41,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:48,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:11:56,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:03,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:10,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:15,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:22,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:26,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:32,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:40,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:45,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:52,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:12:59,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:02,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:08,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:11,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:17,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:21,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:24,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:29,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:32,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:36,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:41,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:45,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:48,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:50,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:13:56,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:00,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:06,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:11,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:14,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:19,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:22,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:29,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:37,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:45,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:51,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:14:55,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:02,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:07,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:11,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:19,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:23,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:29,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:34,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:39,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:46,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:52,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:55,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:15:59,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:03,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:07,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:11,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:16,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:22,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:26,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:30,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:33,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:36,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:39,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:43,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:47,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:52,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:16:55,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:03,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:11,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:19,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:27,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:35,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:42,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:48,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:17:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:00,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:07,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:14,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:21,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:28,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:33,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:40,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:46,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:52,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:18:59,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:05,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:12,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:16,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:21,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:29,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:33,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:38,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:44,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:48,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:50,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:19:54,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:00,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:04,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:10,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:13,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:18,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:23,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:30,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:34,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:37,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:43,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:50,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:20:58,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:02,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:07,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:12,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:18,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:23,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:27,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:30,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:33,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:40,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:47,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:52,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:21:59,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:06,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:13,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:21,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:28,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:35,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:36,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:41,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:45,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:51,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:22:58,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:05,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:11,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:18,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:20,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:25,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:27,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:35,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:38,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:40,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:46,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:50,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:23:57,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:00,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:03,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:10,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:14,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:16,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:19,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:22,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:26,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:31,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:33,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:35,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:38,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:41,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:46,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:51,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:24:54,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:01,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:08,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:14,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:21,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:27,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:33,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:39,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:44,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:48,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:50,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:25:57,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:01,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:07,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:14,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:28,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:35,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:37,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:43,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:50,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:52,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:26:55,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:03,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:07,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:13,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:20,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:25,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:31,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:38,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:44,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:49,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:55,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:27:59,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:03,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:07,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:13,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:19,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:26,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:31,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:34,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:37,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:40,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:47,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:28:54,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:01,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:07,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:13,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:20,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:27,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:31,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:38,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:44,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:51,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:29:57,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:04,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:10,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:17,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:24,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:30,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:37,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:44,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:51,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:30:54,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:00,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:04,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:08,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:10,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:14,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:17,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:20,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:22,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:25,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:28,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:31,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:32,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:36,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:39,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:45,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:51,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:31:57,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:03,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:09,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:12,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:16,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:19,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:24,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:30,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:34,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:37,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:40,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:42,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:45,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:50,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:54,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:32:57,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:00,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:03,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:05,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:07,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:11,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:15,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:17,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:20,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:22,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:27,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:31,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:35,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:38,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:41,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:45,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:49,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:54,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:33:59,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:05,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:06,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:09,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:15,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:18,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:23,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:29,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:32,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:37,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:45,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:51,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:34:56,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:00,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:04,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:11,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:14,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:20,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:25,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:29,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:32,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:35,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:40,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:44,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:50,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:35:56,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:03,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:07,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:12,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:19,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:25,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:28,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:30,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:35,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:40,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:43,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:45,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:49,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:36:55,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:00,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:02,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:05,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:07,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:10,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:15,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:17,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:21,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:25,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:27,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:30,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:34,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:36,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:40,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:44,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:46,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:50,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:37:56,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:02,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:06,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:12,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:17,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:21,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:26,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:32,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:38,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:44,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:50,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:38:56,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:01,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:07,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:11,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:20,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:22,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:24,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:28,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:34,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:37,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:39,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:41,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:46,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:49,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:53,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:39:58,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:02,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:05,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:08,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:14,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:19,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:23,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:26,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:30,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:33,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:38,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:44,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:51,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:40:57,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:03,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:09,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:16,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:22,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:29,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:33,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:39,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:47,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:53,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:41:58,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:01,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:04,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:11,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:14,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:18,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:23,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:26,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:32,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:37,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:40,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:42,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:46,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:48,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:54,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:42:59,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:05,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:10,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:12,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:15,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:18,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:20,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:23,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:31,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:35,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:39,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:41,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:44,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:46,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:52,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:43:58,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:44:04,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:44:08,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 06:44:13,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-18 22:27:11,264][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-18 22:27:11,264][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-18 22:27:11,264][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-18 22:27:13,006][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-18 22:27:18,853][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-18 22:27:18,856][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-18 22:27:18,864][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-18 22:27:18,865][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-18 22:27:24,015][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-18 22:27:24,017][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-18 22:27:24,018][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-18 22:27:24,166][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-18 22:27:24,168][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-18 22:27:24,291][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-18 22:27:24,291][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-18 22:27:24,291][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_5347_loss_0.048072949051856995/model.pt
[2024-12-18 22:27:24,464][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-18 22:27:24,469][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-18 22:27:24,486][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-18 22:27:25,608][root][INFO] - --> Training Set Length = 4
[2024-12-18 22:27:25,609][root][INFO] - =====================================
[2024-12-18 22:28:53,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-19 19:13:48,949][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-19 19:13:48,949][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-19 19:13:48,949][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-19 19:13:50,420][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-19 19:13:56,292][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:13:56,295][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-19 19:13:56,313][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:13:56,319][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-19 19:14:02,826][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:14:02,828][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-19 19:14:02,843][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-19 19:14:02,991][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:14:02,994][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-19 19:25:36,518][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-19 19:25:36,519][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-19 19:25:36,520][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-19 19:25:38,212][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-19 19:25:43,943][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:25:43,945][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-19 19:25:43,953][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:25:43,959][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-19 19:25:49,755][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:25:49,757][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-19 19:25:49,760][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-19 19:25:49,904][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:25:49,907][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-19 19:25:50,028][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-19 19:25:50,028][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-19 19:25:50,029][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_5347_loss_0.048072949051856995/model.pt
[2024-12-19 19:25:50,255][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-19 19:25:50,260][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-19 19:25:50,275][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-19 19:25:52,739][root][INFO] - --> Training Set Length = 4
[2024-12-19 19:25:52,740][root][INFO] - =====================================
[2024-12-19 19:27:14,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-24 00:09:17,448][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-24 00:09:17,448][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-24 00:09:17,451][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-24 00:09:19,335][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-24 00:09:26,518][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-24 00:09:26,523][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-24 00:09:26,532][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-24 00:09:26,539][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-24 00:09:31,872][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-24 00:09:31,873][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-24 00:09:31,876][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-24 00:09:32,043][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-24 00:09:32,046][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-24 00:09:32,177][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-24 00:09:32,178][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-24 00:09:32,178][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_5347_loss_0.048072949051856995/model.pt
[2024-12-24 00:09:32,378][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-24 00:09:32,384][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-24 00:09:32,425][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-24 00:09:33,728][root][INFO] - --> Training Set Length = 2
[2024-12-24 00:09:33,728][root][INFO] - =====================================
[2024-12-24 00:09:57,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-29 23:16:03,227][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-29 23:16:03,228][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-29 23:16:03,231][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft'}
[2024-12-29 23:16:04,608][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-29 23:16:10,747][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-29 23:16:10,749][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-29 23:16:10,752][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-29 23:16:10,756][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-29 23:16:14,401][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-29 23:16:14,402][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-29 23:16:14,404][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-29 23:16:14,555][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-29 23:16:14,558][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-29 23:16:14,677][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-29 23:16:14,677][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-29 23:16:14,678][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_5347_loss_0.048072949051856995/model.pt
[2024-12-29 23:16:15,059][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-29 23:16:15,064][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-29 23:16:15,081][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-29 23:16:15,637][root][INFO] - --> Training Set Length = 2
[2024-12-29 23:16:15,638][root][INFO] - =====================================
[2024-12-29 23:16:38,742][slam_llm.models.slam_model][INFO] - modality encoder
