[2025-02-13 18:50:25,950][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 18:50:25,952][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 18:50:25,952][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100'}
[2025-02-13 18:50:25,952][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_18-50-25.txt', 'log_interval': 5}
[2025-02-13 18:50:51,001][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 18:50:55,923][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 18:50:55,925][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 18:50:55,927][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 18:50:55,928][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 18:51:00,571][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 18:51:00,573][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 18:51:00,574][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-13 18:51:00,690][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 18:51:00,692][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 18:51:00,779][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 18:51:00,779][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 18:51:00,780][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.6441742181777954/model.pt
[2025-02-13 18:51:00,995][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 18:51:00,999][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 18:51:02,652][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 18:51:04,402][root][INFO] - --> Training Set Length = 28539
[2025-02-13 18:51:04,422][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 18:51:04,422][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 18:51:04,423][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 18:51:06,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:06,930][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 0.9019126892089844, acc: 0.8269230723381042)
[2025-02-13 18:51:07,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:07,360][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 0.8121305108070374, acc: 0.8535031676292419)
[2025-02-13 18:51:07,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:07,770][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 0.6978769302368164, acc: 0.8409090638160706)
[2025-02-13 18:51:07,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:08,204][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 0.6046122908592224, acc: 0.8662790656089783)
[2025-02-13 18:51:08,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:08,657][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 0.9372994303703308, acc: 0.7798742055892944)
[2025-02-13 18:51:08,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:09,072][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 0.7100447416305542, acc: 0.8603351712226868)
[2025-02-13 18:51:09,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:09,498][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 0.4683217406272888, acc: 0.8873239159584045)
[2025-02-13 18:51:09,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:09,950][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 1.079785943031311, acc: 0.7526881694793701)
[2025-02-13 18:51:10,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:10,383][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 0.9150280952453613, acc: 0.8109756112098694)
[2025-02-13 18:51:10,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:10,839][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 0.5421480536460876, acc: 0.8807947039604187)
[2025-02-13 18:51:11,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:11,337][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 0.9131265878677368, acc: 0.7869822382926941)
[2025-02-13 18:51:11,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:11,808][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 1.2755078077316284, acc: 0.7583333253860474)
[2025-02-13 18:51:12,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:12,258][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 0.7725328803062439, acc: 0.8265895843505859)
[2025-02-13 18:51:12,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:12,710][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 0.9134834408760071, acc: 0.7977527976036072)
[2025-02-13 18:51:12,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:13,135][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 0.7446181774139404, acc: 0.8175675868988037)
[2025-02-13 18:51:13,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:13,563][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 0.49015045166015625, acc: 0.8768116235733032)
[2025-02-13 18:51:13,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:13,977][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 0.4658382833003998, acc: 0.9122806787490845)
[2025-02-13 18:51:14,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:14,343][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 1.0793969631195068, acc: 0.7765957713127136)
[2025-02-13 18:51:14,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:14,757][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 0.4430907368659973, acc: 0.891566276550293)
[2025-02-13 18:51:14,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:15,183][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 0.8630238771438599, acc: 0.8092485666275024)
[2025-02-13 18:51:15,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:15,601][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 0.7064003944396973, acc: 0.8295454382896423)
[2025-02-13 18:51:15,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:16,021][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 0.9899234771728516, acc: 0.8011363744735718)
[2025-02-13 18:51:16,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:16,408][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 0.3879227638244629, acc: 0.89570552110672)
[2025-02-13 18:51:16,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:16,848][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 0.5717078447341919, acc: 0.8644067645072937)
[2025-02-13 18:51:17,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:17,294][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 0.8836106061935425, acc: 0.7722222208976746)
[2025-02-13 18:51:17,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:17,778][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 0.9287585020065308, acc: 0.8165680766105652)
[2025-02-13 18:51:17,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:18,192][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 0.7153675556182861, acc: 0.8466257452964783)
[2025-02-13 18:51:18,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:18,603][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 0.4291525185108185, acc: 0.9102563858032227)
[2025-02-13 18:51:18,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:19,011][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 1.3572661876678467, acc: 0.720812201499939)
[2025-02-13 18:51:19,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:19,470][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 0.5765016078948975, acc: 0.8757396340370178)
[2025-02-13 18:51:19,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:19,954][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 1.1281874179840088, acc: 0.7777777910232544)
[2025-02-13 18:51:20,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:20,320][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 0.9071044325828552, acc: 0.8186046481132507)
[2025-02-13 18:51:20,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:20,712][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 1.1823369264602661, acc: 0.7663043737411499)
[2025-02-13 18:51:20,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:21,048][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 1.0272108316421509, acc: 0.7692307829856873)
[2025-02-13 18:51:21,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:21,498][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 0.8547794818878174, acc: 0.8142856955528259)
[2025-02-13 18:51:21,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:21,956][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 0.8632139563560486, acc: 0.8156862854957581)
[2025-02-13 18:51:22,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:22,405][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 0.9717245697975159, acc: 0.7880184054374695)
[2025-02-13 18:51:22,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:22,848][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 0.9167193174362183, acc: 0.8042327761650085)
[2025-02-13 18:51:23,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:23,272][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 0.7899074554443359, acc: 0.8364779949188232)
[2025-02-13 18:51:23,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:23,663][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 0.6062451601028442, acc: 0.847953200340271)
[2025-02-13 18:51:23,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:24,182][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 0.6877654194831848, acc: 0.8392857313156128)
[2025-02-13 18:51:24,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:24,613][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 0.6697558760643005, acc: 0.8829787373542786)
[2025-02-13 18:51:24,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:25,020][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 0.4547341763973236, acc: 0.8888888955116272)
[2025-02-13 18:51:25,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:25,396][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 0.5488417744636536, acc: 0.8212290406227112)
[2025-02-13 18:51:25,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:25,784][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 0.4889908730983734, acc: 0.8983957171440125)
[2025-02-13 18:51:25,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:26,192][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 0.7227822542190552, acc: 0.8349056839942932)
[2025-02-13 18:51:26,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:26,601][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 0.8443659543991089, acc: 0.8081395626068115)
[2025-02-13 18:51:26,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:27,009][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 0.6712507605552673, acc: 0.8379888534545898)
[2025-02-13 18:51:27,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:27,467][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 0.6481905579566956, acc: 0.8502415418624878)
[2025-02-13 18:51:27,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:27,948][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 0.6905813813209534, acc: 0.8294117450714111)
[2025-02-13 18:51:28,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:28,411][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 0.6423398852348328, acc: 0.844660222530365)
[2025-02-13 18:51:28,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:28,843][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 0.6839908957481384, acc: 0.8571428656578064)
[2025-02-13 18:51:29,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:29,263][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 0.35589325428009033, acc: 0.9271523356437683)
[2025-02-13 18:51:29,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:29,685][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 0.4524907171726227, acc: 0.8972973227500916)
[2025-02-13 18:51:29,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:30,112][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 0.5914773941040039, acc: 0.8619047403335571)
[2025-02-13 18:51:30,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:30,532][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 0.7783483266830444, acc: 0.8586956262588501)
[2025-02-13 18:51:30,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:30,986][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 0.44905588030815125, acc: 0.8999999761581421)
[2025-02-13 18:51:31,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:31,450][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 1.319800853729248, acc: 0.7637362480163574)
[2025-02-13 18:51:31,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:31,896][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 1.0128127336502075, acc: 0.8212290406227112)
[2025-02-13 18:51:32,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:32,280][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 0.7084370255470276, acc: 0.8502673506736755)
[2025-02-13 18:51:32,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:32,677][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 0.8856815099716187, acc: 0.797468364238739)
[2025-02-13 18:51:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:33,142][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 0.8096670508384705, acc: 0.8435754179954529)
[2025-02-13 18:51:33,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:33,564][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 0.8420912623405457, acc: 0.8370786309242249)
[2025-02-13 18:51:33,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:33,942][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 0.8809944987297058, acc: 0.8131868243217468)
[2025-02-13 18:51:34,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:34,413][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 0.9359240531921387, acc: 0.7823529243469238)
[2025-02-13 18:51:34,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:34,882][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 1.4742705821990967, acc: 0.686170220375061)
[2025-02-13 18:51:35,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:35,305][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 0.9468445181846619, acc: 0.8257575631141663)
[2025-02-13 18:51:35,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:35,784][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 0.975424587726593, acc: 0.7873563170433044)
[2025-02-13 18:51:35,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:36,157][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 1.387322187423706, acc: 0.7191011309623718)
[2025-02-13 18:51:36,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:36,582][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 1.0656925439834595, acc: 0.7883597612380981)
[2025-02-13 18:51:36,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:37,008][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 0.8381526470184326, acc: 0.8125)
[2025-02-13 18:51:37,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:37,438][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 0.740955114364624, acc: 0.8181818127632141)
[2025-02-13 18:51:37,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:37,883][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 0.8220117092132568, acc: 0.8165680766105652)
[2025-02-13 18:51:38,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:38,337][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 1.173948884010315, acc: 0.7919074892997742)
[2025-02-13 18:51:38,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:38,765][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 0.8012929558753967, acc: 0.8720930218696594)
[2025-02-13 18:51:38,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:39,194][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 1.2998857498168945, acc: 0.761904776096344)
[2025-02-13 18:51:39,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:39,572][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 1.09721040725708, acc: 0.7554348111152649)
[2025-02-13 18:51:39,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:40,004][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 1.1125049591064453, acc: 0.8342541456222534)
[2025-02-13 18:51:40,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:40,450][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 0.9394587874412537, acc: 0.7615894079208374)
[2025-02-13 18:51:40,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:40,868][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 0.852863073348999, acc: 0.8399999737739563)
[2025-02-13 18:51:41,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:41,276][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 0.44738367199897766, acc: 0.915730357170105)
[2025-02-13 18:51:41,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:41,680][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 1.1066240072250366, acc: 0.7830687761306763)
[2025-02-13 18:51:41,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:42,069][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 0.7442240715026855, acc: 0.849397599697113)
[2025-02-13 18:51:42,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:42,466][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 1.1592586040496826, acc: 0.792553186416626)
[2025-02-13 18:51:42,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:42,902][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 0.6975273489952087, acc: 0.8345323801040649)
[2025-02-13 18:51:43,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:43,270][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 1.0196492671966553, acc: 0.7699999809265137)
[2025-02-13 18:51:43,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:43,709][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 1.1828476190567017, acc: 0.7142857313156128)
[2025-02-13 18:51:43,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:44,134][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 1.0957353115081787, acc: 0.7467532753944397)
[2025-02-13 18:51:44,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:44,540][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 0.9598627686500549, acc: 0.7650602459907532)
[2025-02-13 18:51:44,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:44,986][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 0.7992634773254395, acc: 0.8399999737739563)
[2025-02-13 18:51:45,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:45,403][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 0.9531844258308411, acc: 0.8151260614395142)
[2025-02-13 18:51:45,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:45,877][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 0.980280339717865, acc: 0.7685950398445129)
[2025-02-13 18:51:46,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:46,280][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 0.9460271596908569, acc: 0.7662337422370911)
[2025-02-13 18:51:46,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:46,688][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 0.8455222845077515, acc: 0.8399999737739563)
[2025-02-13 18:51:46,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:47,042][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 1.3322213888168335, acc: 0.801886796951294)
[2025-02-13 18:51:47,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:47,469][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 0.9373943209648132, acc: 0.8048780560493469)
[2025-02-13 18:51:47,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:47,883][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 0.38909924030303955, acc: 0.9252336621284485)
[2025-02-13 18:51:48,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:48,237][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 2.71097731590271, acc: 0.5)
[2025-02-13 18:51:48,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:48,635][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 1.3252978324890137, acc: 0.7394366264343262)
[2025-02-13 18:51:48,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:48,998][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.8919774889945984, acc: 0.800000011920929)
[2025-02-13 18:51:49,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:49,387][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 1.2740615606307983, acc: 0.7348066568374634)
[2025-02-13 18:51:49,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:49,732][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 1.2426398992538452, acc: 0.734375)
[2025-02-13 18:51:49,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:50,143][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 0.4655967354774475, acc: 0.8907103538513184)
[2025-02-13 18:51:50,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:50,567][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.811650276184082, acc: 0.7876712083816528)
[2025-02-13 18:51:50,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:51,009][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.4993036389350891, acc: 0.8933333158493042)
[2025-02-13 18:51:51,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:51,460][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.45386791229248047, acc: 0.8731343150138855)
[2025-02-13 18:51:51,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:51,869][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.5163083076477051, acc: 0.8819875717163086)
[2025-02-13 18:51:52,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:52,323][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 2.021839141845703, acc: 0.6686046719551086)
[2025-02-13 18:51:52,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:52,764][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.7470939755439758, acc: 0.7783783674240112)
[2025-02-13 18:51:52,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:53,157][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.9397556781768799, acc: 0.8015872836112976)
[2025-02-13 18:51:53,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:53,541][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.7308395504951477, acc: 0.8547486066818237)
[2025-02-13 18:51:53,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:53,927][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.8164733052253723, acc: 0.8030303120613098)
[2025-02-13 18:51:54,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:54,344][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 1.037961721420288, acc: 0.7986111044883728)
[2025-02-13 18:51:54,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:54,726][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.4695141017436981, acc: 0.8882681727409363)
[2025-02-13 18:51:54,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:55,139][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.8524275422096252, acc: 0.7771428823471069)
[2025-02-13 18:51:55,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:55,489][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 1.5748860836029053, acc: 0.6336633563041687)
[2025-02-13 18:51:55,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:55,929][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 1.0695431232452393, acc: 0.7873563170433044)
[2025-02-13 18:51:56,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:56,322][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.6621460914611816, acc: 0.842424213886261)
[2025-02-13 18:51:56,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:56,762][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 1.1157066822052002, acc: 0.7722222208976746)
[2025-02-13 18:51:56,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:57,176][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.7998127341270447, acc: 0.7978723645210266)
[2025-02-13 18:51:57,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:57,571][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.6742401123046875, acc: 0.8690476417541504)
[2025-02-13 18:51:57,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:58,040][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.8210602402687073, acc: 0.8379888534545898)
[2025-02-13 18:51:58,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:58,490][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.7765080332756042, acc: 0.8186812996864319)
[2025-02-13 18:51:58,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:58,903][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.649980902671814, acc: 0.8553459048271179)
[2025-02-13 18:51:59,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:59,311][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.7300766706466675, acc: 0.8194444179534912)
[2025-02-13 18:51:59,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:59,733][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 1.171390414237976, acc: 0.7598039507865906)
[2025-02-13 18:51:59,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:00,130][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.8944870829582214, acc: 0.7978723645210266)
[2025-02-13 18:52:00,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:00,539][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.7980196475982666, acc: 0.84375)
[2025-02-13 18:52:00,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:00,950][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.7131009697914124, acc: 0.8516746163368225)
[2025-02-13 18:52:01,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:01,371][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 0.7481622099876404, acc: 0.8435754179954529)
[2025-02-13 18:52:01,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:01,788][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.5667217373847961, acc: 0.8588235378265381)
[2025-02-13 18:52:01,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:02,186][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.47981610894203186, acc: 0.8771929740905762)
[2025-02-13 18:52:02,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:02,641][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.9276512861251831, acc: 0.8616352081298828)
[2025-02-13 18:52:02,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:03,064][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.7272520065307617, acc: 0.845588207244873)
[2025-02-13 18:52:03,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:03,460][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.5640668272972107, acc: 0.8796992301940918)
[2025-02-13 18:52:03,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:03,862][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.5880385637283325, acc: 0.8730158805847168)
[2025-02-13 18:52:04,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:04,301][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.6780862808227539, acc: 0.85326087474823)
[2025-02-13 18:52:04,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:04,717][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.9852436184883118, acc: 0.7939698696136475)
[2025-02-13 18:52:04,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:05,161][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.8362275958061218, acc: 0.801075279712677)
[2025-02-13 18:52:05,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:05,645][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.8448521494865417, acc: 0.8085106611251831)
[2025-02-13 18:52:05,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:06,084][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.550274133682251, acc: 0.8711656332015991)
[2025-02-13 18:52:06,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:06,510][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.6320724487304688, acc: 0.8450000286102295)
[2025-02-13 18:52:06,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:06,977][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.7654228210449219, acc: 0.8248587846755981)
[2025-02-13 18:52:07,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:07,407][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.8683980703353882, acc: 0.7845304012298584)
[2025-02-13 18:52:07,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:07,839][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 1.0684839487075806, acc: 0.7567567825317383)
[2025-02-13 18:52:07,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:08,213][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 1.5364669561386108, acc: 0.6891191601753235)
[2025-02-13 18:52:08,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:08,683][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 1.2639844417572021, acc: 0.7175140976905823)
[2025-02-13 18:52:08,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:09,085][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 1.203404188156128, acc: 0.7310344576835632)
[2025-02-13 18:52:09,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:09,504][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 2.077833414077759, acc: 0.6129032373428345)
[2025-02-13 18:52:09,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:09,931][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 1.2897247076034546, acc: 0.7258883118629456)
[2025-02-13 18:52:10,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:10,357][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 1.4874656200408936, acc: 0.7015706896781921)
[2025-02-13 18:52:10,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:10,823][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.7994905114173889, acc: 0.8172042965888977)
[2025-02-13 18:52:11,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:11,315][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.6644353270530701, acc: 0.8516746163368225)
[2025-02-13 18:52:11,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:11,704][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 1.1050070524215698, acc: 0.7845304012298584)
[2025-02-13 18:52:11,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:12,083][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 1.1396653652191162, acc: 0.7385621070861816)
[2025-02-13 18:52:12,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:12,469][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 1.0124626159667969, acc: 0.8032786846160889)
[2025-02-13 18:52:12,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:12,843][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.7027908563613892, acc: 0.8374384045600891)
[2025-02-13 18:52:12,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:13,234][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.8241589069366455, acc: 0.820105791091919)
[2025-02-13 18:52:13,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:13,661][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.7372870445251465, acc: 0.8362573385238647)
[2025-02-13 18:52:13,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:14,088][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.8210300803184509, acc: 0.8092485666275024)
[2025-02-13 18:52:14,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:14,473][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.595937967300415, acc: 0.8559321761131287)
[2025-02-13 18:52:14,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:14,872][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.2026684433221817, acc: 0.9459459185600281)
[2025-02-13 18:52:15,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:15,237][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.3470255434513092, acc: 0.9135802388191223)
[2025-02-13 18:52:15,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:15,655][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.5354194045066833, acc: 0.8633093237876892)
[2025-02-13 18:52:15,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:16,024][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.5176241993904114, acc: 0.9090909361839294)
[2025-02-13 18:52:16,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:16,462][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.4831729531288147, acc: 0.8899999856948853)
[2025-02-13 18:52:16,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:16,853][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.33243855834007263, acc: 0.9473684430122375)
[2025-02-13 18:52:17,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:17,299][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 2.9001758098602295, acc: 0.5722891688346863)
[2025-02-13 18:52:17,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:17,675][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 5.467520236968994, acc: 0.26363635063171387)
[2025-02-13 18:52:17,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:18,082][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 4.621039390563965, acc: 0.3671875)
[2025-02-13 18:52:18,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:18,473][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 3.495572328567505, acc: 0.5178571343421936)
[2025-02-13 18:52:18,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:18,885][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 3.7073886394500732, acc: 0.4554455578327179)
[2025-02-13 18:52:19,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:19,302][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 1.3518116474151611, acc: 0.6899224519729614)
[2025-02-13 18:52:19,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:19,668][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 1.3403635025024414, acc: 0.7200000286102295)
[2025-02-13 18:52:19,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:20,043][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 2.4282491207122803, acc: 0.6052631735801697)
[2025-02-13 18:52:20,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:20,453][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 1.327721357345581, acc: 0.7218543291091919)
[2025-02-13 18:52:20,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:20,845][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 1.4500247240066528, acc: 0.693989098072052)
[2025-02-13 18:52:20,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:21,208][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 1.643052339553833, acc: 0.658682644367218)
[2025-02-13 18:52:21,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:21,558][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 1.174690842628479, acc: 0.7792207598686218)
[2025-02-13 18:52:21,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:21,912][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.9122105836868286, acc: 0.7664233446121216)
[2025-02-13 18:52:22,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:22,322][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.7253128886222839, acc: 0.8255033493041992)
[2025-02-13 18:52:22,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:22,754][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.5766994953155518, acc: 0.8689655065536499)
[2025-02-13 18:52:22,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:23,214][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.7327085733413696, acc: 0.8385093212127686)
[2025-02-13 18:52:23,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:23,668][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.6870710253715515, acc: 0.8630136847496033)
[2025-02-13 18:52:23,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:24,051][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.7047881484031677, acc: 0.8203125)
[2025-02-13 18:52:24,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:24,447][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.4880821704864502, acc: 0.8775510191917419)
[2025-02-13 18:52:24,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:24,856][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.7708748579025269, acc: 0.8358209133148193)
[2025-02-13 18:52:25,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:25,302][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.8156408071517944, acc: 0.8496732115745544)
[2025-02-13 18:52:25,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:25,765][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 1.1728014945983887, acc: 0.7621951103210449)
[2025-02-13 18:52:25,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:26,182][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.8656017184257507, acc: 0.8196721076965332)
[2025-02-13 18:52:26,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:26,606][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 1.23762845993042, acc: 0.7611940503120422)
[2025-02-13 18:52:26,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:27,018][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.8595094084739685, acc: 0.7980769276618958)
[2025-02-13 18:52:27,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:27,474][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.5791751742362976, acc: 0.895348846912384)
[2025-02-13 18:52:27,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:27,910][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.9063098430633545, acc: 0.7784430980682373)
[2025-02-13 18:52:28,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:28,338][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 1.1722482442855835, acc: 0.7735849022865295)
[2025-02-13 18:52:28,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:28,709][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.9404245018959045, acc: 0.7578125)
[2025-02-13 18:52:28,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:29,076][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 1.225926160812378, acc: 0.7653631567955017)
[2025-02-13 18:52:29,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:29,471][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.6793047785758972, acc: 0.837837815284729)
[2025-02-13 18:52:29,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:29,909][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.7096331715583801, acc: 0.8407643437385559)
[2025-02-13 18:52:30,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:30,286][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 1.2729642391204834, acc: 0.7573529481887817)
[2025-02-13 18:52:30,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:30,661][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.7797853946685791, acc: 0.8299319744110107)
[2025-02-13 18:52:30,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:31,025][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.4829502999782562, acc: 0.8865247964859009)
[2025-02-13 18:52:31,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:31,387][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.6372147798538208, acc: 0.8709677457809448)
[2025-02-13 18:52:31,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:31,771][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.8935532569885254, acc: 0.8106508851051331)
[2025-02-13 18:52:31,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:32,131][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 1.0795809030532837, acc: 0.7798742055892944)
[2025-02-13 18:52:32,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:32,484][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 1.115307331085205, acc: 0.7450000047683716)
[2025-02-13 18:52:32,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:32,835][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.6403391361236572, acc: 0.8472222089767456)
[2025-02-13 18:52:32,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:33,196][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.6593737006187439, acc: 0.8503401279449463)
[2025-02-13 18:52:33,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:33,597][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 1.2150596380233765, acc: 0.7357142567634583)
[2025-02-13 18:52:33,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:34,041][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 1.3365674018859863, acc: 0.7374301552772522)
[2025-02-13 18:52:34,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:34,471][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 1.0288556814193726, acc: 0.784140944480896)
[2025-02-13 18:52:34,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:34,867][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.8376590609550476, acc: 0.8445945978164673)
[2025-02-13 18:52:35,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:35,275][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 0.9941600561141968, acc: 0.8264462947845459)
[2025-02-13 18:52:35,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:35,656][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 1.1531884670257568, acc: 0.7333333492279053)
[2025-02-13 18:52:35,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:36,029][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.9318845868110657, acc: 0.7722772359848022)
[2025-02-13 18:52:36,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:36,427][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.924148678779602, acc: 0.7857142686843872)
[2025-02-13 18:52:36,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:36,840][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.7160788774490356, acc: 0.8370786309242249)
[2025-02-13 18:52:37,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:37,274][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.6472188830375671, acc: 0.8385416865348816)
[2025-02-13 18:52:37,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:37,681][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.41825631260871887, acc: 0.9197860956192017)
[2025-02-13 18:52:37,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:38,085][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.45569396018981934, acc: 0.8684210777282715)
[2025-02-13 18:52:38,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:38,502][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.47543638944625854, acc: 0.8782608509063721)
[2025-02-13 18:52:38,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:38,870][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.5869627594947815, acc: 0.8500000238418579)
[2025-02-13 18:52:39,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:39,278][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.6475427746772766, acc: 0.8612716794013977)
[2025-02-13 18:52:39,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:39,640][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.9092125296592712, acc: 0.7675675749778748)
[2025-02-13 18:52:39,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:39,990][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 1.3460803031921387, acc: 0.7318435907363892)
[2025-02-13 18:52:40,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:40,349][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 1.2289066314697266, acc: 0.7573964595794678)
[2025-02-13 18:52:40,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:40,802][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.7158864736557007, acc: 0.8571428656578064)
[2025-02-13 18:52:40,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:41,191][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.9580804705619812, acc: 0.7872340679168701)
[2025-02-13 18:52:41,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:41,632][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 1.0549288988113403, acc: 0.772455096244812)
[2025-02-13 18:52:41,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:42,049][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 1.1198253631591797, acc: 0.7257142663002014)
[2025-02-13 18:52:42,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:42,467][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.8176780343055725, acc: 0.7900552749633789)
[2025-02-13 18:52:42,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:42,857][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.7784075736999512, acc: 0.7719298005104065)
[2025-02-13 18:52:43,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:43,274][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.8470577597618103, acc: 0.811188817024231)
[2025-02-13 18:52:43,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:43,666][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.6528781652450562, acc: 0.8613138794898987)
[2025-02-13 18:52:43,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:44,060][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.7692620754241943, acc: 0.8169013857841492)
[2025-02-13 18:52:44,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:44,470][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.8438536524772644, acc: 0.8048780560493469)
[2025-02-13 18:52:44,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:44,879][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.49027150869369507, acc: 0.8675496578216553)
[2025-02-13 18:52:45,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:45,260][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.6145948171615601, acc: 0.8152866363525391)
[2025-02-13 18:52:45,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:45,632][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 1.0074269771575928, acc: 0.7862595319747925)
[2025-02-13 18:52:45,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:46,006][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.808284342288971, acc: 0.801886796951294)
[2025-02-13 18:52:46,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:46,425][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.699144184589386, acc: 0.8543689250946045)
[2025-02-13 18:52:46,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:46,845][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.817905068397522, acc: 0.8059701323509216)
[2025-02-13 18:52:47,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:47,289][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 1.0701799392700195, acc: 0.7785235047340393)
[2025-02-13 18:52:47,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:47,707][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 1.3845343589782715, acc: 0.7103448510169983)
[2025-02-13 18:52:47,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:48,071][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.5166013836860657, acc: 0.8992805480957031)
[2025-02-13 18:52:48,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:48,512][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.7563040852546692, acc: 0.8278688788414001)
[2025-02-13 18:52:48,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:48,926][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.6231201887130737, acc: 0.8623188138008118)
[2025-02-13 18:52:49,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:49,305][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.9327262043952942, acc: 0.7777777910232544)
[2025-02-13 18:52:49,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:49,710][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.755972146987915, acc: 0.8455284833908081)
[2025-02-13 18:52:49,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:50,093][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.7786350846290588, acc: 0.8152173757553101)
[2025-02-13 18:52:50,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:50,507][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.7335624694824219, acc: 0.8211920261383057)
[2025-02-13 18:52:50,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:50,894][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.6022124290466309, acc: 0.8705036044120789)
[2025-02-13 18:52:51,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:51,237][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.5036121606826782, acc: 0.8992805480957031)
[2025-02-13 18:52:51,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:51,599][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.49797940254211426, acc: 0.8640000224113464)
[2025-02-13 18:52:51,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:51,968][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.5801449418067932, acc: 0.8684210777282715)
[2025-02-13 18:52:52,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:52,364][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.5061379671096802, acc: 0.8782051205635071)
[2025-02-13 18:52:52,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:52,748][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 1.294772982597351, acc: 0.7567567825317383)
[2025-02-13 18:52:52,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:53,212][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.6058393716812134, acc: 0.9024389982223511)
[2025-02-13 18:52:53,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:53,627][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.6054975986480713, acc: 0.8790322542190552)
[2025-02-13 18:52:53,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:54,074][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.6939603090286255, acc: 0.8484848737716675)
[2025-02-13 18:52:54,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:54,502][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.7277365326881409, acc: 0.8571428656578064)
[2025-02-13 18:52:54,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:54,983][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.6583605408668518, acc: 0.8148148059844971)
[2025-02-13 18:52:55,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:55,454][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.33266299962997437, acc: 0.9105691313743591)
[2025-02-13 18:52:55,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:55,867][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.4420756697654724, acc: 0.895061731338501)
[2025-02-13 18:52:56,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:56,253][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.25042909383773804, acc: 0.9647058844566345)
[2025-02-13 18:52:56,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:56,621][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.5573073625564575, acc: 0.8764045238494873)
[2025-02-13 18:52:56,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:56,994][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.40879034996032715, acc: 0.9010416865348816)
[2025-02-13 18:52:57,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:57,401][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.5630731582641602, acc: 0.8723404407501221)
[2025-02-13 18:52:57,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:57,814][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.7502787709236145, acc: 0.8484848737716675)
[2025-02-13 18:52:58,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:58,256][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.7086264491081238, acc: 0.8273809552192688)
[2025-02-13 18:52:58,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:58,632][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.723843514919281, acc: 0.856249988079071)
[2025-02-13 18:52:58,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:59,024][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.5334339737892151, acc: 0.9055555462837219)
[2025-02-13 18:52:59,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:59,396][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.6491653919219971, acc: 0.826815664768219)
[2025-02-13 18:52:59,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:59,826][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.5802538394927979, acc: 0.8776595592498779)
[2025-02-13 18:52:59,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:00,206][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.5815573334693909, acc: 0.859649121761322)
[2025-02-13 18:53:00,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:00,610][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.48306748270988464, acc: 0.8832487463951111)
[2025-02-13 18:53:00,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:01,079][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.5119745135307312, acc: 0.8917526006698608)
[2025-02-13 18:53:01,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:01,507][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.8119924068450928, acc: 0.837837815284729)
[2025-02-13 18:53:01,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:01,901][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.7688122391700745, acc: 0.8229166865348816)
[2025-02-13 18:53:02,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:02,293][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.7197362780570984, acc: 0.8108108043670654)
[2025-02-13 18:53:02,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:02,761][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.7503896355628967, acc: 0.8240000009536743)
[2025-02-13 18:53:02,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:03,174][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.6730103492736816, acc: 0.8559321761131287)
[2025-02-13 18:53:03,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:03,552][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 1.04072904586792, acc: 0.797468364238739)
[2025-02-13 18:53:03,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:03,930][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.6704809069633484, acc: 0.847328245639801)
[2025-02-13 18:53:04,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:04,318][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.5764148235321045, acc: 0.8881118893623352)
[2025-02-13 18:53:04,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:04,691][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.7864733934402466, acc: 0.8370370268821716)
[2025-02-13 18:53:04,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:05,056][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.6158493161201477, acc: 0.8402777910232544)
[2025-02-13 18:53:05,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:05,423][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.5875616669654846, acc: 0.8657718300819397)
[2025-02-13 18:53:05,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:05,814][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.6681053042411804, acc: 0.8661417365074158)
[2025-02-13 18:53:05,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:06,210][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.5963464975357056, acc: 0.8449612259864807)
[2025-02-13 18:53:06,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:06,583][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.5607593655586243, acc: 0.8701298832893372)
[2025-02-13 18:53:06,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:06,985][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.8204681277275085, acc: 0.8163265585899353)
[2025-02-13 18:53:07,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:07,340][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.9617876410484314, acc: 0.790123462677002)
[2025-02-13 18:53:07,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:07,711][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.8600752353668213, acc: 0.7964601516723633)
[2025-02-13 18:53:07,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:08,098][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.7649468183517456, acc: 0.805084764957428)
[2025-02-13 18:53:08,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:08,512][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.9474836587905884, acc: 0.7820512652397156)
[2025-02-13 18:53:08,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:08,913][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.41237813234329224, acc: 0.9090909361839294)
[2025-02-13 18:53:09,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:09,296][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.4225768744945526, acc: 0.9008264541625977)
[2025-02-13 18:53:09,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:09,698][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.5783656239509583, acc: 0.8769230842590332)
[2025-02-13 18:53:09,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:10,071][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.2618742883205414, acc: 0.9416058659553528)
[2025-02-13 18:53:10,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:10,457][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.21523964405059814, acc: 0.9275362491607666)
[2025-02-13 18:53:10,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:10,882][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.4732876718044281, acc: 0.8741258978843689)
[2025-02-13 18:53:11,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:11,243][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.3598865866661072, acc: 0.9009901285171509)
[2025-02-13 18:53:11,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:11,609][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.5284061431884766, acc: 0.8648648858070374)
[2025-02-13 18:53:11,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:11,980][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.3236311376094818, acc: 0.9306930899620056)
[2025-02-13 18:53:12,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:12,355][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.5409392714500427, acc: 0.8984375)
[2025-02-13 18:53:12,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:12,733][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.23688852787017822, acc: 0.95652174949646)
[2025-02-13 18:53:12,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:13,129][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.6253372430801392, acc: 0.8691588640213013)
[2025-02-13 18:53:13,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:13,494][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.34174469113349915, acc: 0.913385808467865)
[2025-02-13 18:53:13,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:13,887][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.4091430902481079, acc: 0.8938053250312805)
[2025-02-13 18:53:14,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:14,303][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.40544623136520386, acc: 0.8914728760719299)
[2025-02-13 18:53:14,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:14,682][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.3463086783885956, acc: 0.902255654335022)
[2025-02-13 18:53:14,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:15,053][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.27246707677841187, acc: 0.9508196711540222)
[2025-02-13 18:53:15,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:15,434][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.3299570381641388, acc: 0.9279999732971191)
[2025-02-13 18:53:15,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:15,818][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.40476036071777344, acc: 0.8962963223457336)
[2025-02-13 18:53:15,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:16,221][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.6983152627944946, acc: 0.8500000238418579)
[2025-02-13 18:53:16,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:16,584][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.48337242007255554, acc: 0.8828828930854797)
[2025-02-13 18:53:16,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:16,972][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.17270036041736603, acc: 0.9620253443717957)
[2025-02-13 18:53:17,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:17,374][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.5666846036911011, acc: 0.8548387289047241)
[2025-02-13 18:53:17,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:17,759][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.7384957671165466, acc: 0.8322981595993042)
[2025-02-13 18:53:17,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:18,165][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.6179811358451843, acc: 0.863095223903656)
[2025-02-13 18:53:18,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:18,559][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.6118836402893066, acc: 0.8678160905838013)
[2025-02-13 18:53:18,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:18,954][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.6776738166809082, acc: 0.8235294222831726)
[2025-02-13 18:53:19,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:19,339][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 1.0117974281311035, acc: 0.7551020383834839)
[2025-02-13 18:53:19,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:19,722][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.9450506567955017, acc: 0.8287037014961243)
[2025-02-13 18:53:19,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:20,117][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.5977489948272705, acc: 0.8806818127632141)
[2025-02-13 18:53:20,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:20,496][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.5891013145446777, acc: 0.8595505356788635)
[2025-02-13 18:53:20,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:20,871][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.5581627488136292, acc: 0.8787878751754761)
[2025-02-13 18:53:21,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:21,282][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.5082125663757324, acc: 0.8812500238418579)
[2025-02-13 18:53:21,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:21,661][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.4779382646083832, acc: 0.9004974961280823)
[2025-02-13 18:53:21,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:22,073][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.5173633694648743, acc: 0.890350878238678)
[2025-02-13 18:53:22,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:22,465][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.33074018359184265, acc: 0.90625)
[2025-02-13 18:53:22,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:22,851][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.2112232744693756, acc: 0.9353233575820923)
[2025-02-13 18:53:23,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:23,235][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.5765556693077087, acc: 0.8387096524238586)
[2025-02-13 18:53:23,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:23,650][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.5767192840576172, acc: 0.8516483306884766)
[2025-02-13 18:53:23,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:24,015][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.4136192202568054, acc: 0.912162184715271)
[2025-02-13 18:53:24,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:24,388][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.6924837231636047, acc: 0.8652849793434143)
[2025-02-13 18:53:24,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:24,758][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.6285609006881714, acc: 0.8198757767677307)
[2025-02-13 18:53:24,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:25,134][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.8018808960914612, acc: 0.8376963138580322)
[2025-02-13 18:53:25,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:25,497][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.49971336126327515, acc: 0.8716577291488647)
[2025-02-13 18:53:25,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:25,876][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.5335501432418823, acc: 0.8888888955116272)
[2025-02-13 18:53:26,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:26,269][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.47053542733192444, acc: 0.8775510191917419)
[2025-02-13 18:53:26,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:26,667][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.44378533959388733, acc: 0.893081784248352)
[2025-02-13 18:53:26,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:27,121][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.8064233064651489, acc: 0.8230769038200378)
[2025-02-13 18:53:27,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:27,483][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.5254474878311157, acc: 0.8759689927101135)
[2025-02-13 18:53:27,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:27,839][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.47422611713409424, acc: 0.8652482032775879)
[2025-02-13 18:53:28,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:28,273][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.3327641487121582, acc: 0.9342105388641357)
[2025-02-13 18:53:28,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:28,668][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.7504225373268127, acc: 0.8125)
[2025-02-13 18:53:28,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:29,055][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.4536759853363037, acc: 0.8875739574432373)
[2025-02-13 18:53:29,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:29,468][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.5305808782577515, acc: 0.8875739574432373)
[2025-02-13 18:53:29,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:29,882][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.4246882498264313, acc: 0.9105263352394104)
[2025-02-13 18:53:30,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:30,295][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.48073747754096985, acc: 0.8804348111152649)
[2025-02-13 18:53:30,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:30,702][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.4326886236667633, acc: 0.8947368264198303)
[2025-02-13 18:53:30,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:31,101][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.5333510041236877, acc: 0.8895705342292786)
[2025-02-13 18:53:31,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:31,474][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.4822082221508026, acc: 0.8994975090026855)
[2025-02-13 18:53:31,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:31,909][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.531667172908783, acc: 0.8934911489486694)
[2025-02-13 18:53:32,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:32,343][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.27648651599884033, acc: 0.9281437397003174)
[2025-02-13 18:53:32,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:32,773][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.39948800206184387, acc: 0.9069767594337463)
[2025-02-13 18:53:32,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:33,173][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.4618317782878876, acc: 0.8636363744735718)
[2025-02-13 18:53:33,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:33,602][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.5212429165840149, acc: 0.8999999761581421)
[2025-02-13 18:53:33,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:33,970][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.3407103419303894, acc: 0.915032684803009)
[2025-02-13 18:53:34,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:34,317][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.3788834512233734, acc: 0.8895705342292786)
[2025-02-13 18:53:34,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:34,696][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.4187767803668976, acc: 0.905063271522522)
[2025-02-13 18:53:34,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:35,078][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.4367346465587616, acc: 0.8852459192276001)
[2025-02-13 18:53:35,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:35,441][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.5473805665969849, acc: 0.893203854560852)
[2025-02-13 18:53:35,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:35,832][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.3678884506225586, acc: 0.905063271522522)
[2025-02-13 18:53:36,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:36,231][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.26603469252586365, acc: 0.9363057613372803)
[2025-02-13 18:53:36,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:36,631][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.29835933446884155, acc: 0.9278350472450256)
[2025-02-13 18:53:36,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:37,021][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.17690803110599518, acc: 0.9467455744743347)
[2025-02-13 18:53:37,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:37,434][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.49554476141929626, acc: 0.8895705342292786)
[2025-02-13 18:53:37,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:37,846][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.48642241954803467, acc: 0.8860759735107422)
[2025-02-13 18:53:37,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:38,224][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.3759356737136841, acc: 0.9130434989929199)
[2025-02-13 18:53:38,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:38,592][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.5985998511314392, acc: 0.8616352081298828)
[2025-02-13 18:53:38,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:38,979][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.3208533227443695, acc: 0.9281045794487)
[2025-02-13 18:53:39,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:39,344][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.32929062843322754, acc: 0.912162184715271)
[2025-02-13 18:53:39,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:39,750][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.6129036545753479, acc: 0.8723404407501221)
[2025-02-13 18:53:39,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:40,159][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.5942814350128174, acc: 0.8444444537162781)
[2025-02-13 18:53:40,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:40,572][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.5408604145050049, acc: 0.8715083599090576)
[2025-02-13 18:53:40,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:40,978][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.5645188689231873, acc: 0.8474576473236084)
[2025-02-13 18:53:41,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:41,379][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.18475304543972015, acc: 0.9509803652763367)
[2025-02-13 18:53:41,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:41,816][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.7240212559700012, acc: 0.8041236996650696)
[2025-02-13 18:53:41,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:42,199][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.44896847009658813, acc: 0.9105263352394104)
[2025-02-13 18:53:42,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:42,589][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.3260273337364197, acc: 0.9128205180168152)
[2025-02-13 18:53:42,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:42,946][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.37912073731422424, acc: 0.9069767594337463)
[2025-02-13 18:53:43,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:43,331][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.33709844946861267, acc: 0.9281768202781677)
[2025-02-13 18:53:43,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:43,712][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.2760612368583679, acc: 0.9215686321258545)
[2025-02-13 18:53:43,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:44,059][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.4267629086971283, acc: 0.9099099040031433)
[2025-02-13 18:53:44,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:44,521][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.5425376296043396, acc: 0.8520709872245789)
[2025-02-13 18:53:44,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:44,905][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.37039488554000854, acc: 0.9090909361839294)
[2025-02-13 18:53:45,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:45,285][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.4945102035999298, acc: 0.8971428275108337)
[2025-02-13 18:53:45,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:45,673][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.39707934856414795, acc: 0.8918918967247009)
[2025-02-13 18:53:45,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:46,045][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.4282582998275757, acc: 0.9066666960716248)
[2025-02-13 18:53:46,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:46,444][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.4054214060306549, acc: 0.8779069781303406)
[2025-02-13 18:53:46,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:46,825][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.4405688941478729, acc: 0.8877005577087402)
[2025-02-13 18:53:46,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:47,213][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.3356594741344452, acc: 0.9083969593048096)
[2025-02-13 18:53:47,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:47,597][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.4477035105228424, acc: 0.9042553305625916)
[2025-02-13 18:53:47,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:47,992][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.4220294952392578, acc: 0.8879310488700867)
[2025-02-13 18:53:48,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:48,420][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.6597163081169128, acc: 0.8804348111152649)
[2025-02-13 18:53:48,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:48,792][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.48950064182281494, acc: 0.8989899158477783)
[2025-02-13 18:53:48,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:49,180][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.3590879440307617, acc: 0.9240506291389465)
[2025-02-13 18:53:49,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:49,552][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.7559072971343994, acc: 0.8291139006614685)
[2025-02-13 18:53:49,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:49,951][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.44099706411361694, acc: 0.8970588445663452)
[2025-02-13 18:53:50,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:50,323][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.3231601119041443, acc: 0.909547746181488)
[2025-02-13 18:53:50,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:50,738][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.42292511463165283, acc: 0.903930127620697)
[2025-02-13 18:53:50,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:51,158][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.47693178057670593, acc: 0.8870967626571655)
[2025-02-13 18:53:51,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:51,622][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.675896406173706, acc: 0.819767415523529)
[2025-02-13 18:53:51,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:52,045][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.7733315229415894, acc: 0.8432432413101196)
[2025-02-13 18:53:52,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:52,407][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.921225368976593, acc: 0.8280254602432251)
[2025-02-13 18:53:52,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:52,782][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.6995388269424438, acc: 0.8379888534545898)
[2025-02-13 18:53:52,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:53,143][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.5652292370796204, acc: 0.8999999761581421)
[2025-02-13 18:53:53,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:53,543][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.601707935333252, acc: 0.8727272748947144)
[2025-02-13 18:53:53,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:53,935][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.4837024211883545, acc: 0.8873873949050903)
[2025-02-13 18:53:54,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:54,301][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.4888972342014313, acc: 0.9058296084403992)
[2025-02-13 18:53:54,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:54,670][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.6579817533493042, acc: 0.8272251486778259)
[2025-02-13 18:53:54,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:55,048][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.5805597305297852, acc: 0.8554913401603699)
[2025-02-13 18:53:55,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:55,418][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.2880336344242096, acc: 0.9333333373069763)
[2025-02-13 18:53:55,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:55,810][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.3664339780807495, acc: 0.9230769276618958)
[2025-02-13 18:53:55,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:56,223][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.6265657544136047, acc: 0.8681318759918213)
[2025-02-13 18:53:56,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:56,617][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.3968934118747711, acc: 0.9033816456794739)
[2025-02-13 18:53:56,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:57,030][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.3429934084415436, acc: 0.9306930899620056)
[2025-02-13 18:53:57,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:57,437][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.44324854016304016, acc: 0.8928571343421936)
[2025-02-13 18:53:57,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:57,848][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.4315564036369324, acc: 0.9182389974594116)
[2025-02-13 18:53:58,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:58,284][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.3159210681915283, acc: 0.9230769276618958)
[2025-02-13 18:53:58,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:58,702][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.19512656331062317, acc: 0.9438202381134033)
[2025-02-13 18:53:58,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:59,116][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.42619526386260986, acc: 0.895061731338501)
[2025-02-13 18:53:59,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:59,563][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.16571570932865143, acc: 0.9602272510528564)
[2025-02-13 18:53:59,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:59,972][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.24468302726745605, acc: 0.929411768913269)
[2025-02-13 18:54:00,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:00,371][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.34639453887939453, acc: 0.9182389974594116)
[2025-02-13 18:54:00,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:00,808][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.42040443420410156, acc: 0.910179615020752)
[2025-02-13 18:54:00,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:01,231][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.5444650053977966, acc: 0.8684210777282715)
[2025-02-13 18:54:01,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:01,614][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.3155502378940582, acc: 0.9411764740943909)
[2025-02-13 18:54:01,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:01,991][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.3010939657688141, acc: 0.9112426042556763)
[2025-02-13 18:54:02,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:02,363][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.39152005314826965, acc: 0.9242424368858337)
[2025-02-13 18:54:02,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:02,781][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.5148576498031616, acc: 0.8829787373542786)
[2025-02-13 18:54:02,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:03,193][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.32102692127227783, acc: 0.9320987462997437)
[2025-02-13 18:54:03,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:03,603][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.36086493730545044, acc: 0.8999999761581421)
[2025-02-13 18:54:03,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:03,961][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.310928612947464, acc: 0.9305555820465088)
[2025-02-13 18:54:04,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:04,325][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.3989722728729248, acc: 0.9295774698257446)
[2025-02-13 18:54:04,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:04,681][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.1790303885936737, acc: 0.9527559280395508)
[2025-02-13 18:54:04,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:05,076][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.5536051988601685, acc: 0.8563218116760254)
[2025-02-13 18:54:05,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:05,458][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.3823382556438446, acc: 0.892307698726654)
[2025-02-13 18:54:05,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:05,846][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.21727551519870758, acc: 0.9383561611175537)
[2025-02-13 18:54:06,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:06,240][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.23180438578128815, acc: 0.961240291595459)
[2025-02-13 18:54:06,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:06,610][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.1821126490831375, acc: 0.9591836929321289)
[2025-02-13 18:54:06,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:06,983][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.10966454446315765, acc: 0.9767441749572754)
[2025-02-13 18:54:07,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:07,360][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.4956953823566437, acc: 0.8679245114326477)
[2025-02-13 18:54:07,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:07,770][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.202890545129776, acc: 0.9526627063751221)
[2025-02-13 18:54:07,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:08,167][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.25280073285102844, acc: 0.9263803958892822)
[2025-02-13 18:54:08,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:08,545][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.5554531216621399, acc: 0.8730158805847168)
[2025-02-13 18:54:08,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:08,922][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.5139971971511841, acc: 0.8617021441459656)
[2025-02-13 18:54:09,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:09,312][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.6039745211601257, acc: 0.8301886916160583)
[2025-02-13 18:54:09,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:09,690][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 1.0403164625167847, acc: 0.7881355881690979)
[2025-02-13 18:54:09,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:10,062][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.9481938481330872, acc: 0.7921348214149475)
[2025-02-13 18:54:10,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:10,418][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.5702006220817566, acc: 0.8723404407501221)
[2025-02-13 18:54:10,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:10,813][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.703788697719574, acc: 0.8396624326705933)
[2025-02-13 18:54:10,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:11,194][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.8881493806838989, acc: 0.800000011920929)
[2025-02-13 18:54:11,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:11,600][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.3644160032272339, acc: 0.9018405079841614)
[2025-02-13 18:54:11,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:11,964][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.7328878045082092, acc: 0.8413792848587036)
[2025-02-13 18:54:12,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:12,329][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.5206406712532043, acc: 0.8870967626571655)
[2025-02-13 18:54:12,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:12,718][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.6701772212982178, acc: 0.8253012299537659)
[2025-02-13 18:54:12,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:13,105][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.6330811977386475, acc: 0.8533333539962769)
[2025-02-13 18:54:13,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:13,484][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.492364764213562, acc: 0.8705882430076599)
[2025-02-13 18:54:13,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:13,895][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.5815476775169373, acc: 0.875)
[2025-02-13 18:54:14,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:14,318][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.9901999235153198, acc: 0.7910447716712952)
[2025-02-13 18:54:14,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:14,715][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.6898608207702637, acc: 0.8465909361839294)
[2025-02-13 18:54:14,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:15,115][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.47597500681877136, acc: 0.9050279259681702)
[2025-02-13 18:54:15,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:15,519][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.6974455714225769, acc: 0.8538011908531189)
[2025-02-13 18:54:15,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:15,903][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.6912090182304382, acc: 0.8677248954772949)
[2025-02-13 18:54:16,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:16,253][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.632904589176178, acc: 0.8733333349227905)
[2025-02-13 18:54:16,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:16,609][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.5136768817901611, acc: 0.8888888955116272)
[2025-02-13 18:54:16,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:16,970][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.6185029745101929, acc: 0.8108108043670654)
[2025-02-13 18:54:17,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:17,323][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.7039857506752014, acc: 0.8461538553237915)
[2025-02-13 18:54:17,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:17,703][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.6664640307426453, acc: 0.8533333539962769)
[2025-02-13 18:54:17,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:18,063][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.5649852752685547, acc: 0.8560606241226196)
[2025-02-13 18:54:18,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:18,515][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.7403106689453125, acc: 0.8484848737716675)
[2025-02-13 18:54:18,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:18,949][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.9567469954490662, acc: 0.8041958212852478)
[2025-02-13 18:54:19,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:19,367][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.7722261548042297, acc: 0.8187500238418579)
[2025-02-13 18:54:19,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:19,770][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.7370620965957642, acc: 0.8288770318031311)
[2025-02-13 18:54:19,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:20,144][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.9831591248512268, acc: 0.747826099395752)
[2025-02-13 18:54:20,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:20,499][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.654768705368042, acc: 0.8543046116828918)
[2025-02-13 18:54:20,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:20,958][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.7641435861587524, acc: 0.824999988079071)
[2025-02-13 18:54:21,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:21,349][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.4630238711833954, acc: 0.9068322777748108)
[2025-02-13 18:54:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:21,755][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.3772892355918884, acc: 0.9308176040649414)
[2025-02-13 18:54:21,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:22,134][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.37761038541793823, acc: 0.9230769276618958)
[2025-02-13 18:54:22,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:22,503][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.8079453110694885, acc: 0.8367347121238708)
[2025-02-13 18:54:22,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:22,869][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.8556914925575256, acc: 0.8322981595993042)
[2025-02-13 18:54:23,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:23,250][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.7991499900817871, acc: 0.8064516186714172)
[2025-02-13 18:54:23,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:23,641][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.5149998664855957, acc: 0.8827160596847534)
[2025-02-13 18:54:23,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:23,994][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.8210080862045288, acc: 0.8208954930305481)
[2025-02-13 18:54:24,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:24,369][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.7107293009757996, acc: 0.8633093237876892)
[2025-02-13 18:54:24,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:24,728][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.4800684452056885, acc: 0.8734177350997925)
[2025-02-13 18:54:24,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:25,078][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.622185230255127, acc: 0.8541666865348816)
[2025-02-13 18:54:25,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:25,453][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.18750634789466858, acc: 0.9612902998924255)
[2025-02-13 18:54:25,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:25,864][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.4681604504585266, acc: 0.8875739574432373)
[2025-02-13 18:54:26,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:26,244][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.23146431148052216, acc: 0.95333331823349)
[2025-02-13 18:54:26,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:26,618][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.48827871680259705, acc: 0.8925619721412659)
[2025-02-13 18:54:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:26,962][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 1.104923963546753, acc: 0.7684210538864136)
[2025-02-13 18:54:27,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:27,342][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.4277072548866272, acc: 0.9076923131942749)
[2025-02-13 18:54:27,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:27,723][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.4309709370136261, acc: 0.8926174640655518)
[2025-02-13 18:54:27,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:28,115][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.36206766963005066, acc: 0.9058823585510254)
[2025-02-13 18:54:28,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:28,479][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.49489468336105347, acc: 0.8662790656089783)
[2025-02-13 18:54:28,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:28,881][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.47331300377845764, acc: 0.8786126971244812)
[2025-02-13 18:54:29,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:29,277][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.2250986397266388, acc: 0.9285714030265808)
[2025-02-13 18:54:29,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:29,626][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.4647029936313629, acc: 0.8806818127632141)
[2025-02-13 18:54:29,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:30,015][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.3881129324436188, acc: 0.9378882050514221)
[2025-02-13 18:54:30,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:30,393][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.18438903987407684, acc: 0.9404761791229248)
[2025-02-13 18:54:30,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:30,803][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.4612416923046112, acc: 0.9090909361839294)
[2025-02-13 18:54:30,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:31,199][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.3596452474594116, acc: 0.9180327653884888)
[2025-02-13 18:54:31,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:31,594][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.3370038568973541, acc: 0.9411764740943909)
[2025-02-13 18:54:31,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:32,006][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.37232598662376404, acc: 0.8918918967247009)
[2025-02-13 18:54:32,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:32,377][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.6086068153381348, acc: 0.8780487775802612)
[2025-02-13 18:54:32,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:32,765][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.4845310151576996, acc: 0.8952381014823914)
[2025-02-13 18:54:32,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:33,158][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.7074047923088074, acc: 0.8396946787834167)
[2025-02-13 18:54:33,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:33,560][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.5849385261535645, acc: 0.8791946172714233)
[2025-02-13 18:54:33,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:33,924][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.8052019476890564, acc: 0.8425925970077515)
[2025-02-13 18:54:34,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:34,319][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.44147658348083496, acc: 0.898809552192688)
[2025-02-13 18:54:34,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:34,672][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.6267412304878235, acc: 0.8813559412956238)
[2025-02-13 18:54:34,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:35,056][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.7008405327796936, acc: 0.8623188138008118)
[2025-02-13 18:54:35,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:35,424][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.3705928921699524, acc: 0.9271523356437683)
[2025-02-13 18:54:35,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:35,810][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.3306303322315216, acc: 0.9304347634315491)
[2025-02-13 18:54:35,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:36,197][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.4453488290309906, acc: 0.8815789222717285)
[2025-02-13 18:54:36,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:36,571][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.42183130979537964, acc: 0.8888888955116272)
[2025-02-13 18:54:36,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:36,963][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.3770282566547394, acc: 0.898876428604126)
[2025-02-13 18:54:37,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:37,342][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.38509494066238403, acc: 0.9127907156944275)
[2025-02-13 18:54:37,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:37,688][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.29513880610466003, acc: 0.9435028433799744)
[2025-02-13 18:54:37,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:38,062][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.4307849705219269, acc: 0.89570552110672)
[2025-02-13 18:54:38,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:38,429][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.32877057790756226, acc: 0.9034090638160706)
[2025-02-13 18:54:38,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:38,795][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.3011068105697632, acc: 0.921875)
[2025-02-13 18:54:38,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:39,173][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.3291696012020111, acc: 0.9224137663841248)
[2025-02-13 18:54:39,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:39,584][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.543354868888855, acc: 0.8789808750152588)
[2025-02-13 18:54:39,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:39,948][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.3486977517604828, acc: 0.9153845906257629)
[2025-02-13 18:54:40,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:40,313][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.630221426486969, acc: 0.8698630332946777)
[2025-02-13 18:54:40,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:40,685][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.28437262773513794, acc: 0.8938053250312805)
[2025-02-13 18:54:40,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:41,067][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.6678290367126465, acc: 0.8571428656578064)
[2025-02-13 18:54:41,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:41,441][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.29591625928878784, acc: 0.9221556782722473)
[2025-02-13 18:54:41,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:41,832][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.4136371910572052, acc: 0.8832116723060608)
[2025-02-13 18:54:41,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:42,201][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.4216555058956146, acc: 0.8841463327407837)
[2025-02-13 18:54:42,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:42,566][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.516456663608551, acc: 0.849711000919342)
[2025-02-13 18:54:42,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:42,949][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.32556968927383423, acc: 0.9397590160369873)
[2025-02-13 18:54:43,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:43,340][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.4805600643157959, acc: 0.885496199131012)
[2025-02-13 18:54:43,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:43,694][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.4584195613861084, acc: 0.8584905862808228)
[2025-02-13 18:54:43,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:44,077][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.6362109780311584, acc: 0.8333333134651184)
[2025-02-13 18:54:44,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:44,417][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.828396201133728, acc: 0.8199999928474426)
[2025-02-13 18:54:44,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:44,792][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.8717114329338074, acc: 0.7763158082962036)
[2025-02-13 18:54:44,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:45,170][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.6013321876525879, acc: 0.8428571224212646)
[2025-02-13 18:54:45,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:45,572][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.675937294960022, acc: 0.8421052694320679)
[2025-02-13 18:54:45,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:45,945][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 0.9380962252616882, acc: 0.8055555820465088)
[2025-02-13 18:54:46,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:46,349][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.5645514130592346, acc: 0.8543046116828918)
[2025-02-13 18:54:46,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:46,757][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.4555306136608124, acc: 0.8881579041481018)
[2025-02-13 18:54:46,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:47,173][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 1.0476890802383423, acc: 0.7785714268684387)
[2025-02-13 18:54:47,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:47,585][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.6246393918991089, acc: 0.8188976645469666)
[2025-02-13 18:54:47,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:47,971][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.6494056582450867, acc: 0.875)
[2025-02-13 18:54:48,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:48,352][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.5554131269454956, acc: 0.8435373902320862)
[2025-02-13 18:54:48,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:48,747][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.43071502447128296, acc: 0.9157894849777222)
[2025-02-13 18:54:48,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:49,201][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.770734429359436, acc: 0.8407643437385559)
[2025-02-13 18:54:49,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:49,598][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.8035555481910706, acc: 0.8571428656578064)
[2025-02-13 18:54:49,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:49,992][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.3458332419395447, acc: 0.918749988079071)
[2025-02-13 18:54:50,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:50,396][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.643258810043335, acc: 0.8652482032775879)
[2025-02-13 18:54:50,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:50,817][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.6152824759483337, acc: 0.8653846383094788)
[2025-02-13 18:54:50,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:51,219][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.6431461572647095, acc: 0.8590604066848755)
[2025-02-13 18:54:51,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:51,612][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.274147629737854, acc: 0.9186992049217224)
[2025-02-13 18:54:51,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:52,045][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.5783898830413818, acc: 0.856249988079071)
[2025-02-13 18:54:52,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:52,439][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.38091254234313965, acc: 0.9025974273681641)
[2025-02-13 18:54:52,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:52,822][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.7735201716423035, acc: 0.811188817024231)
[2025-02-13 18:54:52,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:53,247][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.746268093585968, acc: 0.8187134265899658)
[2025-02-13 18:54:53,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:53,612][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.45105159282684326, acc: 0.8976377844810486)
[2025-02-13 18:54:53,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:53,971][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.7960574626922607, acc: 0.792792797088623)
[2025-02-13 18:54:54,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:54,361][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.6499267816543579, acc: 0.8383233547210693)
[2025-02-13 18:54:54,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:54,735][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.5039032101631165, acc: 0.8736842274665833)
[2025-02-13 18:54:54,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:55,121][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.40669816732406616, acc: 0.8938547372817993)
[2025-02-13 18:54:55,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:55,498][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.6064287424087524, acc: 0.8656716346740723)
[2025-02-13 18:54:55,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:55,865][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.38255318999290466, acc: 0.8901098966598511)
[2025-02-13 18:54:55,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:56,223][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.40807127952575684, acc: 0.9028571248054504)
[2025-02-13 18:54:56,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:56,591][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.4998556971549988, acc: 0.8848484754562378)
[2025-02-13 18:54:56,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:56,939][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.3877943158149719, acc: 0.8934911489486694)
[2025-02-13 18:54:57,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:57,342][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.35663607716560364, acc: 0.9181286692619324)
[2025-02-13 18:54:57,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:57,697][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.3605717122554779, acc: 0.897849440574646)
[2025-02-13 18:54:57,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:58,060][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.3909881114959717, acc: 0.9090909361839294)
[2025-02-13 18:54:58,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:58,458][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.3907932639122009, acc: 0.9203979969024658)
[2025-02-13 18:54:58,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:58,812][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.5718331336975098, acc: 0.8763440847396851)
[2025-02-13 18:54:58,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:59,202][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.8027554154396057, acc: 0.8388888835906982)
[2025-02-13 18:54:59,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:59,582][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.30564209818840027, acc: 0.9539473652839661)
[2025-02-13 18:54:59,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:00,017][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.42054176330566406, acc: 0.875)
[2025-02-13 18:55:00,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:00,409][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.4741209149360657, acc: 0.8443113565444946)
[2025-02-13 18:55:00,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:00,814][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.33634689450263977, acc: 0.9398906826972961)
[2025-02-13 18:55:00,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:01,183][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.3256867229938507, acc: 0.9189189076423645)
[2025-02-13 18:55:01,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:01,646][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.27555814385414124, acc: 0.9255319237709045)
[2025-02-13 18:55:01,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:02,083][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.41426798701286316, acc: 0.90055251121521)
[2025-02-13 18:55:02,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:02,522][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.3340444564819336, acc: 0.912162184715271)
[2025-02-13 18:55:02,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:02,949][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.36341843008995056, acc: 0.9325153231620789)
[2025-02-13 18:55:03,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:03,335][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.4150919020175934, acc: 0.9139072895050049)
[2025-02-13 18:55:03,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:03,712][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.3010491132736206, acc: 0.9322034120559692)
[2025-02-13 18:55:03,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:04,105][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.26425182819366455, acc: 0.9602272510528564)
[2025-02-13 18:55:04,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:04,506][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.34819871187210083, acc: 0.9239766001701355)
[2025-02-13 18:55:04,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:04,939][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.5051652193069458, acc: 0.910179615020752)
[2025-02-13 18:55:05,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:05,355][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.40612366795539856, acc: 0.9086021780967712)
[2025-02-13 18:55:05,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:05,794][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.3304993510246277, acc: 0.9166666865348816)
[2025-02-13 18:55:05,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:06,203][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.5308336019515991, acc: 0.8826815485954285)
[2025-02-13 18:55:06,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:06,597][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.8013810515403748, acc: 0.8369565010070801)
[2025-02-13 18:55:06,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:07,032][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.3391043543815613, acc: 0.9301075339317322)
[2025-02-13 18:55:07,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:07,430][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.3880992829799652, acc: 0.8907103538513184)
[2025-02-13 18:55:07,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:07,862][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.393048495054245, acc: 0.8806818127632141)
[2025-02-13 18:55:08,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:08,258][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.356974333524704, acc: 0.9351351261138916)
[2025-02-13 18:55:08,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:08,683][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.46185407042503357, acc: 0.9153439402580261)
[2025-02-13 18:55:08,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:09,043][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.5287608504295349, acc: 0.8666666746139526)
[2025-02-13 18:55:09,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:09,468][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.4703134000301361, acc: 0.8983957171440125)
[2025-02-13 18:55:09,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:09,894][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.35313406586647034, acc: 0.9049999713897705)
[2025-02-13 18:55:10,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:10,376][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.41039952635765076, acc: 0.9137930870056152)
[2025-02-13 18:55:10,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:10,844][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.4318374991416931, acc: 0.9189189076423645)
[2025-02-13 18:55:11,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:11,252][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.5032211542129517, acc: 0.8820512890815735)
[2025-02-13 18:55:11,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:11,662][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.3948926627635956, acc: 0.8768472671508789)
[2025-02-13 18:55:11,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:12,089][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.390194296836853, acc: 0.9209039807319641)
[2025-02-13 18:55:12,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:12,489][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.45061907172203064, acc: 0.89673912525177)
[2025-02-13 18:55:12,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:12,962][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.37700939178466797, acc: 0.9242424368858337)
[2025-02-13 18:55:13,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:13,419][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.3213646709918976, acc: 0.9347826242446899)
[2025-02-13 18:55:13,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:13,906][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.28090423345565796, acc: 0.9263157844543457)
[2025-02-13 18:55:14,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:14,362][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.376427561044693, acc: 0.921658992767334)
[2025-02-13 18:55:14,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:14,827][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.31161922216415405, acc: 0.9282296895980835)
[2025-02-13 18:55:14,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:15,223][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.367842435836792, acc: 0.903743326663971)
[2025-02-13 18:55:15,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:15,641][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.3692621886730194, acc: 0.9098360538482666)
[2025-02-13 18:55:15,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:16,079][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.37001362442970276, acc: 0.9246575236320496)
[2025-02-13 18:55:16,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:16,521][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.43282729387283325, acc: 0.8799999952316284)
[2025-02-13 18:55:16,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:16,929][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.38025379180908203, acc: 0.8947368264198303)
[2025-02-13 18:55:17,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:17,298][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.4855211079120636, acc: 0.8992805480957031)
[2025-02-13 18:55:17,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:17,686][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.3213280141353607, acc: 0.9416058659553528)
[2025-02-13 18:55:17,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:18,104][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.3803445100784302, acc: 0.9125000238418579)
[2025-02-13 18:55:18,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:18,500][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.5183857679367065, acc: 0.8880000114440918)
[2025-02-13 18:55:18,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:18,893][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.351717084646225, acc: 0.9115646481513977)
[2025-02-13 18:55:19,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:19,261][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.377664178609848, acc: 0.9115646481513977)
[2025-02-13 18:55:19,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:19,753][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.5580853223800659, acc: 0.8865247964859009)
[2025-02-13 18:55:19,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:20,204][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.6690793633460999, acc: 0.8471337556838989)
[2025-02-13 18:55:20,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:20,604][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.3820408880710602, acc: 0.9090909361839294)
[2025-02-13 18:55:20,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:21,029][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.3065660893917084, acc: 0.9154929518699646)
[2025-02-13 18:55:21,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:21,417][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.31567496061325073, acc: 0.9437500238418579)
[2025-02-13 18:55:21,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:21,881][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.38668790459632874, acc: 0.9090909361839294)
[2025-02-13 18:55:22,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:22,296][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.49754706025123596, acc: 0.8766233921051025)
[2025-02-13 18:55:22,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:22,706][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.4429026246070862, acc: 0.8922155499458313)
[2025-02-13 18:55:22,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:23,095][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.3636877238750458, acc: 0.9280575513839722)
[2025-02-13 18:55:23,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:23,536][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.5199019312858582, acc: 0.8590604066848755)
[2025-02-13 18:55:23,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:23,916][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.3847860097885132, acc: 0.9071428775787354)
[2025-02-13 18:55:24,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:24,320][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.32542696595191956, acc: 0.9492753744125366)
[2025-02-13 18:55:24,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:24,759][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.263923317193985, acc: 0.9492753744125366)
[2025-02-13 18:55:24,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:25,188][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.2992307245731354, acc: 0.9411764740943909)
[2025-02-13 18:55:25,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:25,580][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.2684697210788727, acc: 0.9269663095474243)
[2025-02-13 18:55:25,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:26,072][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.20588546991348267, acc: 0.953125)
[2025-02-13 18:55:26,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:26,448][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.27317237854003906, acc: 0.9379310607910156)
[2025-02-13 18:55:26,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:26,893][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.23088432848453522, acc: 0.9430379867553711)
[2025-02-13 18:55:27,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:27,320][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.2988179326057434, acc: 0.9407407641410828)
[2025-02-13 18:55:27,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:27,721][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.6900514364242554, acc: 0.8461538553237915)
[2025-02-13 18:55:27,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:28,101][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.6469732522964478, acc: 0.8540540337562561)
[2025-02-13 18:55:28,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:28,497][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.5873768329620361, acc: 0.8698630332946777)
[2025-02-13 18:55:28,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:28,906][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.6030281186103821, acc: 0.8181818127632141)
[2025-02-13 18:55:29,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:29,327][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.5633931159973145, acc: 0.8521126508712769)
[2025-02-13 18:55:29,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:29,714][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.31487682461738586, acc: 0.9151515364646912)
[2025-02-13 18:55:29,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:30,171][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.7213680744171143, acc: 0.8270676732063293)
[2025-02-13 18:55:30,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:30,629][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.29071173071861267, acc: 0.931034505367279)
[2025-02-13 18:55:30,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:31,115][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.47959384322166443, acc: 0.8989899158477783)
[2025-02-13 18:55:31,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:31,554][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.46864795684814453, acc: 0.8802816867828369)
[2025-02-13 18:55:31,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:31,971][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.18364916741847992, acc: 0.9491525292396545)
[2025-02-13 18:55:32,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:32,397][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.16525140404701233, acc: 0.970059871673584)
[2025-02-13 18:55:32,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:32,808][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.34431296586990356, acc: 0.9235293865203857)
[2025-02-13 18:55:32,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:33,196][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.4271021783351898, acc: 0.9025423526763916)
[2025-02-13 18:55:33,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:33,603][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.25926315784454346, acc: 0.949999988079071)
[2025-02-13 18:55:33,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:34,017][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.40942615270614624, acc: 0.917475700378418)
[2025-02-13 18:55:34,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:34,468][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.44399645924568176, acc: 0.881118893623352)
[2025-02-13 18:55:34,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:34,862][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.6128455996513367, acc: 0.8579235076904297)
[2025-02-13 18:55:35,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:35,250][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.603670060634613, acc: 0.8571428656578064)
[2025-02-13 18:55:35,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:35,642][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.6322665810585022, acc: 0.886227548122406)
[2025-02-13 18:55:35,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:36,012][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.2398189902305603, acc: 0.9453551769256592)
[2025-02-13 18:55:36,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:36,393][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.2944048047065735, acc: 0.9273743033409119)
[2025-02-13 18:55:36,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:36,793][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.27619725465774536, acc: 0.913241982460022)
[2025-02-13 18:55:36,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:37,184][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.4374218285083771, acc: 0.8959276080131531)
[2025-02-13 18:55:37,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:37,559][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.3477129638195038, acc: 0.918749988079071)
[2025-02-13 18:55:37,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:37,934][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.330156534910202, acc: 0.9353233575820923)
[2025-02-13 18:55:38,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:38,314][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.2897774279117584, acc: 0.9329268336296082)
[2025-02-13 18:55:38,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:38,664][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.2749970555305481, acc: 0.939226508140564)
[2025-02-13 18:55:38,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:39,028][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.22831276059150696, acc: 0.929347813129425)
[2025-02-13 18:55:39,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:39,402][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.1855151504278183, acc: 0.9576719403266907)
[2025-02-13 18:55:39,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:39,802][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.3656063675880432, acc: 0.9146341681480408)
[2025-02-13 18:55:39,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:40,200][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.2982528507709503, acc: 0.9172932505607605)
[2025-02-13 18:55:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:40,598][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.5022048950195312, acc: 0.8994975090026855)
[2025-02-13 18:55:40,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:41,015][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.2820548117160797, acc: 0.9368420839309692)
[2025-02-13 18:55:41,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:41,433][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.22786080837249756, acc: 0.9435028433799744)
[2025-02-13 18:55:41,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:41,867][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.412930428981781, acc: 0.8972973227500916)
[2025-02-13 18:55:42,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:42,292][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.4177553057670593, acc: 0.894444465637207)
[2025-02-13 18:55:42,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:42,684][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.3145207464694977, acc: 0.9145728349685669)
[2025-02-13 18:55:42,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:43,085][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.1532331258058548, acc: 0.9562841653823853)
[2025-02-13 18:55:43,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:43,499][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.32991769909858704, acc: 0.913385808467865)
[2025-02-13 18:55:43,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:43,972][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.15309350192546844, acc: 0.949367105960846)
[2025-02-13 18:55:44,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:44,367][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.2656114101409912, acc: 0.9259259104728699)
[2025-02-13 18:55:44,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:44,826][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.7665194869041443, acc: 0.85161292552948)
[2025-02-13 18:55:44,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:45,278][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.704769492149353, acc: 0.8766233921051025)
[2025-02-13 18:55:45,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:45,675][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.7575052380561829, acc: 0.8477157354354858)
[2025-02-13 18:55:45,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:46,072][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.41956695914268494, acc: 0.8865247964859009)
[2025-02-13 18:55:46,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:46,473][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.7376360297203064, acc: 0.8181818127632141)
[2025-02-13 18:55:46,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:46,855][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.664307713508606, acc: 0.8208954930305481)
[2025-02-13 18:55:46,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:47,260][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.7787183523178101, acc: 0.8301886916160583)
[2025-02-13 18:55:47,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:47,673][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.464094877243042, acc: 0.885869562625885)
[2025-02-13 18:55:47,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:48,064][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.338409960269928, acc: 0.9242424368858337)
[2025-02-13 18:55:48,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:48,485][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.24949102103710175, acc: 0.931506872177124)
[2025-02-13 18:55:48,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:48,880][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.5753281712532043, acc: 0.8640000224113464)
[2025-02-13 18:55:49,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:49,238][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.6285213232040405, acc: 0.8782608509063721)
[2025-02-13 18:55:49,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:49,626][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.872916042804718, acc: 0.8145695328712463)
[2025-02-13 18:55:49,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:49,998][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.6342990398406982, acc: 0.8445945978164673)
[2025-02-13 18:55:50,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:50,382][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.5171380043029785, acc: 0.9090909361839294)
[2025-02-13 18:55:50,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:50,791][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.5542816519737244, acc: 0.8807339668273926)
[2025-02-13 18:55:50,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:51,195][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.41535043716430664, acc: 0.8867924809455872)
[2025-02-13 18:55:51,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:51,591][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.4508436918258667, acc: 0.896039605140686)
[2025-02-13 18:55:51,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:52,010][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.357878178358078, acc: 0.9269663095474243)
[2025-02-13 18:55:52,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:52,425][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.43660804629325867, acc: 0.9028571248054504)
[2025-02-13 18:55:52,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:52,833][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.25414374470710754, acc: 0.9427083134651184)
[2025-02-13 18:55:52,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:53,218][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.3267265260219574, acc: 0.9166666865348816)
[2025-02-13 18:55:53,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:53,579][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.1796855330467224, acc: 0.953125)
[2025-02-13 18:55:53,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:53,953][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.36394429206848145, acc: 0.929648220539093)
[2025-02-13 18:55:54,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:54,316][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.42152145504951477, acc: 0.8999999761581421)
[2025-02-13 18:55:54,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:54,764][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.13822607696056366, acc: 0.967391312122345)
[2025-02-13 18:55:54,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:55,154][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.1840905249118805, acc: 0.9558823704719543)
[2025-02-13 18:55:55,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:55,522][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.285926878452301, acc: 0.9251337051391602)
[2025-02-13 18:55:55,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:55,917][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.17725355923175812, acc: 0.9470899701118469)
[2025-02-13 18:55:56,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:56,292][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.18121038377285004, acc: 0.9716312289237976)
[2025-02-13 18:55:56,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:56,672][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.26653432846069336, acc: 0.9459459185600281)
[2025-02-13 18:55:56,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:57,046][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.20623885095119476, acc: 0.9578947424888611)
[2025-02-13 18:55:57,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:57,440][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.3913675546646118, acc: 0.8737373948097229)
[2025-02-13 18:55:57,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:57,854][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.21386756002902985, acc: 0.9510869383811951)
[2025-02-13 18:55:58,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:58,289][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.30827754735946655, acc: 0.9308176040649414)
[2025-02-13 18:55:58,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:58,659][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.16707707941532135, acc: 0.9523809552192688)
[2025-02-13 18:55:58,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:59,049][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.16801854968070984, acc: 0.9553072452545166)
[2025-02-13 18:55:59,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:59,498][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.19195055961608887, acc: 0.9653179049491882)
[2025-02-13 18:55:59,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:59,915][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.149820938706398, acc: 0.9599999785423279)
[2025-02-13 18:56:00,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:00,316][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.1789516806602478, acc: 0.9620253443717957)
[2025-02-13 18:56:00,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:00,729][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.12200237065553665, acc: 0.9840425252914429)
[2025-02-13 18:56:00,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:01,132][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.1111559346318245, acc: 0.9774011373519897)
[2025-02-13 18:56:01,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:01,544][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.5097770690917969, acc: 0.868852436542511)
[2025-02-13 18:56:01,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:01,966][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.22615256905555725, acc: 0.9538461565971375)
[2025-02-13 18:56:02,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:02,413][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.3770381212234497, acc: 0.8709677457809448)
[2025-02-13 18:56:02,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:02,806][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.31320685148239136, acc: 0.9295774698257446)
[2025-02-13 18:56:02,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:03,240][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.6919922828674316, acc: 0.8415841460227966)
[2025-02-13 18:56:03,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:03,633][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.7604069113731384, acc: 0.8728813529014587)
[2025-02-13 18:56:03,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:04,002][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.2763703465461731, acc: 0.9245283007621765)
[2025-02-13 18:56:04,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:04,414][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.22919785976409912, acc: 0.939130425453186)
[2025-02-13 18:56:04,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:04,784][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.16980160772800446, acc: 0.9593495726585388)
[2025-02-13 18:56:04,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:05,174][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.22563567757606506, acc: 0.9555555582046509)
[2025-02-13 18:56:05,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:05,554][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.2224338799715042, acc: 0.9263157844543457)
[2025-02-13 18:56:05,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:06,033][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.5148189663887024, acc: 0.8911564350128174)
[2025-02-13 18:56:06,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:06,435][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.3456716239452362, acc: 0.9300000071525574)
[2025-02-13 18:56:06,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:06,814][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.3172902464866638, acc: 0.9411764740943909)
[2025-02-13 18:56:06,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:07,232][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.39520707726478577, acc: 0.8778625726699829)
[2025-02-13 18:56:07,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:07,614][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.4919513463973999, acc: 0.8818897604942322)
[2025-02-13 18:56:07,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:08,019][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.38568729162216187, acc: 0.8731343150138855)
[2025-02-13 18:56:08,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:08,409][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.2698669135570526, acc: 0.9285714030265808)
[2025-02-13 18:56:08,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:08,850][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.30952197313308716, acc: 0.9173553586006165)
[2025-02-13 18:56:08,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:09,219][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.47730353474617004, acc: 0.8799999952316284)
[2025-02-13 18:56:09,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:09,588][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.5138518214225769, acc: 0.8918918967247009)
[2025-02-13 18:56:09,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:09,981][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.3169593811035156, acc: 0.921875)
[2025-02-13 18:56:10,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:10,359][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.31401678919792175, acc: 0.9130434989929199)
[2025-02-13 18:56:10,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:10,791][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.7230332493782043, acc: 0.8560000061988831)
[2025-02-13 18:56:10,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:11,186][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.2738558053970337, acc: 0.9375)
[2025-02-13 18:56:11,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:11,569][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.1579621434211731, acc: 0.9615384340286255)
[2025-02-13 18:56:11,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:11,947][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.2329094558954239, acc: 0.9279279112815857)
[2025-02-13 18:56:12,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:12,312][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.3479323983192444, acc: 0.9099099040031433)
[2025-02-13 18:56:12,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:12,684][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.3179105222225189, acc: 0.9015151262283325)
[2025-02-13 18:56:12,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:13,062][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.3573951721191406, acc: 0.910179615020752)
[2025-02-13 18:56:13,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:13,438][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.36632758378982544, acc: 0.9202898740768433)
[2025-02-13 18:56:13,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:13,814][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.4904557168483734, acc: 0.8881579041481018)
[2025-02-13 18:56:13,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:14,205][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.2928375005722046, acc: 0.9230769276618958)
[2025-02-13 18:56:14,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:14,651][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.16085483133792877, acc: 0.9496855139732361)
[2025-02-13 18:56:14,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:15,046][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.2237258404493332, acc: 0.9375)
[2025-02-13 18:56:15,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:15,421][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.3090705871582031, acc: 0.9107142686843872)
[2025-02-13 18:56:15,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:15,805][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.37373819947242737, acc: 0.9234972596168518)
[2025-02-13 18:56:15,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:16,205][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.24712802469730377, acc: 0.9503105878829956)
[2025-02-13 18:56:16,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:16,619][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.3382139205932617, acc: 0.895061731338501)
[2025-02-13 18:56:16,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:17,005][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.272546648979187, acc: 0.9124087691307068)
[2025-02-13 18:56:17,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:17,387][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.383919894695282, acc: 0.918367326259613)
[2025-02-13 18:56:17,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:17,787][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.46931853890419006, acc: 0.8736842274665833)
[2025-02-13 18:56:17,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:18,193][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.3202856779098511, acc: 0.908450722694397)
[2025-02-13 18:56:18,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:18,580][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.17357125878334045, acc: 0.957446813583374)
[2025-02-13 18:56:18,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:19,014][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.07700493186712265, acc: 0.987261176109314)
[2025-02-13 18:56:19,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:19,485][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.20553730428218842, acc: 0.9599999785423279)
[2025-02-13 18:56:19,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:19,916][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.3079763650894165, acc: 0.9308176040649414)
[2025-02-13 18:56:20,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:20,353][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.28770947456359863, acc: 0.9298245906829834)
[2025-02-13 18:56:20,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:20,765][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.4006343185901642, acc: 0.8864864706993103)
[2025-02-13 18:56:20,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:21,198][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.3522914946079254, acc: 0.9117646813392639)
[2025-02-13 18:56:21,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:21,647][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.39423710107803345, acc: 0.9303797483444214)
[2025-02-13 18:56:21,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:22,066][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.2866553068161011, acc: 0.9158878326416016)
[2025-02-13 18:56:22,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:22,467][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.3423229157924652, acc: 0.9402984976768494)
[2025-02-13 18:56:22,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:22,889][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.37377387285232544, acc: 0.8933333158493042)
[2025-02-13 18:56:23,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:23,336][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.2595025897026062, acc: 0.9253731369972229)
[2025-02-13 18:56:23,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:23,725][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.1379840075969696, acc: 0.9774436354637146)
[2025-02-13 18:56:23,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:24,136][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.38922855257987976, acc: 0.916201114654541)
[2025-02-13 18:56:24,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:24,541][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.3691966235637665, acc: 0.9166666865348816)
[2025-02-13 18:56:24,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:24,937][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.43678051233291626, acc: 0.9054054021835327)
[2025-02-13 18:56:25,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:25,381][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.48186472058296204, acc: 0.869918704032898)
[2025-02-13 18:56:25,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:25,814][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.36442580819129944, acc: 0.9177215099334717)
[2025-02-13 18:56:25,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:26,234][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.19519689679145813, acc: 0.956250011920929)
[2025-02-13 18:56:26,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:26,643][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.43762296438217163, acc: 0.9137930870056152)
[2025-02-13 18:56:26,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:27,051][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.36314108967781067, acc: 0.9113923907279968)
[2025-02-13 18:56:27,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:27,468][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.7222170233726501, acc: 0.8285714387893677)
[2025-02-13 18:56:27,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:27,912][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.5525734424591064, acc: 0.8976377844810486)
[2025-02-13 18:56:28,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:28,347][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.5345263481140137, acc: 0.8888888955116272)
[2025-02-13 18:56:28,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:28,817][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.29023829102516174, acc: 0.9090909361839294)
[2025-02-13 18:56:28,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:29,231][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.4630768895149231, acc: 0.8814814686775208)
[2025-02-13 18:56:29,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:29,667][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.5366915464401245, acc: 0.8580645322799683)
[2025-02-13 18:56:29,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:30,123][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.3632902204990387, acc: 0.9007092118263245)
[2025-02-13 18:56:30,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:30,525][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.27571815252304077, acc: 0.9420289993286133)
[2025-02-13 18:56:30,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:30,940][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.6247665882110596, acc: 0.8527131676673889)
[2025-02-13 18:56:31,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:31,406][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.6144954562187195, acc: 0.8391608595848083)
[2025-02-13 18:56:31,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:31,856][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.36928480863571167, acc: 0.895061731338501)
[2025-02-13 18:56:32,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:32,293][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.3998776376247406, acc: 0.8899999856948853)
[2025-02-13 18:56:32,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:32,673][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.2735660970211029, acc: 0.9543147087097168)
[2025-02-13 18:56:32,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:33,041][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.3991313874721527, acc: 0.9066666960716248)
[2025-02-13 18:56:33,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:33,408][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.3334977924823761, acc: 0.9153439402580261)
[2025-02-13 18:56:33,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:33,775][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.5232835412025452, acc: 0.8482758402824402)
[2025-02-13 18:56:33,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:34,112][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.3510946035385132, acc: 0.8894736766815186)
[2025-02-13 18:56:34,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:34,470][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.31296274065971375, acc: 0.9235293865203857)
[2025-02-13 18:56:34,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:34,941][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.31506893038749695, acc: 0.9209039807319641)
[2025-02-13 18:56:35,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:35,346][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.38299354910850525, acc: 0.8974359035491943)
[2025-02-13 18:56:35,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:35,775][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.4216271936893463, acc: 0.8936170339584351)
[2025-02-13 18:56:35,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:36,184][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.45703426003456116, acc: 0.8848921060562134)
[2025-02-13 18:56:36,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:36,596][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.6757722496986389, acc: 0.8758170008659363)
[2025-02-13 18:56:36,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:37,005][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.4693719446659088, acc: 0.9246575236320496)
[2025-02-13 18:56:37,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:37,504][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.4346223771572113, acc: 0.8896104097366333)
[2025-02-13 18:56:37,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:37,946][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.27091744542121887, acc: 0.9479768872261047)
[2025-02-13 18:56:38,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:38,360][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.37983545660972595, acc: 0.905063271522522)
[2025-02-13 18:56:38,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:38,744][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.24848581850528717, acc: 0.9430894255638123)
[2025-02-13 18:56:38,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:39,136][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.5082560181617737, acc: 0.8787878751754761)
[2025-02-13 18:56:39,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:39,586][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.5262593626976013, acc: 0.875)
[2025-02-13 18:56:39,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:40,033][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.28776147961616516, acc: 0.9356725215911865)
[2025-02-13 18:56:40,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:40,426][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.3500094711780548, acc: 0.9064327478408813)
[2025-02-13 18:56:40,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:40,830][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.31406471133232117, acc: 0.9220778942108154)
[2025-02-13 18:56:40,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:41,239][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.5335984826087952, acc: 0.9038461446762085)
[2025-02-13 18:56:41,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:41,648][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.23561343550682068, acc: 0.9424460530281067)
[2025-02-13 18:56:41,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:42,033][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.26931867003440857, acc: 0.9457831382751465)
[2025-02-13 18:56:42,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:42,421][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.6499635577201843, acc: 0.8367347121238708)
[2025-02-13 18:56:42,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:42,818][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.22902359068393707, acc: 0.9345238208770752)
[2025-02-13 18:56:42,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:43,259][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.4069344103336334, acc: 0.8882352709770203)
[2025-02-13 18:56:43,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:43,659][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.4183413088321686, acc: 0.8888888955116272)
[2025-02-13 18:56:43,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:44,088][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.3722681999206543, acc: 0.8984771370887756)
[2025-02-13 18:56:44,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:44,521][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.38739585876464844, acc: 0.9209039807319641)
[2025-02-13 18:56:44,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:45,003][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.38763925433158875, acc: 0.9019607901573181)
[2025-02-13 18:56:45,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:45,502][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.2649821937084198, acc: 0.9306930899620056)
[2025-02-13 18:56:45,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:45,973][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.34859833121299744, acc: 0.910179615020752)
[2025-02-13 18:56:46,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:46,379][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.2568765878677368, acc: 0.9473684430122375)
[2025-02-13 18:56:46,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:46,788][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.2210891991853714, acc: 0.9470587968826294)
[2025-02-13 18:56:46,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:47,176][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.23465576767921448, acc: 0.9298245906829834)
[2025-02-13 18:56:47,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:47,572][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.23509955406188965, acc: 0.9246575236320496)
[2025-02-13 18:56:47,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:47,973][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.27714434266090393, acc: 0.9407407641410828)
[2025-02-13 18:56:48,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:48,361][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.38645246624946594, acc: 0.9246575236320496)
[2025-02-13 18:56:48,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:48,751][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.4313400685787201, acc: 0.8922155499458313)
[2025-02-13 18:56:48,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:49,127][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.34825000166893005, acc: 0.9122806787490845)
[2025-02-13 18:56:49,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:49,544][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.2668244540691376, acc: 0.9207317233085632)
[2025-02-13 18:56:49,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:49,922][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.1902182698249817, acc: 0.9664429426193237)
[2025-02-13 18:56:50,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:50,354][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.2850135862827301, acc: 0.9127907156944275)
[2025-02-13 18:56:50,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:50,753][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.2584342062473297, acc: 0.9476439952850342)
[2025-02-13 18:56:50,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:51,152][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.36703553795814514, acc: 0.9319371581077576)
[2025-02-13 18:56:51,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:51,596][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.22653551399707794, acc: 0.957317054271698)
[2025-02-13 18:56:51,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:51,983][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.20625130832195282, acc: 0.9421965479850769)
[2025-02-13 18:56:52,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:52,353][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.20031492412090302, acc: 0.9453551769256592)
[2025-02-13 18:56:52,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:52,732][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.34274929761886597, acc: 0.9142857193946838)
[2025-02-13 18:56:52,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:53,151][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.5411245822906494, acc: 0.886904776096344)
[2025-02-13 18:56:53,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:53,609][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.5589663982391357, acc: 0.8644067645072937)
[2025-02-13 18:56:53,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:53,981][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.5951873660087585, acc: 0.8675496578216553)
[2025-02-13 18:56:54,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:54,427][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.3686685562133789, acc: 0.9032257795333862)
[2025-02-13 18:56:54,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:54,835][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.524939239025116, acc: 0.8571428656578064)
[2025-02-13 18:56:54,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:55,262][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.5063621997833252, acc: 0.874316930770874)
[2025-02-13 18:56:55,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:55,670][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.6046728491783142, acc: 0.8809523582458496)
[2025-02-13 18:56:55,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:56,078][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.65911465883255, acc: 0.8488371968269348)
[2025-02-13 18:56:56,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:56,493][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.5928875803947449, acc: 0.8714285492897034)
[2025-02-13 18:56:56,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:57,030][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.5618871450424194, acc: 0.84375)
[2025-02-13 18:56:57,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:57,429][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.8450822234153748, acc: 0.8372092843055725)
[2025-02-13 18:56:57,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:57,821][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 1.0672390460968018, acc: 0.7951807379722595)
[2025-02-13 18:56:57,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:58,222][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.3566516041755676, acc: 0.9064327478408813)
[2025-02-13 18:56:58,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:58,620][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.39632999897003174, acc: 0.8972602486610413)
[2025-02-13 18:56:58,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:58,987][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.2349788397550583, acc: 0.9305555820465088)
[2025-02-13 18:56:59,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:59,341][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.4349207878112793, acc: 0.9151515364646912)
[2025-02-13 18:56:59,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:59,733][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.4668891727924347, acc: 0.8819444179534912)
[2025-02-13 18:56:59,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:00,131][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.237291157245636, acc: 0.9568345546722412)
[2025-02-13 18:57:00,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:00,543][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.3248301148414612, acc: 0.9210526347160339)
[2025-02-13 18:57:00,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:00,948][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.5274816155433655, acc: 0.8895705342292786)
[2025-02-13 18:57:01,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:01,343][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.42796140909194946, acc: 0.8999999761581421)
[2025-02-13 18:57:01,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:01,713][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.4039175808429718, acc: 0.9108280539512634)
[2025-02-13 18:57:01,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:02,110][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.3181956112384796, acc: 0.9263803958892822)
[2025-02-13 18:57:02,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:02,498][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.285666823387146, acc: 0.9268292784690857)
[2025-02-13 18:57:02,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:02,913][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.250846266746521, acc: 0.9274193644523621)
[2025-02-13 18:57:03,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:03,314][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.47038891911506653, acc: 0.9078947305679321)
[2025-02-13 18:57:03,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:03,710][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.37903040647506714, acc: 0.9041916131973267)
[2025-02-13 18:57:03,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:04,118][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.17837366461753845, acc: 0.9451219439506531)
[2025-02-13 18:57:04,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:04,521][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.3444102704524994, acc: 0.9411764740943909)
[2025-02-13 18:57:04,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:04,898][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.24357783794403076, acc: 0.9402984976768494)
[2025-02-13 18:57:05,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:05,266][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.32488635182380676, acc: 0.9395973086357117)
[2025-02-13 18:57:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:05,653][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.3146922290325165, acc: 0.9306930899620056)
[2025-02-13 18:57:05,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:06,124][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.6007280349731445, acc: 0.899328887462616)
[2025-02-13 18:57:06,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:06,557][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.32410338521003723, acc: 0.9497206807136536)
[2025-02-13 18:57:06,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:06,957][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.31510716676712036, acc: 0.9285714030265808)
[2025-02-13 18:57:07,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:07,314][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.3899911642074585, acc: 0.9281045794487)
[2025-02-13 18:57:07,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:07,702][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.22124440968036652, acc: 0.9604519605636597)
[2025-02-13 18:57:07,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:08,097][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.21836870908737183, acc: 0.931506872177124)
[2025-02-13 18:57:08,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:08,452][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.10443456470966339, acc: 0.9842519760131836)
[2025-02-13 18:57:08,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:08,876][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.21823757886886597, acc: 0.9553072452545166)
[2025-02-13 18:57:09,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:09,271][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.4554482698440552, acc: 0.8863636255264282)
[2025-02-13 18:57:09,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:09,661][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.3085302710533142, acc: 0.9358288645744324)
[2025-02-13 18:57:09,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:10,024][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.21178261935710907, acc: 0.9268292784690857)
[2025-02-13 18:57:10,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:10,408][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.47429022192955017, acc: 0.898809552192688)
[2025-02-13 18:57:10,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:10,813][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.3526391088962555, acc: 0.8961039185523987)
[2025-02-13 18:57:10,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:11,227][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.15982922911643982, acc: 0.9595375657081604)
[2025-02-13 18:57:11,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:11,620][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.22153380513191223, acc: 0.9604519605636597)
[2025-02-13 18:57:11,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:12,021][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.15628398954868317, acc: 0.954023003578186)
[2025-02-13 18:57:12,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:12,402][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.2817138135433197, acc: 0.9226519465446472)
[2025-02-13 18:57:12,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:12,795][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.27824246883392334, acc: 0.9408602118492126)
[2025-02-13 18:57:12,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:13,182][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.26714786887168884, acc: 0.9222221970558167)
[2025-02-13 18:57:13,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:13,561][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.30857980251312256, acc: 0.9144737124443054)
[2025-02-13 18:57:13,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:13,947][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.3315698504447937, acc: 0.9125000238418579)
[2025-02-13 18:57:14,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:14,349][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.26236703991889954, acc: 0.948387086391449)
[2025-02-13 18:57:14,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:14,786][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.30654945969581604, acc: 0.9157894849777222)
[2025-02-13 18:57:14,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:15,146][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.26966598629951477, acc: 0.9505494236946106)
[2025-02-13 18:57:15,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:15,517][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.3129749298095703, acc: 0.9272727370262146)
[2025-02-13 18:57:15,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:15,926][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.3586316704750061, acc: 0.8999999761581421)
[2025-02-13 18:57:16,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:16,325][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.1767449826002121, acc: 0.9457364082336426)
[2025-02-13 18:57:16,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:16,684][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.3955274224281311, acc: 0.9285714030265808)
[2025-02-13 18:57:16,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:17,055][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.40413063764572144, acc: 0.9072847962379456)
[2025-02-13 18:57:17,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:17,409][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.18352247774600983, acc: 0.9545454382896423)
[2025-02-13 18:57:17,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:17,808][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.37233686447143555, acc: 0.9127907156944275)
[2025-02-13 18:57:17,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:18,193][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.321732759475708, acc: 0.9239766001701355)
[2025-02-13 18:57:18,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:18,586][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.22811755537986755, acc: 0.9477611780166626)
[2025-02-13 18:57:18,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:18,967][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.21977704763412476, acc: 0.9532163739204407)
[2025-02-13 18:57:19,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:19,395][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.19261623919010162, acc: 0.9604519605636597)
[2025-02-13 18:57:19,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:19,789][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.6498998403549194, acc: 0.8387096524238586)
[2025-02-13 18:57:19,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:20,228][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.3800918459892273, acc: 0.9166666865348816)
[2025-02-13 18:57:20,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:20,638][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.3034157156944275, acc: 0.9136690497398376)
[2025-02-13 18:57:20,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:21,021][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.08349830657243729, acc: 0.9908257126808167)
[2025-02-13 18:57:21,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:21,420][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.24687190353870392, acc: 0.9539473652839661)
[2025-02-13 18:57:21,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:21,830][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.5353371500968933, acc: 0.8709677457809448)
[2025-02-13 18:57:22,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:22,238][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.38436025381088257, acc: 0.904411792755127)
[2025-02-13 18:57:22,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:22,623][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.24851226806640625, acc: 0.9404761791229248)
[2025-02-13 18:57:22,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:22,992][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.39228472113609314, acc: 0.9221556782722473)
[2025-02-13 18:57:23,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:23,384][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.24946598708629608, acc: 0.930232584476471)
[2025-02-13 18:57:23,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:23,791][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.19638167321681976, acc: 0.9457364082336426)
[2025-02-13 18:57:23,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:24,186][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.23623178899288177, acc: 0.9370078444480896)
[2025-02-13 18:57:24,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:24,565][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.19824400544166565, acc: 0.9674796462059021)
[2025-02-13 18:57:24,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:24,956][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.3460707664489746, acc: 0.9166666865348816)
[2025-02-13 18:57:25,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:25,327][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.4433421194553375, acc: 0.9024389982223511)
[2025-02-13 18:57:25,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:25,725][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.4140399992465973, acc: 0.913294792175293)
[2025-02-13 18:57:25,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:26,129][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.2713061571121216, acc: 0.9382022619247437)
[2025-02-13 18:57:26,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:26,514][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.35620400309562683, acc: 0.9127907156944275)
[2025-02-13 18:57:26,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:26,902][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.26546379923820496, acc: 0.9263803958892822)
[2025-02-13 18:57:27,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:27,281][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.22921451926231384, acc: 0.9554139971733093)
[2025-02-13 18:57:27,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:27,643][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.39243558049201965, acc: 0.9271523356437683)
[2025-02-13 18:57:27,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:28,069][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.14849837124347687, acc: 0.9805825352668762)
[2025-02-13 18:57:28,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:28,439][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.385143905878067, acc: 0.9117646813392639)
[2025-02-13 18:57:28,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:28,819][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.28647878766059875, acc: 0.9457831382751465)
[2025-02-13 18:57:29,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:29,231][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.28574007749557495, acc: 0.9437500238418579)
[2025-02-13 18:57:29,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:29,630][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.2920430898666382, acc: 0.9350649118423462)
[2025-02-13 18:57:29,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:30,016][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.2642320692539215, acc: 0.9389312863349915)
[2025-02-13 18:57:30,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:30,381][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.5702129602432251, acc: 0.8823529481887817)
[2025-02-13 18:57:30,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:30,770][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.511610746383667, acc: 0.8841463327407837)
[2025-02-13 18:57:30,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:31,136][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.4223954677581787, acc: 0.9103448390960693)
[2025-02-13 18:57:31,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:31,530][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.4001582860946655, acc: 0.9152542352676392)
[2025-02-13 18:57:31,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:31,936][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.4457088112831116, acc: 0.91847825050354)
[2025-02-13 18:57:32,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:32,317][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.45956066250801086, acc: 0.9117646813392639)
[2025-02-13 18:57:32,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:32,701][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.5963025689125061, acc: 0.858208954334259)
[2025-02-13 18:57:32,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:33,064][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.5981389880180359, acc: 0.8402062058448792)
[2025-02-13 18:57:33,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:33,452][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.7185642123222351, acc: 0.8432835936546326)
[2025-02-13 18:57:33,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:33,826][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.45758819580078125, acc: 0.8791208863258362)
[2025-02-13 18:57:33,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:34,218][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.6318312287330627, acc: 0.8421052694320679)
[2025-02-13 18:57:34,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:34,613][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.4699278473854065, acc: 0.9127907156944275)
[2025-02-13 18:57:34,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:35,026][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.7368762493133545, acc: 0.8475610017776489)
[2025-02-13 18:57:35,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:35,413][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.5313835144042969, acc: 0.8823529481887817)
[2025-02-13 18:57:35,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:35,789][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.40728989243507385, acc: 0.89570552110672)
[2025-02-13 18:57:35,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:36,185][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.4197358787059784, acc: 0.914893627166748)
[2025-02-13 18:57:36,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:36,579][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.5370326042175293, acc: 0.8496240377426147)
[2025-02-13 18:57:36,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:36,931][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.6199538707733154, acc: 0.8799999952316284)
[2025-02-13 18:57:37,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:37,335][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.5815480947494507, acc: 0.8897058963775635)
[2025-02-13 18:57:37,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:37,781][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.5148620009422302, acc: 0.9052132964134216)
[2025-02-13 18:57:37,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:38,214][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.5921010375022888, acc: 0.8564102649688721)
[2025-02-13 18:57:38,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:38,656][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.5995103716850281, acc: 0.8626373410224915)
[2025-02-13 18:57:38,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:39,036][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.49097827076911926, acc: 0.9108911156654358)
[2025-02-13 18:57:39,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:39,415][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.4361869692802429, acc: 0.9255319237709045)
[2025-02-13 18:57:39,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:39,775][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.31950291991233826, acc: 0.9006622433662415)
[2025-02-13 18:57:39,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:40,150][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.7401055097579956, acc: 0.8089887499809265)
[2025-02-13 18:57:40,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:40,529][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.3506767153739929, acc: 0.9226804375648499)
[2025-02-13 18:57:40,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:40,902][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.624291718006134, acc: 0.8486486673355103)
[2025-02-13 18:57:41,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:41,315][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.6124143600463867, acc: 0.8450704216957092)
[2025-02-13 18:57:41,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:41,724][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.5372928977012634, acc: 0.8993710875511169)
[2025-02-13 18:57:41,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:42,190][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.6226881146430969, acc: 0.8914285898208618)
[2025-02-13 18:57:42,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:42,601][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.3866855800151825, acc: 0.9172932505607605)
[2025-02-13 18:57:42,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:43,008][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.29558032751083374, acc: 0.9219858050346375)
[2025-02-13 18:57:43,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:43,376][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.2726147472858429, acc: 0.949999988079071)
[2025-02-13 18:57:43,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:43,763][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.3418259918689728, acc: 0.9441340565681458)
[2025-02-13 18:57:43,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:44,173][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.31436967849731445, acc: 0.9194630980491638)
[2025-02-13 18:57:44,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:44,567][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.5223240256309509, acc: 0.8926553726196289)
[2025-02-13 18:57:44,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:44,958][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.5358715057373047, acc: 0.89570552110672)
[2025-02-13 18:57:45,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:45,353][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.45971205830574036, acc: 0.8846153616905212)
[2025-02-13 18:57:45,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:45,740][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.7772598266601562, acc: 0.8364779949188232)
[2025-02-13 18:57:45,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:46,139][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.4282456636428833, acc: 0.884393036365509)
[2025-02-13 18:57:46,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:46,518][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.32889696955680847, acc: 0.9280575513839722)
[2025-02-13 18:57:46,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:46,920][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.41813141107559204, acc: 0.893203854560852)
[2025-02-13 18:57:47,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:47,319][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.3637329936027527, acc: 0.9047619104385376)
[2025-02-13 18:57:47,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:47,702][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.5037734508514404, acc: 0.895348846912384)
[2025-02-13 18:57:47,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:48,054][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.27411168813705444, acc: 0.9352940917015076)
[2025-02-13 18:57:48,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:48,425][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.32914674282073975, acc: 0.9221556782722473)
[2025-02-13 18:57:48,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:48,798][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.40591248869895935, acc: 0.9076923131942749)
[2025-02-13 18:57:48,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:49,143][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.2367367148399353, acc: 0.970588207244873)
[2025-02-13 18:57:49,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:49,571][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.4416503310203552, acc: 0.9363636374473572)
[2025-02-13 18:57:49,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:49,955][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.26829755306243896, acc: 0.9245283007621765)
[2025-02-13 18:57:50,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:50,344][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.2561768889427185, acc: 0.9375)
[2025-02-13 18:57:50,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:50,786][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.3521239459514618, acc: 0.8938053250312805)
[2025-02-13 18:57:50,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:51,173][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.2789411246776581, acc: 0.9219858050346375)
[2025-02-13 18:57:51,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:51,546][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.39282166957855225, acc: 0.9108280539512634)
[2025-02-13 18:57:51,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:51,897][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.5739230513572693, acc: 0.8591549396514893)
[2025-02-13 18:57:52,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:52,290][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.3531551957130432, acc: 0.9259259104728699)
[2025-02-13 18:57:52,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:52,709][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.25866127014160156, acc: 0.930232584476471)
[2025-02-13 18:57:52,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:53,103][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.4405721426010132, acc: 0.8974359035491943)
[2025-02-13 18:57:53,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:53,497][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.22543299198150635, acc: 0.9496855139732361)
[2025-02-13 18:57:53,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:53,901][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.6197845935821533, acc: 0.8687499761581421)
[2025-02-13 18:57:54,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:54,302][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.4471977949142456, acc: 0.891566276550293)
[2025-02-13 18:57:54,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:54,690][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.41018861532211304, acc: 0.8936170339584351)
[2025-02-13 18:57:54,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:55,070][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.25555843114852905, acc: 0.9337748289108276)
[2025-02-13 18:57:55,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:55,481][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.30076566338539124, acc: 0.924369752407074)
[2025-02-13 18:57:55,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:55,876][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.3277896046638489, acc: 0.9257143139839172)
[2025-02-13 18:57:56,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:56,251][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.5708808898925781, acc: 0.8695651888847351)
[2025-02-13 18:57:56,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:56,657][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.2884020209312439, acc: 0.948051929473877)
[2025-02-13 18:57:56,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:57,072][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.21005506813526154, acc: 0.9363057613372803)
[2025-02-13 18:57:57,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:57,474][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.169621542096138, acc: 0.9673202633857727)
[2025-02-13 18:57:57,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:57,851][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.09843075275421143, acc: 0.9884393215179443)
[2025-02-13 18:57:57,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:58,226][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.23785723745822906, acc: 0.9608938694000244)
[2025-02-13 18:57:58,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:58,662][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.2795918583869934, acc: 0.9477611780166626)
[2025-02-13 18:57:58,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:59,071][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.17874042689800262, acc: 0.9503546357154846)
[2025-02-13 18:57:59,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:59,472][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.4056648910045624, acc: 0.9026548862457275)
[2025-02-13 18:57:59,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:59,897][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.23111845552921295, acc: 0.9647058844566345)
[2025-02-13 18:58:00,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:00,292][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.13019959628582, acc: 0.9748427867889404)
[2025-02-13 18:58:00,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:00,695][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.2045743763446808, acc: 0.9629629850387573)
[2025-02-13 18:58:00,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:01,084][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.0835871770977974, acc: 0.982758641242981)
[2025-02-13 18:58:01,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:01,436][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.42475563287734985, acc: 0.888198733329773)
[2025-02-13 18:58:01,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:01,810][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.2494119107723236, acc: 0.936170220375061)
[2025-02-13 18:58:01,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:02,237][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.2769080102443695, acc: 0.931506872177124)
[2025-02-13 18:58:02,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:02,608][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.6635124087333679, acc: 0.8441558480262756)
[2025-02-13 18:58:02,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:03,010][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 1.29779052734375, acc: 0.7905405163764954)
[2025-02-13 18:58:03,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:03,405][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 1.4596587419509888, acc: 0.7881355881690979)
[2025-02-13 18:58:03,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:03,802][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.33763495087623596, acc: 0.9255319237709045)
[2025-02-13 18:58:03,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:04,202][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.3969991207122803, acc: 0.9200000166893005)
[2025-02-13 18:58:04,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:04,595][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.3012276291847229, acc: 0.9174311757087708)
[2025-02-13 18:58:04,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:05,019][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.15837812423706055, acc: 0.95652174949646)
[2025-02-13 18:58:05,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:05,397][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.20891734957695007, acc: 0.9530201554298401)
[2025-02-13 18:58:05,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:05,811][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.35658204555511475, acc: 0.8882681727409363)
[2025-02-13 18:58:05,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:06,223][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.3154052793979645, acc: 0.9158415794372559)
[2025-02-13 18:58:06,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:06,628][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.1831752210855484, acc: 0.9440993666648865)
[2025-02-13 18:58:06,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:07,063][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.8412213325500488, acc: 0.8136646151542664)
[2025-02-13 18:58:07,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:07,439][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.4754892885684967, acc: 0.8646616339683533)
[2025-02-13 18:58:07,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:07,817][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.8427877426147461, acc: 0.800000011920929)
[2025-02-13 18:58:07,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:08,238][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.5086225271224976, acc: 0.8853503465652466)
[2025-02-13 18:58:08,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:08,601][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.4577850103378296, acc: 0.9144737124443054)
[2025-02-13 18:58:08,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:09,002][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.5409724116325378, acc: 0.8489583134651184)
[2025-02-13 18:58:09,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:09,434][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.2827332317829132, acc: 0.9314285516738892)
[2025-02-13 18:58:09,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:09,907][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.4954144060611725, acc: 0.891566276550293)
[2025-02-13 18:58:10,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:10,361][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.3475896418094635, acc: 0.9041916131973267)
[2025-02-13 18:58:10,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:10,783][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.2991499602794647, acc: 0.9270833134651184)
[2025-02-13 18:58:10,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:11,171][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.22337080538272858, acc: 0.9541984796524048)
[2025-02-13 18:58:11,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:11,563][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.41437187790870667, acc: 0.9069767594337463)
[2025-02-13 18:58:11,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:11,948][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.22670383751392365, acc: 0.9533678889274597)
[2025-02-13 18:58:12,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:12,354][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.23426282405853271, acc: 0.9578947424888611)
[2025-02-13 18:58:12,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:12,778][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.3135169446468353, acc: 0.9573459625244141)
[2025-02-13 18:58:12,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:13,152][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.2712496519088745, acc: 0.9487179517745972)
[2025-02-13 18:58:13,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:13,592][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.2902979552745819, acc: 0.9278350472450256)
[2025-02-13 18:58:13,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:13,969][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.2926858067512512, acc: 0.9277108311653137)
[2025-02-13 18:58:14,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:14,380][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.19210277497768402, acc: 0.9411764740943909)
[2025-02-13 18:58:14,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:14,780][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.4503839910030365, acc: 0.908108115196228)
[2025-02-13 18:58:14,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:15,140][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.39755576848983765, acc: 0.9226519465446472)
[2025-02-13 18:58:15,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:15,534][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.47138258814811707, acc: 0.885496199131012)
[2025-02-13 18:58:15,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:15,879][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.24765506386756897, acc: 0.9411764740943909)
[2025-02-13 18:58:16,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:16,281][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.3648276627063751, acc: 0.921875)
[2025-02-13 18:58:16,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:16,689][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.34550681710243225, acc: 0.8819444179534912)
[2025-02-13 18:58:16,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:17,077][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.38006412982940674, acc: 0.9452054500579834)
[2025-02-13 18:58:17,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:17,515][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.2690504491329193, acc: 0.9375)
[2025-02-13 18:58:17,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:17,880][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.11304184049367905, acc: 0.9756097793579102)
[2025-02-13 18:58:18,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:18,253][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.178018257021904, acc: 0.9751552939414978)
[2025-02-13 18:58:18,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:18,599][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.40670081973075867, acc: 0.914893627166748)
[2025-02-13 18:58:18,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:18,958][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.6697441339492798, acc: 0.8540145754814148)
[2025-02-13 18:58:19,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:19,323][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.5555413961410522, acc: 0.8563829660415649)
[2025-02-13 18:58:19,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:19,721][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.45585763454437256, acc: 0.9203540086746216)
[2025-02-13 18:58:19,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:20,135][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.6917131543159485, acc: 0.849056601524353)
[2025-02-13 18:58:20,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:20,504][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.46199703216552734, acc: 0.8847926259040833)
[2025-02-13 18:58:20,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:20,915][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.36058861017227173, acc: 0.9224806427955627)
[2025-02-13 18:58:21,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:21,301][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.38103920221328735, acc: 0.9337349534034729)
[2025-02-13 18:58:21,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:21,729][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.1475730687379837, acc: 0.9686098694801331)
[2025-02-13 18:58:21,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:22,102][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.36536213755607605, acc: 0.9187816977500916)
[2025-02-13 18:58:22,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:22,509][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.23678521811962128, acc: 0.9414893388748169)
[2025-02-13 18:58:22,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:22,887][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.31921201944351196, acc: 0.9245283007621765)
[2025-02-13 18:58:23,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:23,249][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.5518339276313782, acc: 0.8790322542190552)
[2025-02-13 18:58:23,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:23,626][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.23898479342460632, acc: 0.9516128897666931)
[2025-02-13 18:58:23,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:23,996][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.28719082474708557, acc: 0.9488636255264282)
[2025-02-13 18:58:24,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:24,364][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.6060683131217957, acc: 0.8711656332015991)
[2025-02-13 18:58:24,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:24,759][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.43782132863998413, acc: 0.898809552192688)
[2025-02-13 18:58:24,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:25,134][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.49345600605010986, acc: 0.8757764101028442)
[2025-02-13 18:58:25,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:25,555][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.2105587273836136, acc: 0.9580838084220886)
[2025-02-13 18:58:25,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:25,926][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.3667072653770447, acc: 0.9319728016853333)
[2025-02-13 18:58:26,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:26,309][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.6212396025657654, acc: 0.8796992301940918)
[2025-02-13 18:58:26,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:26,734][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.2876361012458801, acc: 0.9365079402923584)
[2025-02-13 18:58:26,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:27,131][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.3500957190990448, acc: 0.9139785170555115)
[2025-02-13 18:58:27,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:27,513][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.5228535532951355, acc: 0.8797814249992371)
[2025-02-13 18:58:27,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:27,913][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.19472844898700714, acc: 0.9611111283302307)
[2025-02-13 18:58:28,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:28,295][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.4011133015155792, acc: 0.90625)
[2025-02-13 18:58:28,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:28,701][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.34946173429489136, acc: 0.9239766001701355)
[2025-02-13 18:58:28,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:29,108][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.39483681321144104, acc: 0.9179487228393555)
[2025-02-13 18:58:29,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:29,465][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.6035017967224121, acc: 0.8333333134651184)
[2025-02-13 18:58:29,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:29,834][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.6689428687095642, acc: 0.8809523582458496)
[2025-02-13 18:58:29,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:30,214][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.3351774215698242, acc: 0.9200000166893005)
[2025-02-13 18:58:30,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:30,599][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.4145389199256897, acc: 0.9074074029922485)
[2025-02-13 18:58:30,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:31,008][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.22362154722213745, acc: 0.940397322177887)
[2025-02-13 18:58:31,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:31,409][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.22260107100009918, acc: 0.949999988079071)
[2025-02-13 18:58:31,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:31,808][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.2974122166633606, acc: 0.9171597361564636)
[2025-02-13 18:58:31,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:32,248][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.3647274076938629, acc: 0.9248554706573486)
[2025-02-13 18:58:32,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:32,613][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.3837539851665497, acc: 0.9193548560142517)
[2025-02-13 18:58:32,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:32,991][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.35397306084632874, acc: 0.90625)
[2025-02-13 18:58:33,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:33,359][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.28455930948257446, acc: 0.9440559148788452)
[2025-02-13 18:58:33,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:33,728][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.2886557877063751, acc: 0.9245283007621765)
[2025-02-13 18:58:33,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:34,130][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.36601927876472473, acc: 0.920634925365448)
[2025-02-13 18:58:34,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:34,542][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.3055783808231354, acc: 0.9225806593894958)
[2025-02-13 18:58:34,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:34,924][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.3495759963989258, acc: 0.9105691313743591)
[2025-02-13 18:58:35,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:35,319][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.15108127892017365, acc: 0.9473684430122375)
[2025-02-13 18:58:35,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:35,724][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.17235776782035828, acc: 0.9389312863349915)
[2025-02-13 18:58:35,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:36,126][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.22576893866062164, acc: 0.9379310607910156)
[2025-02-13 18:58:36,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:36,495][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.38608184456825256, acc: 0.8778625726699829)
[2025-02-13 18:58:36,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:36,892][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.18348485231399536, acc: 0.9677419066429138)
[2025-02-13 18:58:37,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:37,263][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.15750260651111603, acc: 0.9615384340286255)
[2025-02-13 18:58:37,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:37,637][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.21703889966011047, acc: 0.9379310607910156)
[2025-02-13 18:58:37,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:38,015][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.22781001031398773, acc: 0.9324324131011963)
[2025-02-13 18:58:38,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:38,475][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.14234556257724762, acc: 0.9583333134651184)
[2025-02-13 18:58:38,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:38,870][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.27628856897354126, acc: 0.9101123809814453)
[2025-02-13 18:58:39,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:39,244][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.1440313309431076, acc: 0.9632353186607361)
[2025-02-13 18:58:39,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:39,671][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.22110049426555634, acc: 0.9312977194786072)
[2025-02-13 18:58:39,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:40,117][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.391623318195343, acc: 0.8666666746139526)
[2025-02-13 18:58:40,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:40,551][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.13798387348651886, acc: 0.9784172773361206)
[2025-02-13 18:58:40,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:41,007][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.5291112065315247, acc: 0.9090909361839294)
[2025-02-13 18:58:41,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:41,423][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.6529972553253174, acc: 0.8456375598907471)
[2025-02-13 18:58:41,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:41,841][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.525883138179779, acc: 0.884393036365509)
[2025-02-13 18:58:41,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:42,240][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.5320418477058411, acc: 0.901098906993866)
[2025-02-13 18:58:42,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:42,673][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.37235498428344727, acc: 0.899328887462616)
[2025-02-13 18:58:42,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:43,090][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.5724967122077942, acc: 0.8673469424247742)
[2025-02-13 18:58:43,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:43,482][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.4223916828632355, acc: 0.8962264060974121)
[2025-02-13 18:58:43,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:43,880][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.382735937833786, acc: 0.9200000166893005)
[2025-02-13 18:58:44,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:44,278][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.4843101501464844, acc: 0.8736263513565063)
[2025-02-13 18:58:44,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:44,699][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.13744232058525085, acc: 0.9578947424888611)
[2025-02-13 18:58:44,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:45,115][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.1371612250804901, acc: 0.9727272987365723)
[2025-02-13 18:58:45,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:45,479][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.249216690659523, acc: 0.9277777671813965)
[2025-02-13 18:58:45,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:45,863][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.4687713384628296, acc: 0.891566276550293)
[2025-02-13 18:58:46,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:46,278][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.45824936032295227, acc: 0.9166666865348816)
[2025-02-13 18:58:46,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:46,656][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.22483032941818237, acc: 0.9385474920272827)
[2025-02-13 18:58:46,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:47,015][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.0706956684589386, acc: 0.9924242496490479)
[2025-02-13 18:58:47,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:47,383][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.13134647905826569, acc: 0.9783783555030823)
[2025-02-13 18:58:47,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:47,755][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.17301045358181, acc: 0.9424460530281067)
[2025-02-13 18:58:47,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:48,175][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.40704095363616943, acc: 0.8958333134651184)
[2025-02-13 18:58:48,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:48,581][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.2417072206735611, acc: 0.9411764740943909)
[2025-02-13 18:58:48,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:49,008][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.3373531103134155, acc: 0.9327731132507324)
[2025-02-13 18:58:49,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:49,417][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.20642207562923431, acc: 0.9516128897666931)
[2025-02-13 18:58:49,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:49,795][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.3665109872817993, acc: 0.915032684803009)
[2025-02-13 18:58:49,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:50,203][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.2599005699157715, acc: 0.9609375)
[2025-02-13 18:58:50,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:50,595][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.6635031700134277, acc: 0.8376068472862244)
[2025-02-13 18:58:50,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:50,992][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.22999118268489838, acc: 0.9469696879386902)
[2025-02-13 18:58:51,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:51,374][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.4255470037460327, acc: 0.9241379499435425)
[2025-02-13 18:58:51,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:51,754][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.3420119881629944, acc: 0.939130425453186)
[2025-02-13 18:58:51,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:52,171][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.39096421003341675, acc: 0.9281437397003174)
[2025-02-13 18:58:52,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:52,582][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.22057749330997467, acc: 0.9444444179534912)
[2025-02-13 18:58:52,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:52,971][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.15960238873958588, acc: 0.9637681245803833)
[2025-02-13 18:58:53,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:53,362][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.3879198133945465, acc: 0.8987341523170471)
[2025-02-13 18:58:53,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:53,765][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.26986244320869446, acc: 0.9241379499435425)
[2025-02-13 18:58:53,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:54,130][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.25753161311149597, acc: 0.9281768202781677)
[2025-02-13 18:58:54,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:54,511][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.21107697486877441, acc: 0.9435897469520569)
[2025-02-13 18:58:54,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:54,872][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.172646626830101, acc: 0.9468085169792175)
[2025-02-13 18:58:55,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:55,228][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.15926136076450348, acc: 0.9679999947547913)
[2025-02-13 18:58:55,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:55,627][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.16022202372550964, acc: 0.9735449552536011)
[2025-02-13 18:58:55,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:56,033][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.27516990900039673, acc: 0.936170220375061)
[2025-02-13 18:58:56,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:56,470][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.3605784475803375, acc: 0.909604549407959)
[2025-02-13 18:58:56,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:56,867][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.20618098974227905, acc: 0.9466666579246521)
[2025-02-13 18:58:57,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:57,261][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.22491636872291565, acc: 0.9345238208770752)
[2025-02-13 18:58:57,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:57,651][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.19178858399391174, acc: 0.9576719403266907)
[2025-02-13 18:58:57,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:58,037][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.12472166866064072, acc: 0.9693251252174377)
[2025-02-13 18:58:58,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:58,447][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.32802078127861023, acc: 0.9411764740943909)
[2025-02-13 18:58:58,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:58,817][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.28827062249183655, acc: 0.9375)
[2025-02-13 18:58:58,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:59,198][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.09405196458101273, acc: 0.9692307710647583)
[2025-02-13 18:58:59,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:59,608][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.2082480490207672, acc: 0.942307710647583)
[2025-02-13 18:58:59,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:59,999][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.1454520970582962, acc: 0.975806474685669)
[2025-02-13 18:59:00,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:00,393][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.20234675705432892, acc: 0.9459459185600281)
[2025-02-13 18:59:00,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:00,814][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.17140637338161469, acc: 0.9560439586639404)
[2025-02-13 18:59:00,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:01,251][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.11598387360572815, acc: 0.9727891087532043)
[2025-02-13 18:59:01,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:01,663][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.07791172713041306, acc: 0.9884393215179443)
[2025-02-13 18:59:01,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:02,074][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.23240022361278534, acc: 0.9537572264671326)
[2025-02-13 18:59:02,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:02,522][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.15509335696697235, acc: 0.9757575988769531)
[2025-02-13 18:59:02,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:02,922][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.1152268797159195, acc: 0.9728260636329651)
[2025-02-13 18:59:03,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:03,304][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.1759764403104782, acc: 0.9583333134651184)
[2025-02-13 18:59:03,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:03,690][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.0634150579571724, acc: 0.9838709831237793)
[2025-02-13 18:59:03,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:04,079][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.1797305941581726, acc: 0.9704142212867737)
[2025-02-13 18:59:04,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:04,451][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.23368746042251587, acc: 0.9555555582046509)
[2025-02-13 18:59:04,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:04,838][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.33225515484809875, acc: 0.9387755393981934)
[2025-02-13 18:59:04,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:05,234][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.1716959923505783, acc: 0.9624060392379761)
[2025-02-13 18:59:05,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:05,613][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.3132438659667969, acc: 0.9448819160461426)
[2025-02-13 18:59:05,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:06,022][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.2172907143831253, acc: 0.9731543660163879)
[2025-02-13 18:59:06,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:06,396][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.4229515790939331, acc: 0.9076923131942749)
[2025-02-13 18:59:06,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:06,829][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.3919087052345276, acc: 0.9290780425071716)
[2025-02-13 18:59:06,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:07,233][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.23299303650856018, acc: 0.9452054500579834)
[2025-02-13 18:59:07,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:07,626][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.2443769872188568, acc: 0.9624999761581421)
[2025-02-13 18:59:07,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:08,068][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.26664963364601135, acc: 0.948051929473877)
[2025-02-13 18:59:08,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:08,506][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.16029421985149384, acc: 0.9532710313796997)
[2025-02-13 18:59:08,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:08,918][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.6037896275520325, acc: 0.9056603908538818)
[2025-02-13 18:59:09,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:09,346][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.6278753876686096, acc: 0.8695651888847351)
[2025-02-13 18:59:09,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:09,733][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.2964317202568054, acc: 0.9266666769981384)
[2025-02-13 18:59:09,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:10,121][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.490939199924469, acc: 0.8602941036224365)
[2025-02-13 18:59:10,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:10,511][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.5582902431488037, acc: 0.8620689511299133)
[2025-02-13 18:59:10,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:10,928][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.46461063623428345, acc: 0.8944723606109619)
[2025-02-13 18:59:11,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:11,318][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.28447288274765015, acc: 0.9301075339317322)
[2025-02-13 18:59:11,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:11,713][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.4302446246147156, acc: 0.9160305261611938)
[2025-02-13 18:59:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:12,096][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.6442100405693054, acc: 0.8442623019218445)
[2025-02-13 18:59:12,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:12,484][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.5900194644927979, acc: 0.8533333539962769)
[2025-02-13 18:59:12,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:12,866][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 0.8376146554946899, acc: 0.798561155796051)
[2025-02-13 18:59:13,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:13,219][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.614863395690918, acc: 0.8759124279022217)
[2025-02-13 18:59:13,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:13,628][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.5290156006813049, acc: 0.88165682554245)
[2025-02-13 18:59:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:13,997][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.5031459331512451, acc: 0.846666693687439)
[2025-02-13 18:59:14,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:14,393][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.54297935962677, acc: 0.8882352709770203)
[2025-02-13 18:59:14,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:14,783][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.3247964382171631, acc: 0.9205297827720642)
[2025-02-13 18:59:14,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:15,170][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.48457586765289307, acc: 0.9144737124443054)
[2025-02-13 18:59:15,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:15,541][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.6019364595413208, acc: 0.862500011920929)
[2025-02-13 18:59:15,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:15,950][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.7430352568626404, acc: 0.846666693687439)
[2025-02-13 18:59:16,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:16,329][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.2799641191959381, acc: 0.9484536051750183)
[2025-02-13 18:59:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:16,721][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.44200557470321655, acc: 0.9230769276618958)
[2025-02-13 18:59:16,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:17,103][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.6243433356285095, acc: 0.8883495330810547)
[2025-02-13 18:59:17,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:17,449][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.5448598265647888, acc: 0.8579545617103577)
[2025-02-13 18:59:17,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:17,827][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.47955286502838135, acc: 0.9150943160057068)
[2025-02-13 18:59:17,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:18,204][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.46372905373573303, acc: 0.9086757898330688)
[2025-02-13 18:59:18,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:18,578][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.5417366027832031, acc: 0.89682537317276)
[2025-02-13 18:59:18,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:19,013][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.7924171686172485, acc: 0.8041236996650696)
[2025-02-13 18:59:19,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:19,393][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.4380433261394501, acc: 0.9011628031730652)
[2025-02-13 18:59:19,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:19,790][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.42106345295906067, acc: 0.9009901285171509)
[2025-02-13 18:59:19,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:20,177][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.6201857328414917, acc: 0.8726415038108826)
[2025-02-13 18:59:20,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:20,607][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.5201966762542725, acc: 0.875)
[2025-02-13 18:59:20,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:21,024][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.46034640073776245, acc: 0.8923766613006592)
[2025-02-13 18:59:21,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:21,422][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.6629045009613037, acc: 0.8597285151481628)
[2025-02-13 18:59:21,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:21,840][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.34853044152259827, acc: 0.9166666865348816)
[2025-02-13 18:59:21,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:22,235][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.3909955620765686, acc: 0.8928571343421936)
[2025-02-13 18:59:22,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:22,634][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.6393802762031555, acc: 0.8647058606147766)
[2025-02-13 18:59:22,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:23,006][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.649854838848114, acc: 0.8741722106933594)
[2025-02-13 18:59:23,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:23,372][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 1.3950873613357544, acc: 0.7255814075469971)
[2025-02-13 18:59:23,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:23,760][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.49451228976249695, acc: 0.8658536672592163)
[2025-02-13 18:59:23,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:24,180][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.5681750178337097, acc: 0.8313953280448914)
[2025-02-13 18:59:24,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:24,584][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.34669890999794006, acc: 0.9180327653884888)
[2025-02-13 18:59:24,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:25,016][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.5664266347885132, acc: 0.8691099286079407)
[2025-02-13 18:59:25,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:25,424][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.5284162759780884, acc: 0.8684210777282715)
[2025-02-13 18:59:25,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:25,847][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.5110858082771301, acc: 0.8661971688270569)
[2025-02-13 18:59:25,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:26,265][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.43884897232055664, acc: 0.9040403962135315)
[2025-02-13 18:59:26,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:26,643][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.6042972803115845, acc: 0.8842105269432068)
[2025-02-13 18:59:26,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:27,026][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.31019923090934753, acc: 0.9156626462936401)
[2025-02-13 18:59:27,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:27,396][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.5730568170547485, acc: 0.8899999856948853)
[2025-02-13 18:59:27,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:27,776][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.3597014546394348, acc: 0.8965517282485962)
[2025-02-13 18:59:27,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:28,169][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.2221529185771942, acc: 0.95333331823349)
[2025-02-13 18:59:28,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:28,548][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.4046942889690399, acc: 0.903030276298523)
[2025-02-13 18:59:28,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:28,925][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.3223516345024109, acc: 0.9448819160461426)
[2025-02-13 18:59:29,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:29,299][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.5298977494239807, acc: 0.8717948794364929)
[2025-02-13 18:59:29,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:29,675][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.4257850646972656, acc: 0.8933333158493042)
[2025-02-13 18:59:29,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:30,046][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.42335546016693115, acc: 0.9172413945198059)
[2025-02-13 18:59:30,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:30,410][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.39424505829811096, acc: 0.9007633328437805)
[2025-02-13 18:59:30,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:30,783][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.24179427325725555, acc: 0.9312977194786072)
[2025-02-13 18:59:30,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:31,194][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.5558899641036987, acc: 0.886904776096344)
[2025-02-13 18:59:31,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:31,617][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.5295304656028748, acc: 0.9038461446762085)
[2025-02-13 18:59:31,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:32,044][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.2765379548072815, acc: 0.926174521446228)
[2025-02-13 18:59:32,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:32,438][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.3240707814693451, acc: 0.9324324131011963)
[2025-02-13 18:59:32,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:32,854][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.12030313163995743, acc: 0.9767441749572754)
[2025-02-13 18:59:32,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:33,279][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.20406971871852875, acc: 0.9612902998924255)
[2025-02-13 18:59:33,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:33,701][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.3312011957168579, acc: 0.9259259104728699)
[2025-02-13 18:59:33,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:34,080][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.26104259490966797, acc: 0.9314285516738892)
[2025-02-13 18:59:34,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:34,456][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.2933286428451538, acc: 0.9236640930175781)
[2025-02-13 18:59:34,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:34,857][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.27873241901397705, acc: 0.9285714030265808)
[2025-02-13 18:59:35,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:35,237][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.4223027527332306, acc: 0.9111111164093018)
[2025-02-13 18:59:35,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:35,616][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.7249073386192322, acc: 0.8560606241226196)
[2025-02-13 18:59:35,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:35,986][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.2950461804866791, acc: 0.9041095972061157)
[2025-02-13 18:59:36,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:36,369][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.3763841986656189, acc: 0.9083333611488342)
[2025-02-13 18:59:36,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:36,785][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.20102941989898682, acc: 0.9496855139732361)
[2025-02-13 18:59:36,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:37,158][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.34916484355926514, acc: 0.9037036895751953)
[2025-02-13 18:59:37,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:37,528][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.1705818474292755, acc: 0.942148745059967)
[2025-02-13 18:59:37,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:37,965][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.26680460572242737, acc: 0.95652174949646)
[2025-02-13 18:59:38,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:38,359][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.20789626240730286, acc: 0.9696969985961914)
[2025-02-13 18:59:38,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:38,723][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.11572839319705963, acc: 0.98591548204422)
[2025-02-13 18:59:38,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:39,150][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.5342134237289429, acc: 0.8500000238418579)
[2025-02-13 18:59:39,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:39,516][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.38965705037117004, acc: 0.9239130616188049)
[2025-02-13 18:59:39,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:39,877][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.2191457748413086, acc: 0.9358288645744324)
[2025-02-13 18:59:40,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:40,230][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.3567682206630707, acc: 0.9015151262283325)
[2025-02-13 18:59:40,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:40,616][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.6507483720779419, acc: 0.8469945192337036)
[2025-02-13 18:59:40,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:40,995][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.37066876888275146, acc: 0.8914728760719299)
[2025-02-13 18:59:41,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:41,397][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.3401947617530823, acc: 0.910179615020752)
[2025-02-13 18:59:41,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:41,787][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.2647189497947693, acc: 0.9437500238418579)
[2025-02-13 18:59:41,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:42,191][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.3322451412677765, acc: 0.9207317233085632)
[2025-02-13 18:59:42,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:42,605][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.3306150734424591, acc: 0.8740741014480591)
[2025-02-13 18:59:42,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:42,986][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.33621248602867126, acc: 0.9119170904159546)
[2025-02-13 18:59:43,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:43,374][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.42918482422828674, acc: 0.8896551728248596)
[2025-02-13 18:59:43,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:43,750][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.21488609910011292, acc: 0.9520958065986633)
[2025-02-13 18:59:43,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:44,179][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.2838096022605896, acc: 0.9117646813392639)
[2025-02-13 18:59:44,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:44,587][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.29705381393432617, acc: 0.9175823926925659)
[2025-02-13 18:59:44,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:45,013][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.33286744356155396, acc: 0.9128205180168152)
[2025-02-13 18:59:45,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:45,427][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.26677992939949036, acc: 0.9398906826972961)
[2025-02-13 18:59:45,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:45,816][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.47423747181892395, acc: 0.9179104566574097)
[2025-02-13 18:59:45,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:46,215][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.3082866966724396, acc: 0.9230769276618958)
[2025-02-13 18:59:46,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:46,597][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.38241878151893616, acc: 0.8928571343421936)
[2025-02-13 18:59:46,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:46,954][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.2954643964767456, acc: 0.93388432264328)
[2025-02-13 18:59:47,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:47,303][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.24572093784809113, acc: 0.9397590160369873)
[2025-02-13 18:59:47,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:47,701][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.24948541820049286, acc: 0.9257425665855408)
[2025-02-13 18:59:47,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:48,098][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.17158012092113495, acc: 0.9736841917037964)
[2025-02-13 18:59:48,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:48,474][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.30006837844848633, acc: 0.9504950642585754)
[2025-02-13 18:59:48,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:48,854][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.25048625469207764, acc: 0.9399999976158142)
[2025-02-13 18:59:48,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:49,219][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.3230896294116974, acc: 0.9152542352676392)
[2025-02-13 18:59:49,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:49,628][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.3722570240497589, acc: 0.9278350472450256)
[2025-02-13 18:59:49,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:50,083][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.27355772256851196, acc: 0.9473684430122375)
[2025-02-13 18:59:50,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:50,489][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.22902047634124756, acc: 0.9496402740478516)
[2025-02-13 18:59:50,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:50,872][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.3103097975254059, acc: 0.9161290526390076)
[2025-02-13 18:59:51,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:51,273][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.34793850779533386, acc: 0.8888888955116272)
[2025-02-13 18:59:51,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:51,691][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.5121176242828369, acc: 0.8618784546852112)
[2025-02-13 18:59:51,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:52,100][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.32987162470817566, acc: 0.942307710647583)
[2025-02-13 18:59:52,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:52,498][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.45767438411712646, acc: 0.8944099545478821)
[2025-02-13 18:59:52,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:52,904][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.6224400401115417, acc: 0.8271604776382446)
[2025-02-13 18:59:53,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:53,305][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.22363252937793732, acc: 0.9492753744125366)
[2025-02-13 18:59:53,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:53,738][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.5347028374671936, acc: 0.8540540337562561)
[2025-02-13 18:59:53,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:54,171][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.278777539730072, acc: 0.9512194991111755)
[2025-02-13 18:59:54,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:54,559][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.5637633204460144, acc: 0.89552241563797)
[2025-02-13 18:59:54,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:54,984][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.39274969696998596, acc: 0.8791208863258362)
[2025-02-13 18:59:55,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:55,356][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.3204818665981293, acc: 0.9363636374473572)
[2025-02-13 18:59:55,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:55,726][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.46747633814811707, acc: 0.8675496578216553)
[2025-02-13 18:59:55,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:56,134][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.2935487926006317, acc: 0.9015544056892395)
[2025-02-13 18:59:56,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:56,495][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.49186062812805176, acc: 0.8911564350128174)
[2025-02-13 18:59:56,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:56,865][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.41238924860954285, acc: 0.9295774698257446)
[2025-02-13 18:59:56,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:57,220][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.34200790524482727, acc: 0.9281045794487)
[2025-02-13 18:59:57,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:57,562][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.3339022994041443, acc: 0.893203854560852)
[2025-02-13 18:59:57,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:57,970][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.49822717905044556, acc: 0.913241982460022)
[2025-02-13 18:59:58,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:58,366][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.27879536151885986, acc: 0.9289940595626831)
[2025-02-13 18:59:58,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:58,732][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.28651973605155945, acc: 0.9306930899620056)
[2025-02-13 18:59:58,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:59,121][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.3373945653438568, acc: 0.9100000262260437)
[2025-02-13 18:59:59,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:59,532][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.30635422468185425, acc: 0.9208633303642273)
[2025-02-13 18:59:59,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:59,923][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.13357345759868622, acc: 0.9577465057373047)
[2025-02-13 19:00:00,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:00,295][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.25769779086112976, acc: 0.9385964870452881)
[2025-02-13 19:00:00,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:00,693][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.3248867392539978, acc: 0.9342105388641357)
[2025-02-13 19:00:00,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:01,084][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.5446986556053162, acc: 0.8888888955116272)
[2025-02-13 19:00:01,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:01,515][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.4047340154647827, acc: 0.9057591557502747)
[2025-02-13 19:00:01,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:01,912][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.41963958740234375, acc: 0.9144384860992432)
[2025-02-13 19:00:02,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:02,319][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.3126200735569, acc: 0.9290322661399841)
[2025-02-13 19:00:02,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:02,711][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.5091176629066467, acc: 0.859375)
[2025-02-13 19:00:02,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:03,068][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.4482711851596832, acc: 0.8775510191917419)
[2025-02-13 19:00:03,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:03,450][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.5057264566421509, acc: 0.9090909361839294)
[2025-02-13 19:00:03,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:03,839][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.14661328494548798, acc: 0.9780219793319702)
[2025-02-13 19:00:03,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:04,228][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.16764730215072632, acc: 0.9668874144554138)
[2025-02-13 19:00:04,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:04,602][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.4489811658859253, acc: 0.8970588445663452)
[2025-02-13 19:00:04,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:04,947][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.2818854749202728, acc: 0.9266055226325989)
[2025-02-13 19:00:05,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:05,334][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.40419304370880127, acc: 0.8817204236984253)
[2025-02-13 19:00:05,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:05,765][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.2549591362476349, acc: 0.9453551769256592)
[2025-02-13 19:00:05,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:06,136][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.24565741419792175, acc: 0.9305555820465088)
[2025-02-13 19:00:06,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:06,515][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.1517075151205063, acc: 0.9723756909370422)
[2025-02-13 19:00:06,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:06,940][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.09761740267276764, acc: 0.9716312289237976)
[2025-02-13 19:00:07,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:07,324][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.09276842325925827, acc: 0.9940119981765747)
[2025-02-13 19:00:07,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:07,713][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.11775671690702438, acc: 0.9750000238418579)
[2025-02-13 19:00:07,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:08,090][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.1200299933552742, acc: 0.9878048896789551)
[2025-02-13 19:00:08,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:08,503][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.2027013599872589, acc: 0.9666666388511658)
[2025-02-13 19:00:08,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:08,911][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.08961965888738632, acc: 0.9733333587646484)
[2025-02-13 19:00:09,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:09,340][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.07616894692182541, acc: 0.9870967864990234)
[2025-02-13 19:00:09,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:09,718][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.2148236334323883, acc: 0.930232584476471)
[2025-02-13 19:00:09,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:10,112][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.10093577206134796, acc: 0.9735449552536011)
[2025-02-13 19:00:10,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:10,509][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.11121240258216858, acc: 0.9659090638160706)
[2025-02-13 19:00:10,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:10,911][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.06610332429409027, acc: 0.9834710955619812)
[2025-02-13 19:00:11,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:11,279][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.10797140747308731, acc: 0.976190447807312)
[2025-02-13 19:00:11,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:11,681][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.17545293271541595, acc: 0.9585798978805542)
[2025-02-13 19:00:11,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:12,094][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.24311238527297974, acc: 0.9453551769256592)
[2025-02-13 19:00:12,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:12,509][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.19827520847320557, acc: 0.9712643623352051)
[2025-02-13 19:00:12,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:12,881][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.06879226118326187, acc: 0.9823529124259949)
[2025-02-13 19:00:13,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:13,273][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.05529361963272095, acc: 0.9777777791023254)
[2025-02-13 19:00:13,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:13,661][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.07683423906564713, acc: 0.9837398529052734)
[2025-02-13 19:00:13,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:14,049][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.12082091718912125, acc: 0.970588207244873)
[2025-02-13 19:00:14,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:14,449][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.3869118094444275, acc: 0.9195402264595032)
[2025-02-13 19:00:14,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:14,816][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.1869923621416092, acc: 0.956204354763031)
[2025-02-13 19:00:14,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:15,181][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.2373637557029724, acc: 0.9438775777816772)
[2025-02-13 19:00:15,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:15,576][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.1693025380373001, acc: 0.9567901492118835)
[2025-02-13 19:00:15,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:16,012][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.3532637655735016, acc: 0.9166666865348816)
[2025-02-13 19:00:16,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:16,439][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.25190070271492004, acc: 0.9147287011146545)
[2025-02-13 19:00:16,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:16,864][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.2725520133972168, acc: 0.9166666865348816)
[2025-02-13 19:00:17,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:17,227][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.2691432237625122, acc: 0.9313725233078003)
[2025-02-13 19:00:17,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:17,617][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.5577913522720337, acc: 0.8928571343421936)
[2025-02-13 19:00:17,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:18,038][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.6464816927909851, acc: 0.8627451062202454)
[2025-02-13 19:00:18,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:18,394][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.5321964621543884, acc: 0.9038461446762085)
[2025-02-13 19:00:18,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:18,770][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.3864647150039673, acc: 0.8712871074676514)
[2025-02-13 19:00:18,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:19,189][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.3818452060222626, acc: 0.8703703880310059)
[2025-02-13 19:00:19,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:19,629][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.1723233312368393, acc: 0.9492753744125366)
[2025-02-13 19:00:19,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:20,099][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.3821941912174225, acc: 0.8999999761581421)
[2025-02-13 19:00:20,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:20,517][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.5670863389968872, acc: 0.8823529481887817)
[2025-02-13 19:00:20,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:20,957][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.5116304755210876, acc: 0.8510638475418091)
[2025-02-13 19:00:21,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:21,391][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.595363974571228, acc: 0.8560000061988831)
[2025-02-13 19:00:21,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:21,867][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.470147043466568, acc: 0.9014084339141846)
[2025-02-13 19:00:22,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:22,257][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.28399237990379333, acc: 0.9300699234008789)
[2025-02-13 19:00:22,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:22,641][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.3964832127094269, acc: 0.8960000276565552)
[2025-02-13 19:00:22,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:23,032][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.5479594469070435, acc: 0.858208954334259)
[2025-02-13 19:00:23,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:23,429][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.34869444370269775, acc: 0.9124087691307068)
[2025-02-13 19:00:23,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:23,813][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.625028669834137, acc: 0.845588207244873)
[2025-02-13 19:00:23,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:24,222][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.42215901613235474, acc: 0.8943089246749878)
[2025-02-13 19:00:24,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:24,563][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.4311881959438324, acc: 0.90625)
[2025-02-13 19:00:24,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:24,934][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.38831230998039246, acc: 0.8867924809455872)
[2025-02-13 19:00:25,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:25,366][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.385128378868103, acc: 0.9453125)
[2025-02-13 19:00:25,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:25,739][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.23357145488262177, acc: 0.9520000219345093)
[2025-02-13 19:00:25,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:26,088][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.7750123739242554, acc: 0.8310810923576355)
[2025-02-13 19:00:26,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:26,477][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.23878218233585358, acc: 0.9306930899620056)
[2025-02-13 19:00:26,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:26,906][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.3664349615573883, acc: 0.9064748287200928)
[2025-02-13 19:00:27,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:27,287][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.2507536709308624, acc: 0.9420289993286133)
[2025-02-13 19:00:27,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:27,644][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.22680708765983582, acc: 0.9333333373069763)
[2025-02-13 19:00:27,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:28,049][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.22106443345546722, acc: 0.9402984976768494)
[2025-02-13 19:00:28,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:28,447][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.21083322167396545, acc: 0.9467455744743347)
[2025-02-13 19:00:28,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:28,835][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.16962864995002747, acc: 0.9583333134651184)
[2025-02-13 19:00:28,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:29,226][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.21777915954589844, acc: 0.9583333134651184)
[2025-02-13 19:00:29,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:29,647][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.14880788326263428, acc: 0.9751552939414978)
[2025-02-13 19:00:29,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:30,019][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.1249435693025589, acc: 0.9825581312179565)
[2025-02-13 19:00:30,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:30,397][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.10769161581993103, acc: 0.9931034445762634)
[2025-02-13 19:00:30,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:30,779][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.07238242030143738, acc: 0.9811320900917053)
[2025-02-13 19:00:30,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:31,215][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.22504191100597382, acc: 0.9281768202781677)
[2025-02-13 19:00:31,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:31,583][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.1793498545885086, acc: 0.9599999785423279)
[2025-02-13 19:00:31,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:31,963][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.17917713522911072, acc: 0.9631578922271729)
[2025-02-13 19:00:32,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:32,337][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.1830589920282364, acc: 0.940119743347168)
[2025-02-13 19:00:32,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:32,696][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.11714401096105576, acc: 0.9735099077224731)
[2025-02-13 19:00:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:33,069][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.26134270429611206, acc: 0.9489051103591919)
[2025-02-13 19:00:33,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:33,454][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.06611234694719315, acc: 0.9830508232116699)
[2025-02-13 19:00:33,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:33,818][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.10248055309057236, acc: 0.9791666865348816)
[2025-02-13 19:00:33,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:34,212][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.11406584084033966, acc: 0.9689922332763672)
[2025-02-13 19:00:34,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:34,602][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.1782357096672058, acc: 0.9772727489471436)
[2025-02-13 19:00:34,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:34,974][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.2545722424983978, acc: 0.9263157844543457)
[2025-02-13 19:00:35,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:35,360][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.09958291053771973, acc: 0.9865771532058716)
[2025-02-13 19:00:35,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:35,754][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.4380820393562317, acc: 0.8975903391838074)
[2025-02-13 19:00:35,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:36,149][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.7184702754020691, acc: 0.8549618124961853)
[2025-02-13 19:00:36,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:36,576][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.5241036415100098, acc: 0.8617886304855347)
[2025-02-13 19:00:36,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:36,969][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.34690728783607483, acc: 0.8901734352111816)
[2025-02-13 19:00:37,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:37,397][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.36644747853279114, acc: 0.9278350472450256)
[2025-02-13 19:00:37,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:37,784][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.39782318472862244, acc: 0.887005627155304)
[2025-02-13 19:00:37,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:38,199][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 1.5512239933013916, acc: 0.6532257795333862)
[2025-02-13 19:00:38,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:38,626][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.31800365447998047, acc: 0.9074074029922485)
[2025-02-13 19:00:38,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:39,078][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.3201811611652374, acc: 0.9280575513839722)
[2025-02-13 19:00:39,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:39,462][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.2081807404756546, acc: 0.9516128897666931)
[2025-02-13 19:00:39,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:39,856][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.3126904368400574, acc: 0.9246231317520142)
[2025-02-13 19:00:39,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:40,223][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.32609617710113525, acc: 0.9313725233078003)
[2025-02-13 19:00:40,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:40,590][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.32569557428359985, acc: 0.89552241563797)
[2025-02-13 19:00:40,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:41,036][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.31630799174308777, acc: 0.9354838728904724)
[2025-02-13 19:00:41,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:41,443][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.3667505979537964, acc: 0.9490740895271301)
[2025-02-13 19:00:41,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:41,805][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.2761019766330719, acc: 0.9354838728904724)
[2025-02-13 19:00:41,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:42,229][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.2735987603664398, acc: 0.9515151381492615)
[2025-02-13 19:00:42,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:42,615][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.1946062445640564, acc: 0.9407407641410828)
[2025-02-13 19:00:42,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:43,007][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.27226123213768005, acc: 0.9378882050514221)
[2025-02-13 19:00:43,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:43,420][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.17829656600952148, acc: 0.9712643623352051)
[2025-02-13 19:00:43,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:43,802][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.16318444907665253, acc: 0.9821428656578064)
[2025-02-13 19:00:43,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:44,180][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.28998857736587524, acc: 0.9506173133850098)
[2025-02-13 19:00:44,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:44,565][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.3061773478984833, acc: 0.9444444179534912)
[2025-02-13 19:00:44,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:44,953][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.2958717942237854, acc: 0.9299362897872925)
[2025-02-13 19:00:45,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:45,349][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.18278726935386658, acc: 0.9545454382896423)
[2025-02-13 19:00:45,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:45,735][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.24537016451358795, acc: 0.9247311949729919)
[2025-02-13 19:00:45,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:46,137][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.8750365376472473, acc: 0.8208954930305481)
[2025-02-13 19:00:46,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:46,529][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.5879468321800232, acc: 0.8579235076904297)
[2025-02-13 19:00:46,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:46,890][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.32546740770339966, acc: 0.9197860956192017)
[2025-02-13 19:00:47,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:47,246][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.2800149917602539, acc: 0.9214659929275513)
[2025-02-13 19:00:47,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:47,614][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.30205363035202026, acc: 0.9236111044883728)
[2025-02-13 19:00:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:48,012][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.7233343720436096, acc: 0.854651153087616)
[2025-02-13 19:00:48,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:48,430][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.4408755600452423, acc: 0.875)
[2025-02-13 19:00:48,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:48,825][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.6859932541847229, acc: 0.8413792848587036)
[2025-02-13 19:00:48,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:49,256][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.39251580834388733, acc: 0.9008264541625977)
[2025-02-13 19:00:49,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:49,660][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.21428030729293823, acc: 0.9378882050514221)
[2025-02-13 19:00:49,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:50,061][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.2199094146490097, acc: 0.9289940595626831)
[2025-02-13 19:00:50,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:50,451][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.19606439769268036, acc: 0.9407407641410828)
[2025-02-13 19:00:50,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:50,843][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.5835028886795044, acc: 0.8934911489486694)
[2025-02-13 19:00:50,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:51,234][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.49531862139701843, acc: 0.8741722106933594)
[2025-02-13 19:00:51,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:51,641][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.35425612330436707, acc: 0.9029850959777832)
[2025-02-13 19:00:51,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:52,037][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.5089494585990906, acc: 0.8692810535430908)
[2025-02-13 19:00:52,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:52,425][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.6064788103103638, acc: 0.8674699068069458)
[2025-02-13 19:00:52,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:52,801][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.31860437989234924, acc: 0.9230769276618958)
[2025-02-13 19:00:52,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:53,237][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.1348307579755783, acc: 0.9720670580863953)
[2025-02-13 19:00:53,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:53,630][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.16420073807239532, acc: 0.9696969985961914)
[2025-02-13 19:00:53,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:54,018][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.19579118490219116, acc: 0.949438214302063)
[2025-02-13 19:00:54,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:54,394][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.11217164993286133, acc: 0.9772727489471436)
[2025-02-13 19:00:54,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:54,756][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.3185732364654541, acc: 0.9171974658966064)
[2025-02-13 19:00:54,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:55,163][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.490798681974411, acc: 0.8999999761581421)
[2025-02-13 19:00:55,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:55,563][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.18469902873039246, acc: 0.9649122953414917)
[2025-02-13 19:00:55,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:55,939][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.16825826466083527, acc: 0.939393937587738)
[2025-02-13 19:00:56,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:56,297][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.216372549533844, acc: 0.9379310607910156)
[2025-02-13 19:00:56,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:56,662][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.34521108865737915, acc: 0.9212121367454529)
[2025-02-13 19:00:56,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:57,032][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.328461229801178, acc: 0.9266666769981384)
[2025-02-13 19:00:57,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:57,402][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.22919900715351105, acc: 0.9241379499435425)
[2025-02-13 19:00:57,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:57,796][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.2298152893781662, acc: 0.9736841917037964)
[2025-02-13 19:00:57,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:58,196][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.12928813695907593, acc: 0.9712643623352051)
[2025-02-13 19:00:58,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:58,574][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.15669235587120056, acc: 0.9613259434700012)
[2025-02-13 19:00:58,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:58,989][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.22695420682430267, acc: 0.949999988079071)
[2025-02-13 19:00:59,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:59,387][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.45912471413612366, acc: 0.8875739574432373)
[2025-02-13 19:00:59,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:59,778][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.14413292706012726, acc: 0.9847328066825867)
[2025-02-13 19:00:59,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:00,183][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.3476736545562744, acc: 0.9397590160369873)
[2025-02-13 19:01:00,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:00,575][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.18141096830368042, acc: 0.9590643048286438)
[2025-02-13 19:01:00,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:00,959][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.21413099765777588, acc: 0.9685534834861755)
[2025-02-13 19:01:01,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:01,336][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.31376519799232483, acc: 0.929729700088501)
[2025-02-13 19:01:01,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:01,726][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.2652701139450073, acc: 0.9441860318183899)
[2025-02-13 19:01:01,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:02,097][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.325751394033432, acc: 0.8928571343421936)
[2025-02-13 19:01:02,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:02,479][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.32660728693008423, acc: 0.9292035102844238)
[2025-02-13 19:01:02,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:02,829][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.21940602362155914, acc: 0.9571428298950195)
[2025-02-13 19:01:02,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:03,224][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.29414331912994385, acc: 0.928909957408905)
[2025-02-13 19:01:03,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:03,615][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.2735104560852051, acc: 0.9392523169517517)
[2025-02-13 19:01:03,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:04,005][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.23829929530620575, acc: 0.9430052042007446)
[2025-02-13 19:01:04,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:04,445][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.19230614602565765, acc: 0.9317073225975037)
[2025-02-13 19:01:04,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:04,892][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.2698683440685272, acc: 0.956250011920929)
[2025-02-13 19:01:05,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:05,333][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.14498843252658844, acc: 0.9722222089767456)
[2025-02-13 19:01:05,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:05,726][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.2645273506641388, acc: 0.9513513445854187)
[2025-02-13 19:01:05,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:06,149][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.213068887591362, acc: 0.9488372206687927)
[2025-02-13 19:01:06,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:06,538][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.3927970230579376, acc: 0.9109588861465454)
[2025-02-13 19:01:06,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:06,955][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.4438351094722748, acc: 0.8875739574432373)
[2025-02-13 19:01:07,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:07,341][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.3445296585559845, acc: 0.918181836605072)
[2025-02-13 19:01:07,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:07,746][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.41550612449645996, acc: 0.8971428275108337)
[2025-02-13 19:01:07,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:08,110][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.20696908235549927, acc: 0.9682539701461792)
[2025-02-13 19:01:08,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:08,471][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.3267417252063751, acc: 0.905063271522522)
[2025-02-13 19:01:08,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:08,840][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.5059194564819336, acc: 0.8679245114326477)
[2025-02-13 19:01:08,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:09,245][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.29850754141807556, acc: 0.921875)
[2025-02-13 19:01:09,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:09,652][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.2677071690559387, acc: 0.9205607771873474)
[2025-02-13 19:01:09,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:10,038][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.21553243696689606, acc: 0.9277777671813965)
[2025-02-13 19:01:10,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:10,415][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.4723234474658966, acc: 0.8903225660324097)
[2025-02-13 19:01:10,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:10,790][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.33276888728141785, acc: 0.9217877388000488)
[2025-02-13 19:01:10,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:11,166][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.2281670868396759, acc: 0.9466666579246521)
[2025-02-13 19:01:11,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:11,548][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.09145329892635345, acc: 0.9944751262664795)
[2025-02-13 19:01:11,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:11,924][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.17469163239002228, acc: 0.9637305736541748)
[2025-02-13 19:01:12,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:12,306][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.2061758190393448, acc: 0.9729729890823364)
[2025-02-13 19:01:12,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:12,670][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.3284435570240021, acc: 0.925000011920929)
[2025-02-13 19:01:12,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:13,057][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.45564714074134827, acc: 0.9017857313156128)
[2025-02-13 19:01:13,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:13,498][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.2775234878063202, acc: 0.938144326210022)
[2025-02-13 19:01:13,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:13,913][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.23966142535209656, acc: 0.9708737730979919)
[2025-02-13 19:01:14,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:14,321][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.19311468303203583, acc: 0.956204354763031)
[2025-02-13 19:01:14,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:14,767][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 1.1763375997543335, acc: 0.779411792755127)
[2025-02-13 19:01:14,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:15,163][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.5744487047195435, acc: 0.8492063283920288)
[2025-02-13 19:01:15,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:15,569][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 1.1216075420379639, acc: 0.7960526347160339)
[2025-02-13 19:01:15,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:15,974][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.3338889479637146, acc: 0.931506872177124)
[2025-02-13 19:01:16,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:16,390][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.6929052472114563, acc: 0.8600000143051147)
[2025-02-13 19:01:16,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:16,786][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.8947452902793884, acc: 0.8299319744110107)
[2025-02-13 19:01:16,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:17,193][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 1.3768222332000732, acc: 0.7964601516723633)
[2025-02-13 19:01:17,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:17,591][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.608822762966156, acc: 0.8695651888847351)
[2025-02-13 19:01:17,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:17,969][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.40743109583854675, acc: 0.9095744490623474)
[2025-02-13 19:01:18,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:18,369][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.6598753333091736, acc: 0.8392857313156128)
[2025-02-13 19:01:18,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:18,771][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 1.1314736604690552, acc: 0.7424242496490479)
[2025-02-13 19:01:18,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:19,190][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.32927462458610535, acc: 0.9333333373069763)
[2025-02-13 19:01:19,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:19,550][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.4441570043563843, acc: 0.891566276550293)
[2025-02-13 19:01:19,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:19,923][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.5377120971679688, acc: 0.8848921060562134)
[2025-02-13 19:01:20,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:20,362][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.47927215695381165, acc: 0.8742856979370117)
[2025-02-13 19:01:20,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:20,754][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.336312472820282, acc: 0.9197530746459961)
[2025-02-13 19:01:20,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:21,184][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.429571270942688, acc: 0.9005848169326782)
[2025-02-13 19:01:21,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:21,586][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.5697624683380127, acc: 0.898809552192688)
[2025-02-13 19:01:21,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:21,955][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.3984373211860657, acc: 0.9090909361839294)
[2025-02-13 19:01:22,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:22,332][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.4226000905036926, acc: 0.905063271522522)
[2025-02-13 19:01:22,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:22,698][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.3184880018234253, acc: 0.9210526347160339)
[2025-02-13 19:01:22,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:23,085][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.21931825578212738, acc: 0.9269663095474243)
[2025-02-13 19:01:23,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:23,474][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.23634368181228638, acc: 0.9238095283508301)
[2025-02-13 19:01:23,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:23,904][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.2380058616399765, acc: 0.925000011920929)
[2025-02-13 19:01:24,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:24,276][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.2754863500595093, acc: 0.9166666865348816)
[2025-02-13 19:01:24,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:24,667][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.6220291256904602, acc: 0.8705882430076599)
[2025-02-13 19:01:24,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:25,043][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.4122004210948944, acc: 0.9012345671653748)
[2025-02-13 19:01:25,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:25,426][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.34449654817581177, acc: 0.9473684430122375)
[2025-02-13 19:01:25,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:25,818][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.16929291188716888, acc: 0.9657142758369446)
[2025-02-13 19:01:25,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:26,196][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.2210586816072464, acc: 0.9470587968826294)
[2025-02-13 19:01:26,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:26,578][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.19320020079612732, acc: 0.9470198750495911)
[2025-02-13 19:01:26,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:27,003][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.1749451458454132, acc: 0.9591836929321289)
[2025-02-13 19:01:27,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:27,436][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.1923615038394928, acc: 0.9675324559211731)
[2025-02-13 19:01:27,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:27,842][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.28564760088920593, acc: 0.9364162087440491)
[2025-02-13 19:01:27,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:28,254][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.2328691929578781, acc: 0.956250011920929)
[2025-02-13 19:01:28,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:28,648][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.33954283595085144, acc: 0.9152542352676392)
[2025-02-13 19:01:28,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:29,037][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.3091220557689667, acc: 0.9222221970558167)
[2025-02-13 19:01:29,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:29,412][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.35290810465812683, acc: 0.9041095972061157)
[2025-02-13 19:01:29,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:29,776][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.3767203092575073, acc: 0.9142857193946838)
[2025-02-13 19:01:29,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:30,178][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.1830340176820755, acc: 0.9644669890403748)
[2025-02-13 19:01:30,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:30,579][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.5223522782325745, acc: 0.8965517282485962)
[2025-02-13 19:01:30,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:30,988][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.3439067602157593, acc: 0.9171597361564636)
[2025-02-13 19:01:31,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:31,347][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.3860926032066345, acc: 0.9019607901573181)
[2025-02-13 19:01:31,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:31,715][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.5055980086326599, acc: 0.8564356565475464)
[2025-02-13 19:01:31,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:32,102][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.4116528034210205, acc: 0.8993710875511169)
[2025-02-13 19:01:32,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:32,528][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.3861342668533325, acc: 0.90055251121521)
[2025-02-13 19:01:32,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:32,953][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.5466720461845398, acc: 0.8829268217086792)
[2025-02-13 19:01:33,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:33,397][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.5885505676269531, acc: 0.8557692170143127)
[2025-02-13 19:01:33,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:33,802][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.32294103503227234, acc: 0.9090909361839294)
[2025-02-13 19:01:33,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:34,211][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.6596308350563049, acc: 0.8685445785522461)
[2025-02-13 19:01:34,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:34,629][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.38558125495910645, acc: 0.940092146396637)
[2025-02-13 19:01:34,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:35,053][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.3929259479045868, acc: 0.9436619877815247)
[2025-02-13 19:01:35,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:35,460][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.18507595360279083, acc: 0.9520958065986633)
[2025-02-13 19:01:35,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:35,867][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.4313662052154541, acc: 0.8923766613006592)
[2025-02-13 19:01:35,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:36,244][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.2765154540538788, acc: 0.9409090876579285)
[2025-02-13 19:01:36,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:36,614][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.24329523742198944, acc: 0.9483568072319031)
[2025-02-13 19:01:36,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:37,013][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.18574358522891998, acc: 0.9516907930374146)
[2025-02-13 19:01:37,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:37,392][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.28584492206573486, acc: 0.9204545617103577)
[2025-02-13 19:01:37,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:37,780][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.25772908329963684, acc: 0.932692289352417)
[2025-02-13 19:01:37,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:38,178][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.5972501039505005, acc: 0.8454106450080872)
[2025-02-13 19:01:38,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:38,590][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.7851883769035339, acc: 0.8392857313156128)
[2025-02-13 19:01:38,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:38,994][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.46089494228363037, acc: 0.8799999952316284)
[2025-02-13 19:01:39,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:39,431][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.3622027635574341, acc: 0.8914285898208618)
[2025-02-13 19:01:39,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:39,849][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.37549206614494324, acc: 0.9054054021835327)
[2025-02-13 19:01:39,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:40,219][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.2580474317073822, acc: 0.9271844625473022)
[2025-02-13 19:01:40,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:40,598][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.349509060382843, acc: 0.9130434989929199)
[2025-02-13 19:01:40,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:40,975][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.3884119987487793, acc: 0.9235668778419495)
[2025-02-13 19:01:41,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:41,391][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.13593830168247223, acc: 0.9745222926139832)
[2025-02-13 19:01:41,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:41,768][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.17535296082496643, acc: 0.9695122241973877)
[2025-02-13 19:01:41,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:42,162][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.2837521433830261, acc: 0.9214285612106323)
[2025-02-13 19:01:42,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:42,561][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.1730964481830597, acc: 0.9689922332763672)
[2025-02-13 19:01:42,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:42,972][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.22464175522327423, acc: 0.9675324559211731)
[2025-02-13 19:01:43,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:43,392][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.16329243779182434, acc: 0.9625668525695801)
[2025-02-13 19:01:43,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:43,809][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.13813276588916779, acc: 0.96875)
[2025-02-13 19:01:43,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:44,214][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.16469064354896545, acc: 0.9663865566253662)
[2025-02-13 19:01:44,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:44,609][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.33891671895980835, acc: 0.9305555820465088)
[2025-02-13 19:01:44,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:45,004][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.13954180479049683, acc: 0.9602649211883545)
[2025-02-13 19:01:45,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:45,388][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.32943370938301086, acc: 0.9166666865348816)
[2025-02-13 19:01:45,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:45,774][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.45824846625328064, acc: 0.8689655065536499)
[2025-02-13 19:01:45,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:46,161][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.29220569133758545, acc: 0.9387755393981934)
[2025-02-13 19:01:46,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:46,577][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.1799554079771042, acc: 0.9636363387107849)
[2025-02-13 19:01:46,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:46,969][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.2574026584625244, acc: 0.9358974099159241)
[2025-02-13 19:01:47,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:47,389][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.3737718164920807, acc: 0.9397590160369873)
[2025-02-13 19:01:47,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:47,771][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.1923656463623047, acc: 0.9515151381492615)
[2025-02-13 19:01:47,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:48,176][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.20350044965744019, acc: 0.9529411792755127)
[2025-02-13 19:01:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:48,546][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.16508890688419342, acc: 0.9523809552192688)
[2025-02-13 19:01:48,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:48,952][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.07125106453895569, acc: 0.9909090995788574)
[2025-02-13 19:01:49,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:49,345][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.14605176448822021, acc: 0.9685039520263672)
[2025-02-13 19:01:49,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:49,744][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.33248043060302734, acc: 0.9027777910232544)
[2025-02-13 19:01:49,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:50,139][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.22848311066627502, acc: 0.9389312863349915)
[2025-02-13 19:01:50,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:50,501][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.2462201714515686, acc: 0.9320987462997437)
[2025-02-13 19:01:50,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:50,886][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.22052527964115143, acc: 0.9518072009086609)
[2025-02-13 19:01:51,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:51,252][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.2266397476196289, acc: 0.9503105878829956)
[2025-02-13 19:01:51,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:51,656][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.2570243775844574, acc: 0.9407894611358643)
[2025-02-13 19:01:51,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:52,057][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.23531851172447205, acc: 0.9337349534034729)
[2025-02-13 19:01:52,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:52,486][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.1859370619058609, acc: 0.9612902998924255)
[2025-02-13 19:01:52,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:52,860][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.19129551947116852, acc: 0.9496402740478516)
[2025-02-13 19:01:53,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:53,290][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.4843437969684601, acc: 0.9083969593048096)
[2025-02-13 19:01:53,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:53,694][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.41155409812927246, acc: 0.9024389982223511)
[2025-02-13 19:01:53,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:54,113][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.426180899143219, acc: 0.8896104097366333)
[2025-02-13 19:01:54,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:54,516][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.4984488785266876, acc: 0.8888888955116272)
[2025-02-13 19:01:54,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:54,883][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.3404562771320343, acc: 0.915032684803009)
[2025-02-13 19:01:55,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:55,260][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.3410796523094177, acc: 0.9102563858032227)
[2025-02-13 19:01:55,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:55,650][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.2901940643787384, acc: 0.9285714030265808)
[2025-02-13 19:01:55,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:56,046][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.24233737587928772, acc: 0.9312499761581421)
[2025-02-13 19:01:56,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:56,450][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.3590467870235443, acc: 0.9484536051750183)
[2025-02-13 19:01:56,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:56,870][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.33793190121650696, acc: 0.9024389982223511)
[2025-02-13 19:01:57,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:57,298][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.18324072659015656, acc: 0.9506173133850098)
[2025-02-13 19:01:57,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:57,700][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.2298673391342163, acc: 0.9545454382896423)
[2025-02-13 19:01:57,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:58,103][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.282060831785202, acc: 0.936170220375061)
[2025-02-13 19:01:58,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:58,518][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.37988898158073425, acc: 0.9280575513839722)
[2025-02-13 19:01:58,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:58,947][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.19586995244026184, acc: 0.9626865386962891)
[2025-02-13 19:01:59,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:59,365][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.25421661138534546, acc: 0.9571428298950195)
[2025-02-13 19:01:59,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:59,766][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.19518618285655975, acc: 0.9496855139732361)
[2025-02-13 19:01:59,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:00,130][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.3164384365081787, acc: 0.9074074029922485)
[2025-02-13 19:02:00,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:00,528][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.5072894096374512, acc: 0.8899082541465759)
[2025-02-13 19:02:00,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:00,900][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.18221649527549744, acc: 0.9627329111099243)
[2025-02-13 19:02:01,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:01,270][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.6863889098167419, acc: 0.8582677245140076)
[2025-02-13 19:02:01,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:01,641][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.7561790943145752, acc: 0.8260869383811951)
[2025-02-13 19:02:01,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:02,007][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.3216985762119293, acc: 0.9130434989929199)
[2025-02-13 19:02:02,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:02,387][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.3186974823474884, acc: 0.9144737124443054)
[2025-02-13 19:02:02,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:02,779][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.4017663598060608, acc: 0.8742138147354126)
[2025-02-13 19:02:02,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:03,168][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.6469609141349792, acc: 0.8454545736312866)
[2025-02-13 19:02:03,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:03,554][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.4972592294216156, acc: 0.893081784248352)
[2025-02-13 19:02:03,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:03,946][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.3609425127506256, acc: 0.8965517282485962)
[2025-02-13 19:02:04,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:04,319][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.4021226465702057, acc: 0.9096774458885193)
[2025-02-13 19:02:04,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:04,688][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.26327258348464966, acc: 0.9624413251876831)
[2025-02-13 19:02:04,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:05,061][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.3755185604095459, acc: 0.93034827709198)
[2025-02-13 19:02:05,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:05,426][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.14394038915634155, acc: 0.9690265655517578)
[2025-02-13 19:02:05,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:05,779][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.20389671623706818, acc: 0.9648241400718689)
[2025-02-13 19:02:05,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:06,149][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.2957218885421753, acc: 0.949999988079071)
[2025-02-13 19:02:06,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:06,549][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.36564233899116516, acc: 0.9059829115867615)
[2025-02-13 19:02:06,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:06,949][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.3149281144142151, acc: 0.9289617538452148)
[2025-02-13 19:02:07,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:07,372][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.18699732422828674, acc: 0.9527897238731384)
[2025-02-13 19:02:07,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:07,800][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.2263428419828415, acc: 0.9438775777816772)
[2025-02-13 19:02:07,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:08,230][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.3138898015022278, acc: 0.9058296084403992)
[2025-02-13 19:02:08,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:08,639][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.21492062509059906, acc: 0.930232584476471)
[2025-02-13 19:02:08,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:09,053][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.1927109658718109, acc: 0.9502074718475342)
[2025-02-13 19:02:09,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:09,439][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.20744988322257996, acc: 0.9520000219345093)
[2025-02-13 19:02:09,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:09,809][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.26186805963516235, acc: 0.9375)
[2025-02-13 19:02:09,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:10,193][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.31791889667510986, acc: 0.939393937587738)
[2025-02-13 19:02:10,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:10,556][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.1639232337474823, acc: 0.9573459625244141)
[2025-02-13 19:02:10,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:10,951][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.19904901087284088, acc: 0.9531915187835693)
[2025-02-13 19:02:11,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:11,321][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.2208767682313919, acc: 0.9399999976158142)
[2025-02-13 19:02:11,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:11,681][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.20129506289958954, acc: 0.9570552110671997)
[2025-02-13 19:02:11,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:12,108][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.15313754975795746, acc: 0.9537814855575562)
[2025-02-13 19:02:12,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:12,518][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.19638711214065552, acc: 0.9343434572219849)
[2025-02-13 19:02:12,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:12,914][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.17094174027442932, acc: 0.9596412777900696)
[2025-02-13 19:02:13,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:13,309][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.24670319259166718, acc: 0.9351145029067993)
[2025-02-13 19:02:13,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:13,691][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.22615748643875122, acc: 0.9543726444244385)
[2025-02-13 19:02:13,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:14,067][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.2019464075565338, acc: 0.9438775777816772)
[2025-02-13 19:02:14,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:14,454][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.38723206520080566, acc: 0.9064327478408813)
[2025-02-13 19:02:14,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:14,827][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.30275487899780273, acc: 0.9378882050514221)
[2025-02-13 19:02:14,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:15,167][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.39231324195861816, acc: 0.9342105388641357)
[2025-02-13 19:02:15,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:15,592][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.4698764383792877, acc: 0.8767123222351074)
[2025-02-13 19:02:15,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:15,994][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.4531401991844177, acc: 0.8525640964508057)
[2025-02-13 19:02:16,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:16,411][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.4431415796279907, acc: 0.914893627166748)
[2025-02-13 19:02:16,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:16,827][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.3601106107234955, acc: 0.9281437397003174)
[2025-02-13 19:02:16,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:17,243][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.38587456941604614, acc: 0.8963414430618286)
[2025-02-13 19:02:17,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:17,644][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.3550548255443573, acc: 0.8999999761581421)
[2025-02-13 19:02:17,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:18,014][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.3368687331676483, acc: 0.9308176040649414)
[2025-02-13 19:02:18,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:18,421][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.3054700493812561, acc: 0.9383561611175537)
[2025-02-13 19:02:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:18,859][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.3310324251651764, acc: 0.9172413945198059)
[2025-02-13 19:02:19,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:19,238][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.2646787762641907, acc: 0.914893627166748)
[2025-02-13 19:02:19,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:19,629][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.24592536687850952, acc: 0.9515151381492615)
[2025-02-13 19:02:19,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:19,999][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.17073829472064972, acc: 0.9624060392379761)
[2025-02-13 19:02:20,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:20,392][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.1364927738904953, acc: 0.9646017551422119)
[2025-02-13 19:02:20,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:20,752][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.2187168449163437, acc: 0.931506872177124)
[2025-02-13 19:02:20,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:21,136][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.41778653860092163, acc: 0.8709677457809448)
[2025-02-13 19:02:21,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:21,492][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.23040062189102173, acc: 0.9292035102844238)
[2025-02-13 19:02:21,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:21,864][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.26035159826278687, acc: 0.936170220375061)
[2025-02-13 19:02:22,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:22,225][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.1893579065799713, acc: 0.9512194991111755)
[2025-02-13 19:02:22,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:22,635][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.23014242947101593, acc: 0.9455782175064087)
[2025-02-13 19:02:22,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:23,026][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.41859307885169983, acc: 0.9172413945198059)
[2025-02-13 19:02:23,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:23,424][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.2967331111431122, acc: 0.924369752407074)
[2025-02-13 19:02:23,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:23,778][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.3426688611507416, acc: 0.9172932505607605)
[2025-02-13 19:02:23,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:24,186][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.22599303722381592, acc: 0.9440559148788452)
[2025-02-13 19:02:24,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:24,592][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.1319490522146225, acc: 0.9790209531784058)
[2025-02-13 19:02:24,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:24,976][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.23743082582950592, acc: 0.9402984976768494)
[2025-02-13 19:02:25,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:25,363][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.44289302825927734, acc: 0.89552241563797)
[2025-02-13 19:02:25,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:25,744][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.4235908091068268, acc: 0.9066666960716248)
[2025-02-13 19:02:25,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:26,142][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.34716010093688965, acc: 0.9341317415237427)
[2025-02-13 19:02:26,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:26,544][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.5246117115020752, acc: 0.8940397500991821)
[2025-02-13 19:02:26,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:26,953][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.6670650839805603, acc: 0.8475610017776489)
[2025-02-13 19:02:27,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:27,395][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.4595021605491638, acc: 0.9039999842643738)
[2025-02-13 19:02:27,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:27,785][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.31125468015670776, acc: 0.9571428298950195)
[2025-02-13 19:02:27,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:28,178][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.4033787250518799, acc: 0.9396551847457886)
[2025-02-13 19:02:28,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:28,571][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.2895006835460663, acc: 0.9185185432434082)
[2025-02-13 19:02:28,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:28,998][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.5022834539413452, acc: 0.846666693687439)
[2025-02-13 19:02:29,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:29,468][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.4674873948097229, acc: 0.8648648858070374)
[2025-02-13 19:02:29,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:29,874][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.3469372093677521, acc: 0.9274193644523621)
[2025-02-13 19:02:30,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:30,294][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.2350098043680191, acc: 0.9365079402923584)
[2025-02-13 19:02:30,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:30,732][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.4130357503890991, acc: 0.9047619104385376)
[2025-02-13 19:02:30,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:31,152][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.30785974860191345, acc: 0.9290780425071716)
[2025-02-13 19:02:31,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:31,535][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.23096416890621185, acc: 0.9622641801834106)
[2025-02-13 19:02:31,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:31,888][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.516819417476654, acc: 0.9210526347160339)
[2025-02-13 19:02:32,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:32,234][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.2317122220993042, acc: 0.9379844665527344)
[2025-02-13 19:02:32,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:32,609][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.5643031001091003, acc: 0.9059829115867615)
[2025-02-13 19:02:32,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:33,006][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.34584227204322815, acc: 0.9349112510681152)
[2025-02-13 19:02:33,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:33,394][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.611860454082489, acc: 0.8461538553237915)
[2025-02-13 19:02:33,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:33,777][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.29504311084747314, acc: 0.90625)
[2025-02-13 19:02:33,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:34,186][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.24008607864379883, acc: 0.939393937587738)
[2025-02-13 19:02:34,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:34,570][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.5842905640602112, acc: 0.8888888955116272)
[2025-02-13 19:02:34,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:34,983][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.3600977957248688, acc: 0.9296875)
[2025-02-13 19:02:35,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:35,430][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.19104669988155365, acc: 0.9253731369972229)
[2025-02-13 19:02:35,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:35,831][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.13552570343017578, acc: 0.9677419066429138)
[2025-02-13 19:02:35,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:36,185][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.20858162641525269, acc: 0.9557521939277649)
[2025-02-13 19:02:36,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:36,559][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.18390031158924103, acc: 0.9615384340286255)
[2025-02-13 19:02:36,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:36,968][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.3388911187648773, acc: 0.9189189076423645)
[2025-02-13 19:02:37,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:37,365][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.23636843264102936, acc: 0.9532710313796997)
[2025-02-13 19:02:37,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:37,788][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.7182521820068359, acc: 0.8554216623306274)
[2025-02-13 19:02:37,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:38,192][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.20100809633731842, acc: 0.9351851940155029)
[2025-02-13 19:02:38,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:38,618][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.44419577717781067, acc: 0.8990825414657593)
[2025-02-13 19:02:38,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:39,025][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.3815666437149048, acc: 0.9096385836601257)
[2025-02-13 19:02:39,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:39,442][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.16765035688877106, acc: 0.9536423683166504)
[2025-02-13 19:02:39,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:39,809][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.3433006703853607, acc: 0.9113923907279968)
[2025-02-13 19:02:39,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:40,208][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.3080374002456665, acc: 0.9333333373069763)
[2025-02-13 19:02:40,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:40,617][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.11066250503063202, acc: 0.9776536226272583)
[2025-02-13 19:02:40,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:41,020][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.27918508648872375, acc: 0.9248120188713074)
[2025-02-13 19:02:41,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:41,398][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.09826405346393585, acc: 0.9797297120094299)
[2025-02-13 19:02:41,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:41,803][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.20392648875713348, acc: 0.95652174949646)
[2025-02-13 19:02:41,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:42,224][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.34618380665779114, acc: 0.9139072895050049)
[2025-02-13 19:02:42,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:42,649][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.3600102663040161, acc: 0.9387755393981934)
[2025-02-13 19:02:42,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:43,018][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.25311097502708435, acc: 0.932330846786499)
[2025-02-13 19:02:43,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:43,395][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.23923705518245697, acc: 0.9411764740943909)
[2025-02-13 19:02:43,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:43,807][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.18823640048503876, acc: 0.957446813583374)
[2025-02-13 19:02:43,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:44,201][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.3261244595050812, acc: 0.9268292784690857)
[2025-02-13 19:02:44,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:44,584][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.26361799240112305, acc: 0.9470587968826294)
[2025-02-13 19:02:44,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:44,974][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.3494856655597687, acc: 0.9476439952850342)
[2025-02-13 19:02:45,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:45,437][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.34566324949264526, acc: 0.9408283829689026)
[2025-02-13 19:02:45,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:45,825][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.3998267650604248, acc: 0.9166666865348816)
[2025-02-13 19:02:45,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:46,221][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.4593445360660553, acc: 0.9212121367454529)
[2025-02-13 19:02:46,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:46,610][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.3752669095993042, acc: 0.9139072895050049)
[2025-02-13 19:02:46,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:47,006][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.399397611618042, acc: 0.9215686321258545)
[2025-02-13 19:02:47,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:47,391][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.15024234354496002, acc: 0.9738562107086182)
[2025-02-13 19:02:47,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:47,769][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.36482563614845276, acc: 0.8999999761581421)
[2025-02-13 19:02:47,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:48,208][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.174592062830925, acc: 0.9586777091026306)
[2025-02-13 19:02:48,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:48,603][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.15141825377941132, acc: 0.970588207244873)
[2025-02-13 19:02:48,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:49,033][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.2059321105480194, acc: 0.9539473652839661)
[2025-02-13 19:02:49,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:49,431][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.12853601574897766, acc: 0.9836065769195557)
[2025-02-13 19:02:49,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:49,818][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.2826365828514099, acc: 0.9214285612106323)
[2025-02-13 19:02:49,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:50,185][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.30056703090667725, acc: 0.9320388436317444)
[2025-02-13 19:02:50,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:50,544][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.42064169049263, acc: 0.8698630332946777)
[2025-02-13 19:02:50,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:50,938][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.4217160642147064, acc: 0.8870967626571655)
[2025-02-13 19:02:51,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:51,333][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.3308710753917694, acc: 0.8954248428344727)
[2025-02-13 19:02:51,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:51,769][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.4490126669406891, acc: 0.8961039185523987)
[2025-02-13 19:02:51,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:52,212][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.5272923111915588, acc: 0.887005627155304)
[2025-02-13 19:02:52,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:52,666][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.15508587658405304, acc: 0.9642857313156128)
[2025-02-13 19:02:52,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:53,137][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.29484644532203674, acc: 0.9266666769981384)
[2025-02-13 19:02:53,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:53,560][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.2522236108779907, acc: 0.9383561611175537)
[2025-02-13 19:02:53,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:53,976][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.36109301447868347, acc: 0.9090909361839294)
[2025-02-13 19:02:54,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:54,416][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.33001601696014404, acc: 0.9142857193946838)
[2025-02-13 19:02:54,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:54,834][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.23166523873806, acc: 0.936170220375061)
[2025-02-13 19:02:54,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:55,231][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.13527625799179077, acc: 0.9621211886405945)
[2025-02-13 19:02:55,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:55,652][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.14685122668743134, acc: 0.9794520735740662)
[2025-02-13 19:02:55,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:56,057][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.22185218334197998, acc: 0.931034505367279)
[2025-02-13 19:02:56,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:56,452][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.0721849650144577, acc: 0.987261176109314)
[2025-02-13 19:02:56,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:56,852][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.34818580746650696, acc: 0.9182389974594116)
[2025-02-13 19:02:56,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:57,254][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.24767202138900757, acc: 0.9557521939277649)
[2025-02-13 19:02:57,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:57,718][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.17155329883098602, acc: 0.9580838084220886)
[2025-02-13 19:02:57,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:58,121][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.12922537326812744, acc: 0.9731543660163879)
[2025-02-13 19:02:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:58,524][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.15806886553764343, acc: 0.9615384340286255)
[2025-02-13 19:02:59,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:00,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:00,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:00,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:01,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:01,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:01,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:02,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:02,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:03,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:03,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:03,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:04,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:04,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:05,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:05,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:05,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:06,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:06,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:06,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:07,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:07,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:07,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:08,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:08,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:08,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:09,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:09,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:09,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:10,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:10,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:11,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:11,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:11,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:12,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:12,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:13,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:13,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:13,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:14,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:14,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:14,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:15,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:15,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:15,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:16,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:16,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:17,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:17,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:17,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:18,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:18,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:18,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:19,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:20,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:20,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:20,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:21,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:21,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:21,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:22,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:22,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:23,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:23,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:23,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:24,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:24,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:24,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:25,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:25,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:25,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:26,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:26,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:26,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:27,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:27,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:28,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:28,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:29,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:29,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:30,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:30,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:30,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:31,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:31,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:32,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:32,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:32,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:33,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:33,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:34,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:34,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:34,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:35,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:35,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:36,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:36,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:36,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:37,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:37,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:37,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:38,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:38,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:38,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:39,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:39,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:39,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:40,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:40,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:40,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:41,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:41,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:41,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:42,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:42,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:42,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:43,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:43,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:44,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:44,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:44,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:45,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:45,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:45,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:46,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:46,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:48,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:48,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:49,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:49,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:49,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:50,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:50,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:50,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:51,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:51,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:51,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:52,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:52,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:54,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:54,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:55,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:55,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:55,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:56,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:56,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:57,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:57,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:57,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:58,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:58,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:58,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:59,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:59,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:00,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:00,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:00,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:01,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:01,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:02,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:02,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:02,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:03,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:03,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:04,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:04,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:04,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:05,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:05,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:05,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:06,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:06,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:07,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:07,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:07,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:08,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:08,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:08,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:09,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:09,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:10,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:10,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:10,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:11,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:11,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:11,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:12,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:12,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:13,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:13,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:13,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:14,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:14,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:14,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:15,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:15,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:16,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:16,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:16,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:17,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:17,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:17,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:18,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:18,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:19,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:19,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:20,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:20,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:20,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:21,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:21,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:21,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:22,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:22,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:23,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:23,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:23,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:24,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:24,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:24,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:25,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:25,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:26,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:26,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:26,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:27,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:27,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:27,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:28,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:28,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:28,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:29,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:29,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:30,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:30,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:30,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:31,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:31,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:32,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:32,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:33,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:33,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:33,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:34,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:34,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:34,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:35,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:35,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:36,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:36,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:36,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:37,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:37,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:37,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:38,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:38,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:38,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:39,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:39,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:39,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:40,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:40,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:41,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:41,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:42,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:42,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:42,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:43,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:43,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:43,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:44,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:44,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:45,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:45,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:46,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:46,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:46,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:47,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:47,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:48,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:48,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:49,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:49,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:49,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:50,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:50,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:51,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:51,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:52,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:52,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:52,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:53,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:53,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:53,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:54,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:54,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:54,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:55,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:55,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:55,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:56,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:56,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:57,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:57,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:57,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:58,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:58,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:59,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:59,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:59,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:00,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:00,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:00,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:01,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:01,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:02,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:02,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:02,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:03,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:03,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:04,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:04,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:05,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:05,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:05,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:06,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:06,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:06,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:07,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:07,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:07,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:08,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:08,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:09,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:09,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:10,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:10,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:10,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:11,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:11,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:11,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:12,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:12,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:13,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:13,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:13,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:14,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:14,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:15,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:15,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:16,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:16,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:17,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:17,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:18,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:18,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:18,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:19,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:19,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:20,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:20,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:20,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:21,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:21,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:22,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:22,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:22,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:23,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:23,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:23,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:24,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:24,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:24,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:25,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:25,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:26,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:26,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:26,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:27,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:27,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:27,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:28,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:28,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:28,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:29,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:29,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:29,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:30,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:30,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:31,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:31,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:31,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:32,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:32,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:32,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:33,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:33,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:33,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:34,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:34,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:34,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:35,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:35,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:35,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:36,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:36,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:38,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:38,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:38,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:39,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:39,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:39,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:40,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:40,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:41,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:41,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:41,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:42,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:42,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:42,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:43,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:43,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:43,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:44,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:44,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:44,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:45,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:45,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:45,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:46,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:46,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:46,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:47,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:47,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:48,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:48,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:49,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:49,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:49,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:50,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:50,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:50,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:51,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:51,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:51,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:52,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:52,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:52,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:53,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:53,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:53,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:54,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:54,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:54,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:55,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:55,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:55,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:56,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:56,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:56,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:57,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:57,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:58,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:58,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:58,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:59,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:59,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:00,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:00,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:00,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:01,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:01,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:01,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:02,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:02,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:03,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:03,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:03,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:04,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:04,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:04,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:05,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:05,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:05,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:06,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:06,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:07,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:08,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:08,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:08,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:09,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:09,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:09,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:10,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:10,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:10,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:11,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:11,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:12,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:12,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:12,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:13,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:13,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:13,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:14,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:14,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:14,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:16,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:16,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:16,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:17,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:17,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:17,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:18,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:18,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:19,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:19,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:19,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:20,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:20,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:20,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:21,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:21,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:21,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:22,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:22,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:22,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:23,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:23,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:23,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:24,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:24,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:25,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:25,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:25,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:26,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:26,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:27,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:27,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:27,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:28,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:28,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:30,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:30,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:31,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:31,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:31,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:32,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:32,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:32,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:33,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:33,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:34,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:34,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:34,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:35,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:35,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:36,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:36,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:36,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:37,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:37,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:38,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:38,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:38,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:39,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:39,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:39,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:40,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:40,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:40,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:41,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:41,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:41,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:42,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:42,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:42,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:43,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:43,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:43,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:44,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:44,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:45,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:45,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:45,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:46,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:46,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:47,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:47,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:48,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:48,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:48,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:49,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:49,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:50,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:50,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:50,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:51,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:51,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:52,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:52,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:53,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:53,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:54,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:54,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:54,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:55,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:55,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:56,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:56,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:57,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:57,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:58,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:58,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:58,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:59,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:59,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:59,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:00,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:00,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:00,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:01,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:01,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:02,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:02,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:02,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:03,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:03,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:03,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:04,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:04,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:05,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:05,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:05,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:06,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:06,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:06,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:07,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:07,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:07,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:08,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:08,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:08,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:09,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:09,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:10,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:10,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:10,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:11,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:11,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:12,413][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4500, device='cuda:0') eval_epoch_loss=tensor(0.3716, device='cuda:0') eval_epoch_acc=tensor(0.9162, device='cuda:0')
[2025-02-13 19:07:12,415][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:07:12,415][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:07:12,741][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_1783_loss_0.3715682625770569/model.pt
[2025-02-13 19:07:12,745][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:07:12,745][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.3715682625770569
[2025-02-13 19:07:12,746][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9161514639854431
[2025-02-13 19:07:12,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:13,209][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.30213144421577454, acc: 0.9248554706573486)
[2025-02-13 19:07:13,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:13,620][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.16688436269760132, acc: 0.9871794581413269)
[2025-02-13 19:07:13,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:14,020][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.34097298979759216, acc: 0.9156626462936401)
[2025-02-13 19:07:14,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:14,414][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.1905331164598465, acc: 0.948387086391449)
[2025-02-13 19:07:14,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:14,829][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.3085114061832428, acc: 0.9253731369972229)
[2025-02-13 19:07:14,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:15,232][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.27067747712135315, acc: 0.931034505367279)
[2025-02-13 19:07:15,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:15,635][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.24074873328208923, acc: 0.9407894611358643)
[2025-02-13 19:07:15,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:16,049][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.5234473943710327, acc: 0.896774172782898)
[2025-02-13 19:07:16,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:16,424][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.6357429027557373, acc: 0.8650306463241577)
[2025-02-13 19:07:16,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:16,806][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 0.9629269242286682, acc: 0.826815664768219)
[2025-02-13 19:07:16,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:17,193][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.7836556434631348, acc: 0.8639053106307983)
[2025-02-13 19:07:17,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:17,569][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.5088650584220886, acc: 0.8881579041481018)
[2025-02-13 19:07:17,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:17,950][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.8813314437866211, acc: 0.8187500238418579)
[2025-02-13 19:07:18,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:18,333][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.6667386293411255, acc: 0.8418079018592834)
[2025-02-13 19:07:18,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:18,706][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.6121185421943665, acc: 0.8742856979370117)
[2025-02-13 19:07:18,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:19,085][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.5490043759346008, acc: 0.8527131676673889)
[2025-02-13 19:07:19,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:19,455][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.5798896551132202, acc: 0.868686854839325)
[2025-02-13 19:07:19,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:19,827][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.529543399810791, acc: 0.8761904835700989)
[2025-02-13 19:07:19,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:20,202][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.4372800588607788, acc: 0.9099099040031433)
[2025-02-13 19:07:20,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:20,589][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.40867045521736145, acc: 0.9095744490623474)
[2025-02-13 19:07:20,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:20,976][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.5073217749595642, acc: 0.8475610017776489)
[2025-02-13 19:07:21,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:21,340][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.20395177602767944, acc: 0.9523809552192688)
[2025-02-13 19:07:21,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:21,755][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.27890485525131226, acc: 0.9239766001701355)
[2025-02-13 19:07:21,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:22,153][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.23962664604187012, acc: 0.9508196711540222)
[2025-02-13 19:07:22,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:22,545][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.33321478962898254, acc: 0.9216867685317993)
[2025-02-13 19:07:22,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:22,963][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.17342880368232727, acc: 0.9604519605636597)
[2025-02-13 19:07:23,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:23,357][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.1665414422750473, acc: 0.9479768872261047)
[2025-02-13 19:07:23,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:23,730][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.16070589423179626, acc: 0.9615384340286255)
[2025-02-13 19:07:23,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:24,097][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.3261719346046448, acc: 0.9042553305625916)
[2025-02-13 19:07:24,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:24,462][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.3147548735141754, acc: 0.9435028433799744)
[2025-02-13 19:07:24,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:24,837][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.3516824543476105, acc: 0.9060773253440857)
[2025-02-13 19:07:24,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:25,238][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.22438278794288635, acc: 0.950276255607605)
[2025-02-13 19:07:25,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:25,653][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.36499080061912537, acc: 0.9044585824012756)
[2025-02-13 19:07:25,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:26,041][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.39177995920181274, acc: 0.8680555820465088)
[2025-02-13 19:07:26,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:26,456][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.23452989757061005, acc: 0.9427083134651184)
[2025-02-13 19:07:26,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:26,896][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.32556942105293274, acc: 0.9424083828926086)
[2025-02-13 19:07:27,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:27,327][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.2366437017917633, acc: 0.9534883499145508)
[2025-02-13 19:07:27,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:27,758][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.2252001166343689, acc: 0.9433962106704712)
[2025-02-13 19:07:27,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:28,125][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.19167470932006836, acc: 0.9345794320106506)
[2025-02-13 19:07:28,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:28,503][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.19571362435817719, acc: 0.9539473652839661)
[2025-02-13 19:07:28,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:28,911][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.24496492743492126, acc: 0.9534883499145508)
[2025-02-13 19:07:29,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:29,310][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.1336156129837036, acc: 0.9781420826911926)
[2025-02-13 19:07:29,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:29,697][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.17357683181762695, acc: 0.9626168012619019)
[2025-02-13 19:07:29,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:30,080][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.17031410336494446, acc: 0.9649122953414917)
[2025-02-13 19:07:30,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:30,485][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.23838849365711212, acc: 0.9620853066444397)
[2025-02-13 19:07:30,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:30,887][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.14975589513778687, acc: 0.9555555582046509)
[2025-02-13 19:07:31,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:31,291][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.18716251850128174, acc: 0.9462365508079529)
[2025-02-13 19:07:31,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:31,699][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.18381637334823608, acc: 0.9585492014884949)
[2025-02-13 19:07:31,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:32,084][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.16187599301338196, acc: 0.9696969985961914)
[2025-02-13 19:07:32,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:32,470][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.5305936336517334, acc: 0.8837209343910217)
[2025-02-13 19:07:32,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:32,887][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.9483636617660522, acc: 0.8536585569381714)
[2025-02-13 19:07:33,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:33,305][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.2026626318693161, acc: 0.942307710647583)
[2025-02-13 19:07:33,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:33,757][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.4427192509174347, acc: 0.8961039185523987)
[2025-02-13 19:07:33,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:34,182][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.2943285405635834, acc: 0.9304812550544739)
[2025-02-13 19:07:34,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:34,571][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.29466456174850464, acc: 0.9120879173278809)
[2025-02-13 19:07:34,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:34,947][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.2439362108707428, acc: 0.9449541568756104)
[2025-02-13 19:07:35,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:35,366][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.248061403632164, acc: 0.9340659379959106)
[2025-02-13 19:07:35,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:35,753][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.29104578495025635, acc: 0.9303797483444214)
[2025-02-13 19:07:35,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:36,120][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.5222323536872864, acc: 0.8727272748947144)
[2025-02-13 19:07:36,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:36,519][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.3444085121154785, acc: 0.9085366129875183)
[2025-02-13 19:07:36,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:36,949][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.35201212763786316, acc: 0.9107142686843872)
[2025-02-13 19:07:37,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:37,368][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.18648980557918549, acc: 0.936170220375061)
[2025-02-13 19:07:37,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:37,758][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.22330309450626373, acc: 0.9130434989929199)
[2025-02-13 19:07:37,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:38,162][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.3112567365169525, acc: 0.9166666865348816)
[2025-02-13 19:07:38,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:38,532][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.50889652967453, acc: 0.8636363744735718)
[2025-02-13 19:07:38,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:38,908][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.47530850768089294, acc: 0.8829787373542786)
[2025-02-13 19:07:39,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:39,305][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.3578428626060486, acc: 0.8873239159584045)
[2025-02-13 19:07:39,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:39,698][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.4235045313835144, acc: 0.8556700944900513)
[2025-02-13 19:07:39,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:40,069][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.4929271340370178, acc: 0.8769230842590332)
[2025-02-13 19:07:40,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:40,469][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.5728996992111206, acc: 0.887499988079071)
[2025-02-13 19:07:40,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:40,905][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.3450286388397217, acc: 0.9045225977897644)
[2025-02-13 19:07:41,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:41,283][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.39529871940612793, acc: 0.8914027214050293)
[2025-02-13 19:07:41,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:41,647][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.2690916657447815, acc: 0.9526066184043884)
[2025-02-13 19:07:41,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:42,031][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.272641658782959, acc: 0.918367326259613)
[2025-02-13 19:07:42,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:42,477][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.5388922691345215, acc: 0.8872548937797546)
[2025-02-13 19:07:42,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:42,865][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.2141215205192566, acc: 0.9438775777816772)
[2025-02-13 19:07:43,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:43,283][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.26573988795280457, acc: 0.9186602830886841)
[2025-02-13 19:07:43,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:43,646][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.2149941772222519, acc: 0.9508196711540222)
[2025-02-13 19:07:43,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:44,005][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.20669768750667572, acc: 0.9581151604652405)
[2025-02-13 19:07:44,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:44,371][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.21989287436008453, acc: 0.9523809552192688)
[2025-02-13 19:07:44,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:44,781][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.1865055412054062, acc: 0.9704433679580688)
[2025-02-13 19:07:44,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:45,187][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.8258336782455444, acc: 0.8375634551048279)
[2025-02-13 19:07:45,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:45,548][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.38598930835723877, acc: 0.9328858852386475)
[2025-02-13 19:07:45,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:45,941][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.1388854682445526, acc: 0.9647058844566345)
[2025-02-13 19:07:46,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:46,338][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.24817489087581635, acc: 0.9204545617103577)
[2025-02-13 19:07:46,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:46,731][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.5287821888923645, acc: 0.8834356069564819)
[2025-02-13 19:07:46,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:47,151][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.3100642263889313, acc: 0.9246575236320496)
[2025-02-13 19:07:47,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:47,509][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.20545640587806702, acc: 0.9459459185600281)
[2025-02-13 19:07:47,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:47,888][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.5500760078430176, acc: 0.9225806593894958)
[2025-02-13 19:07:48,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:48,227][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 3.3046648502349854, acc: 0.5797101259231567)
[2025-02-13 19:07:48,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:48,626][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.8242395520210266, acc: 0.8347107172012329)
[2025-02-13 19:07:48,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:49,004][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.19253651797771454, acc: 0.9712643623352051)
[2025-02-13 19:07:49,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:49,420][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.1898839771747589, acc: 0.9464285969734192)
[2025-02-13 19:07:49,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:49,797][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.705107569694519, acc: 0.845714271068573)
[2025-02-13 19:07:49,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:50,204][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.24098746478557587, acc: 0.9491525292396545)
[2025-02-13 19:07:50,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:50,597][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.20407047867774963, acc: 0.9542483687400818)
[2025-02-13 19:07:50,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:50,993][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.9733858704566956, acc: 0.7962962985038757)
[2025-02-13 19:07:51,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:51,411][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.6858013868331909, acc: 0.8943089246749878)
[2025-02-13 19:07:51,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:51,831][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.5172265768051147, acc: 0.9014084339141846)
[2025-02-13 19:07:51,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:52,197][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.550234854221344, acc: 0.8804348111152649)
[2025-02-13 19:07:52,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:52,587][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.5724446773529053, acc: 0.8294573426246643)
[2025-02-13 19:07:52,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:52,931][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.45039016008377075, acc: 0.8918918967247009)
[2025-02-13 19:07:53,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:53,299][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.6679260730743408, acc: 0.8521126508712769)
[2025-02-13 19:07:53,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:53,736][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.6786229610443115, acc: 0.8650793433189392)
[2025-02-13 19:07:53,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:54,131][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.4042520225048065, acc: 0.9271523356437683)
[2025-02-13 19:07:54,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:54,542][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.378078430891037, acc: 0.9145299196243286)
[2025-02-13 19:07:54,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:54,929][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.3861874043941498, acc: 0.9356725215911865)
[2025-02-13 19:07:55,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:55,355][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.28472772240638733, acc: 0.9271523356437683)
[2025-02-13 19:07:55,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:55,732][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.6320576071739197, acc: 0.854651153087616)
[2025-02-13 19:07:55,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:56,108][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.5468304753303528, acc: 0.8682634830474854)
[2025-02-13 19:07:56,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:56,457][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.4618176221847534, acc: 0.895061731338501)
[2025-02-13 19:07:56,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:56,848][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.4370245933532715, acc: 0.8918918967247009)
[2025-02-13 19:07:57,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:57,228][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 1.5415220260620117, acc: 0.7303370833396912)
[2025-02-13 19:07:57,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:57,584][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.3973788619041443, acc: 0.9090909361839294)
[2025-02-13 19:07:57,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:57,966][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.33656609058380127, acc: 0.9295774698257446)
[2025-02-13 19:07:58,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:58,330][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.4219161570072174, acc: 0.9034482836723328)
[2025-02-13 19:07:58,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:58,681][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.2778652310371399, acc: 0.9469696879386902)
[2025-02-13 19:07:58,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:59,064][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.1766253113746643, acc: 0.9779411554336548)
[2025-02-13 19:07:59,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:59,499][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.4256003797054291, acc: 0.9136690497398376)
[2025-02-13 19:07:59,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:59,893][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.24125608801841736, acc: 0.935251772403717)
[2025-02-13 19:08:00,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:00,279][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.2748047411441803, acc: 0.9607843160629272)
[2025-02-13 19:08:00,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:00,631][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.37409812211990356, acc: 0.9142857193946838)
[2025-02-13 19:08:00,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:01,029][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.3836750388145447, acc: 0.9078013896942139)
[2025-02-13 19:08:01,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:01,420][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.23046374320983887, acc: 0.9635036587715149)
[2025-02-13 19:08:01,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:01,788][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.34324249625205994, acc: 0.918367326259613)
[2025-02-13 19:08:01,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:02,161][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.32229384779930115, acc: 0.9252336621284485)
[2025-02-13 19:08:02,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:02,537][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.3688337802886963, acc: 0.8979591727256775)
[2025-02-13 19:08:02,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:02,941][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.20667900145053864, acc: 0.9581395387649536)
[2025-02-13 19:08:03,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:03,322][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.17484354972839355, acc: 0.9593908786773682)
[2025-02-13 19:08:03,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:03,704][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.07989310473203659, acc: 0.9878787994384766)
[2025-02-13 19:08:03,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:04,075][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.32638949155807495, acc: 0.9212598204612732)
[2025-02-13 19:08:04,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:04,431][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.33633920550346375, acc: 0.9180327653884888)
[2025-02-13 19:08:04,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:04,779][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.1528477817773819, acc: 0.9790209531784058)
[2025-02-13 19:08:04,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:05,185][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.2222236543893814, acc: 0.9450549483299255)
[2025-02-13 19:08:05,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:05,600][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.24323388934135437, acc: 0.9575757384300232)
[2025-02-13 19:08:05,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:05,992][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.4061111807823181, acc: 0.940397322177887)
[2025-02-13 19:08:06,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:06,420][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.13379356265068054, acc: 0.9634146094322205)
[2025-02-13 19:08:06,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:06,799][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.21193501353263855, acc: 0.9659863710403442)
[2025-02-13 19:08:06,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:07,160][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.21967342495918274, acc: 0.9707602262496948)
[2025-02-13 19:08:07,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:07,540][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.0977356806397438, acc: 0.9795082211494446)
[2025-02-13 19:08:07,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:07,924][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.24517829716205597, acc: 0.9230769276618958)
[2025-02-13 19:08:08,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:08,321][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.24044819176197052, acc: 0.9296875)
[2025-02-13 19:08:08,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:08,741][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.22664564847946167, acc: 0.9456067085266113)
[2025-02-13 19:08:08,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:09,169][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.12667210400104523, acc: 0.9732142686843872)
[2025-02-13 19:08:09,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:09,558][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.24769416451454163, acc: 0.9528301954269409)
[2025-02-13 19:08:09,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:09,942][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.29675570130348206, acc: 0.9166666865348816)
[2025-02-13 19:08:10,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:10,340][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.17297381162643433, acc: 0.9329608678817749)
[2025-02-13 19:08:10,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:10,750][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.48863157629966736, acc: 0.8792270421981812)
[2025-02-13 19:08:10,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:11,127][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.457658976316452, acc: 0.8842592835426331)
[2025-02-13 19:08:11,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:11,475][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.4219772517681122, acc: 0.9050279259681702)
[2025-02-13 19:08:11,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:11,833][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.5329580307006836, acc: 0.8693467378616333)
[2025-02-13 19:08:11,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:12,215][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.22537146508693695, acc: 0.9428571462631226)
[2025-02-13 19:08:12,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:12,640][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.6688053011894226, acc: 0.9146341681480408)
[2025-02-13 19:08:12,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:13,022][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.28302299976348877, acc: 0.9281045794487)
[2025-02-13 19:08:13,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:13,400][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.3227044641971588, acc: 0.9329268336296082)
[2025-02-13 19:08:13,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:13,795][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.6405254602432251, acc: 0.875)
[2025-02-13 19:08:13,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:14,203][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.30571311712265015, acc: 0.9224137663841248)
[2025-02-13 19:08:14,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:14,603][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.6133404970169067, acc: 0.832335352897644)
[2025-02-13 19:08:14,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:15,020][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.4810877740383148, acc: 0.8520709872245789)
[2025-02-13 19:08:15,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:15,432][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.3178282082080841, acc: 0.8888888955116272)
[2025-02-13 19:08:15,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:15,803][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.4905688166618347, acc: 0.8947368264198303)
[2025-02-13 19:08:15,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:16,174][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.30667707324028015, acc: 0.9180327653884888)
[2025-02-13 19:08:16,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:16,590][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.2981305718421936, acc: 0.9032257795333862)
[2025-02-13 19:08:16,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:16,986][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.31211328506469727, acc: 0.8999999761581421)
[2025-02-13 19:08:17,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:17,372][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.3308030366897583, acc: 0.9421965479850769)
[2025-02-13 19:08:17,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:17,794][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.25504767894744873, acc: 0.9384615421295166)
[2025-02-13 19:08:17,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:18,159][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.2479710578918457, acc: 0.9350649118423462)
[2025-02-13 19:08:18,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:18,539][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.22890087962150574, acc: 0.9637681245803833)
[2025-02-13 19:08:18,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:18,916][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.32352012395858765, acc: 0.902255654335022)
[2025-02-13 19:08:19,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:19,285][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.26206251978874207, acc: 0.9518072009086609)
[2025-02-13 19:08:19,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:19,714][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.21727627515792847, acc: 0.949367105960846)
[2025-02-13 19:08:19,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:20,110][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.10181983560323715, acc: 0.9803921580314636)
[2025-02-13 19:08:20,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:20,522][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.24423086643218994, acc: 0.9402984976768494)
[2025-02-13 19:08:20,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:20,920][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.1031055897474289, acc: 0.97826087474823)
[2025-02-13 19:08:21,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:21,283][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.2987094819545746, acc: 0.9172932505607605)
[2025-02-13 19:08:21,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:21,671][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.32859694957733154, acc: 0.9222221970558167)
[2025-02-13 19:08:21,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:22,068][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.2993927597999573, acc: 0.9285714030265808)
[2025-02-13 19:08:22,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:22,435][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.2472110092639923, acc: 0.949999988079071)
[2025-02-13 19:08:22,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:22,812][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.298920601606369, acc: 0.9113923907279968)
[2025-02-13 19:08:22,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:23,198][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.16694286465644836, acc: 0.9655172228813171)
[2025-02-13 19:08:23,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:23,556][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.4120507538318634, acc: 0.9072847962379456)
[2025-02-13 19:08:23,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:23,921][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.3950623869895935, acc: 0.9141414165496826)
[2025-02-13 19:08:24,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:24,280][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.3799934387207031, acc: 0.8932584524154663)
[2025-02-13 19:08:24,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:24,713][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.413698673248291, acc: 0.8980891704559326)
[2025-02-13 19:08:24,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:25,131][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.30469533801078796, acc: 0.9157894849777222)
[2025-02-13 19:08:25,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:25,527][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.31223997473716736, acc: 0.9095744490623474)
[2025-02-13 19:08:25,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:25,947][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.2971174418926239, acc: 0.9378530979156494)
[2025-02-13 19:08:26,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:26,332][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.25651785731315613, acc: 0.9408283829689026)
[2025-02-13 19:08:26,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:26,716][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.5058587193489075, acc: 0.9162303805351257)
[2025-02-13 19:08:26,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:27,130][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.8984569907188416, acc: 0.8157894611358643)
[2025-02-13 19:08:27,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:27,492][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 1.0583653450012207, acc: 0.7986111044883728)
[2025-02-13 19:08:27,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:27,866][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.28180307149887085, acc: 0.9278350472450256)
[2025-02-13 19:08:28,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:28,226][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.18900756537914276, acc: 0.9437500238418579)
[2025-02-13 19:08:28,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:28,599][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.39556172490119934, acc: 0.9134615659713745)
[2025-02-13 19:08:28,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:28,971][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.3984951674938202, acc: 0.9281437397003174)
[2025-02-13 19:08:29,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:29,397][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.11174330860376358, acc: 0.9883720874786377)
[2025-02-13 19:08:29,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:29,813][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.2540762424468994, acc: 0.9395604133605957)
[2025-02-13 19:08:29,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:30,221][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.20902583003044128, acc: 0.963350772857666)
[2025-02-13 19:08:30,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:30,628][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.2347182184457779, acc: 0.9560439586639404)
[2025-02-13 19:08:30,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:31,016][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.18678145110607147, acc: 0.964102566242218)
[2025-02-13 19:08:31,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:31,438][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.15923984348773956, acc: 0.9784946441650391)
[2025-02-13 19:08:31,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:31,879][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.2081737369298935, acc: 0.9534883499145508)
[2025-02-13 19:08:32,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:32,278][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.2959353029727936, acc: 0.9254658222198486)
[2025-02-13 19:08:32,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:32,695][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.11992798000574112, acc: 0.9720670580863953)
[2025-02-13 19:08:32,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:33,072][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.24459759891033173, acc: 0.949438214302063)
[2025-02-13 19:08:33,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:33,504][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.24001295864582062, acc: 0.9454545378684998)
[2025-02-13 19:08:33,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:33,906][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.2626342177391052, acc: 0.9109588861465454)
[2025-02-13 19:08:34,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:34,304][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.27206650376319885, acc: 0.9447513818740845)
[2025-02-13 19:08:34,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:34,686][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.1336255520582199, acc: 0.9596773982048035)
[2025-02-13 19:08:34,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:35,100][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.5065357685089111, acc: 0.8768116235733032)
[2025-02-13 19:08:35,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:35,484][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.388788640499115, acc: 0.9304347634315491)
[2025-02-13 19:08:35,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:35,857][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.20631763339042664, acc: 0.9534883499145508)
[2025-02-13 19:08:36,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:36,252][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.17247340083122253, acc: 0.9448275566101074)
[2025-02-13 19:08:36,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:36,628][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.14494331181049347, acc: 0.9781022071838379)
[2025-02-13 19:08:36,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:36,995][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.19524069130420685, acc: 0.949999988079071)
[2025-02-13 19:08:37,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:37,360][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.21526190638542175, acc: 0.9507042169570923)
[2025-02-13 19:08:37,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:37,703][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.3269716203212738, acc: 0.8951048851013184)
[2025-02-13 19:08:37,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:38,103][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.21179504692554474, acc: 0.9583333134651184)
[2025-02-13 19:08:38,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:38,517][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.1285412758588791, acc: 0.9724137783050537)
[2025-02-13 19:08:38,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:38,962][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.31137827038764954, acc: 0.9398496150970459)
[2025-02-13 19:08:39,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:39,326][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.26133453845977783, acc: 0.9561403393745422)
[2025-02-13 19:08:39,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:39,714][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 1.075817584991455, acc: 0.8278688788414001)
[2025-02-13 19:08:39,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:40,146][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.646508514881134, acc: 0.8979591727256775)
[2025-02-13 19:08:40,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:40,526][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.17897410690784454, acc: 0.9418604373931885)
[2025-02-13 19:08:40,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:40,975][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.18999138474464417, acc: 0.9495798349380493)
[2025-02-13 19:08:41,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:41,391][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.28565290570259094, acc: 0.9041095972061157)
[2025-02-13 19:08:41,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:41,792][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.17171336710453033, acc: 0.9629629850387573)
[2025-02-13 19:08:41,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:42,213][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.1665440946817398, acc: 0.956204354763031)
[2025-02-13 19:08:42,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:42,634][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.1388806700706482, acc: 0.9603174328804016)
[2025-02-13 19:08:42,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:43,029][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.17491233348846436, acc: 0.9830508232116699)
[2025-02-13 19:08:43,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:43,403][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.12737545371055603, acc: 0.9836065769195557)
[2025-02-13 19:08:43,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:43,793][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.1555360108613968, acc: 0.9567901492118835)
[2025-02-13 19:08:43,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:44,160][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.08337031304836273, acc: 0.981249988079071)
[2025-02-13 19:08:44,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:44,521][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.5713569521903992, acc: 0.884353756904602)
[2025-02-13 19:08:44,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:44,905][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.4724191427230835, acc: 0.887417197227478)
[2025-02-13 19:08:45,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:45,294][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.07534363865852356, acc: 0.9863013625144958)
[2025-02-13 19:08:45,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:45,656][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.1392608880996704, acc: 0.9735099077224731)
[2025-02-13 19:08:45,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:46,025][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.2297227382659912, acc: 0.9504132270812988)
[2025-02-13 19:08:46,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:46,402][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.355000764131546, acc: 0.908108115196228)
[2025-02-13 19:08:46,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:46,787][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.27624374628067017, acc: 0.9253731369972229)
[2025-02-13 19:08:46,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:47,212][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.18652582168579102, acc: 0.9509202241897583)
[2025-02-13 19:08:47,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:47,646][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.21680070459842682, acc: 0.9418604373931885)
[2025-02-13 19:08:47,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:48,041][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.28301891684532166, acc: 0.9590643048286438)
[2025-02-13 19:08:48,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:48,416][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.22578676044940948, acc: 0.9415204524993896)
[2025-02-13 19:08:48,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:48,776][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.22377952933311462, acc: 0.9473684430122375)
[2025-02-13 19:08:48,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:49,149][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.25411590933799744, acc: 0.9386503100395203)
[2025-02-13 19:08:49,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:49,522][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.32167261838912964, acc: 0.9352940917015076)
[2025-02-13 19:08:49,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:49,908][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.3658776879310608, acc: 0.9036144614219666)
[2025-02-13 19:08:50,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:50,287][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.2265302538871765, acc: 0.9558011293411255)
[2025-02-13 19:08:50,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:50,662][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.4006440341472626, acc: 0.9005848169326782)
[2025-02-13 19:08:50,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:51,055][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.09506049752235413, acc: 0.9923664331436157)
[2025-02-13 19:08:51,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:51,463][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.41748765110969543, acc: 0.8926174640655518)
[2025-02-13 19:08:51,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:51,862][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.1932159960269928, acc: 0.9516128897666931)
[2025-02-13 19:08:51,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:52,240][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.1961802840232849, acc: 0.9389312863349915)
[2025-02-13 19:08:52,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:52,634][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.18291230499744415, acc: 0.9509202241897583)
[2025-02-13 19:08:52,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:53,011][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.2103436291217804, acc: 0.9505494236946106)
[2025-02-13 19:08:53,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:53,399][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.17170721292495728, acc: 0.9489051103591919)
[2025-02-13 19:08:53,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:53,799][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.2652395963668823, acc: 0.9396551847457886)
[2025-02-13 19:08:53,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:54,175][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.1835848093032837, acc: 0.9636363387107849)
[2025-02-13 19:08:54,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:54,539][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.32132387161254883, acc: 0.9406779408454895)
[2025-02-13 19:08:54,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:54,891][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.09474875777959824, acc: 0.9798657894134521)
[2025-02-13 19:08:55,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:55,262][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.12951232492923737, acc: 0.9679999947547913)
[2025-02-13 19:08:55,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:55,651][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.313724547624588, acc: 0.925000011920929)
[2025-02-13 19:08:55,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:56,015][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.5586029887199402, acc: 0.8991596698760986)
[2025-02-13 19:08:56,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:56,411][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.4359425902366638, acc: 0.8965517282485962)
[2025-02-13 19:08:56,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:56,833][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.5085851550102234, acc: 0.8928571343421936)
[2025-02-13 19:08:56,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:57,212][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.4505551755428314, acc: 0.9272727370262146)
[2025-02-13 19:08:57,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:57,653][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.30595290660858154, acc: 0.931506872177124)
[2025-02-13 19:08:57,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:58,032][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.32472148537635803, acc: 0.9115044474601746)
[2025-02-13 19:08:58,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:58,424][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.4817703068256378, acc: 0.9027777910232544)
[2025-02-13 19:08:58,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:58,807][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.5466612577438354, acc: 0.8931297659873962)
[2025-02-13 19:08:58,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:59,180][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.5054937601089478, acc: 0.8910256624221802)
[2025-02-13 19:08:59,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:59,547][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.39941078424453735, acc: 0.8630136847496033)
[2025-02-13 19:08:59,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:59,957][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.5222744941711426, acc: 0.887005627155304)
[2025-02-13 19:09:00,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:00,331][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.4525608420372009, acc: 0.8764045238494873)
[2025-02-13 19:09:00,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:00,727][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.6428713798522949, acc: 0.849711000919342)
[2025-02-13 19:09:00,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:01,158][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.7439424395561218, acc: 0.8226950168609619)
[2025-02-13 19:09:01,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:01,550][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.3475892245769501, acc: 0.9108280539512634)
[2025-02-13 19:09:01,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:01,928][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.38394027948379517, acc: 0.8928571343421936)
[2025-02-13 19:09:02,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:02,340][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.6515777707099915, acc: 0.8671875)
[2025-02-13 19:09:02,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:02,711][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.2529689371585846, acc: 0.9219858050346375)
[2025-02-13 19:09:02,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:03,099][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.20274978876113892, acc: 0.9558823704719543)
[2025-02-13 19:09:03,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:03,532][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.24252693355083466, acc: 0.9428571462631226)
[2025-02-13 19:09:03,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:03,894][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.19496317207813263, acc: 0.9674796462059021)
[2025-02-13 19:09:04,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:04,260][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.3369044065475464, acc: 0.8999999761581421)
[2025-02-13 19:09:04,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:04,626][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.2825172543525696, acc: 0.9313725233078003)
[2025-02-13 19:09:04,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:05,051][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.23178353905677795, acc: 0.9395973086357117)
[2025-02-13 19:09:05,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:05,432][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.18229839205741882, acc: 0.9482758641242981)
[2025-02-13 19:09:05,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:05,810][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.11877504736185074, acc: 0.9611650705337524)
[2025-02-13 19:09:05,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:06,165][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.185557022690773, acc: 0.9607843160629272)
[2025-02-13 19:09:06,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:06,517][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.187571182847023, acc: 0.9669421315193176)
[2025-02-13 19:09:06,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:06,917][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.3081711232662201, acc: 0.9476439952850342)
[2025-02-13 19:09:07,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:07,289][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.4006509482860565, acc: 0.9056603908538818)
[2025-02-13 19:09:07,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:07,661][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.2909506559371948, acc: 0.9387755393981934)
[2025-02-13 19:09:07,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:08,003][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.3593880236148834, acc: 0.9265536665916443)
[2025-02-13 19:09:08,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:08,408][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.2254360020160675, acc: 0.9593908786773682)
[2025-02-13 19:09:08,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:08,779][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.1505679190158844, acc: 0.9638554453849792)
[2025-02-13 19:09:08,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:09,150][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.25354164838790894, acc: 0.9477124214172363)
[2025-02-13 19:09:09,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:09,545][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.25707823038101196, acc: 0.9426751732826233)
[2025-02-13 19:09:09,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:09,956][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.45593932271003723, acc: 0.8728813529014587)
[2025-02-13 19:09:10,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:10,324][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.18213900923728943, acc: 0.961240291595459)
[2025-02-13 19:09:10,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:10,684][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.25471577048301697, acc: 0.9333333373069763)
[2025-02-13 19:09:10,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:11,052][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.1458059698343277, acc: 0.9674796462059021)
[2025-02-13 19:09:11,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:11,468][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.1692269891500473, acc: 0.970588207244873)
[2025-02-13 19:09:11,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:11,859][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.08491753041744232, acc: 0.9940476417541504)
[2025-02-13 19:09:12,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:12,247][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.4280956983566284, acc: 0.9408283829689026)
[2025-02-13 19:09:12,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:12,648][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.42791861295700073, acc: 0.8690476417541504)
[2025-02-13 19:09:12,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:13,038][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.14741772413253784, acc: 0.9542483687400818)
[2025-02-13 19:09:13,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:13,399][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.11426166445016861, acc: 0.9724137783050537)
[2025-02-13 19:09:13,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:13,784][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.10259336978197098, acc: 0.9735099077224731)
[2025-02-13 19:09:13,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:14,197][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.25648069381713867, acc: 0.9473684430122375)
[2025-02-13 19:09:14,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:14,588][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.357425719499588, acc: 0.9179104566574097)
[2025-02-13 19:09:14,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:15,007][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.3002873659133911, acc: 0.9154228568077087)
[2025-02-13 19:09:15,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:15,360][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.2884903848171234, acc: 0.9370629191398621)
[2025-02-13 19:09:15,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:15,775][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.13028870522975922, acc: 0.9638554453849792)
[2025-02-13 19:09:15,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:16,168][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.21047630906105042, acc: 0.9539473652839661)
[2025-02-13 19:09:16,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:16,569][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.1981326788663864, acc: 0.9375)
[2025-02-13 19:09:16,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:16,957][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.2395821064710617, acc: 0.9314285516738892)
[2025-02-13 19:09:17,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:17,354][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.3418542146682739, acc: 0.9226190447807312)
[2025-02-13 19:09:17,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:17,719][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.11021382361650467, acc: 0.9567567706108093)
[2025-02-13 19:09:17,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:18,103][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.2521292567253113, acc: 0.9437500238418579)
[2025-02-13 19:09:18,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:18,540][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.12614776194095612, acc: 0.9714285731315613)
[2025-02-13 19:09:18,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:18,936][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.14793041348457336, acc: 0.9503546357154846)
[2025-02-13 19:09:19,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:19,344][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.18659238517284393, acc: 0.9605262875556946)
[2025-02-13 19:09:19,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:19,727][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.1701563447713852, acc: 0.950276255607605)
[2025-02-13 19:09:19,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:20,107][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.26969701051712036, acc: 0.9192546606063843)
[2025-02-13 19:09:20,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:20,482][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.35152217745780945, acc: 0.9057971239089966)
[2025-02-13 19:09:20,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:20,869][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.45772647857666016, acc: 0.8802395462989807)
[2025-02-13 19:09:21,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:21,290][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.4085136353969574, acc: 0.9430052042007446)
[2025-02-13 19:09:21,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:21,682][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.29784682393074036, acc: 0.9333333373069763)
[2025-02-13 19:09:21,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:22,084][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.2695758044719696, acc: 0.9399999976158142)
[2025-02-13 19:09:22,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:22,465][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.2898404598236084, acc: 0.9289940595626831)
[2025-02-13 19:09:22,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:22,852][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.3026575446128845, acc: 0.9195402264595032)
[2025-02-13 19:09:22,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:23,258][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.28731226921081543, acc: 0.9523809552192688)
[2025-02-13 19:09:23,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:23,615][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.31003856658935547, acc: 0.9265536665916443)
[2025-02-13 19:09:23,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:24,023][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.18758806586265564, acc: 0.9585798978805542)
[2025-02-13 19:09:24,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:24,404][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.27337753772735596, acc: 0.9421965479850769)
[2025-02-13 19:09:24,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:24,788][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.34840160608291626, acc: 0.9259259104728699)
[2025-02-13 19:09:24,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:25,203][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.33457037806510925, acc: 0.926174521446228)
[2025-02-13 19:09:25,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:25,597][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.25059381127357483, acc: 0.9548022747039795)
[2025-02-13 19:09:25,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:26,021][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.45790180563926697, acc: 0.8947368264198303)
[2025-02-13 19:09:26,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:26,433][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.2318902611732483, acc: 0.9324324131011963)
[2025-02-13 19:09:26,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:26,809][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.1964370608329773, acc: 0.9580838084220886)
[2025-02-13 19:09:26,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:27,194][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.31962400674819946, acc: 0.9078013896942139)
[2025-02-13 19:09:27,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:27,570][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.18026645481586456, acc: 0.9518072009086609)
[2025-02-13 19:09:27,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:27,937][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.26066601276397705, acc: 0.9555555582046509)
[2025-02-13 19:09:28,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:28,329][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.40299662947654724, acc: 0.9112426042556763)
[2025-02-13 19:09:28,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:28,690][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.394022673368454, acc: 0.8986486196517944)
[2025-02-13 19:09:28,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:29,010][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.14692814648151398, acc: 0.9631901979446411)
[2025-02-13 19:09:29,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:29,393][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.10766228288412094, acc: 0.9695122241973877)
[2025-02-13 19:09:29,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:29,754][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.21584993600845337, acc: 0.9473684430122375)
[2025-02-13 19:09:29,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:30,144][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.17567089200019836, acc: 0.959770143032074)
[2025-02-13 19:09:30,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:30,495][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.11093537509441376, acc: 0.9536423683166504)
[2025-02-13 19:09:30,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:30,892][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.1592637300491333, acc: 0.9670329689979553)
[2025-02-13 19:09:31,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:31,330][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.2166016697883606, acc: 0.9468085169792175)
[2025-02-13 19:09:31,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:31,723][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.09137599915266037, acc: 0.9820359349250793)
[2025-02-13 19:09:31,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:32,107][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.08896687626838684, acc: 0.9767441749572754)
[2025-02-13 19:09:32,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:32,494][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.533682107925415, acc: 0.8983050584793091)
[2025-02-13 19:09:32,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:32,878][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.5263939499855042, acc: 0.8936170339584351)
[2025-02-13 19:09:33,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:33,264][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.368465781211853, acc: 0.9265536665916443)
[2025-02-13 19:09:33,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:33,656][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.16508808732032776, acc: 0.9655172228813171)
[2025-02-13 19:09:33,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:34,033][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.1488790363073349, acc: 0.9659090638160706)
[2025-02-13 19:09:34,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:34,422][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.23395220935344696, acc: 0.9418604373931885)
[2025-02-13 19:09:34,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:34,842][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.09741363674402237, acc: 0.9729729890823364)
[2025-02-13 19:09:34,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:35,225][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.22761936485767365, acc: 0.9343434572219849)
[2025-02-13 19:09:35,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:35,653][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.16978321969509125, acc: 0.9545454382896423)
[2025-02-13 19:09:35,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:36,040][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.1321418732404709, acc: 0.9590643048286438)
[2025-02-13 19:09:36,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:36,431][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.20584821701049805, acc: 0.9548022747039795)
[2025-02-13 19:09:36,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:36,831][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.22422532737255096, acc: 0.9518072009086609)
[2025-02-13 19:09:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:37,228][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.2869061827659607, acc: 0.9419354796409607)
[2025-02-13 19:09:37,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:37,635][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.31023427844047546, acc: 0.9313725233078003)
[2025-02-13 19:09:37,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:38,037][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.2797102928161621, acc: 0.9367815852165222)
[2025-02-13 19:09:38,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:38,466][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.28622373938560486, acc: 0.9230769276618958)
[2025-02-13 19:09:38,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:38,880][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.1494000256061554, acc: 0.9632353186607361)
[2025-02-13 19:09:39,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:39,276][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.28627046942710876, acc: 0.9292929172515869)
[2025-02-13 19:09:39,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:39,628][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.27660396695137024, acc: 0.9432989954948425)
[2025-02-13 19:09:39,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:39,998][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.1067371517419815, acc: 0.976190447807312)
[2025-02-13 19:09:40,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:40,405][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.14878006279468536, acc: 0.9512194991111755)
[2025-02-13 19:09:40,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:40,822][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.2777242362499237, acc: 0.9333333373069763)
[2025-02-13 19:09:40,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:41,212][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.14443932473659515, acc: 0.9741935729980469)
[2025-02-13 19:09:41,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:41,576][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.1367802917957306, acc: 0.9599999785423279)
[2025-02-13 19:09:41,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:41,962][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.10097067058086395, acc: 0.9772727489471436)
[2025-02-13 19:09:42,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:42,340][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.2066473364830017, acc: 0.9634146094322205)
[2025-02-13 19:09:42,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:42,717][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.08904905617237091, acc: 0.9830508232116699)
[2025-02-13 19:09:42,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:43,124][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.24862556159496307, acc: 0.9708737730979919)
[2025-02-13 19:09:43,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:43,518][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.09015821665525436, acc: 0.9856114983558655)
[2025-02-13 19:09:43,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:43,904][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.19896137714385986, acc: 0.9597989916801453)
[2025-02-13 19:09:44,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:44,253][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.19148218631744385, acc: 0.9673202633857727)
[2025-02-13 19:09:44,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:44,642][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.31279832124710083, acc: 0.9459459185600281)
[2025-02-13 19:09:44,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:45,037][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.20598168671131134, acc: 0.9431279897689819)
[2025-02-13 19:09:45,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:45,438][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.2191932052373886, acc: 0.9589040875434875)
[2025-02-13 19:09:45,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:45,856][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.19771301746368408, acc: 0.9520958065986633)
[2025-02-13 19:09:46,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:46,280][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.20277994871139526, acc: 0.9454545378684998)
[2025-02-13 19:09:46,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:46,645][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.22742144763469696, acc: 0.945652186870575)
[2025-02-13 19:09:46,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:47,027][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.222869411110878, acc: 0.9398906826972961)
[2025-02-13 19:09:47,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:47,395][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.14189189672470093, acc: 0.96875)
[2025-02-13 19:09:47,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:47,779][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.1711438149213791, acc: 0.959770143032074)
[2025-02-13 19:09:47,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:48,164][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.2184067815542221, acc: 0.9505494236946106)
[2025-02-13 19:09:48,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:48,575][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.15708449482917786, acc: 0.9751552939414978)
[2025-02-13 19:09:48,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:48,976][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.33739060163497925, acc: 0.928205132484436)
[2025-02-13 19:09:49,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:49,395][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.22669503092765808, acc: 0.939226508140564)
[2025-02-13 19:09:49,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:49,790][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.17769302427768707, acc: 0.9436619877815247)
[2025-02-13 19:09:49,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:50,191][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.3973180651664734, acc: 0.9119496941566467)
[2025-02-13 19:09:50,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:50,566][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.47894832491874695, acc: 0.8866666555404663)
[2025-02-13 19:09:50,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:50,923][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.5055906176567078, acc: 0.8633093237876892)
[2025-02-13 19:09:51,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:51,339][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.1862538903951645, acc: 0.9515151381492615)
[2025-02-13 19:09:51,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:51,739][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.3684341311454773, acc: 0.8896104097366333)
[2025-02-13 19:09:51,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:52,141][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.3352316915988922, acc: 0.9273743033409119)
[2025-02-13 19:09:52,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:52,538][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.3022005558013916, acc: 0.9052631855010986)
[2025-02-13 19:09:52,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:52,934][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.15368974208831787, acc: 0.9555555582046509)
[2025-02-13 19:09:53,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:53,305][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.29550138115882874, acc: 0.9234972596168518)
[2025-02-13 19:09:53,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:53,697][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.30153754353523254, acc: 0.9210526347160339)
[2025-02-13 19:09:53,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:54,116][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.20118266344070435, acc: 0.9444444179534912)
[2025-02-13 19:09:54,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:54,519][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.19023504853248596, acc: 0.9505494236946106)
[2025-02-13 19:09:54,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:54,916][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.20448347926139832, acc: 0.9527559280395508)
[2025-02-13 19:09:55,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:55,300][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.1717807948589325, acc: 0.954023003578186)
[2025-02-13 19:09:55,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:55,711][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.08929242193698883, acc: 0.9810126423835754)
[2025-02-13 19:09:55,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:56,095][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.17987863719463348, acc: 0.9685039520263672)
[2025-02-13 19:09:56,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:56,468][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.15801754593849182, acc: 0.9729729890823364)
[2025-02-13 19:09:56,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:56,899][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.14740312099456787, acc: 0.970802903175354)
[2025-02-13 19:09:57,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:57,295][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.1305876523256302, acc: 0.976047933101654)
[2025-02-13 19:09:57,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:57,664][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.09793715924024582, acc: 0.982758641242981)
[2025-02-13 19:09:57,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:58,069][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.10360424220561981, acc: 0.9645389914512634)
[2025-02-13 19:09:58,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:58,437][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.11513461917638779, acc: 0.9731543660163879)
[2025-02-13 19:09:58,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:58,804][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.06839905679225922, acc: 0.9795918464660645)
[2025-02-13 19:09:58,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:59,178][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.14481660723686218, acc: 0.9741935729980469)
[2025-02-13 19:09:59,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:59,549][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.14756861329078674, acc: 0.9634146094322205)
[2025-02-13 19:09:59,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:59,908][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.21307040750980377, acc: 0.9451219439506531)
[2025-02-13 19:10:00,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:00,306][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.08107159286737442, acc: 0.9759036302566528)
[2025-02-13 19:10:00,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:00,689][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.08833321183919907, acc: 0.9828571677207947)
[2025-02-13 19:10:00,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:01,087][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.16106903553009033, acc: 0.9470587968826294)
[2025-02-13 19:10:01,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:01,447][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.20138370990753174, acc: 0.9459459185600281)
[2025-02-13 19:10:01,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:01,814][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.10843711346387863, acc: 0.9746835231781006)
[2025-02-13 19:10:01,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:02,201][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.1626444309949875, acc: 0.9766082167625427)
[2025-02-13 19:10:02,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:02,572][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.1026073694229126, acc: 0.9820359349250793)
[2025-02-13 19:10:02,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:02,927][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.19776083528995514, acc: 0.9629629850387573)
[2025-02-13 19:10:03,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:03,273][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.19993899762630463, acc: 0.9428571462631226)
[2025-02-13 19:10:03,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:03,624][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.14623582363128662, acc: 0.9777777791023254)
[2025-02-13 19:10:03,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:04,033][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.21857404708862305, acc: 0.9254658222198486)
[2025-02-13 19:10:04,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:04,386][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.3772369921207428, acc: 0.9178082346916199)
[2025-02-13 19:10:04,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:04,762][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.19224441051483154, acc: 0.9395973086357117)
[2025-02-13 19:10:04,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:05,158][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.3127579391002655, acc: 0.9240506291389465)
[2025-02-13 19:10:05,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:05,528][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.2574966847896576, acc: 0.9387755393981934)
[2025-02-13 19:10:05,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:05,951][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.155630424618721, acc: 0.9731543660163879)
[2025-02-13 19:10:06,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:06,334][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.1545613706111908, acc: 0.9675324559211731)
[2025-02-13 19:10:06,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:06,688][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.18625228106975555, acc: 0.9534883499145508)
[2025-02-13 19:10:06,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:07,052][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.12176638841629028, acc: 0.9708737730979919)
[2025-02-13 19:10:07,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:07,404][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.07722720503807068, acc: 0.9784172773361206)
[2025-02-13 19:10:07,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:07,771][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.30810683965682983, acc: 0.924369752407074)
[2025-02-13 19:10:07,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:08,138][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.20540814101696014, acc: 0.9367815852165222)
[2025-02-13 19:10:08,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:08,552][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.21859565377235413, acc: 0.9358974099159241)
[2025-02-13 19:10:08,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:08,957][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.08712726831436157, acc: 0.9740259647369385)
[2025-02-13 19:10:09,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:09,372][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.1960727721452713, acc: 0.9580419659614563)
[2025-02-13 19:10:09,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:09,822][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.16432133316993713, acc: 0.95333331823349)
[2025-02-13 19:10:10,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:10,269][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.21553702652454376, acc: 0.9466666579246521)
[2025-02-13 19:10:10,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:10,684][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.27334678173065186, acc: 0.9382715821266174)
[2025-02-13 19:10:10,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:11,086][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.4595275819301605, acc: 0.893048107624054)
[2025-02-13 19:10:11,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:11,523][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.35658228397369385, acc: 0.9034090638160706)
[2025-02-13 19:10:11,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:11,911][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.5620906949043274, acc: 0.84375)
[2025-02-13 19:10:12,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:12,331][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.6751599311828613, acc: 0.8611111044883728)
[2025-02-13 19:10:12,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:12,738][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.9010164141654968, acc: 0.800000011920929)
[2025-02-13 19:10:12,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:13,116][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.35913363099098206, acc: 0.9041095972061157)
[2025-02-13 19:10:13,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:13,474][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.4994817078113556, acc: 0.9084967374801636)
[2025-02-13 19:10:13,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:13,838][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.7917382717132568, acc: 0.8087431788444519)
[2025-02-13 19:10:13,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:14,205][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.6945493817329407, acc: 0.8381502628326416)
[2025-02-13 19:10:14,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:14,571][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.6602053046226501, acc: 0.8208092451095581)
[2025-02-13 19:10:14,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:14,952][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.48237863183021545, acc: 0.8839778900146484)
[2025-02-13 19:10:15,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:15,374][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.4064688980579376, acc: 0.8989361524581909)
[2025-02-13 19:10:15,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:15,759][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.2243424654006958, acc: 0.9508196711540222)
[2025-02-13 19:10:15,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:16,156][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.1289956122636795, acc: 0.97826087474823)
[2025-02-13 19:10:16,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:16,584][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.26322460174560547, acc: 0.9363057613372803)
[2025-02-13 19:10:16,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:16,960][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.5973037481307983, acc: 0.8709677457809448)
[2025-02-13 19:10:17,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:17,375][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.30264803767204285, acc: 0.9433962106704712)
[2025-02-13 19:10:17,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:17,762][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.5323882102966309, acc: 0.8617886304855347)
[2025-02-13 19:10:17,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:18,166][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.23493696749210358, acc: 0.9382715821266174)
[2025-02-13 19:10:18,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:18,548][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.25783273577690125, acc: 0.9379844665527344)
[2025-02-13 19:10:18,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:18,922][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.414733350276947, acc: 0.9059829115867615)
[2025-02-13 19:10:19,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:19,289][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.5232953429222107, acc: 0.8636363744735718)
[2025-02-13 19:10:19,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:19,656][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.8201151490211487, acc: 0.8086419701576233)
[2025-02-13 19:10:19,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:20,005][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.4095718562602997, acc: 0.902255654335022)
[2025-02-13 19:10:20,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:20,368][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.38857656717300415, acc: 0.902255654335022)
[2025-02-13 19:10:20,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:20,726][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.5094860792160034, acc: 0.8756756782531738)
[2025-02-13 19:10:20,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:21,102][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.5453317165374756, acc: 0.8831169009208679)
[2025-02-13 19:10:21,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:21,481][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.3436935544013977, acc: 0.9116021990776062)
[2025-02-13 19:10:21,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:21,853][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.24817518889904022, acc: 0.9186992049217224)
[2025-02-13 19:10:22,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:22,229][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.21414567530155182, acc: 0.9452054500579834)
[2025-02-13 19:10:22,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:22,596][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.20765984058380127, acc: 0.9329608678817749)
[2025-02-13 19:10:22,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:22,973][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.38641008734703064, acc: 0.921875)
[2025-02-13 19:10:23,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:23,386][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.47689270973205566, acc: 0.8594594597816467)
[2025-02-13 19:10:23,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:23,838][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.4409967362880707, acc: 0.8870967626571655)
[2025-02-13 19:10:23,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:24,265][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.14855456352233887, acc: 0.9444444179534912)
[2025-02-13 19:10:24,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:24,670][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.6682426929473877, acc: 0.8769230842590332)
[2025-02-13 19:10:24,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:25,066][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.28804299235343933, acc: 0.9313725233078003)
[2025-02-13 19:10:25,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:25,468][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.23743465542793274, acc: 0.9470899701118469)
[2025-02-13 19:10:25,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:25,851][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.3452012240886688, acc: 0.928205132484436)
[2025-02-13 19:10:26,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:26,222][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.5995148420333862, acc: 0.8838709592819214)
[2025-02-13 19:10:26,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:26,586][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.2255619466304779, acc: 0.9593908786773682)
[2025-02-13 19:10:26,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:26,963][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.3370887339115143, acc: 0.9266055226325989)
[2025-02-13 19:10:27,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:27,358][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.5737583637237549, acc: 0.8936170339584351)
[2025-02-13 19:10:27,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:27,756][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.12192114442586899, acc: 0.9578947424888611)
[2025-02-13 19:10:27,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:28,123][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.1947682797908783, acc: 0.9471153616905212)
[2025-02-13 19:10:28,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:28,483][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.2840796709060669, acc: 0.9548386931419373)
[2025-02-13 19:10:28,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:28,890][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.16787095367908478, acc: 0.9599999785423279)
[2025-02-13 19:10:29,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:29,290][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.43975701928138733, acc: 0.8908045887947083)
[2025-02-13 19:10:29,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:29,686][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.22662369906902313, acc: 0.9447852969169617)
[2025-02-13 19:10:29,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:30,102][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.1385408192873001, acc: 0.9615384340286255)
[2025-02-13 19:10:30,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:30,489][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.19995804131031036, acc: 0.9518716335296631)
[2025-02-13 19:10:30,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:30,897][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.24936001002788544, acc: 0.9354838728904724)
[2025-02-13 19:10:31,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:31,285][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.2906390428543091, acc: 0.9281768202781677)
[2025-02-13 19:10:31,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:31,644][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.39703601598739624, acc: 0.9191918969154358)
[2025-02-13 19:10:31,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:32,030][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.26352787017822266, acc: 0.9452054500579834)
[2025-02-13 19:10:32,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:32,392][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.21209296584129333, acc: 0.9425837397575378)
[2025-02-13 19:10:32,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:32,776][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.3558829128742218, acc: 0.9140625)
[2025-02-13 19:10:32,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:33,147][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.20425893366336823, acc: 0.9471153616905212)
[2025-02-13 19:10:33,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:33,547][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.22185437381267548, acc: 0.9575471878051758)
[2025-02-13 19:10:33,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:33,919][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.22780366241931915, acc: 0.9476190209388733)
[2025-02-13 19:10:34,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:34,325][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.25503861904144287, acc: 0.9408602118492126)
[2025-02-13 19:10:34,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:34,711][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.15587005019187927, acc: 0.9747474789619446)
[2025-02-13 19:10:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:35,091][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.3373722732067108, acc: 0.907608687877655)
[2025-02-13 19:10:35,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:35,472][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.31446027755737305, acc: 0.9128205180168152)
[2025-02-13 19:10:35,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:35,819][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.35283851623535156, acc: 0.8940092325210571)
[2025-02-13 19:10:35,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:36,214][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.44013792276382446, acc: 0.8898678421974182)
[2025-02-13 19:10:36,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:36,603][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.36109301447868347, acc: 0.9103773832321167)
[2025-02-13 19:10:36,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:37,020][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.15587227046489716, acc: 0.9488372206687927)
[2025-02-13 19:10:37,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:37,415][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.23685193061828613, acc: 0.9364407062530518)
[2025-02-13 19:10:37,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:37,829][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.24660609662532806, acc: 0.9427312612533569)
[2025-02-13 19:10:37,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:38,220][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.1086486354470253, acc: 0.9788359999656677)
[2025-02-13 19:10:38,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:38,607][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.1579793244600296, acc: 0.961904764175415)
[2025-02-13 19:10:38,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:39,045][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.15794378519058228, acc: 0.9556650519371033)
[2025-02-13 19:10:39,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:39,490][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.14492185413837433, acc: 0.9624413251876831)
[2025-02-13 19:10:39,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:39,898][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.26556283235549927, acc: 0.9278846383094788)
[2025-02-13 19:10:40,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:40,313][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.2033711075782776, acc: 0.9495412707328796)
[2025-02-13 19:10:40,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:40,724][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.22050407528877258, acc: 0.934883713722229)
[2025-02-13 19:10:40,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:41,116][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.12653569877147675, acc: 0.9682539701461792)
[2025-02-13 19:10:41,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:41,533][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.24423019587993622, acc: 0.9495798349380493)
[2025-02-13 19:10:41,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:41,959][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.10527960956096649, acc: 0.9737991094589233)
[2025-02-13 19:10:42,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:42,373][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.17667560279369354, acc: 0.9594594836235046)
[2025-02-13 19:10:42,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:42,747][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.11483363807201385, acc: 0.9692307710647583)
[2025-02-13 19:10:42,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:43,144][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.13571175932884216, acc: 0.9647058844566345)
[2025-02-13 19:10:43,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:43,537][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.29339542984962463, acc: 0.9202454090118408)
[2025-02-13 19:10:43,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:43,967][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.3496343493461609, acc: 0.8980891704559326)
[2025-02-13 19:10:44,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:44,404][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.43917593359947205, acc: 0.9518072009086609)
[2025-02-13 19:10:44,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:44,847][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.27036145329475403, acc: 0.9142857193946838)
[2025-02-13 19:10:44,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:45,243][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.17955432832241058, acc: 0.9538461565971375)
[2025-02-13 19:10:45,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:45,637][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.17784327268600464, acc: 0.9530201554298401)
[2025-02-13 19:10:45,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:46,017][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.28665387630462646, acc: 0.9328858852386475)
[2025-02-13 19:10:46,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:46,412][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.26788440346717834, acc: 0.9346405267715454)
[2025-02-13 19:10:46,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:46,805][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.08417001366615295, acc: 0.9757575988769531)
[2025-02-13 19:10:46,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:47,209][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.08510760217905045, acc: 0.9677419066429138)
[2025-02-13 19:10:47,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:47,585][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.11146204173564911, acc: 0.9757575988769531)
[2025-02-13 19:10:47,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:47,985][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.12740230560302734, acc: 0.9756097793579102)
[2025-02-13 19:10:48,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:48,372][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.2949816882610321, acc: 0.9554139971733093)
[2025-02-13 19:10:48,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:48,783][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.13663625717163086, acc: 0.9620253443717957)
[2025-02-13 19:10:48,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:49,186][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.09111552685499191, acc: 0.9798657894134521)
[2025-02-13 19:10:49,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:49,573][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.1007997989654541, acc: 0.9791666865348816)
[2025-02-13 19:10:49,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:49,961][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.12711530923843384, acc: 0.9870129823684692)
[2025-02-13 19:10:50,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:50,353][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.21541115641593933, acc: 0.9487179517745972)
[2025-02-13 19:10:50,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:50,719][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.09642181545495987, acc: 0.9740259647369385)
[2025-02-13 19:10:50,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:51,084][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.08609052002429962, acc: 0.9917355179786682)
[2025-02-13 19:10:51,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:51,460][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.0951765850186348, acc: 0.988095223903656)
[2025-02-13 19:10:51,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:51,868][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.11162048578262329, acc: 0.96875)
[2025-02-13 19:10:52,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:52,272][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.15497691929340363, acc: 0.9382715821266174)
[2025-02-13 19:10:52,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:52,646][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.17421044409275055, acc: 0.9425287246704102)
[2025-02-13 19:10:52,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:53,025][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.1254701465368271, acc: 0.9613259434700012)
[2025-02-13 19:10:53,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:53,428][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.1550481766462326, acc: 0.9577465057373047)
[2025-02-13 19:10:53,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:53,848][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.531029224395752, acc: 0.8711340427398682)
[2025-02-13 19:10:53,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:54,266][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.5292056202888489, acc: 0.893750011920929)
[2025-02-13 19:10:54,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:54,720][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.5421483516693115, acc: 0.905063271522522)
[2025-02-13 19:10:54,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:55,126][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.3902653753757477, acc: 0.9016393423080444)
[2025-02-13 19:10:55,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:55,512][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.45625439286231995, acc: 0.9096774458885193)
[2025-02-13 19:10:55,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:55,928][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.3392491042613983, acc: 0.9225806593894958)
[2025-02-13 19:10:56,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:56,285][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.23943202197551727, acc: 0.9234972596168518)
[2025-02-13 19:10:56,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:56,682][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.3941345512866974, acc: 0.9171270728111267)
[2025-02-13 19:10:56,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:57,115][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.4500104784965515, acc: 0.8840579986572266)
[2025-02-13 19:10:57,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:57,494][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.3388385474681854, acc: 0.89682537317276)
[2025-02-13 19:10:57,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:57,895][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.43838223814964294, acc: 0.8928571343421936)
[2025-02-13 19:10:58,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:58,293][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.3838203549385071, acc: 0.893750011920929)
[2025-02-13 19:10:58,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:58,688][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.2649427056312561, acc: 0.9367088675498962)
[2025-02-13 19:10:58,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:59,066][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.32996752858161926, acc: 0.9345238208770752)
[2025-02-13 19:10:59,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:59,413][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.3635794520378113, acc: 0.9369369149208069)
[2025-02-13 19:10:59,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:59,805][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.24822816252708435, acc: 0.9424083828926086)
[2025-02-13 19:10:59,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:00,174][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.14498941600322723, acc: 0.9803921580314636)
[2025-02-13 19:11:00,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:00,540][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.1775195598602295, acc: 0.9593023061752319)
[2025-02-13 19:11:00,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:00,943][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.14866888523101807, acc: 0.9692307710647583)
[2025-02-13 19:11:01,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:01,308][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.2545912563800812, acc: 0.939393937587738)
[2025-02-13 19:11:01,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:01,722][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.3930199444293976, acc: 0.9195979833602905)
[2025-02-13 19:11:01,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:02,086][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.21341237425804138, acc: 0.9669811129570007)
[2025-02-13 19:11:02,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:02,424][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.260569304227829, acc: 0.9333333373069763)
[2025-02-13 19:11:02,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:02,817][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.19179512560367584, acc: 0.9627329111099243)
[2025-02-13 19:11:02,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:03,197][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.1629701852798462, acc: 0.9700000286102295)
[2025-02-13 19:11:03,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:03,601][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.13477903604507446, acc: 0.9533678889274597)
[2025-02-13 19:11:03,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:03,965][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.1672469824552536, acc: 0.9668508172035217)
[2025-02-13 19:11:04,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:04,369][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.253925085067749, acc: 0.9272727370262146)
[2025-02-13 19:11:04,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:04,776][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.436847060918808, acc: 0.8758170008659363)
[2025-02-13 19:11:04,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:05,196][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.37512585520744324, acc: 0.8971962332725525)
[2025-02-13 19:11:05,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:05,580][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.47782543301582336, acc: 0.8811880946159363)
[2025-02-13 19:11:05,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:05,983][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.6921994686126709, acc: 0.8571428656578064)
[2025-02-13 19:11:06,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:06,417][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.46139347553253174, acc: 0.885869562625885)
[2025-02-13 19:11:06,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:06,768][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.8932796716690063, acc: 0.8012422323226929)
[2025-02-13 19:11:06,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:07,183][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.22928588092327118, acc: 0.9444444179534912)
[2025-02-13 19:11:07,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:07,562][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.2925362288951874, acc: 0.9289617538452148)
[2025-02-13 19:11:07,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:07,935][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.4478103816509247, acc: 0.8965517282485962)
[2025-02-13 19:11:08,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:08,307][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.19337685406208038, acc: 0.9505494236946106)
[2025-02-13 19:11:08,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:08,713][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.39107128977775574, acc: 0.920634925365448)
[2025-02-13 19:11:08,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:09,089][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.26194119453430176, acc: 0.9285714030265808)
[2025-02-13 19:11:09,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:09,458][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.373156875371933, acc: 0.9134615659713745)
[2025-02-13 19:11:09,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:09,909][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.37107032537460327, acc: 0.9146919250488281)
[2025-02-13 19:11:10,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:10,315][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.5117953419685364, acc: 0.8909952640533447)
[2025-02-13 19:11:10,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:10,749][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.42606738209724426, acc: 0.8810811042785645)
[2025-02-13 19:11:10,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:11,109][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.36350324749946594, acc: 0.9289617538452148)
[2025-02-13 19:11:11,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:11,453][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.33552056550979614, acc: 0.9119170904159546)
[2025-02-13 19:11:11,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:11,845][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.250938355922699, acc: 0.9336734414100647)
[2025-02-13 19:11:11,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:12,216][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.7801657319068909, acc: 0.8285714387893677)
[2025-02-13 19:11:12,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:12,599][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.8470140099525452, acc: 0.7828282713890076)
[2025-02-13 19:11:12,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:12,971][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.5625450611114502, acc: 0.8780487775802612)
[2025-02-13 19:11:13,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:13,341][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.1905699372291565, acc: 0.9459459185600281)
[2025-02-13 19:11:13,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:13,720][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.6053043007850647, acc: 0.8756476640701294)
[2025-02-13 19:11:13,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:14,095][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.5523884892463684, acc: 0.8693467378616333)
[2025-02-13 19:11:14,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:14,450][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 1.307163119316101, acc: 0.7067307829856873)
[2025-02-13 19:11:14,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:14,816][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.5401695966720581, acc: 0.8743961453437805)
[2025-02-13 19:11:14,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:15,196][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.3726739287376404, acc: 0.8982036113739014)
[2025-02-13 19:11:15,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:15,619][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.22302334010601044, acc: 0.9473684430122375)
[2025-02-13 19:11:15,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:15,994][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.4016892611980438, acc: 0.8917197585105896)
[2025-02-13 19:11:16,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:16,385][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.14141875505447388, acc: 0.9644970297813416)
[2025-02-13 19:11:16,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:16,729][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.1722276657819748, acc: 0.9477124214172363)
[2025-02-13 19:11:16,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:17,123][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.32256513833999634, acc: 0.9281768202781677)
[2025-02-13 19:11:17,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:17,527][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.20991119742393494, acc: 0.949999988079071)
[2025-02-13 19:11:17,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:17,881][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.19706515967845917, acc: 0.9507042169570923)
[2025-02-13 19:11:18,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:18,241][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.18053625524044037, acc: 0.950276255607605)
[2025-02-13 19:11:18,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:18,600][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.1320073902606964, acc: 0.9545454382896423)
[2025-02-13 19:11:18,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:18,994][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.1989510953426361, acc: 0.9610389471054077)
[2025-02-13 19:11:19,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:19,365][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.18663740158081055, acc: 0.9453551769256592)
[2025-02-13 19:11:19,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:19,736][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.5020537972450256, acc: 0.9146341681480408)
[2025-02-13 19:11:19,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:20,098][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.20386894047260284, acc: 0.9496855139732361)
[2025-02-13 19:11:20,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:20,480][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.2745051681995392, acc: 0.9609755873680115)
[2025-02-13 19:11:20,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:20,822][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.2709052264690399, acc: 0.9370629191398621)
[2025-02-13 19:11:20,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:21,213][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.15970273315906525, acc: 0.9693251252174377)
[2025-02-13 19:11:21,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:21,632][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.22189444303512573, acc: 0.9444444179534912)
[2025-02-13 19:11:21,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:22,048][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.1435324102640152, acc: 0.9621621370315552)
[2025-02-13 19:11:22,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:22,497][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.18734976649284363, acc: 0.9714285731315613)
[2025-02-13 19:11:22,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:22,849][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.17674347758293152, acc: 0.9464285969734192)
[2025-02-13 19:11:22,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:23,212][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.22126737236976624, acc: 0.9431818127632141)
[2025-02-13 19:11:23,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:23,582][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.22556738555431366, acc: 0.949438214302063)
[2025-02-13 19:11:23,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:23,955][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.19946233928203583, acc: 0.954023003578186)
[2025-02-13 19:11:24,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:24,324][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.1000886783003807, acc: 0.9791666865348816)
[2025-02-13 19:11:24,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:24,713][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.16702723503112793, acc: 0.9526315927505493)
[2025-02-13 19:11:24,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:25,134][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.22116586565971375, acc: 0.9367088675498962)
[2025-02-13 19:11:25,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:25,550][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.1884179711341858, acc: 0.9466666579246521)
[2025-02-13 19:11:25,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:25,937][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.17758016288280487, acc: 0.9580419659614563)
[2025-02-13 19:11:26,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:26,317][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.4279748201370239, acc: 0.9008264541625977)
[2025-02-13 19:11:26,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:26,712][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.3155520558357239, acc: 0.9424460530281067)
[2025-02-13 19:11:26,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:27,094][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.47207415103912354, acc: 0.9194630980491638)
[2025-02-13 19:11:27,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:27,502][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.3954594135284424, acc: 0.9205297827720642)
[2025-02-13 19:11:27,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:27,892][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.20610588788986206, acc: 0.9543147087097168)
[2025-02-13 19:11:28,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:28,273][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.33382081985473633, acc: 0.9295774698257446)
[2025-02-13 19:11:28,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:28,674][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.2937640845775604, acc: 0.9320987462997437)
[2025-02-13 19:11:28,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:29,080][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.2430621236562729, acc: 0.9465240836143494)
[2025-02-13 19:11:29,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:29,516][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.27818673849105835, acc: 0.9204545617103577)
[2025-02-13 19:11:29,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:29,985][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.930056095123291, acc: 0.8320000171661377)
[2025-02-13 19:11:30,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:30,405][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.4448995590209961, acc: 0.9112426042556763)
[2025-02-13 19:11:30,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:30,803][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.2973701059818268, acc: 0.9325153231620789)
[2025-02-13 19:11:30,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:31,209][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.27660223841667175, acc: 0.9534883499145508)
[2025-02-13 19:11:31,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:31,622][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.2252667248249054, acc: 0.9554139971733093)
[2025-02-13 19:11:31,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:32,039][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.36698421835899353, acc: 0.9025974273681641)
[2025-02-13 19:11:32,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:32,439][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.2012341320514679, acc: 0.9719101190567017)
[2025-02-13 19:11:32,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:32,808][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.21087506413459778, acc: 0.9530201554298401)
[2025-02-13 19:11:32,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:33,162][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.47300219535827637, acc: 0.9011628031730652)
[2025-02-13 19:11:33,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:33,530][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.17843617498874664, acc: 0.9529411792755127)
[2025-02-13 19:11:33,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:33,900][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.14781531691551208, acc: 0.9679144620895386)
[2025-02-13 19:11:34,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:34,294][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.1878660023212433, acc: 0.9408602118492126)
[2025-02-13 19:11:34,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:34,709][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.31679508090019226, acc: 0.9430052042007446)
[2025-02-13 19:11:34,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:35,141][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.23340828716754913, acc: 0.936170220375061)
[2025-02-13 19:11:35,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:35,548][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.11856700479984283, acc: 0.9754601120948792)
[2025-02-13 19:11:35,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:35,970][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.2804916203022003, acc: 0.9222797751426697)
[2025-02-13 19:11:36,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:36,360][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.41411998867988586, acc: 0.8823529481887817)
[2025-02-13 19:11:36,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:36,783][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.37112781405448914, acc: 0.8999999761581421)
[2025-02-13 19:11:36,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:37,176][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.08702414482831955, acc: 0.988304078578949)
[2025-02-13 19:11:37,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:37,541][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.18770645558834076, acc: 0.9505494236946106)
[2025-02-13 19:11:37,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:37,912][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.09223096817731857, acc: 0.9685534834861755)
[2025-02-13 19:11:38,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:38,286][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.3686949610710144, acc: 0.9212121367454529)
[2025-02-13 19:11:38,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:38,667][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.0651380866765976, acc: 0.9937499761581421)
[2025-02-13 19:11:38,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:39,067][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.26600074768066406, acc: 0.949999988079071)
[2025-02-13 19:11:39,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:39,438][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.26131439208984375, acc: 0.9441340565681458)
[2025-02-13 19:11:39,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:39,819][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.14971190690994263, acc: 0.946107804775238)
[2025-02-13 19:11:39,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:40,196][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.3632851839065552, acc: 0.9044944047927856)
[2025-02-13 19:11:40,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:40,580][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.19403743743896484, acc: 0.9651162624359131)
[2025-02-13 19:11:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:40,963][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.04864194616675377, acc: 0.9883720874786377)
[2025-02-13 19:11:41,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:41,365][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.29880401492118835, acc: 0.9503546357154846)
[2025-02-13 19:11:41,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:41,771][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.3039097785949707, acc: 0.9432623982429504)
[2025-02-13 19:11:41,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:42,167][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.1500229835510254, acc: 0.9662162065505981)
[2025-02-13 19:11:42,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:42,567][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.11398892104625702, acc: 0.9813084006309509)
[2025-02-13 19:11:42,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:42,983][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.08218095451593399, acc: 0.9818181991577148)
[2025-02-13 19:11:43,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:43,376][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.09669775515794754, acc: 0.9836065769195557)
[2025-02-13 19:11:43,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:43,797][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.13225549459457397, acc: 0.9707602262496948)
[2025-02-13 19:11:43,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:44,180][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.11711882054805756, acc: 0.9629629850387573)
[2025-02-13 19:11:44,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:44,558][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.12280350178480148, acc: 0.9664429426193237)
[2025-02-13 19:11:44,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:44,950][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.46156129240989685, acc: 0.9115646481513977)
[2025-02-13 19:11:45,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:45,321][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.12185350060462952, acc: 0.9711538553237915)
[2025-02-13 19:11:45,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:45,699][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.2401210516691208, acc: 0.932692289352417)
[2025-02-13 19:11:45,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:46,072][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.250784307718277, acc: 0.948051929473877)
[2025-02-13 19:11:46,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:46,496][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.13493193686008453, acc: 0.976047933101654)
[2025-02-13 19:11:46,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:46,897][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.17278021574020386, acc: 0.9698795080184937)
[2025-02-13 19:11:47,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:47,247][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.12570500373840332, acc: 0.9555555582046509)
[2025-02-13 19:11:47,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:47,640][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.11585859954357147, acc: 0.9726027250289917)
[2025-02-13 19:11:47,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:48,035][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.4119625985622406, acc: 0.9166666865348816)
[2025-02-13 19:11:48,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:48,415][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.2048833966255188, acc: 0.9515151381492615)
[2025-02-13 19:11:48,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:48,816][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.12387995421886444, acc: 0.9644970297813416)
[2025-02-13 19:11:48,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:49,200][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.18152284622192383, acc: 0.9675324559211731)
[2025-02-13 19:11:49,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:49,591][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.4340135157108307, acc: 0.9029850959777832)
[2025-02-13 19:11:49,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:49,959][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.2363966703414917, acc: 0.955974817276001)
[2025-02-13 19:11:50,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:50,342][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.20373770594596863, acc: 0.9523809552192688)
[2025-02-13 19:11:50,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:50,713][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.2304360717535019, acc: 0.9320987462997437)
[2025-02-13 19:11:50,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:51,087][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.27104490995407104, acc: 0.9352940917015076)
[2025-02-13 19:11:51,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:51,448][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.27998730540275574, acc: 0.9367088675498962)
[2025-02-13 19:11:51,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:51,809][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.14834946393966675, acc: 0.977011501789093)
[2025-02-13 19:11:51,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:52,185][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.17729522287845612, acc: 0.9599999785423279)
[2025-02-13 19:11:52,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:52,560][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.15317267179489136, acc: 0.9698795080184937)
[2025-02-13 19:11:52,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:52,961][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.18752798438072205, acc: 0.9662162065505981)
[2025-02-13 19:11:53,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:53,375][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.33004504442214966, acc: 0.915730357170105)
[2025-02-13 19:11:53,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:53,791][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.19903522729873657, acc: 0.9572649598121643)
[2025-02-13 19:11:53,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:54,186][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.12640433013439178, acc: 0.9710144996643066)
[2025-02-13 19:11:54,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:54,570][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.19655926525592804, acc: 0.949367105960846)
[2025-02-13 19:11:54,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:54,947][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.19415892660617828, acc: 0.9651162624359131)
[2025-02-13 19:11:55,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:55,357][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.21758107841014862, acc: 0.9508196711540222)
[2025-02-13 19:11:55,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:55,723][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.07369795441627502, acc: 0.9833333492279053)
[2025-02-13 19:11:55,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:56,104][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.1804051548242569, acc: 0.9661017060279846)
[2025-02-13 19:11:56,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:56,473][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.08160501718521118, acc: 0.9764705896377563)
[2025-02-13 19:11:56,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:56,848][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.08302170783281326, acc: 0.9785714149475098)
[2025-02-13 19:11:56,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:57,225][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.26163750886917114, acc: 0.9617486596107483)
[2025-02-13 19:11:57,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:57,600][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.27914074063301086, acc: 0.9176470637321472)
[2025-02-13 19:11:57,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:57,978][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.1748618185520172, acc: 0.9516128897666931)
[2025-02-13 19:11:58,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:58,354][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.18677207827568054, acc: 0.9426751732826233)
[2025-02-13 19:11:58,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:58,772][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.17803528904914856, acc: 0.9675324559211731)
[2025-02-13 19:11:58,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:59,198][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.1848212033510208, acc: 0.9608938694000244)
[2025-02-13 19:11:59,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:59,616][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.2260863035917282, acc: 0.9444444179534912)
[2025-02-13 19:11:59,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:00,042][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.15540529787540436, acc: 0.9542483687400818)
[2025-02-13 19:12:00,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:00,482][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.09057106077671051, acc: 0.9806451797485352)
[2025-02-13 19:12:00,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:00,880][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.16276037693023682, acc: 0.9671052694320679)
[2025-02-13 19:12:01,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:01,289][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.3502131998538971, acc: 0.9230769276618958)
[2025-02-13 19:12:01,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:01,655][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.1529112011194229, acc: 0.951724112033844)
[2025-02-13 19:12:01,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:02,035][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.17366717755794525, acc: 0.9570552110671997)
[2025-02-13 19:12:02,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:02,401][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.08896005153656006, acc: 0.9817073345184326)
[2025-02-13 19:12:02,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:02,836][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.24155624210834503, acc: 0.9383561611175537)
[2025-02-13 19:12:02,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:03,209][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.05277744308114052, acc: 0.9898989796638489)
[2025-02-13 19:12:03,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:03,633][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.1290942281484604, acc: 0.9726775884628296)
[2025-02-13 19:12:03,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:04,036][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.09116458147764206, acc: 0.9819276928901672)
[2025-02-13 19:12:04,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:04,431][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.11132675409317017, acc: 0.9702380895614624)
[2025-02-13 19:12:04,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:04,842][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.13801905512809753, acc: 0.966292142868042)
[2025-02-13 19:12:04,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:05,209][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.07191582024097443, acc: 0.9638554453849792)
[2025-02-13 19:12:05,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:05,591][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.10606875270605087, acc: 0.9866666793823242)
[2025-02-13 19:12:05,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:05,954][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.1666630655527115, acc: 0.9615384340286255)
[2025-02-13 19:12:06,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:06,329][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.13652180135250092, acc: 0.9664804339408875)
[2025-02-13 19:12:06,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:06,722][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.09132126718759537, acc: 0.9830508232116699)
[2025-02-13 19:12:06,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:07,093][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.2584030032157898, acc: 0.9222797751426697)
[2025-02-13 19:12:07,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:07,494][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.22955507040023804, acc: 0.9452054500579834)
[2025-02-13 19:12:07,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:07,885][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.33015376329421997, acc: 0.9080459475517273)
[2025-02-13 19:12:08,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:08,281][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.19887395203113556, acc: 0.9548872113227844)
[2025-02-13 19:12:08,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:08,656][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.20550796389579773, acc: 0.9668874144554138)
[2025-02-13 19:12:08,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:09,011][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.10572247207164764, acc: 0.9632353186607361)
[2025-02-13 19:12:09,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:09,376][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.24426543712615967, acc: 0.9437500238418579)
[2025-02-13 19:12:09,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:09,773][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.27265408635139465, acc: 0.9245283007621765)
[2025-02-13 19:12:09,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:10,166][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.1393924355506897, acc: 0.9702380895614624)
[2025-02-13 19:12:10,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:10,562][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.1307024359703064, acc: 0.9626865386962891)
[2025-02-13 19:12:10,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:10,967][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.5110296010971069, acc: 0.8826530575752258)
[2025-02-13 19:12:11,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:11,364][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.38293901085853577, acc: 0.9064748287200928)
[2025-02-13 19:12:11,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:11,747][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.17627288401126862, acc: 0.9646017551422119)
[2025-02-13 19:12:11,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:12,189][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.22061069309711456, acc: 0.9430894255638123)
[2025-02-13 19:12:12,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:12,547][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.09481551498174667, acc: 0.9589040875434875)
[2025-02-13 19:12:12,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:12,954][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.22815650701522827, acc: 0.9492753744125366)
[2025-02-13 19:12:13,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:13,321][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.14554400742053986, acc: 0.9620253443717957)
[2025-02-13 19:12:13,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:13,758][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.3699791133403778, acc: 0.9342105388641357)
[2025-02-13 19:12:13,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:14,134][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.3070603013038635, acc: 0.9448819160461426)
[2025-02-13 19:12:14,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:14,491][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.13968072831630707, acc: 0.9639639854431152)
[2025-02-13 19:12:14,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:14,848][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.15216495096683502, acc: 0.9700000286102295)
[2025-02-13 19:12:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:15,239][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.18366947770118713, acc: 0.9591836929321289)
[2025-02-13 19:12:15,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:15,637][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.07487864792346954, acc: 0.985401451587677)
[2025-02-13 19:12:15,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:16,003][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.13088181614875793, acc: 0.96875)
[2025-02-13 19:12:16,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:16,377][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.19492202997207642, acc: 0.9459459185600281)
[2025-02-13 19:12:16,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:16,769][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.18108493089675903, acc: 0.9375)
[2025-02-13 19:12:16,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:17,162][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.10760088264942169, acc: 0.9599999785423279)
[2025-02-13 19:12:17,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:17,554][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.12749677896499634, acc: 0.9822485446929932)
[2025-02-13 19:12:17,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:17,928][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.39847612380981445, acc: 0.9136690497398376)
[2025-02-13 19:12:18,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:18,299][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.14385563135147095, acc: 0.9640287756919861)
[2025-02-13 19:12:18,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:18,688][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.13388560712337494, acc: 0.9759036302566528)
[2025-02-13 19:12:18,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:19,074][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.0834403857588768, acc: 0.9849624037742615)
[2025-02-13 19:12:19,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:19,439][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.22986148297786713, acc: 0.9487179517745972)
[2025-02-13 19:12:19,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:19,848][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.06775858998298645, acc: 1.0)
[2025-02-13 19:12:19,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:20,261][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.15047980844974518, acc: 0.9536423683166504)
[2025-02-13 19:12:20,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:20,620][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.16293834149837494, acc: 0.9508196711540222)
[2025-02-13 19:12:20,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:21,029][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.17772753536701202, acc: 0.9659863710403442)
[2025-02-13 19:12:21,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:21,412][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.11122388392686844, acc: 0.9856114983558655)
[2025-02-13 19:12:21,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:21,798][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.14942681789398193, acc: 0.9494949579238892)
[2025-02-13 19:12:21,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:22,168][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.15117304027080536, acc: 0.9836065769195557)
[2025-02-13 19:12:22,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:22,576][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.10982848703861237, acc: 0.9837398529052734)
[2025-02-13 19:12:22,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:22,933][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.33726072311401367, acc: 0.9291338324546814)
[2025-02-13 19:12:23,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:23,308][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.6173460483551025, acc: 0.8627451062202454)
[2025-02-13 19:12:23,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:23,722][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.3900647461414337, acc: 0.8974359035491943)
[2025-02-13 19:12:23,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:24,091][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.26261523365974426, acc: 0.9306358098983765)
[2025-02-13 19:12:24,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:24,428][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.40553146600723267, acc: 0.909604549407959)
[2025-02-13 19:12:24,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:24,800][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.3705115020275116, acc: 0.903954803943634)
[2025-02-13 19:12:24,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:25,187][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.3967226445674896, acc: 0.9137930870056152)
[2025-02-13 19:12:25,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:25,558][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.5190295577049255, acc: 0.8794326186180115)
[2025-02-13 19:12:25,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:25,920][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.33494776487350464, acc: 0.9337748289108276)
[2025-02-13 19:12:26,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:26,315][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.4019985795021057, acc: 0.8846153616905212)
[2025-02-13 19:12:26,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:26,717][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.18010935187339783, acc: 0.949999988079071)
[2025-02-13 19:12:26,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:27,084][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.22134137153625488, acc: 0.9591836929321289)
[2025-02-13 19:12:27,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:27,510][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.47177210450172424, acc: 0.9047619104385376)
[2025-02-13 19:12:27,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:27,858][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.2484651356935501, acc: 0.9333333373069763)
[2025-02-13 19:12:28,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:28,302][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.44981563091278076, acc: 0.9064327478408813)
[2025-02-13 19:12:28,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:28,676][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.42930683493614197, acc: 0.916201114654541)
[2025-02-13 19:12:28,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:29,045][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.21773245930671692, acc: 0.9756097793579102)
[2025-02-13 19:12:29,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:29,458][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.5018940567970276, acc: 0.8758170008659363)
[2025-02-13 19:12:29,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:29,807][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.2179616540670395, acc: 0.9451219439506531)
[2025-02-13 19:12:29,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:30,239][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.2380077987909317, acc: 0.9352940917015076)
[2025-02-13 19:12:30,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:30,646][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.2855585515499115, acc: 0.903954803943634)
[2025-02-13 19:12:30,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:31,052][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.35234972834587097, acc: 0.8940397500991821)
[2025-02-13 19:12:31,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:31,454][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.05182531476020813, acc: 1.0)
[2025-02-13 19:12:31,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:31,857][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.4041415750980377, acc: 0.8932584524154663)
[2025-02-13 19:12:32,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:32,232][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.29548099637031555, acc: 0.9290322661399841)
[2025-02-13 19:12:32,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:32,580][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.2191692590713501, acc: 0.9285714030265808)
[2025-02-13 19:12:32,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:32,956][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.2580741345882416, acc: 0.9281045794487)
[2025-02-13 19:12:33,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:33,357][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.2586151361465454, acc: 0.9259259104728699)
[2025-02-13 19:12:33,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:33,752][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.4645523726940155, acc: 0.8849557638168335)
[2025-02-13 19:12:33,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:34,121][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.3273818790912628, acc: 0.8933333158493042)
[2025-02-13 19:12:34,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:34,552][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.29059627652168274, acc: 0.9438202381134033)
[2025-02-13 19:12:34,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:34,942][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.49060937762260437, acc: 0.9047619104385376)
[2025-02-13 19:12:35,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:35,332][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.4081239700317383, acc: 0.915032684803009)
[2025-02-13 19:12:35,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:35,762][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.23659296333789825, acc: 0.9568345546722412)
[2025-02-13 19:12:35,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:36,162][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.41646990180015564, acc: 0.8938547372817993)
[2025-02-13 19:12:36,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:36,539][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.38058802485466003, acc: 0.8949999809265137)
[2025-02-13 19:12:36,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:36,928][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.3059658706188202, acc: 0.9464285969734192)
[2025-02-13 19:12:37,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:37,300][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.5167137980461121, acc: 0.8882978558540344)
[2025-02-13 19:12:37,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:37,670][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.3144385516643524, acc: 0.9269663095474243)
[2025-02-13 19:12:37,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:38,074][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.4539269804954529, acc: 0.924369752407074)
[2025-02-13 19:12:38,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:38,490][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.5820751786231995, acc: 0.8477157354354858)
[2025-02-13 19:12:38,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:38,859][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.1916363537311554, acc: 0.9572649598121643)
[2025-02-13 19:12:38,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:39,215][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.2798340320587158, acc: 0.95652174949646)
[2025-02-13 19:12:39,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:39,576][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.29118701815605164, acc: 0.9312169551849365)
[2025-02-13 19:12:39,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:39,941][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.4447212815284729, acc: 0.903743326663971)
[2025-02-13 19:12:40,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:40,307][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.34657058119773865, acc: 0.9246575236320496)
[2025-02-13 19:12:40,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:40,688][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.4344072937965393, acc: 0.8807339668273926)
[2025-02-13 19:12:40,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:41,107][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.458045095205307, acc: 0.8619047403335571)
[2025-02-13 19:12:41,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:41,488][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.32225459814071655, acc: 0.9107142686843872)
[2025-02-13 19:12:41,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:41,870][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.28652405738830566, acc: 0.931034505367279)
[2025-02-13 19:12:42,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:42,293][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.2949598729610443, acc: 0.907975435256958)
[2025-02-13 19:12:42,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:42,684][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.370315283536911, acc: 0.9277777671813965)
[2025-02-13 19:12:42,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:43,103][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.22212959825992584, acc: 0.946107804775238)
[2025-02-13 19:12:43,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:43,479][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.304080605506897, acc: 0.9202898740768433)
[2025-02-13 19:12:43,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:43,870][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.3814384341239929, acc: 0.9289340376853943)
[2025-02-13 19:12:44,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:44,263][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.10703939944505692, acc: 0.9739130139350891)
[2025-02-13 19:12:44,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:44,666][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.29747486114501953, acc: 0.9395604133605957)
[2025-02-13 19:12:44,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:45,040][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.17537851631641388, acc: 0.9644970297813416)
[2025-02-13 19:12:45,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:45,438][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.5386306047439575, acc: 0.9055555462837219)
[2025-02-13 19:12:45,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:45,856][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.38374534249305725, acc: 0.9273743033409119)
[2025-02-13 19:12:46,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:46,294][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.5846276879310608, acc: 0.8527607321739197)
[2025-02-13 19:12:46,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:46,747][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.49368661642074585, acc: 0.8855721354484558)
[2025-02-13 19:12:46,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:47,171][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.7386385202407837, acc: 0.8496240377426147)
[2025-02-13 19:12:47,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:47,576][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.5621543526649475, acc: 0.9004974961280823)
[2025-02-13 19:12:47,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:47,977][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.36660492420196533, acc: 0.9085714221000671)
[2025-02-13 19:12:48,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:48,371][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.9131731390953064, acc: 0.8333333134651184)
[2025-02-13 19:12:48,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:48,786][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.5303257703781128, acc: 0.8720930218696594)
[2025-02-13 19:12:48,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:49,256][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.3532384932041168, acc: 0.9259259104728699)
[2025-02-13 19:12:49,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:49,652][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.34828296303749084, acc: 0.9314285516738892)
[2025-02-13 19:12:49,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:50,032][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.665154755115509, acc: 0.8447204828262329)
[2025-02-13 19:12:50,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:50,400][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.36729782819747925, acc: 0.91847825050354)
[2025-02-13 19:12:50,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:50,803][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.43467313051223755, acc: 0.905940592288971)
[2025-02-13 19:12:50,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:51,231][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.6852902173995972, acc: 0.8216215968132019)
[2025-02-13 19:12:51,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:51,664][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.6393570899963379, acc: 0.8588235378265381)
[2025-02-13 19:12:51,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:52,072][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.22769486904144287, acc: 0.9463414549827576)
[2025-02-13 19:12:52,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:52,474][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.3372488021850586, acc: 0.932584285736084)
[2025-02-13 19:12:52,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:52,869][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.3105521500110626, acc: 0.9215686321258545)
[2025-02-13 19:12:53,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:53,234][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.23401805758476257, acc: 0.9285714030265808)
[2025-02-13 19:12:53,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:53,593][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.2072291225194931, acc: 0.9316770434379578)
[2025-02-13 19:12:53,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:54,026][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.3689958453178406, acc: 0.9360465407371521)
[2025-02-13 19:12:54,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:54,448][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.22474487125873566, acc: 0.9177215099334717)
[2025-02-13 19:12:54,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:54,854][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.6976506114006042, acc: 0.8636363744735718)
[2025-02-13 19:12:54,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:55,249][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.2806171774864197, acc: 0.9179104566574097)
[2025-02-13 19:12:55,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:55,709][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.4533211588859558, acc: 0.8888888955116272)
[2025-02-13 19:12:55,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:56,101][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.15991419553756714, acc: 0.9526627063751221)
[2025-02-13 19:12:56,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:56,503][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.2095935046672821, acc: 0.9629629850387573)
[2025-02-13 19:12:56,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:56,881][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.24200482666492462, acc: 0.9378238320350647)
[2025-02-13 19:12:57,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:57,322][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.3125267028808594, acc: 0.9186046719551086)
[2025-02-13 19:12:57,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:57,694][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.1754923164844513, acc: 0.9457831382751465)
[2025-02-13 19:12:57,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:58,076][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.1078098863363266, acc: 0.9767441749572754)
[2025-02-13 19:12:58,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:58,459][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.09794767200946808, acc: 0.9810126423835754)
[2025-02-13 19:12:58,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:58,827][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.0872245654463768, acc: 0.9714285731315613)
[2025-02-13 19:12:58,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:59,198][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.07028597593307495, acc: 0.9850746393203735)
[2025-02-13 19:12:59,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:59,570][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.15697577595710754, acc: 0.954285740852356)
[2025-02-13 19:12:59,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:59,988][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.25838905572891235, acc: 0.9485714435577393)
[2025-02-13 19:13:00,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:00,377][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.09959883987903595, acc: 0.9698795080184937)
[2025-02-13 19:13:00,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:00,756][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.11932271718978882, acc: 0.9723756909370422)
[2025-02-13 19:13:00,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:01,112][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.1874171793460846, acc: 0.9567901492118835)
[2025-02-13 19:13:01,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:01,516][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.05467066168785095, acc: 0.9878787994384766)
[2025-02-13 19:13:01,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:01,922][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.1258750855922699, acc: 0.9715909361839294)
[2025-02-13 19:13:02,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:02,328][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.038807254284620285, acc: 0.9938650131225586)
[2025-02-13 19:13:02,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:02,802][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.05436008796095848, acc: 0.994350254535675)
[2025-02-13 19:13:02,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:03,194][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.07140795141458511, acc: 0.9848484992980957)
[2025-02-13 19:13:03,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:03,613][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.10502482205629349, acc: 0.9835164546966553)
[2025-02-13 19:13:03,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:03,976][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.04811635985970497, acc: 0.9810126423835754)
[2025-02-13 19:13:04,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:04,332][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.08481352031230927, acc: 0.9731543660163879)
[2025-02-13 19:13:04,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:04,764][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.06040879338979721, acc: 0.9867549538612366)
[2025-02-13 19:13:04,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:05,151][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.24149875342845917, acc: 0.9453125)
[2025-02-13 19:13:05,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:05,524][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.10729774087667465, acc: 0.9822485446929932)
[2025-02-13 19:13:05,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:05,898][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.32815879583358765, acc: 0.9350000023841858)
[2025-02-13 19:13:06,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:06,244][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.2513226568698883, acc: 0.9411764740943909)
[2025-02-13 19:13:06,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:06,626][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.26577267050743103, acc: 0.9473684430122375)
[2025-02-13 19:13:06,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:06,977][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.22702357172966003, acc: 0.934959352016449)
[2025-02-13 19:13:07,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:07,373][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.2550140619277954, acc: 0.9208633303642273)
[2025-02-13 19:13:07,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:07,769][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.48705920577049255, acc: 0.8938053250312805)
[2025-02-13 19:13:07,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:08,167][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.5996126532554626, acc: 0.8677685856819153)
[2025-02-13 19:13:08,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:08,538][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.521766722202301, acc: 0.8849557638168335)
[2025-02-13 19:13:08,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:08,865][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.4180413484573364, acc: 0.9139072895050049)
[2025-02-13 19:13:09,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:09,244][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.21649940311908722, acc: 0.9629629850387573)
[2025-02-13 19:13:09,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:09,617][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.2930053472518921, acc: 0.9428571462631226)
[2025-02-13 19:13:09,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:09,968][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.42605358362197876, acc: 0.9090909361839294)
[2025-02-13 19:13:10,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:10,334][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.3119067847728729, acc: 0.9251700639724731)
[2025-02-13 19:13:10,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:10,702][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.1641235649585724, acc: 0.982758641242981)
[2025-02-13 19:13:10,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:11,108][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.28468549251556396, acc: 0.9142857193946838)
[2025-02-13 19:13:11,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:11,473][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.31862661242485046, acc: 0.9548872113227844)
[2025-02-13 19:13:11,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:11,773][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.5127450227737427, acc: 0.8703703880310059)
[2025-02-13 19:13:11,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:12,136][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.12210782617330551, acc: 0.9599999785423279)
[2025-02-13 19:13:12,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:12,529][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.3051852881908417, acc: 0.9345238208770752)
[2025-02-13 19:13:12,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:12,906][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.47862550616264343, acc: 0.9014084339141846)
[2025-02-13 19:13:13,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:13,343][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.14849701523780823, acc: 0.9794520735740662)
[2025-02-13 19:13:13,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:13,722][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.32046154141426086, acc: 0.925000011920929)
[2025-02-13 19:13:13,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:14,145][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.3079240322113037, acc: 0.9200000166893005)
[2025-02-13 19:13:14,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:14,521][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.1718180924654007, acc: 0.9507042169570923)
[2025-02-13 19:13:14,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:14,891][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.41709938645362854, acc: 0.8500000238418579)
[2025-02-13 19:13:15,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:15,260][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.15638640522956848, acc: 0.9793814420700073)
[2025-02-13 19:13:15,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:15,677][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.3103269636631012, acc: 0.9255319237709045)
[2025-02-13 19:13:15,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:16,089][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.2697046995162964, acc: 0.949999988079071)
[2025-02-13 19:13:16,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:16,484][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.09002725034952164, acc: 0.9852941036224365)
[2025-02-13 19:13:16,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:16,878][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.19698160886764526, acc: 0.9241379499435425)
[2025-02-13 19:13:17,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:17,253][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.13620789349079132, acc: 0.9709302186965942)
[2025-02-13 19:13:17,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:17,606][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.184361070394516, acc: 0.9487179517745972)
[2025-02-13 19:13:17,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:18,033][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.4973711371421814, acc: 0.9236111044883728)
[2025-02-13 19:13:18,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:18,443][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.32852867245674133, acc: 0.9424460530281067)
[2025-02-13 19:13:18,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:18,866][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.4234132468700409, acc: 0.9124087691307068)
[2025-02-13 19:13:18,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:19,216][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.2233542948961258, acc: 0.9735099077224731)
[2025-02-13 19:13:19,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:19,570][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.25659167766571045, acc: 0.9255319237709045)
[2025-02-13 19:13:19,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:19,938][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.48389214277267456, acc: 0.9281437397003174)
[2025-02-13 19:13:20,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:20,335][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.08595407754182816, acc: 0.9887005686759949)
[2025-02-13 19:13:20,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:20,723][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.24572893977165222, acc: 0.9328358173370361)
[2025-02-13 19:13:20,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:21,104][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.30334189534187317, acc: 0.934959352016449)
[2025-02-13 19:13:21,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:21,486][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.11674083024263382, acc: 0.9824561476707458)
[2025-02-13 19:13:21,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:21,902][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 1.5480237007141113, acc: 0.7037037014961243)
[2025-02-13 19:13:22,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:22,319][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.983811616897583, acc: 0.8142856955528259)
[2025-02-13 19:13:22,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:22,682][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.6865178942680359, acc: 0.8318583965301514)
[2025-02-13 19:13:22,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:23,028][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.7463924288749695, acc: 0.844660222530365)
[2025-02-13 19:13:23,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:23,394][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.648747444152832, acc: 0.8627451062202454)
[2025-02-13 19:13:23,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:23,794][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.46864286065101624, acc: 0.9111111164093018)
[2025-02-13 19:13:23,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:24,153][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.5352543592453003, acc: 0.8677685856819153)
[2025-02-13 19:13:24,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:24,557][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.3415229618549347, acc: 0.9451219439506531)
[2025-02-13 19:13:24,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:24,996][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.6253803968429565, acc: 0.8616352081298828)
[2025-02-13 19:13:25,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:25,376][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.5419325232505798, acc: 0.8928571343421936)
[2025-02-13 19:13:25,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:25,725][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.434123694896698, acc: 0.916167676448822)
[2025-02-13 19:13:25,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:26,111][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.5270330309867859, acc: 0.8623188138008118)
[2025-02-13 19:13:26,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:26,535][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.26696300506591797, acc: 0.9251700639724731)
[2025-02-13 19:13:26,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:26,944][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.27967751026153564, acc: 0.9396551847457886)
[2025-02-13 19:13:27,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:27,346][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.2887280583381653, acc: 0.924369752407074)
[2025-02-13 19:13:27,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:27,779][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.26323169469833374, acc: 0.914893627166748)
[2025-02-13 19:13:27,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:28,203][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.17631162703037262, acc: 0.9354838728904724)
[2025-02-13 19:13:28,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:28,578][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.739047110080719, acc: 0.8214285969734192)
[2025-02-13 19:13:28,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:28,941][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.4573013484477997, acc: 0.875)
[2025-02-13 19:13:29,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:29,305][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.5961209535598755, acc: 0.8981481194496155)
[2025-02-13 19:13:29,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:29,717][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.22450050711631775, acc: 0.948051929473877)
[2025-02-13 19:13:29,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:30,134][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.43566784262657166, acc: 0.9175257682800293)
[2025-02-13 19:13:30,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:30,534][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.265071302652359, acc: 0.9518072009086609)
[2025-02-13 19:13:30,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:30,928][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.525519073009491, acc: 0.8738738894462585)
[2025-02-13 19:13:31,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:31,348][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.34833353757858276, acc: 0.9124087691307068)
[2025-02-13 19:13:31,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:31,748][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.3870839774608612, acc: 0.8907563090324402)
[2025-02-13 19:13:31,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:32,135][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.37242260575294495, acc: 0.9083333611488342)
[2025-02-13 19:13:32,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:32,501][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.29368939995765686, acc: 0.9296875)
[2025-02-13 19:13:32,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:32,882][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.405528724193573, acc: 0.9256756901741028)
[2025-02-13 19:13:32,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:33,244][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.431537926197052, acc: 0.8804348111152649)
[2025-02-13 19:13:33,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:33,608][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.37272587418556213, acc: 0.9262295365333557)
[2025-02-13 19:13:33,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:33,951][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.25405895709991455, acc: 0.9285714030265808)
[2025-02-13 19:13:34,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:34,314][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.2777411937713623, acc: 0.9222221970558167)
[2025-02-13 19:13:34,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:34,674][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.5406571626663208, acc: 0.8684210777282715)
[2025-02-13 19:13:34,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:35,063][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.4310997724533081, acc: 0.9014778137207031)
[2025-02-13 19:13:35,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:35,471][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.23335406184196472, acc: 0.9344262480735779)
[2025-02-13 19:13:35,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:35,868][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.34264981746673584, acc: 0.921875)
[2025-02-13 19:13:36,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:36,270][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.3338364064693451, acc: 0.9235293865203857)
[2025-02-13 19:13:36,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:36,705][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.12609758973121643, acc: 0.9808917045593262)
[2025-02-13 19:13:36,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:37,099][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.22744612395763397, acc: 0.9700000286102295)
[2025-02-13 19:13:37,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:37,521][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.2685423791408539, acc: 0.9397590160369873)
[2025-02-13 19:13:37,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:37,927][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.46280360221862793, acc: 0.9263157844543457)
[2025-02-13 19:13:38,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:38,316][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.2156876027584076, acc: 0.9444444179534912)
[2025-02-13 19:13:38,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:38,711][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.5253968834877014, acc: 0.8862559199333191)
[2025-02-13 19:13:38,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:39,095][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.34844276309013367, acc: 0.9399999976158142)
[2025-02-13 19:13:39,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:39,471][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.33441534638404846, acc: 0.9396985173225403)
[2025-02-13 19:13:39,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:39,845][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.44372138381004333, acc: 0.9219512343406677)
[2025-02-13 19:13:39,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:40,214][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.2911050021648407, acc: 0.9234972596168518)
[2025-02-13 19:13:40,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:40,601][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.1991928219795227, acc: 0.9523809552192688)
[2025-02-13 19:13:40,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:40,965][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.1480233073234558, acc: 0.9657142758369446)
[2025-02-13 19:13:41,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:41,345][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.45971807837486267, acc: 0.9040403962135315)
[2025-02-13 19:13:41,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:41,720][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.2905290424823761, acc: 0.9269406199455261)
[2025-02-13 19:13:41,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:42,104][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.35967695713043213, acc: 0.9115044474601746)
[2025-02-13 19:13:42,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:42,462][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.27408522367477417, acc: 0.9130434989929199)
[2025-02-13 19:13:42,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:42,828][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.14067427814006805, acc: 0.96875)
[2025-02-13 19:13:42,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:43,194][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.16760961711406708, acc: 0.9727891087532043)
[2025-02-13 19:13:43,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:43,561][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.17867588996887207, acc: 0.9613526463508606)
[2025-02-13 19:13:43,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:43,928][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.20006166398525238, acc: 0.9514563083648682)
[2025-02-13 19:13:44,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:44,311][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.3376573324203491, acc: 0.9111111164093018)
[2025-02-13 19:13:44,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:44,696][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.22276146709918976, acc: 0.9421965479850769)
[2025-02-13 19:13:44,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:45,062][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.43670421838760376, acc: 0.8789808750152588)
[2025-02-13 19:13:45,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:45,448][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 1.8324265480041504, acc: 0.561904788017273)
[2025-02-13 19:13:45,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:45,828][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 1.4313806295394897, acc: 0.675000011920929)
[2025-02-13 19:13:45,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:46,233][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.5195672512054443, acc: 0.8741722106933594)
[2025-02-13 19:13:46,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:46,662][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.23651054501533508, acc: 0.9360465407371521)
[2025-02-13 19:13:46,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:47,046][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.21699386835098267, acc: 0.9520958065986633)
[2025-02-13 19:13:47,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:47,446][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.17343725264072418, acc: 0.939393937587738)
[2025-02-13 19:13:47,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:47,838][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.5782942771911621, acc: 0.8703703880310059)
[2025-02-13 19:13:47,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:48,189][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.3766981363296509, acc: 0.8943089246749878)
[2025-02-13 19:13:48,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:48,617][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.6669228076934814, acc: 0.8536585569381714)
[2025-02-13 19:13:48,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:49,017][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.4517778158187866, acc: 0.8916666507720947)
[2025-02-13 19:13:49,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:49,381][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.3491816222667694, acc: 0.921875)
[2025-02-13 19:13:49,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:49,775][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.33263155817985535, acc: 0.8978102207183838)
[2025-02-13 19:13:49,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:50,178][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.4061649441719055, acc: 0.8961748480796814)
[2025-02-13 19:13:50,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:50,598][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.4132550358772278, acc: 0.9045225977897644)
[2025-02-13 19:13:50,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:50,994][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.14305812120437622, acc: 0.9629629850387573)
[2025-02-13 19:13:51,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:51,398][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.18691828846931458, acc: 0.9653179049491882)
[2025-02-13 19:13:51,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:51,811][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.27634909749031067, acc: 0.9476743936538696)
[2025-02-13 19:13:51,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:52,186][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.25027069449424744, acc: 0.9338235259056091)
[2025-02-13 19:13:52,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:52,572][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.2265656441450119, acc: 0.9611650705337524)
[2025-02-13 19:13:52,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:52,975][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.38393285870552063, acc: 0.9195979833602905)
[2025-02-13 19:13:53,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:53,349][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.28915634751319885, acc: 0.9291338324546814)
[2025-02-13 19:13:53,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:53,753][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.24172785878181458, acc: 0.9562841653823853)
[2025-02-13 19:13:53,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:54,160][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.10955080389976501, acc: 0.9668508172035217)
[2025-02-13 19:13:54,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:54,517][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.08268088847398758, acc: 0.9801324605941772)
[2025-02-13 19:13:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:54,943][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.25943371653556824, acc: 0.9482758641242981)
[2025-02-13 19:13:55,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:55,361][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.25347939133644104, acc: 0.9476743936538696)
[2025-02-13 19:13:55,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:55,756][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.3330243229866028, acc: 0.9505494236946106)
[2025-02-13 19:13:55,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:56,135][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.22986635565757751, acc: 0.9346405267715454)
[2025-02-13 19:13:56,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:56,495][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.2565135955810547, acc: 0.9441624283790588)
[2025-02-13 19:13:56,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:56,865][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.200368732213974, acc: 0.957446813583374)
[2025-02-13 19:13:57,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:57,253][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.14164723455905914, acc: 0.9520547986030579)
[2025-02-13 19:13:57,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:57,608][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.46847590804100037, acc: 0.8895705342292786)
[2025-02-13 19:13:57,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:57,982][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.4600365161895752, acc: 0.9067357778549194)
[2025-02-13 19:13:58,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:58,353][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.5024681091308594, acc: 0.9243243336677551)
[2025-02-13 19:13:58,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:58,771][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.37550127506256104, acc: 0.9192546606063843)
[2025-02-13 19:13:58,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:59,151][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.8597309589385986, acc: 0.8113207817077637)
[2025-02-13 19:13:59,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:59,548][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.23318222165107727, acc: 0.9444444179534912)
[2025-02-13 19:13:59,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:59,919][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.38468512892723083, acc: 0.9022988677024841)
[2025-02-13 19:14:00,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:00,313][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.2643072307109833, acc: 0.9377990365028381)
[2025-02-13 19:14:00,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:00,700][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.2705850899219513, acc: 0.9303797483444214)
[2025-02-13 19:14:00,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:01,105][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.2695736885070801, acc: 0.9464285969734192)
[2025-02-13 19:14:01,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:01,502][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.09259558469057083, acc: 0.9701492786407471)
[2025-02-13 19:14:01,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:01,880][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.2170349359512329, acc: 0.9459459185600281)
[2025-02-13 19:14:02,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:02,323][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.3003520667552948, acc: 0.9301075339317322)
[2025-02-13 19:14:02,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:02,756][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.27989107370376587, acc: 0.9470899701118469)
[2025-02-13 19:14:02,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:03,131][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.30367353558540344, acc: 0.9333333373069763)
[2025-02-13 19:14:03,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:03,524][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.15751546621322632, acc: 0.9407894611358643)
[2025-02-13 19:14:03,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:03,941][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.3651960492134094, acc: 0.9300000071525574)
[2025-02-13 19:14:04,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:04,318][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.1970522403717041, acc: 0.9532163739204407)
[2025-02-13 19:14:04,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:04,693][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.2512798309326172, acc: 0.9507389068603516)
[2025-02-13 19:14:04,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:05,044][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.3374517261981964, acc: 0.9253731369972229)
[2025-02-13 19:14:05,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:05,429][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.19635115563869476, acc: 0.9529411792755127)
[2025-02-13 19:14:05,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:05,792][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.25210899114608765, acc: 0.9433962106704712)
[2025-02-13 19:14:05,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:06,240][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.3508130609989166, acc: 0.9166666865348816)
[2025-02-13 19:14:06,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:06,623][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.2131357043981552, acc: 0.9433962106704712)
[2025-02-13 19:14:06,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:07,027][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.5008739233016968, acc: 0.885869562625885)
[2025-02-13 19:14:07,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:07,402][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.27203166484832764, acc: 0.9256756901741028)
[2025-02-13 19:14:07,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:07,810][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.22703945636749268, acc: 0.9346405267715454)
[2025-02-13 19:14:08,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:08,298][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.17774711549282074, acc: 0.9745222926139832)
[2025-02-13 19:14:08,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:08,684][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.28667131066322327, acc: 0.9082568883895874)
[2025-02-13 19:14:08,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:09,079][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.3006226122379303, acc: 0.9314285516738892)
[2025-02-13 19:14:09,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:09,481][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.5511978268623352, acc: 0.8632478713989258)
[2025-02-13 19:14:09,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:09,900][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.34313833713531494, acc: 0.9111111164093018)
[2025-02-13 19:14:10,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:10,297][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.319560706615448, acc: 0.934959352016449)
[2025-02-13 19:14:10,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:10,725][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.16928181052207947, acc: 0.9591836929321289)
[2025-02-13 19:14:10,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:11,116][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.5254523158073425, acc: 0.9047619104385376)
[2025-02-13 19:14:11,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:11,484][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.1992189884185791, acc: 0.9527027010917664)
[2025-02-13 19:14:11,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:11,927][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.1802331954240799, acc: 0.9813664555549622)
[2025-02-13 19:14:12,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:12,329][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.27741315960884094, acc: 0.9520958065986633)
[2025-02-13 19:14:12,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:12,683][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.1521945744752884, acc: 0.9545454382896423)
[2025-02-13 19:14:12,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:13,051][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.5015799403190613, acc: 0.9047619104385376)
[2025-02-13 19:14:13,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:13,418][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.33158236742019653, acc: 0.9140625)
[2025-02-13 19:14:13,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:13,787][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.1778733730316162, acc: 0.9464285969734192)
[2025-02-13 19:14:13,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:14,168][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.16987545788288116, acc: 0.9624060392379761)
[2025-02-13 19:14:14,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:14,549][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.1641441434621811, acc: 0.950276255607605)
[2025-02-13 19:14:14,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:14,928][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.18340006470680237, acc: 0.9684210419654846)
[2025-02-13 19:14:15,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:15,328][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.20985636115074158, acc: 0.9304347634315491)
[2025-02-13 19:14:15,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:15,711][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.17392051219940186, acc: 0.9677419066429138)
[2025-02-13 19:14:15,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:16,115][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.1322382539510727, acc: 0.9753086566925049)
[2025-02-13 19:14:16,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:16,504][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.20288102328777313, acc: 0.9411764740943909)
[2025-02-13 19:14:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:16,855][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.25578275322914124, acc: 0.9655172228813171)
[2025-02-13 19:14:16,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:17,226][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.6043803691864014, acc: 0.918181836605072)
[2025-02-13 19:14:17,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:17,572][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.224842831492424, acc: 0.9448275566101074)
[2025-02-13 19:14:17,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:17,983][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.14609962701797485, acc: 0.9734513163566589)
[2025-02-13 19:14:18,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:18,352][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.2523263990879059, acc: 0.9457364082336426)
[2025-02-13 19:14:18,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:18,786][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.12519249320030212, acc: 0.9583333134651184)
[2025-02-13 19:14:18,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:19,198][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.25134390592575073, acc: 0.9285714030265808)
[2025-02-13 19:14:19,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:19,567][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.3973556160926819, acc: 0.9056603908538818)
[2025-02-13 19:14:19,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:19,992][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.15089371800422668, acc: 0.9658119678497314)
[2025-02-13 19:14:20,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:20,389][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.14724916219711304, acc: 0.9736841917037964)
[2025-02-13 19:14:20,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:20,775][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.1561093032360077, acc: 0.9591836929321289)
[2025-02-13 19:14:20,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:21,180][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.09352920204401016, acc: 0.9794520735740662)
[2025-02-13 19:14:21,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:21,580][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.2663162052631378, acc: 0.9285714030265808)
[2025-02-13 19:14:21,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:22,002][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.4052645266056061, acc: 0.8959537744522095)
[2025-02-13 19:14:22,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:22,401][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.4052324891090393, acc: 0.916167676448822)
[2025-02-13 19:14:22,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:22,795][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.3980882167816162, acc: 0.939393937587738)
[2025-02-13 19:14:22,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:23,205][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.5950989723205566, acc: 0.8727272748947144)
[2025-02-13 19:14:23,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:23,566][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.44129031896591187, acc: 0.9058823585510254)
[2025-02-13 19:14:23,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:23,946][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.39629510045051575, acc: 0.9170305728912354)
[2025-02-13 19:14:24,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:24,325][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.3731203079223633, acc: 0.9105263352394104)
[2025-02-13 19:14:24,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:24,693][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.44185978174209595, acc: 0.8994413614273071)
[2025-02-13 19:14:24,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:25,095][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.24681468307971954, acc: 0.9384615421295166)
[2025-02-13 19:14:25,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:25,496][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.3399149179458618, acc: 0.9259259104728699)
[2025-02-13 19:14:25,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:25,887][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.3925624191761017, acc: 0.8994082808494568)
[2025-02-13 19:14:26,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:26,280][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.35596203804016113, acc: 0.8852459192276001)
[2025-02-13 19:14:26,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:26,706][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.40199410915374756, acc: 0.9037036895751953)
[2025-02-13 19:14:26,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:27,082][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.11973430961370468, acc: 0.9745762944221497)
[2025-02-13 19:14:27,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:27,459][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.19878429174423218, acc: 0.940119743347168)
[2025-02-13 19:14:27,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:27,846][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.47175726294517517, acc: 0.8872180581092834)
[2025-02-13 19:14:27,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:28,228][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.18912287056446075, acc: 0.967391312122345)
[2025-02-13 19:14:28,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:28,592][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.20806781947612762, acc: 0.9520958065986633)
[2025-02-13 19:14:28,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:28,960][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.23627537488937378, acc: 0.957446813583374)
[2025-02-13 19:14:29,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:29,330][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.24650682508945465, acc: 0.9662162065505981)
[2025-02-13 19:14:29,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:29,734][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.19445064663887024, acc: 0.9487179517745972)
[2025-02-13 19:14:29,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:30,133][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.17611242830753326, acc: 0.9606741666793823)
[2025-02-13 19:14:30,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:30,507][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.14675579965114594, acc: 0.9513513445854187)
[2025-02-13 19:14:30,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:30,875][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.2793191373348236, acc: 0.9465240836143494)
[2025-02-13 19:14:31,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:31,256][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.16579732298851013, acc: 0.9790576100349426)
[2025-02-13 19:14:31,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:31,639][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.21100609004497528, acc: 0.940119743347168)
[2025-02-13 19:14:31,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:32,037][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.21781134605407715, acc: 0.954023003578186)
[2025-02-13 19:14:32,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:32,493][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.1939079612493515, acc: 0.9726775884628296)
[2025-02-13 19:14:32,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:32,924][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.2214914858341217, acc: 0.9465240836143494)
[2025-02-13 19:14:33,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:33,301][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.18037058413028717, acc: 0.9580419659614563)
[2025-02-13 19:14:33,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:33,677][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.18611983954906464, acc: 0.9452054500579834)
[2025-02-13 19:14:33,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:34,075][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.5025227069854736, acc: 0.8776978254318237)
[2025-02-13 19:14:34,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:34,473][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.23882876336574554, acc: 0.9272727370262146)
[2025-02-13 19:14:34,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:34,838][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.46605268120765686, acc: 0.9103448390960693)
[2025-02-13 19:14:34,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:35,264][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.24300913512706757, acc: 0.9298245906829834)
[2025-02-13 19:14:35,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:35,780][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.19776344299316406, acc: 0.9561403393745422)
[2025-02-13 19:14:35,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:36,167][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.24997450411319733, acc: 0.9181286692619324)
[2025-02-13 19:14:36,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:36,576][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.24172690510749817, acc: 0.9337349534034729)
[2025-02-13 19:14:36,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:36,966][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.1934998482465744, acc: 0.9371069073677063)
[2025-02-13 19:14:37,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:37,395][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.22914338111877441, acc: 0.9642857313156128)
[2025-02-13 19:14:37,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:37,755][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.45225369930267334, acc: 0.9202454090118408)
[2025-02-13 19:14:37,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:38,109][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.7751361131668091, acc: 0.8661971688270569)
[2025-02-13 19:14:38,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:38,469][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.27927911281585693, acc: 0.9451219439506531)
[2025-02-13 19:14:38,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:38,860][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.22355054318904877, acc: 0.970588207244873)
[2025-02-13 19:14:39,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:39,293][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.6806032061576843, acc: 0.8994975090026855)
[2025-02-13 19:14:39,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:39,723][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.1957177221775055, acc: 0.9710982441902161)
[2025-02-13 19:14:39,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:40,132][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.4253428876399994, acc: 0.9027777910232544)
[2025-02-13 19:14:40,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:40,568][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.15393602848052979, acc: 0.9583333134651184)
[2025-02-13 19:14:40,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:40,990][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.21544204652309418, acc: 0.9455782175064087)
[2025-02-13 19:14:41,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:41,397][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.5402212142944336, acc: 0.8717948794364929)
[2025-02-13 19:14:41,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:41,799][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.26876816153526306, acc: 0.9144384860992432)
[2025-02-13 19:14:41,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:42,206][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.19755816459655762, acc: 0.9489051103591919)
[2025-02-13 19:14:42,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:42,651][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.16869719326496124, acc: 0.9642857313156128)
[2025-02-13 19:14:42,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:43,044][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.1322992593050003, acc: 0.9735449552536011)
[2025-02-13 19:14:43,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:43,461][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.3214484751224518, acc: 0.9336283206939697)
[2025-02-13 19:14:43,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:43,865][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.2474837601184845, acc: 0.9338235259056091)
[2025-02-13 19:14:43,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:44,252][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.30259403586387634, acc: 0.9344262480735779)
[2025-02-13 19:14:44,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:44,684][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.21958377957344055, acc: 0.940397322177887)
[2025-02-13 19:14:44,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:45,089][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.25232985615730286, acc: 0.9453551769256592)
[2025-02-13 19:14:45,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:45,467][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.3268018364906311, acc: 0.9213483333587646)
[2025-02-13 19:14:45,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:45,888][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.1836043894290924, acc: 0.9454545378684998)
[2025-02-13 19:14:46,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:46,265][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.3010134994983673, acc: 0.9054054021835327)
[2025-02-13 19:14:46,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:46,664][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.22853431105613708, acc: 0.9397590160369873)
[2025-02-13 19:14:46,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:47,045][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.3214209973812103, acc: 0.9395973086357117)
[2025-02-13 19:14:47,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:47,482][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.32254841923713684, acc: 0.9197530746459961)
[2025-02-13 19:14:47,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:47,910][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.3222264349460602, acc: 0.918749988079071)
[2025-02-13 19:14:48,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:48,288][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.2515937089920044, acc: 0.9166666865348816)
[2025-02-13 19:14:48,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:48,648][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.13698799908161163, acc: 0.9673202633857727)
[2025-02-13 19:14:48,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:49,050][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.2619315981864929, acc: 0.9385474920272827)
[2025-02-13 19:14:49,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:49,463][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.16550523042678833, acc: 0.95652174949646)
[2025-02-13 19:14:49,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:49,910][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.1544286608695984, acc: 0.9490445852279663)
[2025-02-13 19:14:50,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:50,321][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.17896266281604767, acc: 0.9591836929321289)
[2025-02-13 19:14:50,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:50,752][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.2508990168571472, acc: 0.9278350472450256)
[2025-02-13 19:14:50,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:51,128][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.2514478862285614, acc: 0.9515151381492615)
[2025-02-13 19:14:51,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:51,515][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.1351431906223297, acc: 0.9570552110671997)
[2025-02-13 19:14:51,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:51,944][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.16627225279808044, acc: 0.9655172228813171)
[2025-02-13 19:14:52,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:52,387][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.0976661890745163, acc: 0.9722222089767456)
[2025-02-13 19:14:52,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:52,853][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.12072274833917618, acc: 0.9620253443717957)
[2025-02-13 19:14:53,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:53,276][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.2085256278514862, acc: 0.9340659379959106)
[2025-02-13 19:14:53,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:53,751][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.25145477056503296, acc: 0.9520958065986633)
[2025-02-13 19:14:53,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:54,163][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.14351435005664825, acc: 0.9702380895614624)
[2025-02-13 19:14:54,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:54,593][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.20148040354251862, acc: 0.9385474920272827)
[2025-02-13 19:14:54,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:55,022][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.0709557756781578, acc: 0.9866666793823242)
[2025-02-13 19:14:55,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:55,487][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.2183011919260025, acc: 0.9545454382896423)
[2025-02-13 19:14:55,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:55,899][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.09995108842849731, acc: 0.9743589758872986)
[2025-02-13 19:14:56,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:56,324][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.4678648114204407, acc: 0.8907103538513184)
[2025-02-13 19:14:56,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:56,725][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.6902749538421631, acc: 0.862500011920929)
[2025-02-13 19:14:56,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:57,137][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 1.431002140045166, acc: 0.7815533876419067)
[2025-02-13 19:14:57,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:57,558][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 1.4800323247909546, acc: 0.7388059496879578)
[2025-02-13 19:14:57,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:57,924][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.33201009035110474, acc: 0.9281768202781677)
[2025-02-13 19:14:58,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:58,309][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.4563407301902771, acc: 0.8799999952316284)
[2025-02-13 19:14:58,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:58,700][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.23468706011772156, acc: 0.9454545378684998)
[2025-02-13 19:14:58,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:59,072][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.32994338870048523, acc: 0.9239766001701355)
[2025-02-13 19:14:59,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:59,445][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.2860020697116852, acc: 0.9069767594337463)
[2025-02-13 19:14:59,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:59,832][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.25190141797065735, acc: 0.9386792182922363)
[2025-02-13 19:14:59,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:00,195][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.22238948941230774, acc: 0.9448275566101074)
[2025-02-13 19:15:00,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:00,617][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.3236532509326935, acc: 0.9277108311653137)
[2025-02-13 19:15:00,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:01,001][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.27340763807296753, acc: 0.931034505367279)
[2025-02-13 19:15:01,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:01,417][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.39019230008125305, acc: 0.8914285898208618)
[2025-02-13 19:15:01,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:01,788][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.44797366857528687, acc: 0.931034505367279)
[2025-02-13 19:15:01,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:02,165][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.5477514266967773, acc: 0.8626373410224915)
[2025-02-13 19:15:02,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:02,561][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.36239445209503174, acc: 0.907975435256958)
[2025-02-13 19:15:02,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:03,011][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.41272562742233276, acc: 0.8901098966598511)
[2025-02-13 19:15:03,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:03,384][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.2044403851032257, acc: 0.9512194991111755)
[2025-02-13 19:15:03,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:03,772][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.20684832334518433, acc: 0.9432989954948425)
[2025-02-13 19:15:03,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:04,145][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.23650430142879486, acc: 0.967391312122345)
[2025-02-13 19:15:04,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:04,507][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.2262326180934906, acc: 0.9459459185600281)
[2025-02-13 19:15:04,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:04,873][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.2690623700618744, acc: 0.916167676448822)
[2025-02-13 19:15:05,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:05,215][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.5027658343315125, acc: 0.8918918967247009)
[2025-02-13 19:15:05,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:05,582][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 1.00448739528656, acc: 0.765625)
[2025-02-13 19:15:05,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:05,971][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.5136799812316895, acc: 0.8993710875511169)
[2025-02-13 19:15:06,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:06,355][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.29291602969169617, acc: 0.9246231317520142)
[2025-02-13 19:15:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:06,719][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.21734006702899933, acc: 0.9603174328804016)
[2025-02-13 19:15:06,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:07,109][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.27996572852134705, acc: 0.9563106894493103)
[2025-02-13 19:15:07,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:07,468][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.2138487696647644, acc: 0.9278350472450256)
[2025-02-13 19:15:07,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:07,872][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.13428165018558502, acc: 0.9669811129570007)
[2025-02-13 19:15:08,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:08,234][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.2465287148952484, acc: 0.9523809552192688)
[2025-02-13 19:15:08,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:08,607][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.1365181803703308, acc: 0.96875)
[2025-02-13 19:15:08,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:08,991][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.42988255620002747, acc: 0.8888888955116272)
[2025-02-13 19:15:09,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:09,372][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.22243143618106842, acc: 0.9441624283790588)
[2025-02-13 19:15:09,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:09,784][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.3275151550769806, acc: 0.9354838728904724)
[2025-02-13 19:15:09,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:10,182][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.06214883178472519, acc: 0.989847719669342)
[2025-02-13 19:15:10,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:10,574][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.07209626585245132, acc: 0.9851484894752502)
[2025-02-13 19:15:10,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:10,966][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.05787869170308113, acc: 0.984455943107605)
[2025-02-13 19:15:11,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:11,377][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.07785216718912125, acc: 0.9828571677207947)
[2025-02-13 19:15:11,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:11,800][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.19875772297382355, acc: 0.9534883499145508)
[2025-02-13 19:15:11,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:12,210][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.2976669669151306, acc: 0.918367326259613)
[2025-02-13 19:15:12,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:12,592][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.37682709097862244, acc: 0.9404761791229248)
[2025-02-13 19:15:12,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:12,964][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.17322960495948792, acc: 0.9714285731315613)
[2025-02-13 19:15:13,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:13,307][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.7060134410858154, acc: 0.8484848737716675)
[2025-02-13 19:15:13,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:13,718][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.4386743903160095, acc: 0.8992805480957031)
[2025-02-13 19:15:13,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:14,115][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.06305374205112457, acc: 0.9883720874786377)
[2025-02-13 19:15:14,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:14,501][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.16947400569915771, acc: 0.9683544039726257)
[2025-02-13 19:15:14,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:14,858][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.1098145991563797, acc: 0.9788732528686523)
[2025-02-13 19:15:15,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:15,252][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.14186891913414001, acc: 0.965753436088562)
[2025-02-13 19:15:15,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:15,632][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.10401961952447891, acc: 0.9752066135406494)
[2025-02-13 19:15:15,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:16,000][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.13210032880306244, acc: 0.9541984796524048)
[2025-02-13 19:15:16,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:16,378][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.17812618613243103, acc: 0.9485294222831726)
[2025-02-13 19:15:16,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:16,756][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.23277831077575684, acc: 0.9640287756919861)
[2025-02-13 19:15:16,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:17,152][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.25625553727149963, acc: 0.9469696879386902)
[2025-02-13 19:15:17,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:17,537][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.1685478389263153, acc: 0.9473684430122375)
[2025-02-13 19:15:17,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:17,947][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.3295133709907532, acc: 0.9076923131942749)
[2025-02-13 19:15:18,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:18,358][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.1454952508211136, acc: 0.9666666388511658)
[2025-02-13 19:15:18,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:18,744][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.2703210413455963, acc: 0.9461538195610046)
[2025-02-13 19:15:18,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:19,141][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.20511315762996674, acc: 0.9590163826942444)
[2025-02-13 19:15:19,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:19,525][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.2720150351524353, acc: 0.9375)
[2025-02-13 19:15:19,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:19,880][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.08528152108192444, acc: 1.0)
[2025-02-13 19:15:20,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:20,261][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.2679920196533203, acc: 0.9452054500579834)
[2025-02-13 19:15:20,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:20,649][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.13073280453681946, acc: 0.9729729890823364)
[2025-02-13 19:15:20,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:21,011][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.12368360161781311, acc: 0.9595959782600403)
[2025-02-13 19:15:21,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:21,410][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.2482791692018509, acc: 0.9354838728904724)
[2025-02-13 19:15:21,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:21,835][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.22565367817878723, acc: 0.9609375)
[2025-02-13 19:15:21,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:22,209][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.2212357223033905, acc: 0.9459459185600281)
[2025-02-13 19:15:22,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:22,582][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.14429965615272522, acc: 0.9455782175064087)
[2025-02-13 19:15:22,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:22,956][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.24017927050590515, acc: 0.9359999895095825)
[2025-02-13 19:15:23,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:23,371][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.19401608407497406, acc: 0.9530201554298401)
[2025-02-13 19:15:23,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:23,755][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.23654384911060333, acc: 0.9477611780166626)
[2025-02-13 19:15:23,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:24,147][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.21703483164310455, acc: 0.9662162065505981)
[2025-02-13 19:15:24,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:24,544][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.2562730610370636, acc: 0.930232584476471)
[2025-02-13 19:15:24,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:24,974][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.3204946219921112, acc: 0.9428571462631226)
[2025-02-13 19:15:25,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:25,371][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.1712290495634079, acc: 0.9615384340286255)
[2025-02-13 19:15:25,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:25,757][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.20434488356113434, acc: 0.9464285969734192)
[2025-02-13 19:15:25,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:26,202][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.18517781794071198, acc: 0.9548872113227844)
[2025-02-13 19:15:26,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:26,587][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.10601247102022171, acc: 0.9689922332763672)
[2025-02-13 19:15:26,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:27,080][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.17352394759655, acc: 0.9526627063751221)
[2025-02-13 19:15:27,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:27,432][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.21337701380252838, acc: 0.9491525292396545)
[2025-02-13 19:15:27,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:27,861][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.3376866281032562, acc: 0.9052631855010986)
[2025-02-13 19:15:28,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:28,259][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.38249117136001587, acc: 0.8999999761581421)
[2025-02-13 19:15:28,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:28,641][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.1993262618780136, acc: 0.9340101480484009)
[2025-02-13 19:15:28,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:29,007][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.2799726724624634, acc: 0.9047619104385376)
[2025-02-13 19:15:29,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:29,364][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.24093219637870789, acc: 0.9351351261138916)
[2025-02-13 19:15:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:29,754][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.16240893304347992, acc: 0.9601989984512329)
[2025-02-13 19:15:29,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:30,129][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.12491786479949951, acc: 0.978723406791687)
[2025-02-13 19:15:30,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:30,498][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.173619344830513, acc: 0.9324324131011963)
[2025-02-13 19:15:30,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:30,874][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.23630119860172272, acc: 0.9513513445854187)
[2025-02-13 19:15:31,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:31,289][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.1640787422657013, acc: 0.9386503100395203)
[2025-02-13 19:15:31,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:31,654][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.19126486778259277, acc: 0.9473684430122375)
[2025-02-13 19:15:31,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:32,025][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.06147577613592148, acc: 1.0)
[2025-02-13 19:15:32,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:32,474][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.2247231900691986, acc: 0.9371727705001831)
[2025-02-13 19:15:32,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:32,864][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.10952216386795044, acc: 0.9750000238418579)
[2025-02-13 19:15:33,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:33,227][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.09497936069965363, acc: 0.9811320900917053)
[2025-02-13 19:15:33,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:33,590][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.19078531861305237, acc: 0.9488636255264282)
[2025-02-13 19:15:33,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:33,967][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.10272620618343353, acc: 0.976331353187561)
[2025-02-13 19:15:34,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:34,347][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.05240092799067497, acc: 0.9887640476226807)
[2025-02-13 19:15:34,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:34,784][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.105318084359169, acc: 0.9735099077224731)
[2025-02-13 19:15:34,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:35,193][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.08284887671470642, acc: 0.9632353186607361)
[2025-02-13 19:15:35,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:35,565][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.2344500720500946, acc: 0.9441340565681458)
[2025-02-13 19:15:35,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:35,927][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.046174176037311554, acc: 0.9897959232330322)
[2025-02-13 19:15:36,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:36,306][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.03737109899520874, acc: 1.0)
[2025-02-13 19:15:36,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:36,678][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.10225053131580353, acc: 0.9833333492279053)
[2025-02-13 19:15:36,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:37,095][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.07341229915618896, acc: 0.98591548204422)
[2025-02-13 19:15:37,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:37,526][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.06677243113517761, acc: 0.9887005686759949)
[2025-02-13 19:15:37,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:37,915][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.05356394499540329, acc: 0.9867549538612366)
[2025-02-13 19:15:38,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:38,286][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.1386907398700714, acc: 0.9629629850387573)
[2025-02-13 19:15:38,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:38,671][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.11002807319164276, acc: 0.9784946441650391)
[2025-02-13 19:15:38,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:39,058][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.27085256576538086, acc: 0.9319371581077576)
[2025-02-13 19:15:39,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:39,435][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.1718783974647522, acc: 0.9427083134651184)
[2025-02-13 19:15:39,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:39,828][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.2783172130584717, acc: 0.9470899701118469)
[2025-02-13 19:15:39,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:40,228][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.3254573345184326, acc: 0.9212962985038757)
[2025-02-13 19:15:40,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:40,610][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.33357566595077515, acc: 0.9108911156654358)
[2025-02-13 19:15:40,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:41,007][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.40324556827545166, acc: 0.89552241563797)
[2025-02-13 19:15:41,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:41,383][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.21016445755958557, acc: 0.9351351261138916)
[2025-02-13 19:15:41,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:41,774][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.3149701952934265, acc: 0.9109947681427002)
[2025-02-13 19:15:41,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:42,244][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.34622859954833984, acc: 0.9225806593894958)
[2025-02-13 19:15:42,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:42,662][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.2640463709831238, acc: 0.9543378949165344)
[2025-02-13 19:15:42,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:43,102][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.14885325729846954, acc: 0.9467455744743347)
[2025-02-13 19:15:43,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:43,517][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.22404880821704865, acc: 0.9392523169517517)
[2025-02-13 19:15:43,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:43,944][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.18829724192619324, acc: 0.9627906680107117)
[2025-02-13 19:15:44,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:44,352][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.5563310384750366, acc: 0.887005627155304)
[2025-02-13 19:15:44,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:44,787][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.7532892823219299, acc: 0.8111587762832642)
[2025-02-13 19:15:44,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:45,182][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.5331760048866272, acc: 0.885496199131012)
[2025-02-13 19:15:45,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:45,590][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.28658536076545715, acc: 0.9436619877815247)
[2025-02-13 19:15:45,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:45,968][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.12919148802757263, acc: 0.9884393215179443)
[2025-02-13 19:15:46,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:46,298][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.255993127822876, acc: 0.949999988079071)
[2025-02-13 19:15:46,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:46,663][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.34076398611068726, acc: 0.926701545715332)
[2025-02-13 19:15:46,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:47,060][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.29433032870292664, acc: 0.9390243887901306)
[2025-02-13 19:15:47,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:47,432][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.24998997151851654, acc: 0.9281045794487)
[2025-02-13 19:15:47,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:47,781][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.20707820355892181, acc: 0.9343065619468689)
[2025-02-13 19:15:47,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:48,196][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.2626459002494812, acc: 0.9417040348052979)
[2025-02-13 19:15:48,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:48,627][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.18109248578548431, acc: 0.9441624283790588)
[2025-02-13 19:15:48,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:49,042][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.14596900343894958, acc: 0.9570552110671997)
[2025-02-13 19:15:49,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:49,445][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.18732458353042603, acc: 0.9568965435028076)
[2025-02-13 19:15:49,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:49,893][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.08505858480930328, acc: 0.9767441749572754)
[2025-02-13 19:15:50,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:50,268][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.2813916802406311, acc: 0.939130425453186)
[2025-02-13 19:15:50,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:50,681][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.20491541922092438, acc: 0.9530201554298401)
[2025-02-13 19:15:50,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:51,073][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.13187743723392487, acc: 0.9418604373931885)
[2025-02-13 19:15:51,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:51,469][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.17881187796592712, acc: 0.9659863710403442)
[2025-02-13 19:15:51,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:51,931][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.26436471939086914, acc: 0.9384615421295166)
[2025-02-13 19:15:52,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:52,369][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.1898878514766693, acc: 0.9514563083648682)
[2025-02-13 19:15:52,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:52,788][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.5591998100280762, acc: 0.8623188138008118)
[2025-02-13 19:15:52,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:53,166][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.45069581270217896, acc: 0.9124087691307068)
[2025-02-13 19:15:53,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:53,595][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.23958934843540192, acc: 0.9329268336296082)
[2025-02-13 19:15:53,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:53,967][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.20705540478229523, acc: 0.9548872113227844)
[2025-02-13 19:15:54,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:54,384][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.23041339218616486, acc: 0.9605262875556946)
[2025-02-13 19:15:54,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:54,806][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.32404401898384094, acc: 0.9378530979156494)
[2025-02-13 19:15:54,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:55,222][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.3442545533180237, acc: 0.9382022619247437)
[2025-02-13 19:15:55,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:55,589][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.2644387483596802, acc: 0.9613259434700012)
[2025-02-13 19:15:55,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:55,984][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.3974323272705078, acc: 0.9312169551849365)
[2025-02-13 19:15:56,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:56,382][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.3817041516304016, acc: 0.9012345671653748)
[2025-02-13 19:15:56,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:56,767][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.20352211594581604, acc: 0.9318181872367859)
[2025-02-13 19:15:56,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:57,215][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.2857055962085724, acc: 0.9368420839309692)
[2025-02-13 19:15:57,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:57,621][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.3622182011604309, acc: 0.9189189076423645)
[2025-02-13 19:15:57,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:58,017][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.38109350204467773, acc: 0.9277777671813965)
[2025-02-13 19:15:58,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:58,404][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.14610348641872406, acc: 0.9659863710403442)
[2025-02-13 19:15:58,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:58,826][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.45256757736206055, acc: 0.8994413614273071)
[2025-02-13 19:15:58,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:59,212][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.20232446491718292, acc: 0.9389671087265015)
[2025-02-13 19:15:59,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:59,619][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.22399096190929413, acc: 0.9518716335296631)
[2025-02-13 19:15:59,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:00,001][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.21675100922584534, acc: 0.9431818127632141)
[2025-02-13 19:16:00,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:00,412][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.11608991026878357, acc: 0.9695431590080261)
[2025-02-13 19:16:00,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:00,802][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.31171128153800964, acc: 0.9152542352676392)
[2025-02-13 19:16:00,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:01,168][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.14295949041843414, acc: 0.9646017551422119)
[2025-02-13 19:16:01,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:01,570][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.3699702322483063, acc: 0.9375)
[2025-02-13 19:16:01,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:01,984][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.22640573978424072, acc: 0.921875)
[2025-02-13 19:16:02,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:02,402][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.23151087760925293, acc: 0.9278350472450256)
[2025-02-13 19:16:02,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:02,813][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.17793133854866028, acc: 0.9465649127960205)
[2025-02-13 19:16:02,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:03,217][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.35066089034080505, acc: 0.9007092118263245)
[2025-02-13 19:16:03,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:03,603][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.32275453209877014, acc: 0.9333333373069763)
[2025-02-13 19:16:03,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:03,981][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.40621376037597656, acc: 0.8992805480957031)
[2025-02-13 19:16:04,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:04,379][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.2218288779258728, acc: 0.9726027250289917)
[2025-02-13 19:16:04,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:04,761][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.29206860065460205, acc: 0.9391891956329346)
[2025-02-13 19:16:04,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:05,151][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.14045463502407074, acc: 0.9580838084220886)
[2025-02-13 19:16:05,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:05,560][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.26238778233528137, acc: 0.9225806593894958)
[2025-02-13 19:16:05,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:05,945][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.3748268187046051, acc: 0.9166666865348816)
[2025-02-13 19:16:06,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:06,313][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.3614925146102905, acc: 0.8918918967247009)
[2025-02-13 19:16:06,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:06,681][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.3505077064037323, acc: 0.9242424368858337)
[2025-02-13 19:16:06,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:07,081][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.1861170530319214, acc: 0.965753436088562)
[2025-02-13 19:16:07,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:07,497][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.15129996836185455, acc: 0.9608938694000244)
[2025-02-13 19:16:07,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:07,915][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.1608261913061142, acc: 0.9741935729980469)
[2025-02-13 19:16:08,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:08,315][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 1.4066410064697266, acc: 0.7714285850524902)
[2025-02-13 19:16:08,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:08,716][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.5098738670349121, acc: 0.885496199131012)
[2025-02-13 19:16:08,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:09,077][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.27007579803466797, acc: 0.9366196990013123)
[2025-02-13 19:16:09,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:09,443][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.33701395988464355, acc: 0.9438202381134033)
[2025-02-13 19:16:09,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:09,819][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.3920533061027527, acc: 0.9172932505607605)
[2025-02-13 19:16:09,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:10,183][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.6550225615501404, acc: 0.8571428656578064)
[2025-02-13 19:16:10,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:10,561][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.46904826164245605, acc: 0.895348846912384)
[2025-02-13 19:16:10,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:10,951][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.27952805161476135, acc: 0.947826087474823)
[2025-02-13 19:16:11,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:11,329][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.20605231821537018, acc: 0.9432623982429504)
[2025-02-13 19:16:11,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:11,705][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.34376609325408936, acc: 0.9484536051750183)
[2025-02-13 19:16:11,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:12,093][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.1790267825126648, acc: 0.942307710647583)
[2025-02-13 19:16:12,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:12,455][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.27859199047088623, acc: 0.9490445852279663)
[2025-02-13 19:16:12,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:12,849][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.1520589292049408, acc: 0.9794520735740662)
[2025-02-13 19:16:12,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:13,208][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.18050776422023773, acc: 0.9590163826942444)
[2025-02-13 19:16:13,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:13,572][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.3340129554271698, acc: 0.9420289993286133)
[2025-02-13 19:16:13,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:13,940][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.268206924200058, acc: 0.9300699234008789)
[2025-02-13 19:16:14,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:14,316][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.5577508211135864, acc: 0.8860759735107422)
[2025-02-13 19:16:14,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:14,679][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.6830682158470154, acc: 0.8703703880310059)
[2025-02-13 19:16:14,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:15,050][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.4358164966106415, acc: 0.8954248428344727)
[2025-02-13 19:16:15,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:15,431][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.4863758087158203, acc: 0.8770492076873779)
[2025-02-13 19:16:15,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:15,822][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.2716607451438904, acc: 0.9264705777168274)
[2025-02-13 19:16:15,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:16,222][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.24002358317375183, acc: 0.9395604133605957)
[2025-02-13 19:16:16,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:16,579][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.15912069380283356, acc: 0.9603174328804016)
[2025-02-13 19:16:16,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:16,975][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.21926532685756683, acc: 0.9494949579238892)
[2025-02-13 19:16:17,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:17,364][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.38290849328041077, acc: 0.8975903391838074)
[2025-02-13 19:16:17,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:17,761][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.3348093330860138, acc: 0.930232584476471)
[2025-02-13 19:16:17,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:18,129][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.5706505179405212, acc: 0.8895348906517029)
[2025-02-13 19:16:18,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:18,510][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.5059153437614441, acc: 0.9074074029922485)
[2025-02-13 19:16:18,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:18,913][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.2813495993614197, acc: 0.9333333373069763)
[2025-02-13 19:16:19,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:19,318][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.5186835527420044, acc: 0.8928571343421936)
[2025-02-13 19:16:19,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:19,682][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.25471872091293335, acc: 0.9320987462997437)
[2025-02-13 19:16:19,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:20,095][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.4545767307281494, acc: 0.8784530162811279)
[2025-02-13 19:16:20,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:20,485][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.2008262276649475, acc: 0.9451219439506531)
[2025-02-13 19:16:20,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:20,896][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.3514220714569092, acc: 0.9124087691307068)
[2025-02-13 19:16:21,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:21,262][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.30857306718826294, acc: 0.925000011920929)
[2025-02-13 19:16:21,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:21,623][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.14933426678180695, acc: 0.9685534834861755)
[2025-02-13 19:16:21,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:22,021][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.19924914836883545, acc: 0.9615384340286255)
[2025-02-13 19:16:22,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:22,375][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.20492464303970337, acc: 0.9357143044471741)
[2025-02-13 19:16:22,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:22,740][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.1947837471961975, acc: 0.9441340565681458)
[2025-02-13 19:16:22,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:23,102][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.31527334451675415, acc: 0.9320987462997437)
[2025-02-13 19:16:23,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:23,471][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.2540850043296814, acc: 0.9437500238418579)
[2025-02-13 19:16:23,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:23,819][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.12455321848392487, acc: 0.9813664555549622)
[2025-02-13 19:16:23,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:24,185][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.4925279915332794, acc: 0.8905109763145447)
[2025-02-13 19:16:24,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:24,552][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.3943944573402405, acc: 0.9027026891708374)
[2025-02-13 19:16:24,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:24,938][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.223429337143898, acc: 0.9333333373069763)
[2025-02-13 19:16:25,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:25,362][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.511345624923706, acc: 0.888198733329773)
[2025-02-13 19:16:25,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:25,761][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.2148292511701584, acc: 0.9347826242446899)
[2025-02-13 19:16:25,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:26,163][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.21637631952762604, acc: 0.940119743347168)
[2025-02-13 19:16:26,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:26,568][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.24980787932872772, acc: 0.9312499761581421)
[2025-02-13 19:16:26,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:26,925][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.16777054965496063, acc: 0.9593023061752319)
[2025-02-13 19:16:27,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:27,315][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.19359911978244781, acc: 0.9645389914512634)
[2025-02-13 19:16:27,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:27,714][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.42943769693374634, acc: 0.8976377844810486)
[2025-02-13 19:16:27,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:28,109][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.2231014519929886, acc: 0.9438202381134033)
[2025-02-13 19:16:28,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:28,508][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.28970956802368164, acc: 0.8999999761581421)
[2025-02-13 19:16:28,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:28,881][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.2948688566684723, acc: 0.9383561611175537)
[2025-02-13 19:16:29,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:29,281][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.10024455934762955, acc: 0.9716312289237976)
[2025-02-13 19:16:29,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:29,678][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.1838502436876297, acc: 0.9636363387107849)
[2025-02-13 19:16:29,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:30,041][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.2821539342403412, acc: 0.9386503100395203)
[2025-02-13 19:16:30,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:30,445][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.19870539009571075, acc: 0.9496402740478516)
[2025-02-13 19:16:30,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:30,852][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.21787965297698975, acc: 0.9375)
[2025-02-13 19:16:30,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:31,240][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.1037287563085556, acc: 0.9612902998924255)
[2025-02-13 19:16:31,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:31,632][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.20699484646320343, acc: 0.9663865566253662)
[2025-02-13 19:16:31,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:32,002][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.1827208250761032, acc: 0.9532163739204407)
[2025-02-13 19:16:32,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:32,373][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.2308150976896286, acc: 0.9404761791229248)
[2025-02-13 19:16:32,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:32,776][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.2026856690645218, acc: 0.9532163739204407)
[2025-02-13 19:16:32,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:33,207][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.31758061051368713, acc: 0.9289940595626831)
[2025-02-13 19:16:33,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:33,621][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.12582965195178986, acc: 0.9691358208656311)
[2025-02-13 19:16:33,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:34,047][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.16375456750392914, acc: 0.9719101190567017)
[2025-02-13 19:16:34,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:34,432][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.16501401364803314, acc: 0.948387086391449)
[2025-02-13 19:16:34,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:34,822][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.2263820767402649, acc: 0.9404761791229248)
[2025-02-13 19:16:34,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:35,209][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.22261032462120056, acc: 0.9435028433799744)
[2025-02-13 19:16:35,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:35,630][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.21633252501487732, acc: 0.9354838728904724)
[2025-02-13 19:16:35,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:35,992][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.4435745179653168, acc: 0.8905472755432129)
[2025-02-13 19:16:36,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:36,372][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.38729602098464966, acc: 0.9141104221343994)
[2025-02-13 19:16:36,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:36,812][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.4214390516281128, acc: 0.9017341136932373)
[2025-02-13 19:16:36,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:37,188][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.17954690754413605, acc: 0.949999988079071)
[2025-02-13 19:16:37,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:37,579][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.30534154176712036, acc: 0.9205297827720642)
[2025-02-13 19:16:37,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:37,999][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.23101165890693665, acc: 0.9547325372695923)
[2025-02-13 19:16:38,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:38,382][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.1798166036605835, acc: 0.9444444179534912)
[2025-02-13 19:16:38,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:38,759][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.26339006423950195, acc: 0.9343434572219849)
[2025-02-13 19:16:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:39,126][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.17863427102565765, acc: 0.9624999761581421)
[2025-02-13 19:16:39,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:39,494][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.34728556871414185, acc: 0.9285714030265808)
[2025-02-13 19:16:39,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:39,893][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.1383323222398758, acc: 0.9629629850387573)
[2025-02-13 19:16:40,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:40,321][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.2998999059200287, acc: 0.9291338324546814)
[2025-02-13 19:16:40,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:40,691][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.10622207075357437, acc: 0.9811320900917053)
[2025-02-13 19:16:40,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:41,054][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.1853199154138565, acc: 0.936170220375061)
[2025-02-13 19:16:41,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:41,443][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.3105987310409546, acc: 0.9435028433799744)
[2025-02-13 19:16:41,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:41,867][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.1423211246728897, acc: 0.9545454382896423)
[2025-02-13 19:16:42,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:42,252][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.2647465467453003, acc: 0.9222221970558167)
[2025-02-13 19:16:42,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:42,656][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.14141353964805603, acc: 0.9634146094322205)
[2025-02-13 19:16:42,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:43,027][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.08111323416233063, acc: 0.9888268113136292)
[2025-02-13 19:16:43,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:43,392][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.11711825430393219, acc: 0.9588235020637512)
[2025-02-13 19:16:43,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:43,764][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.09410810470581055, acc: 0.9740259647369385)
[2025-02-13 19:16:43,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:44,176][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.10797677934169769, acc: 0.9732620120048523)
[2025-02-13 19:16:44,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:44,572][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.22717690467834473, acc: 0.9595375657081604)
[2025-02-13 19:16:44,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:44,953][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.14615897834300995, acc: 0.9743589758872986)
[2025-02-13 19:16:45,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:45,338][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.14188680052757263, acc: 0.9487179517745972)
[2025-02-13 19:16:45,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:45,744][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.149371936917305, acc: 0.9551281929016113)
[2025-02-13 19:16:45,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:46,108][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.08983303606510162, acc: 0.9781022071838379)
[2025-02-13 19:16:46,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:46,475][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.12105712294578552, acc: 0.9704142212867737)
[2025-02-13 19:16:46,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:46,840][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.235164076089859, acc: 0.940119743347168)
[2025-02-13 19:16:46,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:47,247][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.2004382312297821, acc: 0.9551281929016113)
[2025-02-13 19:16:47,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:47,641][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.07403307408094406, acc: 0.9879518151283264)
[2025-02-13 19:16:47,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:48,072][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.19783589243888855, acc: 0.9539473652839661)
[2025-02-13 19:16:48,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:48,480][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.19188448786735535, acc: 0.9545454382896423)
[2025-02-13 19:16:48,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:48,848][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.18494220077991486, acc: 0.9829545617103577)
[2025-02-13 19:16:48,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:49,228][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.13209831714630127, acc: 0.9875776171684265)
[2025-02-13 19:16:49,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:49,610][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.37343570590019226, acc: 0.939393937587738)
[2025-02-13 19:16:49,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:49,994][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.43275541067123413, acc: 0.9105691313743591)
[2025-02-13 19:16:50,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:50,382][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.2773624360561371, acc: 0.9316770434379578)
[2025-02-13 19:16:50,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:50,782][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.4755418300628662, acc: 0.9103448390960693)
[2025-02-13 19:16:50,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:51,184][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.33514729142189026, acc: 0.9197080135345459)
[2025-02-13 19:16:51,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:51,578][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.2962090075016022, acc: 0.9135802388191223)
[2025-02-13 19:16:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:51,937][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.40228068828582764, acc: 0.9171597361564636)
[2025-02-13 19:16:52,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:52,287][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.24844679236412048, acc: 0.9451219439506531)
[2025-02-13 19:16:52,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:52,665][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.33547961711883545, acc: 0.9108280539512634)
[2025-02-13 19:16:52,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:53,080][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.3817566931247711, acc: 0.931034505367279)
[2025-02-13 19:16:53,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:53,473][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.2536067068576813, acc: 0.939393937587738)
[2025-02-13 19:16:53,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:53,848][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.18474504351615906, acc: 0.9589040875434875)
[2025-02-13 19:16:53,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:54,210][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.2358929067850113, acc: 0.9521276354789734)
[2025-02-13 19:16:54,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:54,597][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.11104533076286316, acc: 0.9729729890823364)
[2025-02-13 19:16:54,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:54,957][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.16175439953804016, acc: 0.9718309640884399)
[2025-02-13 19:16:55,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:55,352][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.1743529587984085, acc: 0.9666666388511658)
[2025-02-13 19:16:55,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:55,731][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.1308068335056305, acc: 0.9657142758369446)
[2025-02-13 19:16:55,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:56,179][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.20869289338588715, acc: 0.9440993666648865)
[2025-02-13 19:16:56,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:56,591][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.8056884407997131, acc: 0.826347291469574)
[2025-02-13 19:16:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:56,977][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.23182441294193268, acc: 0.9542483687400818)
[2025-02-13 19:16:57,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:57,371][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.30180132389068604, acc: 0.9424460530281067)
[2025-02-13 19:16:57,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:57,718][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.268794983625412, acc: 0.9112903475761414)
[2025-02-13 19:16:57,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:58,128][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.371468186378479, acc: 0.9235293865203857)
[2025-02-13 19:16:58,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:58,544][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.6068097949028015, acc: 0.8557692170143127)
[2025-02-13 19:16:58,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:58,949][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.4294489324092865, acc: 0.9251700639724731)
[2025-02-13 19:16:59,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:59,360][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.15562699735164642, acc: 0.9618320465087891)
[2025-02-13 19:16:59,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:59,773][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.24530838429927826, acc: 0.9411764740943909)
[2025-02-13 19:16:59,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:00,210][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.3090716004371643, acc: 0.931034505367279)
[2025-02-13 19:17:00,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:00,676][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.3885546326637268, acc: 0.8823529481887817)
[2025-02-13 19:17:00,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:01,105][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.22822944819927216, acc: 0.9461538195610046)
[2025-02-13 19:17:01,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:01,551][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.20147301256656647, acc: 0.9407407641410828)
[2025-02-13 19:17:01,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:01,977][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.2667243182659149, acc: 0.9568345546722412)
[2025-02-13 19:17:02,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:02,388][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.2628232538700104, acc: 0.9485294222831726)
[2025-02-13 19:17:02,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:02,770][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.3053087592124939, acc: 0.9327731132507324)
[2025-02-13 19:17:02,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:03,137][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.341795414686203, acc: 0.9207921028137207)
[2025-02-13 19:17:03,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:03,544][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.3690204322338104, acc: 0.9411764740943909)
[2025-02-13 19:17:03,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:03,958][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.23450958728790283, acc: 0.9166666865348816)
[2025-02-13 19:17:04,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:04,368][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.4762672185897827, acc: 0.9166666865348816)
[2025-02-13 19:17:04,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:04,752][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.06874258816242218, acc: 0.9838709831237793)
[2025-02-13 19:17:04,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:05,150][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.32545205950737, acc: 0.8846153616905212)
[2025-02-13 19:17:05,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:05,503][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.31240618228912354, acc: 0.9056603908538818)
[2025-02-13 19:17:05,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:05,884][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.3656843304634094, acc: 0.9473684430122375)
[2025-02-13 19:17:06,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:06,285][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.21190699934959412, acc: 0.9370078444480896)
[2025-02-13 19:17:06,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:06,671][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.32712894678115845, acc: 0.9172932505607605)
[2025-02-13 19:17:06,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:07,053][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.16119399666786194, acc: 0.9514563083648682)
[2025-02-13 19:17:07,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:07,407][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.3223085403442383, acc: 0.9300699234008789)
[2025-02-13 19:17:07,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:07,781][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.26441729068756104, acc: 0.9306930899620056)
[2025-02-13 19:17:07,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:08,183][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.15921002626419067, acc: 0.9541284441947937)
[2025-02-13 19:17:08,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:08,585][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.28188377618789673, acc: 0.9371069073677063)
[2025-02-13 19:17:08,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:08,979][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.17873765528202057, acc: 0.9425287246704102)
[2025-02-13 19:17:09,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:09,386][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.21508702635765076, acc: 0.9555555582046509)
[2025-02-13 19:17:09,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:09,813][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.32213106751441956, acc: 0.9117646813392639)
[2025-02-13 19:17:09,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:10,197][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.330041766166687, acc: 0.9230769276618958)
[2025-02-13 19:17:10,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:10,589][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.1942702829837799, acc: 0.9363636374473572)
[2025-02-13 19:17:10,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:10,998][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.23389695584774017, acc: 0.9207921028137207)
[2025-02-13 19:17:11,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:11,389][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.28719526529312134, acc: 0.9172932505607605)
[2025-02-13 19:17:11,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:11,773][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.37946876883506775, acc: 0.9081632494926453)
[2025-02-13 19:17:11,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:12,135][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.2694874107837677, acc: 0.9541284441947937)
[2025-02-13 19:17:12,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:12,511][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.34561580419540405, acc: 0.9230769276618958)
[2025-02-13 19:17:12,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:12,882][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.36068257689476013, acc: 0.9253731369972229)
[2025-02-13 19:17:13,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:13,250][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.5337938070297241, acc: 0.8842975497245789)
[2025-02-13 19:17:13,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:13,621][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.36484140157699585, acc: 0.9322034120559692)
[2025-02-13 19:17:13,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:14,027][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.3021049201488495, acc: 0.9275362491607666)
[2025-02-13 19:17:14,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:14,443][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.2910747826099396, acc: 0.9322034120559692)
[2025-02-13 19:17:14,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:14,840][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.40416258573532104, acc: 0.9357143044471741)
[2025-02-13 19:17:15,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:15,234][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.3362251818180084, acc: 0.9210526347160339)
[2025-02-13 19:17:15,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:15,597][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.28531017899513245, acc: 0.9005848169326782)
[2025-02-13 19:17:15,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:15,970][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.4266388714313507, acc: 0.8704662919044495)
[2025-02-13 19:17:16,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:16,366][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.26271751523017883, acc: 0.9580838084220886)
[2025-02-13 19:17:16,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:16,781][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.4428706765174866, acc: 0.9090909361839294)
[2025-02-13 19:17:16,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:17,200][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.2708773910999298, acc: 0.9116021990776062)
[2025-02-13 19:17:17,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:17,629][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.17436224222183228, acc: 0.978723406791687)
[2025-02-13 19:17:17,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:18,059][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.25976070761680603, acc: 0.9166666865348816)
[2025-02-13 19:17:18,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:18,451][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.21365799009799957, acc: 0.9589743614196777)
[2025-02-13 19:17:18,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:18,811][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.6374332904815674, acc: 0.8461538553237915)
[2025-02-13 19:17:18,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:19,178][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.4011193811893463, acc: 0.9266666769981384)
[2025-02-13 19:17:19,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:19,531][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.4116337299346924, acc: 0.886956512928009)
[2025-02-13 19:17:19,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:19,936][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.2480187565088272, acc: 0.9510869383811951)
[2025-02-13 19:17:20,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:20,335][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.11578484624624252, acc: 0.9666666388511658)
[2025-02-13 19:17:20,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:20,694][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.1105007454752922, acc: 0.970059871673584)
[2025-02-13 19:17:20,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:21,109][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.22671134769916534, acc: 0.9663865566253662)
[2025-02-13 19:17:21,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:21,520][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.11713691800832748, acc: 0.9637305736541748)
[2025-02-13 19:17:21,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:21,903][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.269868940114975, acc: 0.9227052927017212)
[2025-02-13 19:17:22,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:22,301][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.4268821179866791, acc: 0.9047619104385376)
[2025-02-13 19:17:22,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:22,693][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.2649502754211426, acc: 0.9580838084220886)
[2025-02-13 19:17:22,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:23,069][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.15557730197906494, acc: 0.965753436088562)
[2025-02-13 19:17:23,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:23,455][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.178696408867836, acc: 0.9568965435028076)
[2025-02-13 19:17:23,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:23,853][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.2059641033411026, acc: 0.9338235259056091)
[2025-02-13 19:17:23,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:24,262][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.22718507051467896, acc: 0.9580419659614563)
[2025-02-13 19:17:24,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:24,628][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.1408052146434784, acc: 0.9634146094322205)
[2025-02-13 19:17:24,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:25,046][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.45310041308403015, acc: 0.9084967374801636)
[2025-02-13 19:17:25,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:25,440][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.19522660970687866, acc: 0.9683544039726257)
[2025-02-13 19:17:25,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:25,816][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.2347433716058731, acc: 0.9457364082336426)
[2025-02-13 19:17:25,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:26,252][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.14201310276985168, acc: 0.9669421315193176)
[2025-02-13 19:17:26,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:26,648][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.4018881320953369, acc: 0.8936170339584351)
[2025-02-13 19:17:26,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:27,005][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.14037474989891052, acc: 0.970370352268219)
[2025-02-13 19:17:27,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:27,372][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.23371157050132751, acc: 0.9512194991111755)
[2025-02-13 19:17:27,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:27,764][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.17746101319789886, acc: 0.9620253443717957)
[2025-02-13 19:17:27,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:28,141][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.0745646059513092, acc: 0.9817073345184326)
[2025-02-13 19:17:28,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:28,530][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.04472897946834564, acc: 1.0)
[2025-02-13 19:17:28,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:28,945][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.15012259781360626, acc: 0.946107804775238)
[2025-02-13 19:17:29,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:29,334][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.5097295045852661, acc: 0.8970588445663452)
[2025-02-13 19:17:29,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:29,698][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.4309841990470886, acc: 0.9178082346916199)
[2025-02-13 19:17:29,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:30,077][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.3767283260822296, acc: 0.939130425453186)
[2025-02-13 19:17:30,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:30,433][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.5172470211982727, acc: 0.9017857313156128)
[2025-02-13 19:17:30,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:30,784][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.3855120539665222, acc: 0.8999999761581421)
[2025-02-13 19:17:30,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:31,144][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.17320454120635986, acc: 0.9719626307487488)
[2025-02-13 19:17:31,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:31,551][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.542376697063446, acc: 0.8758170008659363)
[2025-02-13 19:17:31,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:31,940][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.2758813798427582, acc: 0.9115044474601746)
[2025-02-13 19:17:32,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:32,344][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.18769234418869019, acc: 0.9722222089767456)
[2025-02-13 19:17:32,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:32,756][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.25224190950393677, acc: 0.9251337051391602)
[2025-02-13 19:17:32,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:33,137][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.28131169080734253, acc: 0.9192546606063843)
[2025-02-13 19:17:33,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:33,520][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.234153151512146, acc: 0.9351351261138916)
[2025-02-13 19:17:33,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:33,897][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.17457567155361176, acc: 0.9509202241897583)
[2025-02-13 19:17:34,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:34,274][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.22942869365215302, acc: 0.9444444179534912)
[2025-02-13 19:17:34,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:34,664][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.2599688172340393, acc: 0.9387755393981934)
[2025-02-13 19:17:34,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:35,026][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.12913104891777039, acc: 0.9738562107086182)
[2025-02-13 19:17:35,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:35,404][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.08605773001909256, acc: 0.9885714054107666)
[2025-02-13 19:17:35,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:35,807][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.19934822618961334, acc: 0.946107804775238)
[2025-02-13 19:17:35,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:36,223][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.1265619546175003, acc: 0.9788359999656677)
[2025-02-13 19:17:36,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:36,638][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.12974657118320465, acc: 0.979899525642395)
[2025-02-13 19:17:36,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:36,986][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.06931423395872116, acc: 0.9934210777282715)
[2025-02-13 19:17:37,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:37,357][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.25632593035697937, acc: 0.9054054021835327)
[2025-02-13 19:17:37,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:37,709][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.1903802752494812, acc: 0.9492753744125366)
[2025-02-13 19:17:37,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:38,115][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.1692539006471634, acc: 0.9634146094322205)
[2025-02-13 19:17:38,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:38,504][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.15883700549602509, acc: 0.9453125)
[2025-02-13 19:17:38,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:38,898][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.15786202251911163, acc: 0.9720279574394226)
[2025-02-13 19:17:39,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:39,256][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.27047011256217957, acc: 0.9270073175430298)
[2025-02-13 19:17:39,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:39,631][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.2129366397857666, acc: 0.9444444179534912)
[2025-02-13 19:17:39,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:40,005][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.1120477169752121, acc: 0.9704142212867737)
[2025-02-13 19:17:40,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:40,374][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.16966676712036133, acc: 0.9476743936538696)
[2025-02-13 19:17:40,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:40,753][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.1300249546766281, acc: 0.9743589758872986)
[2025-02-13 19:17:40,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:41,180][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.19377568364143372, acc: 0.9617486596107483)
[2025-02-13 19:17:41,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:41,572][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.18766823410987854, acc: 0.9488636255264282)
[2025-02-13 19:17:41,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:41,916][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.16758912801742554, acc: 0.9695122241973877)
[2025-02-13 19:17:42,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:42,273][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.14294816553592682, acc: 0.9770992398262024)
[2025-02-13 19:17:42,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:42,666][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.2893710434436798, acc: 0.9300699234008789)
[2025-02-13 19:17:42,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:43,045][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.26195043325424194, acc: 0.9214285612106323)
[2025-02-13 19:17:43,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:43,390][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.3649540841579437, acc: 0.9144737124443054)
[2025-02-13 19:17:43,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:43,757][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.18414467573165894, acc: 0.9711538553237915)
[2025-02-13 19:17:43,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:44,135][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.11390763521194458, acc: 0.9701492786407471)
[2025-02-13 19:17:44,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:44,526][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.1806824654340744, acc: 0.9420289993286133)
[2025-02-13 19:17:44,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:44,887][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.5205768346786499, acc: 0.8958333134651184)
[2025-02-13 19:17:45,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:45,247][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.3898909091949463, acc: 0.8976377844810486)
[2025-02-13 19:17:45,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:45,647][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.26705557107925415, acc: 0.9251700639724731)
[2025-02-13 19:17:45,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:46,034][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.6948410272598267, acc: 0.866310179233551)
[2025-02-13 19:17:46,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:46,444][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.4883194863796234, acc: 0.8939393758773804)
[2025-02-13 19:17:46,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:46,847][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.12704366445541382, acc: 0.9622641801834106)
[2025-02-13 19:17:47,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:47,211][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.4538402259349823, acc: 0.9117646813392639)
[2025-02-13 19:17:47,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:47,586][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.40516340732574463, acc: 0.8899999856948853)
[2025-02-13 19:17:47,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:47,929][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.2786836624145508, acc: 0.9202454090118408)
[2025-02-13 19:17:48,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:48,316][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.22252769768238068, acc: 0.9339622855186462)
[2025-02-13 19:17:48,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:48,720][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.1897411197423935, acc: 0.9482758641242981)
[2025-02-13 19:17:48,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:49,088][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.3509749472141266, acc: 0.9268292784690857)
[2025-02-13 19:17:49,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:49,453][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.18992358446121216, acc: 0.9505494236946106)
[2025-02-13 19:17:49,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:49,813][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.4353574812412262, acc: 0.9083333611488342)
[2025-02-13 19:17:49,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:50,186][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.23672373592853546, acc: 0.9658119678497314)
[2025-02-13 19:17:50,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:50,572][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.24021989107131958, acc: 0.9305555820465088)
[2025-02-13 19:17:50,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:50,968][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.11522384732961655, acc: 0.9642857313156128)
[2025-02-13 19:17:51,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:51,372][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.24031922221183777, acc: 0.9453551769256592)
[2025-02-13 19:17:51,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:51,759][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.1701018512248993, acc: 0.9640718698501587)
[2025-02-13 19:17:51,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:52,190][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.30847108364105225, acc: 0.9325153231620789)
[2025-02-13 19:17:52,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:52,608][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.12019553780555725, acc: 0.9578313231468201)
[2025-02-13 19:17:52,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:53,024][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.05911332741379738, acc: 0.9933775067329407)
[2025-02-13 19:17:53,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:53,414][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.28161218762397766, acc: 0.9551281929016113)
[2025-02-13 19:17:53,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:53,804][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.07638544589281082, acc: 0.970588207244873)
[2025-02-13 19:17:53,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:54,196][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.04857971891760826, acc: 0.9934210777282715)
[2025-02-13 19:17:54,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:54,592][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.41060012578964233, acc: 0.9133333563804626)
[2025-02-13 19:17:54,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:54,969][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.37632256746292114, acc: 0.9368420839309692)
[2025-02-13 19:17:55,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:55,349][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.22509600222110748, acc: 0.9642857313156128)
[2025-02-13 19:17:55,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:55,726][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.21072746813297272, acc: 0.9675324559211731)
[2025-02-13 19:17:55,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:56,110][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.16906453669071198, acc: 0.970588207244873)
[2025-02-13 19:17:56,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:56,516][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.22051389515399933, acc: 0.9507042169570923)
[2025-02-13 19:17:56,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:56,922][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.3318261206150055, acc: 0.9520547986030579)
[2025-02-13 19:17:57,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:57,335][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.22996243834495544, acc: 0.9420289993286133)
[2025-02-13 19:17:57,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:57,729][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.17859488725662231, acc: 0.9631901979446411)
[2025-02-13 19:17:57,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:58,147][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.35048505663871765, acc: 0.9117646813392639)
[2025-02-13 19:17:58,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:58,490][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.2673552334308624, acc: 0.9357143044471741)
[2025-02-13 19:17:58,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:58,861][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.2771560847759247, acc: 0.931034505367279)
[2025-02-13 19:17:59,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:59,233][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.24579675495624542, acc: 0.9197080135345459)
[2025-02-13 19:17:59,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:59,599][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.35234683752059937, acc: 0.9173553586006165)
[2025-02-13 19:17:59,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:59,973][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.2757020890712738, acc: 0.9385964870452881)
[2025-02-13 19:18:00,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:00,322][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.39246344566345215, acc: 0.9285714030265808)
[2025-02-13 19:18:00,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:00,687][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.4471818208694458, acc: 0.8793103694915771)
[2025-02-13 19:18:00,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:01,052][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.34987884759902954, acc: 0.9069767594337463)
[2025-02-13 19:18:01,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:01,420][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.20308515429496765, acc: 0.9432623982429504)
[2025-02-13 19:18:01,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:01,775][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.4378640651702881, acc: 0.9101123809814453)
[2025-02-13 19:18:01,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:02,150][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.2271714210510254, acc: 0.9363057613372803)
[2025-02-13 19:18:02,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:02,496][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.3705374300479889, acc: 0.9172932505607605)
[2025-02-13 19:18:02,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:02,872][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.4195536673069, acc: 0.8933333158493042)
[2025-02-13 19:18:03,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:03,244][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.47896063327789307, acc: 0.8888888955116272)
[2025-02-13 19:18:03,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:03,630][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.3792705535888672, acc: 0.913294792175293)
[2025-02-13 19:18:03,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:04,036][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.32665833830833435, acc: 0.9207921028137207)
[2025-02-13 19:18:04,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:04,423][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.3572379946708679, acc: 0.9019607901573181)
[2025-02-13 19:18:04,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:04,824][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.4200299382209778, acc: 0.9007092118263245)
[2025-02-13 19:18:04,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:05,222][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.5862116813659668, acc: 0.8861788511276245)
[2025-02-13 19:18:05,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:05,617][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.32578176259994507, acc: 0.9305555820465088)
[2025-02-13 19:18:05,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:06,026][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.2911983132362366, acc: 0.9337349534034729)
[2025-02-13 19:18:06,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:06,464][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.09795038402080536, acc: 0.9751552939414978)
[2025-02-13 19:18:06,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:06,870][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.11768363416194916, acc: 0.979899525642395)
[2025-02-13 19:18:07,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:07,296][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.11506348103284836, acc: 0.9647887349128723)
[2025-02-13 19:18:07,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:07,674][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.3442811667919159, acc: 0.9340659379959106)
[2025-02-13 19:18:07,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:08,035][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.2629489302635193, acc: 0.9433962106704712)
[2025-02-13 19:18:08,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:08,384][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 1.1707916259765625, acc: 0.7676767706871033)
[2025-02-13 19:18:08,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:08,724][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 1.1289777755737305, acc: 0.7615384459495544)
[2025-02-13 19:18:08,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:09,081][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.46304625272750854, acc: 0.9005848169326782)
[2025-02-13 19:18:09,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:09,437][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.10988367348909378, acc: 0.9814814925193787)
[2025-02-13 19:18:09,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:09,853][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.22523798048496246, acc: 0.932584285736084)
[2025-02-13 19:18:09,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:10,244][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.4227263033390045, acc: 0.8888888955116272)
[2025-02-13 19:18:10,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:10,647][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.30481860041618347, acc: 0.9320987462997437)
[2025-02-13 19:18:10,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:11,017][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.135429248213768, acc: 0.9576271176338196)
[2025-02-13 19:18:11,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:11,399][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.09247420728206635, acc: 0.9736841917037964)
[2025-02-13 19:18:11,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:11,781][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.19538980722427368, acc: 0.9398496150970459)
[2025-02-13 19:18:11,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:12,207][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.0556589812040329, acc: 1.0)
[2025-02-13 19:18:12,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:12,577][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.2519867718219757, acc: 0.9333333373069763)
[2025-02-13 19:18:12,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:12,963][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.10551978647708893, acc: 0.9681528806686401)
[2025-02-13 19:18:13,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:13,433][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.13176943361759186, acc: 0.9605262875556946)
[2025-02-13 19:18:13,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:13,801][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.24645009636878967, acc: 0.9347826242446899)
[2025-02-13 19:18:13,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:14,196][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.1500004082918167, acc: 0.9545454382896423)
[2025-02-13 19:18:14,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:14,563][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.4045344293117523, acc: 0.931506872177124)
[2025-02-13 19:18:14,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:14,930][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.2303318828344345, acc: 0.9662162065505981)
[2025-02-13 19:18:15,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:15,374][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.09105872362852097, acc: 0.9820359349250793)
[2025-02-13 19:18:15,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:15,759][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.03164124861359596, acc: 1.0)
[2025-02-13 19:18:15,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:16,147][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.2212272435426712, acc: 0.9395604133605957)
[2025-02-13 19:18:16,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:16,521][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.076960489153862, acc: 0.9924812316894531)
[2025-02-13 19:18:16,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:16,893][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.034532107412815094, acc: 0.9921875)
[2025-02-13 19:18:17,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:17,311][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.058406658470630646, acc: 0.9935483932495117)
[2025-02-13 19:18:17,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:17,708][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.23637042939662933, acc: 0.9642857313156128)
[2025-02-13 19:18:17,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:18,075][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.2216198593378067, acc: 0.9454545378684998)
[2025-02-13 19:18:18,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:18,480][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.34702807664871216, acc: 0.9230769276618958)
[2025-02-13 19:18:18,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:18,855][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.3617246747016907, acc: 0.9096385836601257)
[2025-02-13 19:18:19,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:19,222][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.19399355351924896, acc: 0.9642857313156128)
[2025-02-13 19:18:19,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:19,584][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.3714594542980194, acc: 0.8814814686775208)
[2025-02-13 19:18:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:19,970][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.26482391357421875, acc: 0.9440559148788452)
[2025-02-13 19:18:20,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:20,383][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.25354859232902527, acc: 0.9363057613372803)
[2025-02-13 19:18:20,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:20,738][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.18989697098731995, acc: 0.9724770784378052)
[2025-02-13 19:18:20,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:21,120][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.23424817621707916, acc: 0.9259259104728699)
[2025-02-13 19:18:21,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:21,460][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.23176522552967072, acc: 0.9391891956329346)
[2025-02-13 19:18:21,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:21,835][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.09724175930023193, acc: 0.9793814420700073)
[2025-02-13 19:18:21,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:22,186][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.21282391250133514, acc: 0.9607843160629272)
[2025-02-13 19:18:22,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:22,548][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.2277524471282959, acc: 0.949367105960846)
[2025-02-13 19:18:22,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:22,950][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.35803741216659546, acc: 0.905063271522522)
[2025-02-13 19:18:23,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:23,320][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.3203011453151703, acc: 0.9281045794487)
[2025-02-13 19:18:23,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:23,688][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.3131674826145172, acc: 0.9356725215911865)
[2025-02-13 19:18:23,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:24,056][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.31034189462661743, acc: 0.9242424368858337)
[2025-02-13 19:18:24,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:24,413][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.46688950061798096, acc: 0.8783783912658691)
[2025-02-13 19:18:24,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:24,866][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.387722373008728, acc: 0.9136690497398376)
[2025-02-13 19:18:25,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:25,260][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.09753740578889847, acc: 0.9733333587646484)
[2025-02-13 19:18:25,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:25,662][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.18380741775035858, acc: 0.9523809552192688)
[2025-02-13 19:18:25,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:26,125][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.18820372223854065, acc: 0.961240291595459)
[2025-02-13 19:18:26,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:26,562][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.10264690220355988, acc: 0.9791666865348816)
[2025-02-13 19:18:26,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:26,968][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.10210530459880829, acc: 0.9801324605941772)
[2025-02-13 19:18:27,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:27,371][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.293044775724411, acc: 0.9447852969169617)
[2025-02-13 19:18:27,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:27,788][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.2319067269563675, acc: 0.9473684430122375)
[2025-02-13 19:18:27,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:28,222][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.07086256891489029, acc: 0.9779411554336548)
[2025-02-13 19:18:28,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:28,624][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.40725353360176086, acc: 0.9294871687889099)
[2025-02-13 19:18:28,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:29,032][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.11982474476099014, acc: 0.9710144996643066)
[2025-02-13 19:18:29,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:29,410][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.1292581856250763, acc: 0.9760765433311462)
[2025-02-13 19:18:29,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:29,783][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.08292435109615326, acc: 0.9774011373519897)
[2025-02-13 19:18:29,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:30,167][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.10096731781959534, acc: 0.9736841917037964)
[2025-02-13 19:18:30,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:30,566][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.13360974192619324, acc: 0.9595959782600403)
[2025-02-13 19:18:30,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:30,940][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.16627027094364166, acc: 0.9629629850387573)
[2025-02-13 19:18:31,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:31,320][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.11984912306070328, acc: 0.9688888788223267)
[2025-02-13 19:18:31,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:31,750][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.2173348218202591, acc: 0.9545454382896423)
[2025-02-13 19:18:31,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:32,133][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.07585397362709045, acc: 0.9820359349250793)
[2025-02-13 19:18:32,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:32,494][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.11574988812208176, acc: 0.9666666388511658)
[2025-02-13 19:18:32,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:32,866][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.03588775172829628, acc: 0.9867549538612366)
[2025-02-13 19:18:33,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:33,231][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.14002425968647003, acc: 0.9725274443626404)
[2025-02-13 19:18:33,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:33,671][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.1165611669421196, acc: 0.9729729890823364)
[2025-02-13 19:18:33,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:34,092][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.16140130162239075, acc: 0.9595959782600403)
[2025-02-13 19:18:34,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:34,503][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.16843873262405396, acc: 0.9627659320831299)
[2025-02-13 19:18:34,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:34,907][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.03250115364789963, acc: 1.0)
[2025-02-13 19:18:35,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:35,298][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.16928286850452423, acc: 0.9545454382896423)
[2025-02-13 19:18:35,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:35,690][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.3363868296146393, acc: 0.8982300758361816)
[2025-02-13 19:18:35,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:36,056][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.17819061875343323, acc: 0.949999988079071)
[2025-02-13 19:18:36,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:36,442][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.14683951437473297, acc: 0.9594594836235046)
[2025-02-13 19:18:36,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:36,811][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.15163001418113708, acc: 0.9688888788223267)
[2025-02-13 19:18:36,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:37,230][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.1642727553844452, acc: 0.9532163739204407)
[2025-02-13 19:18:37,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:37,614][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.1813320517539978, acc: 0.9568345546722412)
[2025-02-13 19:18:37,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:38,018][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.2526543438434601, acc: 0.9320987462997437)
[2025-02-13 19:18:38,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:38,435][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.1217178925871849, acc: 0.96875)
[2025-02-13 19:18:38,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:38,847][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.1210697740316391, acc: 0.9747899174690247)
[2025-02-13 19:18:38,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:39,187][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.2486991286277771, acc: 0.931034505367279)
[2025-02-13 19:18:39,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:39,544][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.17318196594715118, acc: 0.9692307710647583)
[2025-02-13 19:18:39,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:39,917][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.17051109671592712, acc: 0.9599999785423279)
[2025-02-13 19:18:40,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:40,312][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.11454011499881744, acc: 0.9726027250289917)
[2025-02-13 19:18:40,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:40,691][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.2481904923915863, acc: 0.9366196990013123)
[2025-02-13 19:18:40,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:41,090][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.3159242272377014, acc: 0.9271523356437683)
[2025-02-13 19:18:41,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:41,477][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.1312483251094818, acc: 0.9583333134651184)
[2025-02-13 19:18:41,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:41,851][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.3484342098236084, acc: 0.9280575513839722)
[2025-02-13 19:18:42,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:42,258][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.3952271640300751, acc: 0.9152542352676392)
[2025-02-13 19:18:42,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:42,668][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.27591076493263245, acc: 0.9266055226325989)
[2025-02-13 19:18:42,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:43,079][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.2735922932624817, acc: 0.9624060392379761)
[2025-02-13 19:18:43,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:43,455][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.2418563961982727, acc: 0.956204354763031)
[2025-02-13 19:18:43,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:43,837][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.43107226490974426, acc: 0.8911564350128174)
[2025-02-13 19:18:43,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:44,251][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.31312838196754456, acc: 0.913385808467865)
[2025-02-13 19:18:44,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:44,644][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.25218310952186584, acc: 0.9264705777168274)
[2025-02-13 19:18:44,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:45,053][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.4541679918766022, acc: 0.8895705342292786)
[2025-02-13 19:18:45,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:45,431][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.4906417727470398, acc: 0.8903225660324097)
[2025-02-13 19:18:45,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:45,804][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.28533247113227844, acc: 0.9090909361839294)
[2025-02-13 19:18:45,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:46,206][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.28436753153800964, acc: 0.9078013896942139)
[2025-02-13 19:18:46,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:46,605][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.46891292929649353, acc: 0.8799999952316284)
[2025-02-13 19:18:46,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:47,019][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.3456951677799225, acc: 0.9166666865348816)
[2025-02-13 19:18:47,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:47,381][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.2151043862104416, acc: 0.9562841653823853)
[2025-02-13 19:18:47,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:47,765][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.2517808973789215, acc: 0.9491525292396545)
[2025-02-13 19:18:47,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:48,128][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.2603868246078491, acc: 0.9558823704719543)
[2025-02-13 19:18:48,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:48,516][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.3165440559387207, acc: 0.9290780425071716)
[2025-02-13 19:18:48,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:48,925][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.4614911675453186, acc: 0.8809523582458496)
[2025-02-13 19:18:49,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:49,318][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.4433231055736542, acc: 0.8823529481887817)
[2025-02-13 19:18:50,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:50,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:50,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:51,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:51,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:51,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:52,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:52,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:52,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:53,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:53,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:53,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:54,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:54,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:54,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:55,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:55,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:55,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:56,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:56,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:56,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53,734][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3415, device='cuda:0') eval_epoch_loss=tensor(0.2938, device='cuda:0') eval_epoch_acc=tensor(0.9299, device='cuda:0')
[2025-02-13 19:22:53,735][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:22:53,736][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:22:54,094][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_3566_loss_0.2937663793563843/model.pt
[2025-02-13 19:22:54,103][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:22:54,105][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.2937663793563843
[2025-02-13 19:22:54,105][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.929926872253418
[2025-02-13 19:22:54,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54,567][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.45324188470840454, acc: 0.8823529481887817)
[2025-02-13 19:22:54,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54,901][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.22354291379451752, acc: 0.949438214302063)
[2025-02-13 19:22:55,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55,313][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.38358765840530396, acc: 0.914893627166748)
[2025-02-13 19:22:55,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55,786][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.29842355847358704, acc: 0.9333333373069763)
[2025-02-13 19:22:55,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56,174][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.5469440221786499, acc: 0.845588207244873)
[2025-02-13 19:22:56,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56,561][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.4838346540927887, acc: 0.882022500038147)
[2025-02-13 19:22:56,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56,949][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.1649400144815445, acc: 0.9642857313156128)
[2025-02-13 19:22:57,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57,315][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.2562585473060608, acc: 0.9539473652839661)
[2025-02-13 19:22:57,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57,693][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.5055402517318726, acc: 0.9173553586006165)
[2025-02-13 19:22:57,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58,060][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.6230276226997375, acc: 0.8881579041481018)
[2025-02-13 19:22:58,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58,431][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.34974175691604614, acc: 0.9179104566574097)
[2025-02-13 19:22:58,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58,824][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.3015594780445099, acc: 0.9123711585998535)
[2025-02-13 19:22:58,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59,233][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.1944332867860794, acc: 0.9638554453849792)
[2025-02-13 19:22:59,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59,647][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.3676125705242157, acc: 0.930232584476471)
[2025-02-13 19:22:59,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00,042][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.5650973916053772, acc: 0.849711000919342)
[2025-02-13 19:23:00,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00,408][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.41096165776252747, acc: 0.8647058606147766)
[2025-02-13 19:23:00,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00,781][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.4283583462238312, acc: 0.8888888955116272)
[2025-02-13 19:23:00,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01,155][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.40797293186187744, acc: 0.886227548122406)
[2025-02-13 19:23:01,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01,576][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.539626955986023, acc: 0.8829787373542786)
[2025-02-13 19:23:01,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01,993][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.51700359582901, acc: 0.9141104221343994)
[2025-02-13 19:23:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02,397][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.3572048544883728, acc: 0.9171974658966064)
[2025-02-13 19:23:02,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02,785][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.3303460478782654, acc: 0.9300699234008789)
[2025-02-13 19:23:02,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03,172][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.2504907250404358, acc: 0.9306358098983765)
[2025-02-13 19:23:03,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03,557][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.38635796308517456, acc: 0.9344262480735779)
[2025-02-13 19:23:03,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03,971][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.11461829394102097, acc: 0.9777777791023254)
[2025-02-13 19:23:04,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04,360][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.20639535784721375, acc: 0.9649122953414917)
[2025-02-13 19:23:04,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04,784][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.0716731920838356, acc: 0.9857142567634583)
[2025-02-13 19:23:04,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05,168][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.4113750755786896, acc: 0.9366196990013123)
[2025-02-13 19:23:05,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05,573][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.177161306142807, acc: 0.940397322177887)
[2025-02-13 19:23:05,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06,001][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.2972283661365509, acc: 0.9542483687400818)
[2025-02-13 19:23:06,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06,399][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.3864153325557709, acc: 0.9117646813392639)
[2025-02-13 19:23:06,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06,777][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.1834639459848404, acc: 0.9539473652839661)
[2025-02-13 19:23:06,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07,186][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.36400049924850464, acc: 0.9241706132888794)
[2025-02-13 19:23:07,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07,568][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.40016260743141174, acc: 0.9135135412216187)
[2025-02-13 19:23:07,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07,958][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.37759339809417725, acc: 0.9222797751426697)
[2025-02-13 19:23:08,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08,358][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.7871689796447754, acc: 0.8258426785469055)
[2025-02-13 19:23:08,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08,774][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.4666866958141327, acc: 0.8883720636367798)
[2025-02-13 19:23:08,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09,140][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.44094139337539673, acc: 0.9119496941566467)
[2025-02-13 19:23:09,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09,495][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.4250505566596985, acc: 0.913385808467865)
[2025-02-13 19:23:09,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09,866][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.34768056869506836, acc: 0.9237667918205261)
[2025-02-13 19:23:10,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10,260][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.7558403611183167, acc: 0.8424657583236694)
[2025-02-13 19:23:10,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10,626][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.2709842026233673, acc: 0.931506872177124)
[2025-02-13 19:23:10,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11,009][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.36916905641555786, acc: 0.9223300814628601)
[2025-02-13 19:23:11,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11,399][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.24076122045516968, acc: 0.9368420839309692)
[2025-02-13 19:23:11,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11,798][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.4038344919681549, acc: 0.8956043720245361)
[2025-02-13 19:23:11,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12,237][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.31960731744766235, acc: 0.9371428489685059)
[2025-02-13 19:23:12,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12,662][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.5335208773612976, acc: 0.8836206793785095)
[2025-02-13 19:23:12,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13,050][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.25687605142593384, acc: 0.9289617538452148)
[2025-02-13 19:23:13,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13,441][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.19595162570476532, acc: 0.9680851101875305)
[2025-02-13 19:23:13,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13,819][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.22662299871444702, acc: 0.9459459185600281)
[2025-02-13 19:23:13,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14,220][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.42813554406166077, acc: 0.8757764101028442)
[2025-02-13 19:23:14,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14,636][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.2260385900735855, acc: 0.9405405521392822)
[2025-02-13 19:23:14,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15,040][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.4109354317188263, acc: 0.9018405079841614)
[2025-02-13 19:23:15,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15,437][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.5245577692985535, acc: 0.8689956068992615)
[2025-02-13 19:23:15,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15,821][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.27793771028518677, acc: 0.940119743347168)
[2025-02-13 19:23:15,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16,181][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.26731106638908386, acc: 0.9235668778419495)
[2025-02-13 19:23:16,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16,581][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.1617729663848877, acc: 0.9610389471054077)
[2025-02-13 19:23:16,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16,993][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.20317746698856354, acc: 0.9464285969734192)
[2025-02-13 19:23:17,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17,380][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.27980026602745056, acc: 0.935251772403717)
[2025-02-13 19:23:17,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17,764][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.8145503997802734, acc: 0.8151260614395142)
[2025-02-13 19:23:17,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18,201][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 1.0389173030853271, acc: 0.7865168452262878)
[2025-02-13 19:23:18,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18,594][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.5160561203956604, acc: 0.8584905862808228)
[2025-02-13 19:23:18,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18,983][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.559061586856842, acc: 0.8285714387893677)
[2025-02-13 19:23:19,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19,355][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.47460854053497314, acc: 0.9193548560142517)
[2025-02-13 19:23:19,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19,764][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.37231138348579407, acc: 0.9032257795333862)
[2025-02-13 19:23:19,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20,173][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.4282782971858978, acc: 0.9285714030265808)
[2025-02-13 19:23:20,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20,585][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.2386917620897293, acc: 0.9590163826942444)
[2025-02-13 19:23:20,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20,950][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.32581931352615356, acc: 0.9171974658966064)
[2025-02-13 19:23:21,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21,355][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.4523031413555145, acc: 0.8971962332725525)
[2025-02-13 19:23:21,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21,769][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.3367154002189636, acc: 0.9051724076271057)
[2025-02-13 19:23:21,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22,151][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.40223968029022217, acc: 0.8867924809455872)
[2025-02-13 19:23:22,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22,560][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.40444421768188477, acc: 0.8909090757369995)
[2025-02-13 19:23:22,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22,922][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.25222569704055786, acc: 0.9340659379959106)
[2025-02-13 19:23:23,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23,296][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.269974946975708, acc: 0.957446813583374)
[2025-02-13 19:23:23,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23,739][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.27197903394699097, acc: 0.93388432264328)
[2025-02-13 19:23:23,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24,108][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.12326721847057343, acc: 0.9838709831237793)
[2025-02-13 19:23:24,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24,516][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.1300112009048462, acc: 0.9731543660163879)
[2025-02-13 19:23:24,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24,914][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.2726532816886902, acc: 0.9140625)
[2025-02-13 19:23:25,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25,312][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.32543009519577026, acc: 0.9189189076423645)
[2025-02-13 19:23:25,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25,722][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.583175778388977, acc: 0.890625)
[2025-02-13 19:23:25,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26,146][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.5801931619644165, acc: 0.8636363744735718)
[2025-02-13 19:23:26,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26,517][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.19304892420768738, acc: 0.966292142868042)
[2025-02-13 19:23:26,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26,891][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.38643595576286316, acc: 0.9053254723548889)
[2025-02-13 19:23:27,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27,265][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.427059143781662, acc: 0.9328858852386475)
[2025-02-13 19:23:27,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27,694][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.2782580554485321, acc: 0.9171597361564636)
[2025-02-13 19:23:27,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28,080][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.3553006649017334, acc: 0.9156626462936401)
[2025-02-13 19:23:28,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28,511][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.3467431664466858, acc: 0.9251700639724731)
[2025-02-13 19:23:28,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28,876][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.5987990498542786, acc: 0.8466257452964783)
[2025-02-13 19:23:29,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29,259][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.0640764981508255, acc: 0.984000027179718)
[2025-02-13 19:23:29,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29,635][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.46187007427215576, acc: 0.9041916131973267)
[2025-02-13 19:23:29,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29,982][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.4835146963596344, acc: 0.8985507488250732)
[2025-02-13 19:23:30,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30,345][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.18660202622413635, acc: 0.9440559148788452)
[2025-02-13 19:23:30,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30,710][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.30372560024261475, acc: 0.9375)
[2025-02-13 19:23:30,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31,089][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.3571104407310486, acc: 0.918367326259613)
[2025-02-13 19:23:31,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31,463][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.5463446974754333, acc: 0.8395061492919922)
[2025-02-13 19:23:31,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31,868][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.31617096066474915, acc: 0.8999999761581421)
[2025-02-13 19:23:32,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32,281][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.5256189107894897, acc: 0.8757764101028442)
[2025-02-13 19:23:32,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32,652][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.3803945779800415, acc: 0.90625)
[2025-02-13 19:23:32,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33,045][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.5709106922149658, acc: 0.8441558480262756)
[2025-02-13 19:23:33,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33,422][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.27980831265449524, acc: 0.912162184715271)
[2025-02-13 19:23:33,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33,806][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.3770414888858795, acc: 0.8972602486610413)
[2025-02-13 19:23:33,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34,164][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.7792654037475586, acc: 0.8062015771865845)
[2025-02-13 19:23:34,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34,567][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.49135488271713257, acc: 0.8758620619773865)
[2025-02-13 19:23:34,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34,960][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.14743998646736145, acc: 0.9586777091026306)
[2025-02-13 19:23:35,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35,331][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.16252142190933228, acc: 0.9457831382751465)
[2025-02-13 19:23:35,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35,671][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.2142190784215927, acc: 0.9367088675498962)
[2025-02-13 19:23:35,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36,059][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.17492936551570892, acc: 0.939226508140564)
[2025-02-13 19:23:36,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36,424][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.2539253532886505, acc: 0.9379310607910156)
[2025-02-13 19:23:36,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36,828][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.22884507477283478, acc: 0.9527027010917664)
[2025-02-13 19:23:36,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37,219][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.13837504386901855, acc: 0.9613259434700012)
[2025-02-13 19:23:37,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37,599][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.24804997444152832, acc: 0.9230769276618958)
[2025-02-13 19:23:37,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37,994][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.31761273741722107, acc: 0.9204545617103577)
[2025-02-13 19:23:38,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38,395][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.4656447768211365, acc: 0.9240506291389465)
[2025-02-13 19:23:38,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38,819][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.22504229843616486, acc: 0.9615384340286255)
[2025-02-13 19:23:38,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39,237][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.30098238587379456, acc: 0.9266666769981384)
[2025-02-13 19:23:39,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39,648][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.2106839418411255, acc: 0.9437500238418579)
[2025-02-13 19:23:39,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40,067][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.1359042376279831, acc: 0.976190447807312)
[2025-02-13 19:23:40,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40,481][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.11270938068628311, acc: 0.9632353186607361)
[2025-02-13 19:23:40,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40,883][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.07898728549480438, acc: 0.9868420958518982)
[2025-02-13 19:23:41,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41,263][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.19869354367256165, acc: 0.934959352016449)
[2025-02-13 19:23:41,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41,651][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.1681460589170456, acc: 0.9647887349128723)
[2025-02-13 19:23:41,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42,062][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.16979025304317474, acc: 0.9378882050514221)
[2025-02-13 19:23:42,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42,448][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.10517771542072296, acc: 0.9833333492279053)
[2025-02-13 19:23:42,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42,837][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.1776602566242218, acc: 0.957446813583374)
[2025-02-13 19:23:42,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43,226][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.14158105850219727, acc: 0.9556962251663208)
[2025-02-13 19:23:43,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43,582][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.14549313485622406, acc: 0.9504950642585754)
[2025-02-13 19:23:43,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44,009][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.08621420711278915, acc: 0.987730085849762)
[2025-02-13 19:23:44,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44,406][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.09627760946750641, acc: 0.9848484992980957)
[2025-02-13 19:23:44,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44,802][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.14008773863315582, acc: 0.9624999761581421)
[2025-02-13 19:23:44,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45,173][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.18273165822029114, acc: 0.9617834687232971)
[2025-02-13 19:23:45,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45,582][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.19875623285770416, acc: 0.9655172228813171)
[2025-02-13 19:23:45,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45,956][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.08049595355987549, acc: 0.9817073345184326)
[2025-02-13 19:23:46,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46,321][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.10294884443283081, acc: 0.9724137783050537)
[2025-02-13 19:23:46,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46,690][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.13303278386592865, acc: 0.9685039520263672)
[2025-02-13 19:23:46,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47,085][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.1472744196653366, acc: 0.9685534834861755)
[2025-02-13 19:23:47,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47,471][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.16001670062541962, acc: 0.956204354763031)
[2025-02-13 19:23:47,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47,859][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.135213240981102, acc: 0.9818181991577148)
[2025-02-13 19:23:48,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48,253][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.17434769868850708, acc: 0.9488636255264282)
[2025-02-13 19:23:48,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48,604][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.10532697290182114, acc: 0.9801980257034302)
[2025-02-13 19:23:48,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49,010][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.29382097721099854, acc: 0.9447852969169617)
[2025-02-13 19:23:49,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49,423][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.4335033595561981, acc: 0.8910890817642212)
[2025-02-13 19:23:49,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49,819][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.13152152299880981, acc: 0.9800000190734863)
[2025-02-13 19:23:49,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50,238][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.34604573249816895, acc: 0.9230769276618958)
[2025-02-13 19:23:50,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50,653][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.15256132185459137, acc: 0.9583333134651184)
[2025-02-13 19:23:50,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51,025][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.23544611036777496, acc: 0.9259259104728699)
[2025-02-13 19:23:51,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51,425][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.235380157828331, acc: 0.9395604133605957)
[2025-02-13 19:23:51,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51,807][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.348554402589798, acc: 0.91847825050354)
[2025-02-13 19:23:51,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52,182][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.18377186357975006, acc: 0.9424460530281067)
[2025-02-13 19:23:52,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52,581][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.24451926350593567, acc: 0.9378238320350647)
[2025-02-13 19:23:52,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52,979][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.16303257644176483, acc: 0.9680851101875305)
[2025-02-13 19:23:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53,361][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.38477206230163574, acc: 0.9340101480484009)
[2025-02-13 19:23:53,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53,789][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.22213609516620636, acc: 0.9512194991111755)
[2025-02-13 19:23:53,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54,208][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.24172662198543549, acc: 0.9635416865348816)
[2025-02-13 19:23:54,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54,618][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.3390548825263977, acc: 0.9181286692619324)
[2025-02-13 19:23:54,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55,045][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.11494828760623932, acc: 0.9620253443717957)
[2025-02-13 19:23:55,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55,426][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.22229786217212677, acc: 0.9421965479850769)
[2025-02-13 19:23:55,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55,826][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.2134874314069748, acc: 0.9378238320350647)
[2025-02-13 19:23:55,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56,191][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.2177785038948059, acc: 0.9548386931419373)
[2025-02-13 19:23:56,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56,549][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.12225719541311264, acc: 0.9759036302566528)
[2025-02-13 19:23:56,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56,938][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.38700833916664124, acc: 0.9390243887901306)
[2025-02-13 19:23:57,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57,303][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.1759386658668518, acc: 0.9547738432884216)
[2025-02-13 19:23:57,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57,665][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.1616794466972351, acc: 0.9595375657081604)
[2025-02-13 19:23:57,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58,065][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.134517639875412, acc: 0.9556650519371033)
[2025-02-13 19:23:58,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58,477][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.1715978980064392, acc: 0.9622641801834106)
[2025-02-13 19:23:58,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58,894][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.2385365068912506, acc: 0.9333333373069763)
[2025-02-13 19:23:59,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59,292][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.09859409183263779, acc: 0.959770143032074)
[2025-02-13 19:23:59,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59,654][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.12461192160844803, acc: 0.9743589758872986)
[2025-02-13 19:23:59,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00,008][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.28789475560188293, acc: 0.9378882050514221)
[2025-02-13 19:24:00,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00,375][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.40359073877334595, acc: 0.9358974099159241)
[2025-02-13 19:24:00,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00,737][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.25678691267967224, acc: 0.9415584206581116)
[2025-02-13 19:24:00,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01,141][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.22147901356220245, acc: 0.9338235259056091)
[2025-02-13 19:24:01,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01,549][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.17079684138298035, acc: 0.9515151381492615)
[2025-02-13 19:24:01,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01,954][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.10192704945802689, acc: 0.9861111044883728)
[2025-02-13 19:24:02,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02,353][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.21726445853710175, acc: 0.9468085169792175)
[2025-02-13 19:24:02,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02,739][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.1742117553949356, acc: 0.9426751732826233)
[2025-02-13 19:24:02,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03,155][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.21841531991958618, acc: 0.9655172228813171)
[2025-02-13 19:24:03,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03,550][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.2240712195634842, acc: 0.9591836929321289)
[2025-02-13 19:24:03,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03,893][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.3471224009990692, acc: 0.9176470637321472)
[2025-02-13 19:24:04,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04,321][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.2839092016220093, acc: 0.9418604373931885)
[2025-02-13 19:24:04,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04,682][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.40760573744773865, acc: 0.9333333373069763)
[2025-02-13 19:24:04,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05,070][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.22700513899326324, acc: 0.9491525292396545)
[2025-02-13 19:24:05,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05,455][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.1913810670375824, acc: 0.9722222089767456)
[2025-02-13 19:24:05,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05,798][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.23580001294612885, acc: 0.9399999976158142)
[2025-02-13 19:24:05,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06,158][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.4171231687068939, acc: 0.898809552192688)
[2025-02-13 19:24:06,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06,552][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.1083076000213623, acc: 0.9693251252174377)
[2025-02-13 19:24:06,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06,948][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.27761563658714294, acc: 0.9337349534034729)
[2025-02-13 19:24:07,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07,306][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.28249165415763855, acc: 0.9371069073677063)
[2025-02-13 19:24:07,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07,667][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.40865135192871094, acc: 0.9018405079841614)
[2025-02-13 19:24:07,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08,024][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.10315766930580139, acc: 0.9677419066429138)
[2025-02-13 19:24:08,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08,376][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.25635141134262085, acc: 0.9173553586006165)
[2025-02-13 19:24:08,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08,765][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.2899028956890106, acc: 0.9190751314163208)
[2025-02-13 19:24:08,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09,156][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.3817969560623169, acc: 0.9285714030265808)
[2025-02-13 19:24:09,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09,517][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.2104189693927765, acc: 0.9448819160461426)
[2025-02-13 19:24:09,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09,935][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.26234591007232666, acc: 0.9160839319229126)
[2025-02-13 19:24:10,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10,315][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.31817716360092163, acc: 0.9146341681480408)
[2025-02-13 19:24:10,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10,675][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.1326856166124344, acc: 0.9772727489471436)
[2025-02-13 19:24:10,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11,010][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.1914581060409546, acc: 0.9407894611358643)
[2025-02-13 19:24:11,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11,388][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.13929204642772675, acc: 0.9642857313156128)
[2025-02-13 19:24:11,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11,794][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.3559722900390625, acc: 0.9281437397003174)
[2025-02-13 19:24:11,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12,259][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.17500054836273193, acc: 0.9715909361839294)
[2025-02-13 19:24:12,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12,660][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.20295263826847076, acc: 0.9463087320327759)
[2025-02-13 19:24:12,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13,048][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.17973601818084717, acc: 0.9441340565681458)
[2025-02-13 19:24:13,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13,420][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.22677375376224518, acc: 0.9519230723381042)
[2025-02-13 19:24:13,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13,797][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.288163423538208, acc: 0.929411768913269)
[2025-02-13 19:24:13,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14,217][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.16648606956005096, acc: 0.9597315192222595)
[2025-02-13 19:24:14,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14,627][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.10285995155572891, acc: 0.9868420958518982)
[2025-02-13 19:24:14,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15,039][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.39193153381347656, acc: 0.9142857193946838)
[2025-02-13 19:24:15,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15,444][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.19099082052707672, acc: 0.9468085169792175)
[2025-02-13 19:24:15,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15,862][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.28548464179039, acc: 0.9395604133605957)
[2025-02-13 19:24:16,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16,242][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.27635684609413147, acc: 0.9254658222198486)
[2025-02-13 19:24:16,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16,608][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.3056429624557495, acc: 0.9102563858032227)
[2025-02-13 19:24:16,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16,995][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.1396215409040451, acc: 0.9588235020637512)
[2025-02-13 19:24:17,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17,359][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.2715821862220764, acc: 0.9360465407371521)
[2025-02-13 19:24:17,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17,757][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.1958087980747223, acc: 0.9437500238418579)
[2025-02-13 19:24:17,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18,139][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.1473788321018219, acc: 0.9575757384300232)
[2025-02-13 19:24:18,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18,534][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.2944445312023163, acc: 0.9316770434379578)
[2025-02-13 19:24:18,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18,891][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.1870502531528473, acc: 0.9308176040649414)
[2025-02-13 19:24:19,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19,274][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.24955934286117554, acc: 0.9467455744743347)
[2025-02-13 19:24:19,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19,628][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.2659772038459778, acc: 0.9328858852386475)
[2025-02-13 19:24:19,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20,064][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.17763717472553253, acc: 0.9645389914512634)
[2025-02-13 19:24:20,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20,463][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.108951136469841, acc: 0.9821428656578064)
[2025-02-13 19:24:20,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20,878][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.19714175164699554, acc: 0.9384615421295166)
[2025-02-13 19:24:21,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21,329][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.27047616243362427, acc: 0.9447852969169617)
[2025-02-13 19:24:21,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21,772][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.3220245838165283, acc: 0.9235293865203857)
[2025-02-13 19:24:21,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22,176][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.487817645072937, acc: 0.8941176533699036)
[2025-02-13 19:24:22,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22,513][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.1806982010602951, acc: 0.9421965479850769)
[2025-02-13 19:24:22,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22,908][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.20252200961112976, acc: 0.9624060392379761)
[2025-02-13 19:24:23,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23,344][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.3710205554962158, acc: 0.9252873659133911)
[2025-02-13 19:24:23,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23,739][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.4232015907764435, acc: 0.9111111164093018)
[2025-02-13 19:24:23,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24,147][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.271967351436615, acc: 0.9222221970558167)
[2025-02-13 19:24:24,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24,550][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.22774766385555267, acc: 0.9415204524993896)
[2025-02-13 19:24:24,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24,978][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.1686231642961502, acc: 0.9575757384300232)
[2025-02-13 19:24:25,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25,349][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.4723062217235565, acc: 0.893081784248352)
[2025-02-13 19:24:25,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25,719][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.4533890187740326, acc: 0.887005627155304)
[2025-02-13 19:24:25,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26,085][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.37531593441963196, acc: 0.8758620619773865)
[2025-02-13 19:24:26,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26,477][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.2280341535806656, acc: 0.9337016344070435)
[2025-02-13 19:24:26,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26,869][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.2552729845046997, acc: 0.9270833134651184)
[2025-02-13 19:24:27,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27,254][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.2549939751625061, acc: 0.9363057613372803)
[2025-02-13 19:24:27,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27,651][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.3516911268234253, acc: 0.8943662047386169)
[2025-02-13 19:24:27,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28,033][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.280813068151474, acc: 0.9257143139839172)
[2025-02-13 19:24:28,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28,421][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.25372767448425293, acc: 0.9444444179534912)
[2025-02-13 19:24:28,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28,777][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.10578428208827972, acc: 0.9794520735740662)
[2025-02-13 19:24:28,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29,162][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.2806417942047119, acc: 0.929347813129425)
[2025-02-13 19:24:29,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29,601][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.24169644713401794, acc: 0.9593023061752319)
[2025-02-13 19:24:29,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29,966][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.2718946635723114, acc: 0.9415584206581116)
[2025-02-13 19:24:30,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30,351][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.39360901713371277, acc: 0.916167676448822)
[2025-02-13 19:24:30,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30,753][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.6327592730522156, acc: 0.8775510191917419)
[2025-02-13 19:24:30,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31,124][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.5367523431777954, acc: 0.8918918967247009)
[2025-02-13 19:24:31,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31,502][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.14676949381828308, acc: 0.9657142758369446)
[2025-02-13 19:24:31,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31,871][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.2396387755870819, acc: 0.9407894611358643)
[2025-02-13 19:24:32,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32,317][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.20009003579616547, acc: 0.9552238583564758)
[2025-02-13 19:24:32,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32,738][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.3313179910182953, acc: 0.9438202381134033)
[2025-02-13 19:24:32,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33,122][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.39016395807266235, acc: 0.9035087823867798)
[2025-02-13 19:24:33,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33,489][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.3925097584724426, acc: 0.9285714030265808)
[2025-02-13 19:24:33,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33,891][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.32065677642822266, acc: 0.9074074029922485)
[2025-02-13 19:24:34,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34,302][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.22117607295513153, acc: 0.9520547986030579)
[2025-02-13 19:24:34,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34,684][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.221415713429451, acc: 0.9433962106704712)
[2025-02-13 19:24:34,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35,080][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.09206610172986984, acc: 0.9932885766029358)
[2025-02-13 19:24:35,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35,444][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.2761133015155792, acc: 0.9453125)
[2025-02-13 19:24:35,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35,818][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.23012734949588776, acc: 0.9351851940155029)
[2025-02-13 19:24:35,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36,186][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.19166292250156403, acc: 0.9577465057373047)
[2025-02-13 19:24:36,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36,569][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.12078636139631271, acc: 0.9745222926139832)
[2025-02-13 19:24:36,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36,997][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.18354985117912292, acc: 0.9375)
[2025-02-13 19:24:37,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37,401][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.2669447064399719, acc: 0.921875)
[2025-02-13 19:24:37,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37,790][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.22444064915180206, acc: 0.9481481313705444)
[2025-02-13 19:24:37,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38,160][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.26239651441574097, acc: 0.9407894611358643)
[2025-02-13 19:24:38,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38,567][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.15223746001720428, acc: 0.9732142686843872)
[2025-02-13 19:24:38,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38,983][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.09958112239837646, acc: 0.9781420826911926)
[2025-02-13 19:24:39,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39,362][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.019035473465919495, acc: 1.0)
[2025-02-13 19:24:39,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39,714][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.08399349451065063, acc: 0.984375)
[2025-02-13 19:24:39,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40,069][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.36371034383773804, acc: 0.9629629850387573)
[2025-02-13 19:24:40,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40,462][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.20492379367351532, acc: 0.9681528806686401)
[2025-02-13 19:24:40,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40,861][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.15410688519477844, acc: 0.966292142868042)
[2025-02-13 19:24:41,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41,246][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.10906361788511276, acc: 0.9689440727233887)
[2025-02-13 19:24:41,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41,599][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.10928814113140106, acc: 0.9818181991577148)
[2025-02-13 19:24:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41,999][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.19545386731624603, acc: 0.9648241400718689)
[2025-02-13 19:24:42,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42,318][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.05742081254720688, acc: 0.9896907210350037)
[2025-02-13 19:24:42,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42,704][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.25143131613731384, acc: 0.9639639854431152)
[2025-02-13 19:24:42,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43,073][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.31802111864089966, acc: 0.9109947681427002)
[2025-02-13 19:24:43,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43,463][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.237221360206604, acc: 0.9714285731315613)
[2025-02-13 19:24:43,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43,819][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.10127653181552887, acc: 0.985401451587677)
[2025-02-13 19:24:43,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44,222][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.15733753144741058, acc: 0.9716312289237976)
[2025-02-13 19:24:44,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44,606][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.15423880517482758, acc: 0.9715909361839294)
[2025-02-13 19:24:44,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44,998][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.2582018971443176, acc: 0.9481481313705444)
[2025-02-13 19:24:45,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45,371][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.27315810322761536, acc: 0.9398906826972961)
[2025-02-13 19:24:45,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45,773][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.28180840611457825, acc: 0.9226190447807312)
[2025-02-13 19:24:45,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46,133][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.1038428395986557, acc: 0.9784172773361206)
[2025-02-13 19:24:46,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46,498][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.3645537495613098, acc: 0.8938547372817993)
[2025-02-13 19:24:46,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46,864][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.1740015745162964, acc: 0.9426751732826233)
[2025-02-13 19:24:47,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47,262][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.22763493657112122, acc: 0.9281437397003174)
[2025-02-13 19:24:47,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47,623][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.17406733334064484, acc: 0.9426751732826233)
[2025-02-13 19:24:47,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47,997][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.21960529685020447, acc: 0.9459459185600281)
[2025-02-13 19:24:48,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48,367][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.4491350054740906, acc: 0.918749988079071)
[2025-02-13 19:24:48,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48,745][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.26336392760276794, acc: 0.9322034120559692)
[2025-02-13 19:24:48,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49,163][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.17623764276504517, acc: 0.9528796076774597)
[2025-02-13 19:24:49,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49,568][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.30618858337402344, acc: 0.9215686321258545)
[2025-02-13 19:24:49,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49,973][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.1785142719745636, acc: 0.9692307710647583)
[2025-02-13 19:24:50,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50,379][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.08433995395898819, acc: 0.9709302186965942)
[2025-02-13 19:24:50,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50,777][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.17893430590629578, acc: 0.9610389471054077)
[2025-02-13 19:24:50,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51,145][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.3153001666069031, acc: 0.9196428656578064)
[2025-02-13 19:24:51,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51,508][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.250460147857666, acc: 0.9449999928474426)
[2025-02-13 19:24:51,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51,884][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.1620778888463974, acc: 0.9727272987365723)
[2025-02-13 19:24:52,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52,261][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.1610962301492691, acc: 0.9485294222831726)
[2025-02-13 19:24:52,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52,629][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.26828497648239136, acc: 0.9370629191398621)
[2025-02-13 19:24:52,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53,031][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.40078553557395935, acc: 0.9215686321258545)
[2025-02-13 19:24:53,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53,399][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.22856763005256653, acc: 0.9425287246704102)
[2025-02-13 19:24:53,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53,772][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.2756047546863556, acc: 0.9343434572219849)
[2025-02-13 19:24:53,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54,154][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.17682160437107086, acc: 0.9575757384300232)
[2025-02-13 19:24:54,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54,506][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.15823064744472504, acc: 0.9707602262496948)
[2025-02-13 19:24:54,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54,906][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.2602349519729614, acc: 0.949367105960846)
[2025-02-13 19:24:55,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55,264][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.1299927979707718, acc: 0.9679999947547913)
[2025-02-13 19:24:55,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55,656][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.26338574290275574, acc: 0.9360465407371521)
[2025-02-13 19:24:55,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56,024][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.7802248001098633, acc: 0.8372092843055725)
[2025-02-13 19:24:56,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56,410][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.24567563831806183, acc: 0.9253731369972229)
[2025-02-13 19:24:56,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56,805][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.34908148646354675, acc: 0.9358974099159241)
[2025-02-13 19:24:56,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57,177][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.5041243433952332, acc: 0.893048107624054)
[2025-02-13 19:24:57,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57,552][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.41044554114341736, acc: 0.9226804375648499)
[2025-02-13 19:24:57,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57,939][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.2766564190387726, acc: 0.9477124214172363)
[2025-02-13 19:24:58,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58,315][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.19079452753067017, acc: 0.9599999785423279)
[2025-02-13 19:24:58,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58,734][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.10542438179254532, acc: 0.9808917045593262)
[2025-02-13 19:24:58,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59,174][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.271157830953598, acc: 0.9365079402923584)
[2025-02-13 19:24:59,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59,612][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.28314533829689026, acc: 0.9433962106704712)
[2025-02-13 19:24:59,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00,038][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.2536819279193878, acc: 0.9418604373931885)
[2025-02-13 19:25:00,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00,430][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.10917281359434128, acc: 0.9830508232116699)
[2025-02-13 19:25:00,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00,814][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.18390437960624695, acc: 0.9484536051750183)
[2025-02-13 19:25:00,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01,201][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.13354413211345673, acc: 0.9595375657081604)
[2025-02-13 19:25:01,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01,573][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.25699084997177124, acc: 0.9466666579246521)
[2025-02-13 19:25:01,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01,966][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.3279366195201874, acc: 0.9329897165298462)
[2025-02-13 19:25:02,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02,309][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.14888447523117065, acc: 0.9620253443717957)
[2025-02-13 19:25:02,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02,673][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.2462766468524933, acc: 0.9281768202781677)
[2025-02-13 19:25:02,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03,049][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.1882323920726776, acc: 0.9573459625244141)
[2025-02-13 19:25:03,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03,441][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.4220682680606842, acc: 0.9418604373931885)
[2025-02-13 19:25:03,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03,879][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.20843854546546936, acc: 0.954023003578186)
[2025-02-13 19:25:04,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04,277][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.17972023785114288, acc: 0.9537572264671326)
[2025-02-13 19:25:04,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04,707][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.1826026886701584, acc: 0.9484536051750183)
[2025-02-13 19:25:04,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05,094][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.5067224502563477, acc: 0.9096385836601257)
[2025-02-13 19:25:05,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05,466][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.4128837585449219, acc: 0.875)
[2025-02-13 19:25:05,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05,859][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.6865478754043579, acc: 0.8290155529975891)
[2025-02-13 19:25:05,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06,211][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.3938139081001282, acc: 0.9005848169326782)
[2025-02-13 19:25:06,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06,613][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.12339279055595398, acc: 0.9611111283302307)
[2025-02-13 19:25:06,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06,984][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.1974058449268341, acc: 0.932584285736084)
[2025-02-13 19:25:07,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07,346][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.29709044098854065, acc: 0.9278350472450256)
[2025-02-13 19:25:07,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07,761][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.395129531621933, acc: 0.9075144529342651)
[2025-02-13 19:25:07,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08,157][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.2098199427127838, acc: 0.9371069073677063)
[2025-02-13 19:25:08,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08,560][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.17344699800014496, acc: 0.9545454382896423)
[2025-02-13 19:25:08,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08,961][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.15443843603134155, acc: 0.9685534834861755)
[2025-02-13 19:25:09,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09,376][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.06310240924358368, acc: 0.9823529124259949)
[2025-02-13 19:25:09,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09,752][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.11257658898830414, acc: 0.9747474789619446)
[2025-02-13 19:25:09,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10,177][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.1159389540553093, acc: 0.9811320900917053)
[2025-02-13 19:25:10,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10,562][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.07891697436571121, acc: 0.982758641242981)
[2025-02-13 19:25:10,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10,961][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.12722954154014587, acc: 0.9693877696990967)
[2025-02-13 19:25:11,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11,350][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.10953038185834885, acc: 0.9767441749572754)
[2025-02-13 19:25:11,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11,739][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.13723649084568024, acc: 0.961904764175415)
[2025-02-13 19:25:11,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12,113][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.38994958996772766, acc: 0.9408283829689026)
[2025-02-13 19:25:12,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12,498][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.4088214337825775, acc: 0.8982036113739014)
[2025-02-13 19:25:12,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12,876][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.304463654756546, acc: 0.9154929518699646)
[2025-02-13 19:25:13,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13,295][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.3809656798839569, acc: 0.9155844449996948)
[2025-02-13 19:25:13,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13,703][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.5604163408279419, acc: 0.8928571343421936)
[2025-02-13 19:25:13,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14,087][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.2664780020713806, acc: 0.9484536051750183)
[2025-02-13 19:25:14,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14,462][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.24900934100151062, acc: 0.9337016344070435)
[2025-02-13 19:25:14,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14,828][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.23376919329166412, acc: 0.9430379867553711)
[2025-02-13 19:25:14,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15,203][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.29688259959220886, acc: 0.9515151381492615)
[2025-02-13 19:25:15,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15,580][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.4376402199268341, acc: 0.910179615020752)
[2025-02-13 19:25:15,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15,935][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.14172756671905518, acc: 0.9610389471054077)
[2025-02-13 19:25:16,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16,302][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.4265102744102478, acc: 0.8813559412956238)
[2025-02-13 19:25:16,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16,713][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.18393857777118683, acc: 0.9683544039726257)
[2025-02-13 19:25:16,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17,084][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.14753060042858124, acc: 0.9636363387107849)
[2025-02-13 19:25:17,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17,447][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.05420134589076042, acc: 0.9857142567634583)
[2025-02-13 19:25:17,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17,860][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.1496421992778778, acc: 0.9537037014961243)
[2025-02-13 19:25:18,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18,266][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.1052326112985611, acc: 0.9726027250289917)
[2025-02-13 19:25:18,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18,668][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.23515896499156952, acc: 0.9435483813285828)
[2025-02-13 19:25:18,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19,044][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.37965306639671326, acc: 0.9322034120559692)
[2025-02-13 19:25:19,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19,393][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.29950380325317383, acc: 0.9459459185600281)
[2025-02-13 19:25:19,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19,777][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.3231481909751892, acc: 0.9185185432434082)
[2025-02-13 19:25:19,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20,128][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.20779068768024445, acc: 0.95652174949646)
[2025-02-13 19:25:20,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20,487][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.25170591473579407, acc: 0.9390243887901306)
[2025-02-13 19:25:20,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20,876][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.16998988389968872, acc: 0.9440000057220459)
[2025-02-13 19:25:21,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21,286][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.2602730691432953, acc: 0.9577465057373047)
[2025-02-13 19:25:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21,690][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.2893469035625458, acc: 0.9552238583564758)
[2025-02-13 19:25:21,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22,110][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.11088044941425323, acc: 0.9802631735801697)
[2025-02-13 19:25:22,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22,493][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.1364438533782959, acc: 0.9464285969734192)
[2025-02-13 19:25:22,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22,858][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.0664696991443634, acc: 0.9780219793319702)
[2025-02-13 19:25:22,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23,244][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.03342847898602486, acc: 1.0)
[2025-02-13 19:25:23,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23,650][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.16590645909309387, acc: 0.9609375)
[2025-02-13 19:25:23,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24,075][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.23170189559459686, acc: 0.9736841917037964)
[2025-02-13 19:25:24,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24,468][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.08288004994392395, acc: 0.969072163105011)
[2025-02-13 19:25:24,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24,856][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.17338956892490387, acc: 0.9591836929321289)
[2025-02-13 19:25:24,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25,215][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.08400893211364746, acc: 0.97826087474823)
[2025-02-13 19:25:25,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25,572][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.11789857596158981, acc: 0.9587628841400146)
[2025-02-13 19:25:25,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25,925][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.11338072270154953, acc: 0.9842519760131836)
[2025-02-13 19:25:26,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26,322][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.257440984249115, acc: 0.9411764740943909)
[2025-02-13 19:25:26,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26,723][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.13484355807304382, acc: 0.9829059839248657)
[2025-02-13 19:25:26,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27,109][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.49517813324928284, acc: 0.9047619104385376)
[2025-02-13 19:25:27,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27,474][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.2462051659822464, acc: 0.931034505367279)
[2025-02-13 19:25:27,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27,868][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.26187655329704285, acc: 0.9497206807136536)
[2025-02-13 19:25:28,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28,261][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.11022008210420609, acc: 0.9691358208656311)
[2025-02-13 19:25:28,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28,665][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.14598946273326874, acc: 0.9597315192222595)
[2025-02-13 19:25:28,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29,067][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.3746488690376282, acc: 0.91847825050354)
[2025-02-13 19:25:29,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29,464][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.3468836843967438, acc: 0.9307692050933838)
[2025-02-13 19:25:29,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29,832][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.18860729038715363, acc: 0.9575757384300232)
[2025-02-13 19:25:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30,198][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.20791099965572357, acc: 0.9453125)
[2025-02-13 19:25:30,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30,569][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.4126659035682678, acc: 0.9057971239089966)
[2025-02-13 19:25:30,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30,947][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.07867449522018433, acc: 0.9832402467727661)
[2025-02-13 19:25:31,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31,289][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.39328381419181824, acc: 0.8867924809455872)
[2025-02-13 19:25:31,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31,666][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.2958543002605438, acc: 0.9479768872261047)
[2025-02-13 19:25:31,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32,072][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.6743161678314209, acc: 0.8820512890815735)
[2025-02-13 19:25:32,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32,452][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.8995411992073059, acc: 0.8557692170143127)
[2025-02-13 19:25:32,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32,812][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.37996360659599304, acc: 0.908450722694397)
[2025-02-13 19:25:32,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33,177][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.4775030016899109, acc: 0.8769230842590332)
[2025-02-13 19:25:33,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33,558][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.34939563274383545, acc: 0.9085714221000671)
[2025-02-13 19:25:33,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33,946][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.6362678408622742, acc: 0.891566276550293)
[2025-02-13 19:25:34,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34,310][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.19561710953712463, acc: 0.9347826242446899)
[2025-02-13 19:25:34,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34,675][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.18514646589756012, acc: 0.9502487778663635)
[2025-02-13 19:25:34,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35,068][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.2030038833618164, acc: 0.9416058659553528)
[2025-02-13 19:25:35,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35,472][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.12849153578281403, acc: 0.9677419066429138)
[2025-02-13 19:25:35,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35,866][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.274535596370697, acc: 0.9337349534034729)
[2025-02-13 19:25:36,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36,250][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.4791802763938904, acc: 0.9011628031730652)
[2025-02-13 19:25:36,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36,666][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 1.3605973720550537, acc: 0.7674418687820435)
[2025-02-13 19:25:36,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37,069][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.5701797604560852, acc: 0.8829787373542786)
[2025-02-13 19:25:37,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37,494][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.18520064651966095, acc: 0.9538461565971375)
[2025-02-13 19:25:37,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37,911][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.7550525069236755, acc: 0.8409090638160706)
[2025-02-13 19:25:38,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38,315][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.31097981333732605, acc: 0.8888888955116272)
[2025-02-13 19:25:38,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38,715][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.45201465487480164, acc: 0.9043062329292297)
[2025-02-13 19:25:38,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39,117][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.599020779132843, acc: 0.8760330677032471)
[2025-02-13 19:25:39,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39,505][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.36873993277549744, acc: 0.9139072895050049)
[2025-02-13 19:25:39,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39,877][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.2673282027244568, acc: 0.9466666579246521)
[2025-02-13 19:25:40,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40,237][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.3389061391353607, acc: 0.9120879173278809)
[2025-02-13 19:25:40,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40,586][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.39201679825782776, acc: 0.9166666865348816)
[2025-02-13 19:25:40,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40,977][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.2931268513202667, acc: 0.9313725233078003)
[2025-02-13 19:25:41,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41,355][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.6867056488990784, acc: 0.8449612259864807)
[2025-02-13 19:25:41,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41,729][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.3004346191883087, acc: 0.93388432264328)
[2025-02-13 19:25:41,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42,131][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.4741077423095703, acc: 0.8965517282485962)
[2025-02-13 19:25:42,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42,507][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.3422148823738098, acc: 0.8846153616905212)
[2025-02-13 19:25:42,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42,888][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.26019540429115295, acc: 0.9435483813285828)
[2025-02-13 19:25:43,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43,281][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.23869238793849945, acc: 0.9430894255638123)
[2025-02-13 19:25:43,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43,651][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.5986670255661011, acc: 0.8467742204666138)
[2025-02-13 19:25:43,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44,012][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.5801234245300293, acc: 0.8691588640213013)
[2025-02-13 19:25:44,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44,384][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.474659264087677, acc: 0.8947368264198303)
[2025-02-13 19:25:44,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44,740][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.32269200682640076, acc: 0.9100000262260437)
[2025-02-13 19:25:44,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45,138][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.11595981568098068, acc: 0.9738562107086182)
[2025-02-13 19:25:45,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45,548][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.1510043889284134, acc: 0.9594594836235046)
[2025-02-13 19:25:45,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45,879][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.14591874182224274, acc: 0.9642857313156128)
[2025-02-13 19:25:46,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46,247][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.1337987184524536, acc: 0.9686098694801331)
[2025-02-13 19:25:46,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46,586][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.08283039927482605, acc: 0.9832402467727661)
[2025-02-13 19:25:46,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46,967][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.12037558108568192, acc: 0.9760765433311462)
[2025-02-13 19:25:47,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47,369][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.1917935460805893, acc: 0.9485714435577393)
[2025-02-13 19:25:47,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47,771][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.12216760963201523, acc: 0.9578313231468201)
[2025-02-13 19:25:47,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48,167][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.05519125610589981, acc: 0.9860140085220337)
[2025-02-13 19:25:48,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48,554][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.1695992797613144, acc: 0.9599999785423279)
[2025-02-13 19:25:48,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48,954][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.07447342574596405, acc: 0.9862068891525269)
[2025-02-13 19:25:49,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49,347][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.3454643189907074, acc: 0.9430052042007446)
[2025-02-13 19:25:49,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49,734][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.19507618248462677, acc: 0.9587628841400146)
[2025-02-13 19:25:49,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50,105][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.28284752368927, acc: 0.9227052927017212)
[2025-02-13 19:25:50,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50,460][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.17962908744812012, acc: 0.949999988079071)
[2025-02-13 19:25:50,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50,796][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.2619675397872925, acc: 0.9204545617103577)
[2025-02-13 19:25:50,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51,198][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.16288506984710693, acc: 0.9411764740943909)
[2025-02-13 19:25:51,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51,616][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.19491346180438995, acc: 0.942307710647583)
[2025-02-13 19:25:51,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52,009][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.2504326105117798, acc: 0.9428571462631226)
[2025-02-13 19:25:52,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52,357][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.3020636737346649, acc: 0.9022988677024841)
[2025-02-13 19:25:52,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52,704][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.22703927755355835, acc: 0.9371727705001831)
[2025-02-13 19:25:52,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53,060][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.39529216289520264, acc: 0.9192546606063843)
[2025-02-13 19:25:53,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53,433][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.24174636602401733, acc: 0.9438202381134033)
[2025-02-13 19:25:53,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53,799][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.3484988808631897, acc: 0.9055117964744568)
[2025-02-13 19:25:53,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54,170][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.14230549335479736, acc: 0.9783783555030823)
[2025-02-13 19:25:54,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54,578][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.12604127824306488, acc: 0.9615384340286255)
[2025-02-13 19:25:54,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54,962][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.050164494663476944, acc: 0.9937888383865356)
[2025-02-13 19:25:55,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55,332][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.27212005853652954, acc: 0.9534883499145508)
[2025-02-13 19:25:55,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55,717][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.11428346484899521, acc: 0.9786096215248108)
[2025-02-13 19:25:55,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56,076][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.08668208867311478, acc: 0.9846938848495483)
[2025-02-13 19:25:56,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56,506][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.3818593919277191, acc: 0.9219512343406677)
[2025-02-13 19:25:56,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56,866][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.2401326298713684, acc: 0.9307692050933838)
[2025-02-13 19:25:57,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57,254][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.1872899979352951, acc: 0.9635416865348816)
[2025-02-13 19:25:57,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57,640][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.25969111919403076, acc: 0.9431818127632141)
[2025-02-13 19:25:57,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58,029][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.3201926648616791, acc: 0.9230769276618958)
[2025-02-13 19:25:58,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58,438][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.1308417171239853, acc: 0.9562841653823853)
[2025-02-13 19:25:58,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58,833][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.1552535593509674, acc: 0.9595375657081604)
[2025-02-13 19:25:58,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59,234][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.1860511153936386, acc: 0.9797297120094299)
[2025-02-13 19:25:59,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59,577][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.2680135667324066, acc: 0.9367088675498962)
[2025-02-13 19:25:59,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59,944][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.1777278631925583, acc: 0.9670329689979553)
[2025-02-13 19:26:00,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00,366][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.20008961856365204, acc: 0.9371428489685059)
[2025-02-13 19:26:00,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00,760][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.24079065024852753, acc: 0.9298245906829834)
[2025-02-13 19:26:00,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01,161][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.21982476115226746, acc: 0.9672130942344666)
[2025-02-13 19:26:01,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01,548][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.09915539622306824, acc: 0.9644669890403748)
[2025-02-13 19:26:01,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01,982][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.2592954635620117, acc: 0.9502487778663635)
[2025-02-13 19:26:02,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02,371][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.242082878947258, acc: 0.9312499761581421)
[2025-02-13 19:26:02,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02,788][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.27514225244522095, acc: 0.9329608678817749)
[2025-02-13 19:26:02,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03,184][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.2530156672000885, acc: 0.9322034120559692)
[2025-02-13 19:26:03,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03,539][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.25949549674987793, acc: 0.9226190447807312)
[2025-02-13 19:26:03,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03,919][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.4182790219783783, acc: 0.895652174949646)
[2025-02-13 19:26:04,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04,294][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.20624154806137085, acc: 0.9468085169792175)
[2025-02-13 19:26:04,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04,714][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.17337150871753693, acc: 0.9526315927505493)
[2025-02-13 19:26:04,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05,088][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.198772132396698, acc: 0.9411764740943909)
[2025-02-13 19:26:05,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05,492][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.35440781712532043, acc: 0.9015151262283325)
[2025-02-13 19:26:05,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05,899][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.05848650261759758, acc: 1.0)
[2025-02-13 19:26:06,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06,283][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.1618005335330963, acc: 0.9662162065505981)
[2025-02-13 19:26:06,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06,717][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.03930845111608505, acc: 1.0)
[2025-02-13 19:26:06,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07,154][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.06279820203781128, acc: 0.9934210777282715)
[2025-02-13 19:26:07,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07,552][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.07430512458086014, acc: 0.9849624037742615)
[2025-02-13 19:26:07,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07,954][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.06186065077781677, acc: 0.9934640526771545)
[2025-02-13 19:26:08,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08,343][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.06303048878908157, acc: 0.9870967864990234)
[2025-02-13 19:26:08,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08,768][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.07058203965425491, acc: 0.9934640526771545)
[2025-02-13 19:26:08,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09,146][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.18295811116695404, acc: 0.9383561611175537)
[2025-02-13 19:26:09,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09,529][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.37242016196250916, acc: 0.8760330677032471)
[2025-02-13 19:26:09,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09,913][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.14017078280448914, acc: 0.9647887349128723)
[2025-02-13 19:26:10,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10,295][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.16623272001743317, acc: 0.9470198750495911)
[2025-02-13 19:26:10,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10,684][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.09324344247579575, acc: 0.9794520735740662)
[2025-02-13 19:26:10,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11,067][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.13371004164218903, acc: 0.9695122241973877)
[2025-02-13 19:26:11,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11,437][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.04243386164307594, acc: 0.9939758777618408)
[2025-02-13 19:26:11,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11,809][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.10864189267158508, acc: 0.9668874144554138)
[2025-02-13 19:26:11,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12,208][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.05818933621048927, acc: 0.9800000190734863)
[2025-02-13 19:26:12,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12,585][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.04777495563030243, acc: 1.0)
[2025-02-13 19:26:12,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12,968][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.1738305687904358, acc: 0.9455782175064087)
[2025-02-13 19:26:13,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13,335][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.1480259895324707, acc: 0.9685534834861755)
[2025-02-13 19:26:13,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13,723][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.07490401715040207, acc: 0.9923664331436157)
[2025-02-13 19:26:13,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14,114][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.0729609876871109, acc: 0.9793103337287903)
[2025-02-13 19:26:14,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14,483][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.09662687033414841, acc: 0.9652777910232544)
[2025-02-13 19:26:14,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14,857][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.5103718042373657, acc: 0.9078947305679321)
[2025-02-13 19:26:14,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15,239][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.4068264365196228, acc: 0.9005848169326782)
[2025-02-13 19:26:15,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15,614][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.438217431306839, acc: 0.8791946172714233)
[2025-02-13 19:26:15,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16,016][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.18778996169567108, acc: 0.9368420839309692)
[2025-02-13 19:26:16,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16,407][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.36067014932632446, acc: 0.9230769276618958)
[2025-02-13 19:26:16,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16,797][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.1677553355693817, acc: 0.9892473220825195)
[2025-02-13 19:26:16,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17,160][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.39209166169166565, acc: 0.893401026725769)
[2025-02-13 19:26:17,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17,550][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.5551822185516357, acc: 0.8939393758773804)
[2025-02-13 19:26:17,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17,984][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.5858906507492065, acc: 0.8899521827697754)
[2025-02-13 19:26:18,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18,353][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.7126353979110718, acc: 0.8539325594902039)
[2025-02-13 19:26:18,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18,730][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.24875900149345398, acc: 0.9476439952850342)
[2025-02-13 19:26:18,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19,126][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.21940992772579193, acc: 0.9520000219345093)
[2025-02-13 19:26:19,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19,492][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.37809517979621887, acc: 0.8902438879013062)
[2025-02-13 19:26:19,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19,878][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.20170697569847107, acc: 0.9554139971733093)
[2025-02-13 19:26:20,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20,258][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.4078978896141052, acc: 0.9350649118423462)
[2025-02-13 19:26:20,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20,657][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.43009141087532043, acc: 0.9133333563804626)
[2025-02-13 19:26:20,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21,047][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.1497022807598114, acc: 0.9655172228813171)
[2025-02-13 19:26:21,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21,433][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.211152583360672, acc: 0.939393937587738)
[2025-02-13 19:26:21,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21,852][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.07193905860185623, acc: 0.9803921580314636)
[2025-02-13 19:26:21,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22,217][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.23103375732898712, acc: 0.9440000057220459)
[2025-02-13 19:26:22,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22,609][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.280180960893631, acc: 0.9593495726585388)
[2025-02-13 19:26:22,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23,001][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.11837747693061829, acc: 0.9673202633857727)
[2025-02-13 19:26:23,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23,419][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.20567816495895386, acc: 0.9275362491607666)
[2025-02-13 19:26:23,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23,820][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.15636877715587616, acc: 0.9599999785423279)
[2025-02-13 19:26:23,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24,211][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.16698531806468964, acc: 0.9387755393981934)
[2025-02-13 19:26:24,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24,564][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.17204660177230835, acc: 0.9632353186607361)
[2025-02-13 19:26:24,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24,941][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.11742127686738968, acc: 0.9795918464660645)
[2025-02-13 19:26:25,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25,353][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.0953030213713646, acc: 0.9863945841789246)
[2025-02-13 19:26:25,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25,718][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.10249330848455429, acc: 0.9689922332763672)
[2025-02-13 19:26:25,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26,094][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.15062446892261505, acc: 0.951724112033844)
[2025-02-13 19:26:26,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26,470][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.13977734744548798, acc: 0.9534883499145508)
[2025-02-13 19:26:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26,849][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.09033247083425522, acc: 0.9650349617004395)
[2025-02-13 19:26:26,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27,206][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.6218286156654358, acc: 0.8695651888847351)
[2025-02-13 19:26:27,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27,565][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.49075156450271606, acc: 0.9152542352676392)
[2025-02-13 19:26:27,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27,966][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.1277010142803192, acc: 0.9694656729698181)
[2025-02-13 19:26:28,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28,323][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.172092467546463, acc: 0.9513888955116272)
[2025-02-13 19:26:28,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28,693][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.09867242723703384, acc: 0.9791666865348816)
[2025-02-13 19:26:28,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29,083][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.30352431535720825, acc: 0.9256756901741028)
[2025-02-13 19:26:29,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29,458][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.14163987338542938, acc: 0.9774436354637146)
[2025-02-13 19:26:29,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29,842][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.10675971955060959, acc: 0.9695122241973877)
[2025-02-13 19:26:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30,204][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.0460677333176136, acc: 0.9915966391563416)
[2025-02-13 19:26:30,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30,569][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.09614257514476776, acc: 0.9675324559211731)
[2025-02-13 19:26:30,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30,971][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.12903085350990295, acc: 0.9624999761581421)
[2025-02-13 19:26:31,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31,371][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.10221055150032043, acc: 0.9748427867889404)
[2025-02-13 19:26:31,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31,793][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.10518883913755417, acc: 0.9671052694320679)
[2025-02-13 19:26:31,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32,177][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.10590248554944992, acc: 0.9731543660163879)
[2025-02-13 19:26:32,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32,550][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.08983351290225983, acc: 0.9849624037742615)
[2025-02-13 19:26:32,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32,930][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.08953624963760376, acc: 0.9798657894134521)
[2025-02-13 19:26:33,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33,274][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.11348149180412292, acc: 0.982758641242981)
[2025-02-13 19:26:33,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33,639][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.2763786315917969, acc: 0.9481481313705444)
[2025-02-13 19:26:33,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34,038][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.14457890391349792, acc: 0.9537037014961243)
[2025-02-13 19:26:34,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34,394][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.36881980299949646, acc: 0.9370629191398621)
[2025-02-13 19:26:34,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34,780][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.7332924604415894, acc: 0.8636363744735718)
[2025-02-13 19:26:34,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35,123][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.09503655880689621, acc: 0.9785714149475098)
[2025-02-13 19:26:35,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35,541][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.12303372472524643, acc: 0.9624060392379761)
[2025-02-13 19:26:35,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35,937][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.11317767202854156, acc: 0.970588207244873)
[2025-02-13 19:26:36,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36,333][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.40660545229911804, acc: 0.9281045794487)
[2025-02-13 19:26:36,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36,750][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.20424781739711761, acc: 0.9714285731315613)
[2025-02-13 19:26:36,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37,146][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.4157988131046295, acc: 0.8813559412956238)
[2025-02-13 19:26:37,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37,543][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.278427392244339, acc: 0.9294871687889099)
[2025-02-13 19:26:37,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37,935][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.32272252440452576, acc: 0.925000011920929)
[2025-02-13 19:26:38,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38,333][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.22677403688430786, acc: 0.9519650936126709)
[2025-02-13 19:26:38,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38,720][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.43319085240364075, acc: 0.8820512890815735)
[2025-02-13 19:26:38,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39,093][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.366649329662323, acc: 0.9152542352676392)
[2025-02-13 19:26:39,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39,491][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.7223586440086365, acc: 0.8303571343421936)
[2025-02-13 19:26:39,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39,860][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.8799853324890137, acc: 0.8474576473236084)
[2025-02-13 19:26:40,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40,256][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.8997530341148376, acc: 0.7986577153205872)
[2025-02-13 19:26:40,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40,633][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.644973874092102, acc: 0.8372092843055725)
[2025-02-13 19:26:40,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41,044][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.9501487016677856, acc: 0.752136766910553)
[2025-02-13 19:26:41,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41,448][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.4916830062866211, acc: 0.8842975497245789)
[2025-02-13 19:26:41,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41,832][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.3219872713088989, acc: 0.9115646481513977)
[2025-02-13 19:26:41,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42,222][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.36715492606163025, acc: 0.9428571462631226)
[2025-02-13 19:26:42,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42,638][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.2549435496330261, acc: 0.9330143332481384)
[2025-02-13 19:26:42,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43,016][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.33852627873420715, acc: 0.9194630980491638)
[2025-02-13 19:26:43,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43,407][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.3917572796344757, acc: 0.8888888955116272)
[2025-02-13 19:26:43,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43,795][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.30121317505836487, acc: 0.9159663915634155)
[2025-02-13 19:26:43,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44,210][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.6375593543052673, acc: 0.8571428656578064)
[2025-02-13 19:26:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44,564][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.29939061403274536, acc: 0.9172932505607605)
[2025-02-13 19:26:44,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44,894][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.2965659201145172, acc: 0.9075630307197571)
[2025-02-13 19:26:45,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45,266][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.1206267848610878, acc: 0.982758641242981)
[2025-02-13 19:26:45,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45,628][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.21670284867286682, acc: 0.9520958065986633)
[2025-02-13 19:26:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45,992][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.29932931065559387, acc: 0.9395973086357117)
[2025-02-13 19:26:46,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46,386][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.2603376507759094, acc: 0.9378882050514221)
[2025-02-13 19:26:46,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46,772][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.2570231556892395, acc: 0.9193548560142517)
[2025-02-13 19:26:46,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47,161][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.3256242871284485, acc: 0.9102563858032227)
[2025-02-13 19:26:47,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47,580][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.19555756449699402, acc: 0.9593495726585388)
[2025-02-13 19:26:47,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47,962][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.21680551767349243, acc: 0.9248120188713074)
[2025-02-13 19:26:48,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48,401][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.2946338355541229, acc: 0.9539170265197754)
[2025-02-13 19:26:48,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48,805][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.38586702942848206, acc: 0.8952381014823914)
[2025-02-13 19:26:48,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49,219][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.11370739340782166, acc: 0.9802955389022827)
[2025-02-13 19:26:49,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49,633][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.29215577244758606, acc: 0.9222221970558167)
[2025-02-13 19:26:49,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50,013][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.2512216866016388, acc: 0.9444444179534912)
[2025-02-13 19:26:50,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50,407][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.03811930492520332, acc: 0.9920634627342224)
[2025-02-13 19:26:50,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50,772][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.15886659920215607, acc: 0.9436619877815247)
[2025-02-13 19:26:50,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51,133][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.11431042850017548, acc: 0.9677419066429138)
[2025-02-13 19:26:51,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51,559][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.10379865020513535, acc: 0.9707602262496948)
[2025-02-13 19:26:51,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51,925][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.1590358316898346, acc: 0.9736841917037964)
[2025-02-13 19:26:52,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52,288][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.044702835381031036, acc: 1.0)
[2025-02-13 19:26:52,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52,652][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.0956743136048317, acc: 0.981249988079071)
[2025-02-13 19:26:52,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53,009][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.03216351568698883, acc: 0.9927536249160767)
[2025-02-13 19:26:53,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53,405][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.051134321838617325, acc: 0.9935064911842346)
[2025-02-13 19:26:53,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53,816][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.17154991626739502, acc: 0.9570552110671997)
[2025-02-13 19:26:53,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54,231][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.11086183786392212, acc: 0.970588207244873)
[2025-02-13 19:26:54,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54,611][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.08047179877758026, acc: 0.9809523820877075)
[2025-02-13 19:26:54,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54,987][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.19009822607040405, acc: 0.969072163105011)
[2025-02-13 19:26:55,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55,323][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.04516042023897171, acc: 0.9919354915618896)
[2025-02-13 19:26:55,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55,706][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.06020566448569298, acc: 0.9736841917037964)
[2025-02-13 19:26:55,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56,080][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.1211736872792244, acc: 0.9820359349250793)
[2025-02-13 19:26:56,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56,480][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.10011137276887894, acc: 0.9707602262496948)
[2025-02-13 19:26:56,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56,880][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.1273711621761322, acc: 0.9615384340286255)
[2025-02-13 19:26:57,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57,272][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.06632623076438904, acc: 0.9794520735740662)
[2025-02-13 19:26:57,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57,673][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.12335643917322159, acc: 0.9776536226272583)
[2025-02-13 19:26:57,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58,085][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.04473734647035599, acc: 0.9935897588729858)
[2025-02-13 19:26:58,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58,478][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.2644871175289154, acc: 0.9485714435577393)
[2025-02-13 19:26:58,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58,859][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.08568522334098816, acc: 0.9784172773361206)
[2025-02-13 19:26:59,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59,269][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.11468885093927383, acc: 0.9728260636329651)
[2025-02-13 19:26:59,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59,632][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.11484409123659134, acc: 0.9885714054107666)
[2025-02-13 19:26:59,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59,986][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.3258669972419739, acc: 0.9366196990013123)
[2025-02-13 19:27:00,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00,374][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.29156574606895447, acc: 0.9379310607910156)
[2025-02-13 19:27:00,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00,775][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.09140899032354355, acc: 0.9716312289237976)
[2025-02-13 19:27:00,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01,192][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.19049720466136932, acc: 0.9398496150970459)
[2025-02-13 19:27:01,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01,581][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.14878813922405243, acc: 0.9683544039726257)
[2025-02-13 19:27:01,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01,965][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.23943418264389038, acc: 0.9599999785423279)
[2025-02-13 19:27:02,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02,332][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.16460566222667694, acc: 0.970588207244873)
[2025-02-13 19:27:02,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02,711][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.19893260300159454, acc: 0.9623655676841736)
[2025-02-13 19:27:02,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03,099][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.17860041558742523, acc: 0.9748427867889404)
[2025-02-13 19:27:03,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03,528][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.23211251199245453, acc: 0.9655172228813171)
[2025-02-13 19:27:03,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03,930][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.15071608126163483, acc: 0.9668508172035217)
[2025-02-13 19:27:04,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04,293][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.08899121731519699, acc: 0.9736841917037964)
[2025-02-13 19:27:04,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04,709][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.11640854924917221, acc: 0.9672130942344666)
[2025-02-13 19:27:04,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05,114][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.1297348439693451, acc: 0.9741935729980469)
[2025-02-13 19:27:05,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05,497][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.10128670930862427, acc: 0.977011501789093)
[2025-02-13 19:27:05,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05,884][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.27777713537216187, acc: 0.9356725215911865)
[2025-02-13 19:27:06,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06,252][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.11490561813116074, acc: 0.9842519760131836)
[2025-02-13 19:27:06,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06,648][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.20788569748401642, acc: 0.9846153855323792)
[2025-02-13 19:27:06,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07,057][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.05064398795366287, acc: 0.9833333492279053)
[2025-02-13 19:27:07,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07,387][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.19609464704990387, acc: 0.9776119589805603)
[2025-02-13 19:27:07,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07,705][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.12183192372322083, acc: 0.9855072498321533)
[2025-02-13 19:27:07,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08,097][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.2912690043449402, acc: 0.9545454382896423)
[2025-02-13 19:27:08,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08,489][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.1949777603149414, acc: 0.9391891956329346)
[2025-02-13 19:27:08,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08,840][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.5147839784622192, acc: 0.9122806787490845)
[2025-02-13 19:27:08,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09,230][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.4401923716068268, acc: 0.8944723606109619)
[2025-02-13 19:27:09,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09,614][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.3493461310863495, acc: 0.9435483813285828)
[2025-02-13 19:27:09,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10,032][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.36180025339126587, acc: 0.8846153616905212)
[2025-02-13 19:27:10,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10,432][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.38210824131965637, acc: 0.9187816977500916)
[2025-02-13 19:27:10,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10,826][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.6421083211898804, acc: 0.8584905862808228)
[2025-02-13 19:27:10,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11,205][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.6169283390045166, acc: 0.8525640964508057)
[2025-02-13 19:27:11,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11,589][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.4459015130996704, acc: 0.9039999842643738)
[2025-02-13 19:27:11,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11,972][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.41335803270339966, acc: 0.895348846912384)
[2025-02-13 19:27:12,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12,346][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.6980040669441223, acc: 0.8926174640655518)
[2025-02-13 19:27:12,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12,732][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.4024898409843445, acc: 0.8999999761581421)
[2025-02-13 19:27:12,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13,135][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.36440664529800415, acc: 0.9290780425071716)
[2025-02-13 19:27:13,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13,529][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.14772644639015198, acc: 0.9716312289237976)
[2025-02-13 19:27:13,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13,913][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.13553766906261444, acc: 0.9710144996643066)
[2025-02-13 19:27:14,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14,269][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.11882371455430984, acc: 0.9677419066429138)
[2025-02-13 19:27:14,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14,621][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.13171060383319855, acc: 0.9719626307487488)
[2025-02-13 19:27:14,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14,993][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.24432162940502167, acc: 0.9461538195610046)
[2025-02-13 19:27:15,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15,353][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.1250600963830948, acc: 0.9760000109672546)
[2025-02-13 19:27:15,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15,716][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.2560807466506958, acc: 0.9577465057373047)
[2025-02-13 19:27:15,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16,125][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.07193075865507126, acc: 0.9900000095367432)
[2025-02-13 19:27:16,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16,524][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.2567124664783478, acc: 0.9285714030265808)
[2025-02-13 19:27:16,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16,918][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.24751746654510498, acc: 0.93388432264328)
[2025-02-13 19:27:17,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17,294][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.09812388569116592, acc: 0.9905660152435303)
[2025-02-13 19:27:17,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17,657][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.13983796536922455, acc: 0.9684210419654846)
[2025-02-13 19:27:17,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18,031][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.13463911414146423, acc: 0.9794520735740662)
[2025-02-13 19:27:18,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18,394][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.11581923067569733, acc: 0.9760000109672546)
[2025-02-13 19:27:18,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18,769][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.24126240611076355, acc: 0.9520547986030579)
[2025-02-13 19:27:18,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19,192][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.32106155157089233, acc: 0.9411764740943909)
[2025-02-13 19:27:19,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19,594][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.39451783895492554, acc: 0.9454545378684998)
[2025-02-13 19:27:19,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19,966][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.11858238279819489, acc: 0.9774436354637146)
[2025-02-13 19:27:20,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20,338][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.1515703946352005, acc: 0.9596773982048035)
[2025-02-13 19:27:20,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20,699][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.16243302822113037, acc: 0.9652174115180969)
[2025-02-13 19:27:20,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21,060][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.1073274314403534, acc: 0.9886363744735718)
[2025-02-13 19:27:21,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21,449][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.30902111530303955, acc: 0.9307692050933838)
[2025-02-13 19:27:21,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21,875][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.14665572345256805, acc: 0.9658119678497314)
[2025-02-13 19:27:22,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22,322][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.13404279947280884, acc: 0.9696969985961914)
[2025-02-13 19:27:22,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22,708][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.0584569089114666, acc: 1.0)
[2025-02-13 19:27:22,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23,105][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.24664881825447083, acc: 0.949999988079071)
[2025-02-13 19:27:23,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23,502][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.12159093469381332, acc: 0.9541284441947937)
[2025-02-13 19:27:23,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23,858][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.16025763750076294, acc: 0.947826087474823)
[2025-02-13 19:27:24,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24,229][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.2177819311618805, acc: 0.9370629191398621)
[2025-02-13 19:27:24,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24,593][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.1631743609905243, acc: 0.9629629850387573)
[2025-02-13 19:27:24,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24,976][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.2799030542373657, acc: 0.932584285736084)
[2025-02-13 19:27:25,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25,333][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.39307186007499695, acc: 0.9244186282157898)
[2025-02-13 19:27:25,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25,713][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.1452520340681076, acc: 0.9581151604652405)
[2025-02-13 19:27:25,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26,115][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.1678282469511032, acc: 0.939393937587738)
[2025-02-13 19:27:26,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26,485][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.2394605278968811, acc: 0.9512194991111755)
[2025-02-13 19:27:26,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26,868][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.31315502524375916, acc: 0.9248554706573486)
[2025-02-13 19:27:27,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27,238][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.1813926249742508, acc: 0.9583333134651184)
[2025-02-13 19:27:27,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27,641][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.16648723185062408, acc: 0.9405405521392822)
[2025-02-13 19:27:27,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28,029][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.18758408725261688, acc: 0.9402984976768494)
[2025-02-13 19:27:28,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28,404][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.1036258339881897, acc: 0.9735449552536011)
[2025-02-13 19:27:28,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28,814][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.2838501036167145, acc: 0.9313725233078003)
[2025-02-13 19:27:28,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29,206][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.1457793414592743, acc: 0.9608938694000244)
[2025-02-13 19:27:29,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29,579][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.20923717319965363, acc: 0.9621621370315552)
[2025-02-13 19:27:29,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29,987][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.07419510930776596, acc: 0.9718309640884399)
[2025-02-13 19:27:30,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30,378][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.11683868616819382, acc: 0.9744898080825806)
[2025-02-13 19:27:30,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30,776][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.11683988571166992, acc: 0.9751552939414978)
[2025-02-13 19:27:30,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31,209][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.04149322584271431, acc: 0.9929078221321106)
[2025-02-13 19:27:31,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31,606][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.1630522906780243, acc: 0.9731183052062988)
[2025-02-13 19:27:31,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31,996][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.11764874309301376, acc: 0.9482758641242981)
[2025-02-13 19:27:32,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32,386][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.08609797060489655, acc: 0.9846938848495483)
[2025-02-13 19:27:32,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32,772][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.16962112486362457, acc: 0.9689922332763672)
[2025-02-13 19:27:32,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33,153][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.08088751137256622, acc: 0.9922480583190918)
[2025-02-13 19:27:33,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33,556][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.1979246437549591, acc: 0.9579831957817078)
[2025-02-13 19:27:33,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33,922][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.16820493340492249, acc: 0.967391312122345)
[2025-02-13 19:27:34,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34,310][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.1368677318096161, acc: 0.9588235020637512)
[2025-02-13 19:27:34,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34,715][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.18290871381759644, acc: 0.9693251252174377)
[2025-02-13 19:27:34,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35,086][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.1413884162902832, acc: 0.9537572264671326)
[2025-02-13 19:27:35,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35,451][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.08923034369945526, acc: 0.9826589822769165)
[2025-02-13 19:27:35,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35,842][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.19358232617378235, acc: 0.9612902998924255)
[2025-02-13 19:27:35,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36,205][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.21707113087177277, acc: 0.9583333134651184)
[2025-02-13 19:27:36,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36,586][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.29424265027046204, acc: 0.9485714435577393)
[2025-02-13 19:27:36,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36,975][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.21902760863304138, acc: 0.9571428298950195)
[2025-02-13 19:27:37,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37,377][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.10666549205780029, acc: 0.9801980257034302)
[2025-02-13 19:27:37,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37,765][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.08980871737003326, acc: 0.9714285731315613)
[2025-02-13 19:27:37,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38,141][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.18920421600341797, acc: 0.9602272510528564)
[2025-02-13 19:27:38,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38,562][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.1732787936925888, acc: 0.9462365508079529)
[2025-02-13 19:27:38,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38,931][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.2940041720867157, acc: 0.9327731132507324)
[2025-02-13 19:27:39,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39,298][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.12710653245449066, acc: 0.9718309640884399)
[2025-02-13 19:27:39,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39,658][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.14287370443344116, acc: 0.9745222926139832)
[2025-02-13 19:27:39,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40,027][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.21089740097522736, acc: 0.9455445408821106)
[2025-02-13 19:27:40,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40,398][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.16190168261528015, acc: 0.9585798978805542)
[2025-02-13 19:27:40,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40,805][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.1324261575937271, acc: 0.9700000286102295)
[2025-02-13 19:27:40,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41,181][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.21808892488479614, acc: 0.9528796076774597)
[2025-02-13 19:27:41,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41,555][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.14355354011058807, acc: 0.9542483687400818)
[2025-02-13 19:27:41,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41,958][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.1467103511095047, acc: 0.9518716335296631)
[2025-02-13 19:27:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42,341][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.17756912112236023, acc: 0.9545454382896423)
[2025-02-13 19:27:42,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42,712][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.17311517894268036, acc: 0.9593908786773682)
[2025-02-13 19:27:42,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43,072][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.1120779886841774, acc: 0.978723406791687)
[2025-02-13 19:27:43,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43,481][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.13071687519550323, acc: 0.9631901979446411)
[2025-02-13 19:27:43,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43,852][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.08452703803777695, acc: 0.9878787994384766)
[2025-02-13 19:27:43,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44,205][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.17440052330493927, acc: 0.9738562107086182)
[2025-02-13 19:27:44,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44,582][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.16382372379302979, acc: 0.9583333134651184)
[2025-02-13 19:27:44,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44,990][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.17060595750808716, acc: 0.9640718698501587)
[2025-02-13 19:27:45,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45,382][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.12130139768123627, acc: 0.9709302186965942)
[2025-02-13 19:27:45,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45,788][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.15193413197994232, acc: 0.9642857313156128)
[2025-02-13 19:27:45,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46,171][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.09443888068199158, acc: 0.9802955389022827)
[2025-02-13 19:27:46,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46,566][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.1616833359003067, acc: 0.9615384340286255)
[2025-02-13 19:27:46,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46,936][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.48400795459747314, acc: 0.9108911156654358)
[2025-02-13 19:27:47,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47,316][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.278936505317688, acc: 0.9182389974594116)
[2025-02-13 19:27:47,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47,683][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.2072819024324417, acc: 0.9248120188713074)
[2025-02-13 19:27:47,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48,045][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.1454349309206009, acc: 0.95652174949646)
[2025-02-13 19:27:48,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48,414][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.2702285349369049, acc: 0.9383561611175537)
[2025-02-13 19:27:48,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48,801][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.28443560004234314, acc: 0.9242424368858337)
[2025-02-13 19:27:48,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49,186][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.12027395516633987, acc: 0.9635036587715149)
[2025-02-13 19:27:49,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49,564][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.13784009218215942, acc: 0.96875)
[2025-02-13 19:27:49,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49,939][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.1976737082004547, acc: 0.9356725215911865)
[2025-02-13 19:27:50,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50,349][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.13769814372062683, acc: 0.954954981803894)
[2025-02-13 19:27:50,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50,749][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.12492374330759048, acc: 0.9629629850387573)
[2025-02-13 19:27:50,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51,114][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.23980574309825897, acc: 0.9365079402923584)
[2025-02-13 19:27:51,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51,485][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.09039244055747986, acc: 0.981249988079071)
[2025-02-13 19:27:51,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51,936][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.17868950963020325, acc: 0.9693251252174377)
[2025-02-13 19:27:52,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52,356][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.16883274912834167, acc: 0.9669421315193176)
[2025-02-13 19:27:52,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52,782][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.037445563822984695, acc: 0.9928057789802551)
[2025-02-13 19:27:52,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53,182][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.11274977028369904, acc: 0.9650349617004395)
[2025-02-13 19:27:53,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53,603][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.15057772397994995, acc: 0.9636363387107849)
[2025-02-13 19:27:53,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53,994][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.1412748545408249, acc: 0.970588207244873)
[2025-02-13 19:27:54,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54,412][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.15922409296035767, acc: 0.9655172228813171)
[2025-02-13 19:27:54,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54,814][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.19504354894161224, acc: 0.9523809552192688)
[2025-02-13 19:27:54,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55,207][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.2334185242652893, acc: 0.9337748289108276)
[2025-02-13 19:27:55,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55,603][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.24753016233444214, acc: 0.9407407641410828)
[2025-02-13 19:27:55,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56,004][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.20641161501407623, acc: 0.947826087474823)
[2025-02-13 19:27:56,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56,356][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.127071350812912, acc: 0.9639639854431152)
[2025-02-13 19:27:56,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56,717][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.12437861412763596, acc: 0.9819819927215576)
[2025-02-13 19:27:56,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57,072][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.2396974265575409, acc: 0.9408283829689026)
[2025-02-13 19:27:57,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57,423][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.13055646419525146, acc: 0.9795918464660645)
[2025-02-13 19:27:57,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57,789][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.1832038313150406, acc: 0.9583333134651184)
[2025-02-13 19:27:57,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58,191][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.1619105339050293, acc: 0.9719101190567017)
[2025-02-13 19:27:58,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58,617][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.19710494577884674, acc: 0.942307710647583)
[2025-02-13 19:27:58,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59,029][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.29972347617149353, acc: 0.9130434989929199)
[2025-02-13 19:27:59,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59,423][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.233892560005188, acc: 0.946107804775238)
[2025-02-13 19:27:59,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59,801][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.20625829696655273, acc: 0.9570552110671997)
[2025-02-13 19:27:59,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00,167][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.3022736608982086, acc: 0.9254658222198486)
[2025-02-13 19:28:00,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00,543][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.14306458830833435, acc: 0.9825581312179565)
[2025-02-13 19:28:00,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00,920][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.5177239179611206, acc: 0.8724831938743591)
[2025-02-13 19:28:01,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01,347][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.2787995934486389, acc: 0.9390243887901306)
[2025-02-13 19:28:01,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01,722][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.12690487504005432, acc: 0.9682539701461792)
[2025-02-13 19:28:01,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02,102][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.14492012560367584, acc: 0.9490445852279663)
[2025-02-13 19:28:02,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02,460][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.18563838303089142, acc: 0.9470198750495911)
[2025-02-13 19:28:02,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02,855][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.19227366149425507, acc: 0.9619565010070801)
[2025-02-13 19:28:02,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03,224][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.16321977972984314, acc: 0.9772727489471436)
[2025-02-13 19:28:03,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03,588][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.1559179276227951, acc: 0.9689922332763672)
[2025-02-13 19:28:03,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03,984][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.11925815045833588, acc: 0.9640287756919861)
[2025-02-13 19:28:04,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04,369][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.21771368384361267, acc: 0.9476743936538696)
[2025-02-13 19:28:04,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04,762][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.2080184817314148, acc: 0.926174521446228)
[2025-02-13 19:28:04,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05,179][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.1398163139820099, acc: 0.9880239367485046)
[2025-02-13 19:28:05,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05,548][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.17870956659317017, acc: 0.9512194991111755)
[2025-02-13 19:28:05,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05,935][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.2889852225780487, acc: 0.9418604373931885)
[2025-02-13 19:28:06,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06,319][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.1782761514186859, acc: 0.9512194991111755)
[2025-02-13 19:28:06,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06,699][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.1344434916973114, acc: 0.9590163826942444)
[2025-02-13 19:28:06,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07,101][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.24169480800628662, acc: 0.9447513818740845)
[2025-02-13 19:28:07,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07,501][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.2716403901576996, acc: 0.9207317233085632)
[2025-02-13 19:28:07,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07,907][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.15382498502731323, acc: 0.9583333134651184)
[2025-02-13 19:28:08,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08,331][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.17285169661045074, acc: 0.9375)
[2025-02-13 19:28:08,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08,708][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.2834380269050598, acc: 0.9647058844566345)
[2025-02-13 19:28:08,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09,072][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.2941634953022003, acc: 0.9248120188713074)
[2025-02-13 19:28:09,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09,461][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.4493103325366974, acc: 0.875)
[2025-02-13 19:28:09,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09,804][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.2345062792301178, acc: 0.9510489702224731)
[2025-02-13 19:28:09,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10,226][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.29464617371559143, acc: 0.9520958065986633)
[2025-02-13 19:28:10,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10,586][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.3272069990634918, acc: 0.9419354796409607)
[2025-02-13 19:28:10,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10,973][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.411634236574173, acc: 0.913705587387085)
[2025-02-13 19:28:11,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11,365][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.14611853659152985, acc: 0.9715909361839294)
[2025-02-13 19:28:11,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11,731][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.34590983390808105, acc: 0.9015544056892395)
[2025-02-13 19:28:11,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12,138][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.2846527397632599, acc: 0.9368932247161865)
[2025-02-13 19:28:12,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12,520][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.32558658719062805, acc: 0.9176470637321472)
[2025-02-13 19:28:12,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12,895][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.457040011882782, acc: 0.9025974273681641)
[2025-02-13 19:28:13,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13,254][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.45104390382766724, acc: 0.9130434989929199)
[2025-02-13 19:28:13,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13,621][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.49913835525512695, acc: 0.8895705342292786)
[2025-02-13 19:28:13,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13,983][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.5252177715301514, acc: 0.8767123222351074)
[2025-02-13 19:28:14,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14,330][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.18866625428199768, acc: 0.9576719403266907)
[2025-02-13 19:28:14,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14,705][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.21492840349674225, acc: 0.9399999976158142)
[2025-02-13 19:28:14,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15,083][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.24799910187721252, acc: 0.9463087320327759)
[2025-02-13 19:28:15,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15,514][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.3107714354991913, acc: 0.9254658222198486)
[2025-02-13 19:28:15,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15,885][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.488033264875412, acc: 0.8994709253311157)
[2025-02-13 19:28:16,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16,251][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 1.1015671491622925, acc: 0.8108108043670654)
[2025-02-13 19:28:16,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16,601][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.33379513025283813, acc: 0.9555555582046509)
[2025-02-13 19:28:16,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16,981][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.2655041217803955, acc: 0.9305555820465088)
[2025-02-13 19:28:17,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17,411][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.2695920169353485, acc: 0.9631578922271729)
[2025-02-13 19:28:17,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17,811][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.3196752965450287, acc: 0.9290322661399841)
[2025-02-13 19:28:17,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18,164][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.6230182647705078, acc: 0.8765432238578796)
[2025-02-13 19:28:18,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18,538][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.6478314399719238, acc: 0.8636363744735718)
[2025-02-13 19:28:18,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18,905][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.5054857134819031, acc: 0.8917526006698608)
[2025-02-13 19:28:19,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19,295][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.2966112494468689, acc: 0.9356435537338257)
[2025-02-13 19:28:19,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19,681][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.16716250777244568, acc: 0.9609755873680115)
[2025-02-13 19:28:19,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20,083][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.4005962312221527, acc: 0.9234693646430969)
[2025-02-13 19:28:20,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20,453][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.4530966877937317, acc: 0.8990384340286255)
[2025-02-13 19:28:20,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20,824][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.21930640935897827, acc: 0.9583333134651184)
[2025-02-13 19:28:20,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21,191][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.347421795129776, acc: 0.9160839319229126)
[2025-02-13 19:28:21,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21,608][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.5118361115455627, acc: 0.897849440574646)
[2025-02-13 19:28:21,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22,013][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.8561610579490662, acc: 0.8190954923629761)
[2025-02-13 19:28:22,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22,426][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.540142297744751, acc: 0.8756476640701294)
[2025-02-13 19:28:22,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22,811][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.28160184621810913, acc: 0.9354838728904724)
[2025-02-13 19:28:22,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23,204][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.26686763763427734, acc: 0.9269663095474243)
[2025-02-13 19:28:23,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23,601][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.21976247429847717, acc: 0.9274611473083496)
[2025-02-13 19:28:23,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23,948][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.46557486057281494, acc: 0.8989899158477783)
[2025-02-13 19:28:24,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24,358][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.2808477580547333, acc: 0.945652186870575)
[2025-02-13 19:28:24,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24,755][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.33665162324905396, acc: 0.9121951460838318)
[2025-02-13 19:28:24,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25,131][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.4111003279685974, acc: 0.8963730335235596)
[2025-02-13 19:28:25,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25,498][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.2694483697414398, acc: 0.9403669834136963)
[2025-02-13 19:28:25,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25,866][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.30669599771499634, acc: 0.9041916131973267)
[2025-02-13 19:28:26,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26,245][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.2616730332374573, acc: 0.9553571343421936)
[2025-02-13 19:28:26,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26,625][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.1657341867685318, acc: 0.9659090638160706)
[2025-02-13 19:28:26,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27,014][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.10316256433725357, acc: 0.9702970385551453)
[2025-02-13 19:28:27,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27,437][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.30505210161209106, acc: 0.9279999732971191)
[2025-02-13 19:28:27,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27,789][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.6190149784088135, acc: 0.8793103694915771)
[2025-02-13 19:28:27,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28,178][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.17428286373615265, acc: 0.9537572264671326)
[2025-02-13 19:28:28,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28,552][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.3860979378223419, acc: 0.8952381014823914)
[2025-02-13 19:28:28,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28,970][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.343136191368103, acc: 0.915730357170105)
[2025-02-13 19:28:29,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29,353][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.2682516574859619, acc: 0.9476743936538696)
[2025-02-13 19:28:29,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29,731][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.15353231132030487, acc: 0.9554139971733093)
[2025-02-13 19:28:29,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30,110][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.2092771828174591, acc: 0.9428571462631226)
[2025-02-13 19:28:30,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30,496][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.33315563201904297, acc: 0.9200000166893005)
[2025-02-13 19:28:30,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30,886][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.2051641345024109, acc: 0.9583333134651184)
[2025-02-13 19:28:31,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31,281][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.25689443945884705, acc: 0.9289940595626831)
[2025-02-13 19:28:31,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31,671][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.0732584297657013, acc: 0.9890710115432739)
[2025-02-13 19:28:31,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32,060][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.22270376980304718, acc: 0.9776119589805603)
[2025-02-13 19:28:32,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32,481][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.06327351182699203, acc: 0.978723406791687)
[2025-02-13 19:28:32,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32,847][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.1563967764377594, acc: 0.9578313231468201)
[2025-02-13 19:28:33,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33,244][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.11970820277929306, acc: 0.9622641801834106)
[2025-02-13 19:28:33,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33,646][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.3295057415962219, acc: 0.9382022619247437)
[2025-02-13 19:28:33,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34,025][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.27465707063674927, acc: 0.9352940917015076)
[2025-02-13 19:28:34,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34,400][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.18166568875312805, acc: 0.9416058659553528)
[2025-02-13 19:28:34,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34,793][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.4496293365955353, acc: 0.8875739574432373)
[2025-02-13 19:28:34,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35,212][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.12368224561214447, acc: 0.9767441749572754)
[2025-02-13 19:28:35,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35,637][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.11315788328647614, acc: 0.9712643623352051)
[2025-02-13 19:28:35,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36,050][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.33635368943214417, acc: 0.9457364082336426)
[2025-02-13 19:28:36,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36,443][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.2813176214694977, acc: 0.9406779408454895)
[2025-02-13 19:28:36,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36,878][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.1765655130147934, acc: 0.9414893388748169)
[2025-02-13 19:28:37,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37,260][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.4715941548347473, acc: 0.9108280539512634)
[2025-02-13 19:28:37,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37,623][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.26079633831977844, acc: 0.9481481313705444)
[2025-02-13 19:28:37,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37,966][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.1924888789653778, acc: 0.9477124214172363)
[2025-02-13 19:28:38,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38,336][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.4897446632385254, acc: 0.9192546606063843)
[2025-02-13 19:28:38,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38,702][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.34006616473197937, acc: 0.9629629850387573)
[2025-02-13 19:28:38,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39,062][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.08148514479398727, acc: 0.9818181991577148)
[2025-02-13 19:28:39,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39,442][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.057902026921510696, acc: 0.9870129823684692)
[2025-02-13 19:28:39,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39,864][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.06686394661664963, acc: 0.9860140085220337)
[2025-02-13 19:28:40,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40,296][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.15599201619625092, acc: 0.9651162624359131)
[2025-02-13 19:28:40,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40,750][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.1516374796628952, acc: 0.9704433679580688)
[2025-02-13 19:28:40,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41,128][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.15307192504405975, acc: 0.9635416865348816)
[2025-02-13 19:28:41,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41,510][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.14125683903694153, acc: 0.954285740852356)
[2025-02-13 19:28:41,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41,948][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.08891142904758453, acc: 0.9735449552536011)
[2025-02-13 19:28:42,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42,346][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.18922674655914307, acc: 0.9383561611175537)
[2025-02-13 19:28:42,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42,740][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.12866869568824768, acc: 0.9595959782600403)
[2025-02-13 19:28:42,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43,126][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.13002179563045502, acc: 0.9646464586257935)
[2025-02-13 19:28:43,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43,533][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.10353057831525803, acc: 0.9732620120048523)
[2025-02-13 19:28:43,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43,900][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.07883314043283463, acc: 0.9893048405647278)
[2025-02-13 19:28:44,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44,283][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.12026955187320709, acc: 0.9702970385551453)
[2025-02-13 19:28:44,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44,657][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.06573016941547394, acc: 0.9903846383094788)
[2025-02-13 19:28:44,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45,050][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.05881034955382347, acc: 0.9898989796638489)
[2025-02-13 19:28:45,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45,428][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.07156344503164291, acc: 0.9833333492279053)
[2025-02-13 19:28:45,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45,798][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.2116374373435974, acc: 0.9467455744743347)
[2025-02-13 19:28:45,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46,200][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.22778289020061493, acc: 0.9440993666648865)
[2025-02-13 19:28:46,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46,566][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.35898858308792114, acc: 0.8989361524581909)
[2025-02-13 19:28:46,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46,950][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.34122100472450256, acc: 0.9285714030265808)
[2025-02-13 19:28:47,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47,319][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.07519363611936569, acc: 0.9897959232330322)
[2025-02-13 19:28:47,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47,699][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.13090725243091583, acc: 0.9581151604652405)
[2025-02-13 19:28:47,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48,067][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.15549036860466003, acc: 0.9666666388511658)
[2025-02-13 19:28:48,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48,435][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.235243558883667, acc: 0.9504950642585754)
[2025-02-13 19:28:48,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48,829][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.1472427397966385, acc: 0.9750000238418579)
[2025-02-13 19:28:48,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49,233][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.09623714536428452, acc: 0.9801980257034302)
[2025-02-13 19:28:49,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49,652][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.26872843503952026, acc: 0.9192546606063843)
[2025-02-13 19:28:49,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50,044][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.16645583510398865, acc: 0.9488636255264282)
[2025-02-13 19:28:50,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50,425][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.050793759524822235, acc: 0.9934640526771545)
[2025-02-13 19:28:50,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50,751][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.09656523168087006, acc: 0.9627329111099243)
[2025-02-13 19:28:50,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51,136][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.09186962991952896, acc: 0.9815950989723206)
[2025-02-13 19:28:51,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51,493][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.16669684648513794, acc: 0.961240291595459)
[2025-02-13 19:28:51,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51,908][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.06557115167379379, acc: 0.9793103337287903)
[2025-02-13 19:28:52,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52,302][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.13689205050468445, acc: 0.9679999947547913)
[2025-02-13 19:28:52,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52,672][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.18708065152168274, acc: 0.9694656729698181)
[2025-02-13 19:28:52,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53,091][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.13523873686790466, acc: 0.969924807548523)
[2025-02-13 19:28:53,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53,466][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.30452635884284973, acc: 0.9418604373931885)
[2025-02-13 19:28:53,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53,865][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.13493353128433228, acc: 0.9719101190567017)
[2025-02-13 19:28:54,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54,251][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.09135060757398605, acc: 0.9793814420700073)
[2025-02-13 19:28:54,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54,637][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.07293828576803207, acc: 0.9824561476707458)
[2025-02-13 19:28:54,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55,010][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.059661343693733215, acc: 0.9788359999656677)
[2025-02-13 19:28:55,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55,440][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.05706406012177467, acc: 0.9884393215179443)
[2025-02-13 19:28:55,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55,810][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.09516502916812897, acc: 0.978723406791687)
[2025-02-13 19:28:55,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56,184][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.0837993249297142, acc: 0.9839572310447693)
[2025-02-13 19:28:56,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56,564][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.05291169136762619, acc: 0.988950252532959)
[2025-02-13 19:28:56,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56,921][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.05891093239188194, acc: 0.9941176176071167)
[2025-02-13 19:28:57,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57,317][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.041087497025728226, acc: 0.9937106966972351)
[2025-02-13 19:28:57,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57,698][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.24548499286174774, acc: 0.9316770434379578)
[2025-02-13 19:28:57,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58,079][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.20328418910503387, acc: 0.9513513445854187)
[2025-02-13 19:28:58,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58,460][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.19002000987529755, acc: 0.9447852969169617)
[2025-02-13 19:28:58,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58,852][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.09133721888065338, acc: 0.9935064911842346)
[2025-02-13 19:28:58,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59,217][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.1814240664243698, acc: 0.9503105878829956)
[2025-02-13 19:28:59,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59,598][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.15717241168022156, acc: 0.9575757384300232)
[2025-02-13 19:28:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00,017][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.28807276487350464, acc: 0.9226190447807312)
[2025-02-13 19:29:00,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00,395][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.2770652174949646, acc: 0.9318181872367859)
[2025-02-13 19:29:00,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00,713][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.36774885654449463, acc: 0.8888888955116272)
[2025-02-13 19:29:00,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01,086][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.20963755249977112, acc: 0.9473684430122375)
[2025-02-13 19:29:01,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01,472][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.3879024386405945, acc: 0.9248554706573486)
[2025-02-13 19:29:01,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01,852][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.2567618191242218, acc: 0.9407894611358643)
[2025-02-13 19:29:01,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02,208][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.17269957065582275, acc: 0.9572192430496216)
[2025-02-13 19:29:02,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02,614][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.2130034863948822, acc: 0.9350000023841858)
[2025-02-13 19:29:02,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02,995][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.16220761835575104, acc: 0.9639175534248352)
[2025-02-13 19:29:03,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03,383][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.13658301532268524, acc: 0.96517413854599)
[2025-02-13 19:29:03,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03,772][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.12789444625377655, acc: 0.9679144620895386)
[2025-02-13 19:29:03,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04,169][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.22207212448120117, acc: 0.9449999928474426)
[2025-02-13 19:29:04,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04,562][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.16376131772994995, acc: 0.9620853066444397)
[2025-02-13 19:29:04,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04,974][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.09335105866193771, acc: 0.9718309640884399)
[2025-02-13 19:29:05,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05,360][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.17832373082637787, acc: 0.9528796076774597)
[2025-02-13 19:29:05,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05,763][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.23484353721141815, acc: 0.935960590839386)
[2025-02-13 19:29:05,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06,167][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.2786993384361267, acc: 0.9340659379959106)
[2025-02-13 19:29:06,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06,588][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.22436630725860596, acc: 0.9641255736351013)
[2025-02-13 19:29:06,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06,967][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.2608798146247864, acc: 0.9408866763114929)
[2025-02-13 19:29:07,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07,327][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.2746627926826477, acc: 0.9319371581077576)
[2025-02-13 19:29:07,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07,702][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.10704915225505829, acc: 0.9756097793579102)
[2025-02-13 19:29:07,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08,068][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.09800969064235687, acc: 0.9609375)
[2025-02-13 19:29:08,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08,475][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.1854490488767624, acc: 0.9620253443717957)
[2025-02-13 19:29:08,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08,862][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.08497127890586853, acc: 0.9861111044883728)
[2025-02-13 19:29:08,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09,245][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.1335059553384781, acc: 0.9644669890403748)
[2025-02-13 19:29:09,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09,633][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.10954582691192627, acc: 0.9807692170143127)
[2025-02-13 19:29:09,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10,024][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.0836290717124939, acc: 0.9799196720123291)
[2025-02-13 19:29:10,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10,407][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.06914078444242477, acc: 0.9728506803512573)
[2025-02-13 19:29:10,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10,810][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.16208453476428986, acc: 0.966183602809906)
[2025-02-13 19:29:10,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11,207][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.16248348355293274, acc: 0.9671361446380615)
[2025-02-13 19:29:11,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11,579][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.2270832061767578, acc: 0.9404761791229248)
[2025-02-13 19:29:11,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11,965][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.27461767196655273, acc: 0.9426229596138)
[2025-02-13 19:29:12,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12,374][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.33304470777511597, acc: 0.9035087823867798)
[2025-02-13 19:29:12,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12,765][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.44914737343788147, acc: 0.8896104097366333)
[2025-02-13 19:29:12,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13,165][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.1985005885362625, acc: 0.9391891956329346)
[2025-02-13 19:29:13,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13,606][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.16596783697605133, acc: 0.9351851940155029)
[2025-02-13 19:29:13,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13,995][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.20134200155735016, acc: 0.9685039520263672)
[2025-02-13 19:29:14,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14,388][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.21975865960121155, acc: 0.9237288236618042)
[2025-02-13 19:29:14,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14,802][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.2231599986553192, acc: 0.960629940032959)
[2025-02-13 19:29:14,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15,187][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.3418143391609192, acc: 0.9210526347160339)
[2025-02-13 19:29:15,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15,612][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.3220187723636627, acc: 0.9473684430122375)
[2025-02-13 19:29:15,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15,966][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.2702867090702057, acc: 0.9459459185600281)
[2025-02-13 19:29:16,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16,360][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.1688200682401657, acc: 0.9473684430122375)
[2025-02-13 19:29:16,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16,773][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.16001656651496887, acc: 0.9545454382896423)
[2025-02-13 19:29:16,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17,167][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.21125130355358124, acc: 0.9343065619468689)
[2025-02-13 19:29:17,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17,556][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.30803945660591125, acc: 0.9270833134651184)
[2025-02-13 19:29:17,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17,930][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.4004627764225006, acc: 0.9032257795333862)
[2025-02-13 19:29:18,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18,308][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.13687506318092346, acc: 0.9552238583564758)
[2025-02-13 19:29:18,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18,684][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.22899985313415527, acc: 0.9457364082336426)
[2025-02-13 19:29:18,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19,051][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.33522069454193115, acc: 0.9318181872367859)
[2025-02-13 19:29:19,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19,415][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.18504598736763, acc: 0.9354838728904724)
[2025-02-13 19:29:19,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19,806][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.1982022374868393, acc: 0.9508196711540222)
[2025-02-13 19:29:19,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20,176][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.25427567958831787, acc: 0.934959352016449)
[2025-02-13 19:29:20,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20,530][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.15832248330116272, acc: 0.957446813583374)
[2025-02-13 19:29:20,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20,904][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.29954054951667786, acc: 0.948051929473877)
[2025-02-13 19:29:21,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21,307][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.2508920729160309, acc: 0.9380530714988708)
[2025-02-13 19:29:21,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21,710][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.2595829665660858, acc: 0.932330846786499)
[2025-02-13 19:29:21,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22,115][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.055218808352947235, acc: 0.9909909963607788)
[2025-02-13 19:29:22,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22,478][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.3182615041732788, acc: 0.9263157844543457)
[2025-02-13 19:29:22,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22,839][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.4148119390010834, acc: 0.9380530714988708)
[2025-02-13 19:29:22,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23,197][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.1871803104877472, acc: 0.9545454382896423)
[2025-02-13 19:29:23,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23,603][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.1943848431110382, acc: 0.9482758641242981)
[2025-02-13 19:29:23,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23,997][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.17579704523086548, acc: 0.9407894611358643)
[2025-02-13 19:29:24,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24,395][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.16412286460399628, acc: 0.9615384340286255)
[2025-02-13 19:29:24,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24,828][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.08875724673271179, acc: 0.9696969985961914)
[2025-02-13 19:29:24,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25,228][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.11390108615159988, acc: 0.9723756909370422)
[2025-02-13 19:29:25,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25,622][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.08098097890615463, acc: 0.9885714054107666)
[2025-02-13 19:29:25,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25,984][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.23486514389514923, acc: 0.9418604373931885)
[2025-02-13 19:29:26,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26,342][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.16896580159664154, acc: 0.939393937587738)
[2025-02-13 19:29:26,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26,707][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.22107045352458954, acc: 0.9620253443717957)
[2025-02-13 19:29:26,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27,072][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.10912029445171356, acc: 0.9810126423835754)
[2025-02-13 19:29:27,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27,463][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.0674515962600708, acc: 0.9790209531784058)
[2025-02-13 19:29:27,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27,893][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.1328536868095398, acc: 0.9722222089767456)
[2025-02-13 19:29:28,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28,257][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.13195887207984924, acc: 0.9466666579246521)
[2025-02-13 19:29:28,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28,620][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.10929828882217407, acc: 0.9724137783050537)
[2025-02-13 19:29:28,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28,976][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.033059075474739075, acc: 0.9935897588729858)
[2025-02-13 19:29:29,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29,361][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.17513802647590637, acc: 0.9526627063751221)
[2025-02-13 19:29:29,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29,752][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.3403358459472656, acc: 0.9418604373931885)
[2025-02-13 19:29:29,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30,167][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.17470550537109375, acc: 0.956250011920929)
[2025-02-13 19:29:30,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30,577][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.21621616184711456, acc: 0.9195402264595032)
[2025-02-13 19:29:30,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30,992][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.20981791615486145, acc: 0.9464285969734192)
[2025-02-13 19:29:31,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31,386][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.2326303869485855, acc: 0.9375)
[2025-02-13 19:29:31,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31,779][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.23865725100040436, acc: 0.9390243887901306)
[2025-02-13 19:29:31,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32,146][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.4310286045074463, acc: 0.8947368264198303)
[2025-02-13 19:29:32,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32,545][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.27036190032958984, acc: 0.920634925365448)
[2025-02-13 19:29:32,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32,930][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.1746223121881485, acc: 0.9556962251663208)
[2025-02-13 19:29:33,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33,298][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.21672388911247253, acc: 0.9437500238418579)
[2025-02-13 19:29:33,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33,696][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.16475434601306915, acc: 0.9487179517745972)
[2025-02-13 19:29:33,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34,069][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.35560616850852966, acc: 0.9135135412216187)
[2025-02-13 19:29:34,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34,410][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.09753711521625519, acc: 0.97826087474823)
[2025-02-13 19:29:34,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34,797][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.23750115931034088, acc: 0.9371069073677063)
[2025-02-13 19:29:34,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35,195][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.14491957426071167, acc: 0.9604519605636597)
[2025-02-13 19:29:35,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35,620][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.1961887776851654, acc: 0.949999988079071)
[2025-02-13 19:29:35,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36,022][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.2397920936346054, acc: 0.9476743936538696)
[2025-02-13 19:29:36,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36,385][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.20056940615177155, acc: 0.9370629191398621)
[2025-02-13 19:29:36,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36,778][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.19507871568202972, acc: 0.9454545378684998)
[2025-02-13 19:29:36,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37,152][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.1703653186559677, acc: 0.9685534834861755)
[2025-02-13 19:29:37,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37,567][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.1900988519191742, acc: 0.9432623982429504)
[2025-02-13 19:29:37,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37,954][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.1519654244184494, acc: 0.9649122953414917)
[2025-02-13 19:29:38,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38,331][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.22319285571575165, acc: 0.9389312863349915)
[2025-02-13 19:29:38,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38,698][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.0742335170507431, acc: 0.9832402467727661)
[2025-02-13 19:29:38,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39,072][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.1000288799405098, acc: 0.9724137783050537)
[2025-02-13 19:29:39,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39,425][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.17046073079109192, acc: 0.9555555582046509)
[2025-02-13 19:29:39,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39,809][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.14289651811122894, acc: 0.9814814925193787)
[2025-02-13 19:29:39,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40,200][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.14273233711719513, acc: 0.9538461565971375)
[2025-02-13 19:29:40,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40,576][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.3274437189102173, acc: 0.910179615020752)
[2025-02-13 19:29:40,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40,968][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.27889809012413025, acc: 0.9186046719551086)
[2025-02-13 19:29:41,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41,364][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.37175774574279785, acc: 0.9341317415237427)
[2025-02-13 19:29:41,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41,776][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.3325962722301483, acc: 0.8914728760719299)
[2025-02-13 19:29:41,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42,170][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.3367140591144562, acc: 0.9375)
[2025-02-13 19:29:42,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42,555][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.09318535774946213, acc: 0.9647058844566345)
[2025-02-13 19:29:42,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42,922][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.14277805387973785, acc: 0.9817073345184326)
[2025-02-13 19:29:43,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43,292][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.1205366849899292, acc: 0.9666666388511658)
[2025-02-13 19:29:43,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43,728][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.20566533505916595, acc: 0.9529411792755127)
[2025-02-13 19:29:43,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44,183][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.3478150963783264, acc: 0.9352940917015076)
[2025-02-13 19:29:44,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44,549][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.5174468159675598, acc: 0.8466257452964783)
[2025-02-13 19:29:44,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44,957][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.4022173583507538, acc: 0.9021739363670349)
[2025-02-13 19:29:45,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45,373][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.336069256067276, acc: 0.9419354796409607)
[2025-02-13 19:29:45,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45,772][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.2873832583427429, acc: 0.9329268336296082)
[2025-02-13 19:29:45,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46,176][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.21118497848510742, acc: 0.9350649118423462)
[2025-02-13 19:29:46,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46,591][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.19851286709308624, acc: 0.9555555582046509)
[2025-02-13 19:29:46,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47,009][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.19005168974399567, acc: 0.9674418568611145)
[2025-02-13 19:29:47,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47,424][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.2384936511516571, acc: 0.9457831382751465)
[2025-02-13 19:29:47,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47,823][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.32960274815559387, acc: 0.9086757898330688)
[2025-02-13 19:29:47,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48,220][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.15376128256320953, acc: 0.970370352268219)
[2025-02-13 19:29:48,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48,548][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.2674013674259186, acc: 0.9634146094322205)
[2025-02-13 19:29:48,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48,941][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.062042880803346634, acc: 0.9893617033958435)
[2025-02-13 19:29:49,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49,362][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.22650806605815887, acc: 0.9603174328804016)
[2025-02-13 19:29:49,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49,779][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.2623903751373291, acc: 0.9653179049491882)
[2025-02-13 19:29:49,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50,193][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.18610689043998718, acc: 0.9621621370315552)
[2025-02-13 19:29:50,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50,583][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.24700027704238892, acc: 0.9365079402923584)
[2025-02-13 19:29:50,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50,954][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.11995482444763184, acc: 0.9722222089767456)
[2025-02-13 19:29:51,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51,317][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.08492439985275269, acc: 0.9661017060279846)
[2025-02-13 19:29:51,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51,701][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.07424554228782654, acc: 0.9804878234863281)
[2025-02-13 19:29:51,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52,110][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.10266439616680145, acc: 0.9802955389022827)
[2025-02-13 19:29:52,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52,501][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.06062862277030945, acc: 0.9756097793579102)
[2025-02-13 19:29:52,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52,920][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.15961873531341553, acc: 0.9571428298950195)
[2025-02-13 19:29:53,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53,315][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.2513049840927124, acc: 0.9508196711540222)
[2025-02-13 19:29:53,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53,707][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.2171243280172348, acc: 0.9490445852279663)
[2025-02-13 19:29:53,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54,087][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.1955973356962204, acc: 0.9512194991111755)
[2025-02-13 19:29:54,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54,463][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.29099467396736145, acc: 0.912162184715271)
[2025-02-13 19:29:54,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54,809][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.29797711968421936, acc: 0.9329608678817749)
[2025-02-13 19:29:54,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55,205][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.15057483315467834, acc: 0.9567901492118835)
[2025-02-13 19:29:55,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55,596][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.15556590259075165, acc: 0.9613526463508606)
[2025-02-13 19:29:55,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56,021][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.32408252358436584, acc: 0.9189189076423645)
[2025-02-13 19:29:56,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56,405][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.23745177686214447, acc: 0.9485714435577393)
[2025-02-13 19:29:56,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56,815][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.22395415604114532, acc: 0.9237288236618042)
[2025-02-13 19:29:56,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57,180][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.22255903482437134, acc: 0.931034505367279)
[2025-02-13 19:29:57,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57,594][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.22629807889461517, acc: 0.9204545617103577)
[2025-02-13 19:29:57,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57,981][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.350793719291687, acc: 0.9100528955459595)
[2025-02-13 19:29:58,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58,347][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.3063306212425232, acc: 0.9230769276618958)
[2025-02-13 19:29:58,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58,692][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.18458107113838196, acc: 0.9447852969169617)
[2025-02-13 19:29:58,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59,040][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.4694764018058777, acc: 0.9219858050346375)
[2025-02-13 19:29:59,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59,414][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.2839357852935791, acc: 0.9289617538452148)
[2025-02-13 19:29:59,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59,808][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.2169065922498703, acc: 0.9567567706108093)
[2025-02-13 19:29:59,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00,214][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.19351640343666077, acc: 0.9615384340286255)
[2025-02-13 19:30:00,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00,627][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.41458505392074585, acc: 0.9222797751426697)
[2025-02-13 19:30:00,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01,011][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.29455044865608215, acc: 0.9256756901741028)
[2025-02-13 19:30:01,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01,365][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.17282889783382416, acc: 0.9530201554298401)
[2025-02-13 19:30:01,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01,756][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.17816898226737976, acc: 0.9491525292396545)
[2025-02-13 19:30:01,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02,170][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.2140054702758789, acc: 0.9477611780166626)
[2025-02-13 19:30:02,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02,526][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.1466357260942459, acc: 0.9398906826972961)
[2025-02-13 19:30:02,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02,900][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.15083251893520355, acc: 0.9586206674575806)
[2025-02-13 19:30:03,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03,282][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.21445052325725555, acc: 0.9433962106704712)
[2025-02-13 19:30:03,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03,680][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.1194726750254631, acc: 0.9610389471054077)
[2025-02-13 19:30:03,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04,089][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.19809316098690033, acc: 0.964102566242218)
[2025-02-13 19:30:04,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04,514][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.10265348106622696, acc: 0.9805194735527039)
[2025-02-13 19:30:04,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04,885][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.11349496245384216, acc: 0.9663865566253662)
[2025-02-13 19:30:05,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05,296][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.30833175778388977, acc: 0.946107804775238)
[2025-02-13 19:30:05,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05,700][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.14567585289478302, acc: 0.9768785834312439)
[2025-02-13 19:30:05,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06,076][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.26352250576019287, acc: 0.9272727370262146)
[2025-02-13 19:30:06,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06,436][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.27565377950668335, acc: 0.9350649118423462)
[2025-02-13 19:30:06,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06,826][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.18531714379787445, acc: 0.9613259434700012)
[2025-02-13 19:30:06,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07,191][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.23264305293560028, acc: 0.9399999976158142)
[2025-02-13 19:30:07,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07,574][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.18650756776332855, acc: 0.9639175534248352)
[2025-02-13 19:30:07,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07,956][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.11277482658624649, acc: 0.9802955389022827)
[2025-02-13 19:30:08,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08,393][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.14161288738250732, acc: 0.9682539701461792)
[2025-02-13 19:30:08,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08,766][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.7405053973197937, acc: 0.8478260636329651)
[2025-02-13 19:30:08,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09,138][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.9177177548408508, acc: 0.8515625)
[2025-02-13 19:30:09,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09,513][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.46890610456466675, acc: 0.9022988677024841)
[2025-02-13 19:30:09,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09,880][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.5223393440246582, acc: 0.9210526347160339)
[2025-02-13 19:30:10,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10,271][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.20020583271980286, acc: 0.9692307710647583)
[2025-02-13 19:30:10,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10,693][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.9061702489852905, acc: 0.8095238208770752)
[2025-02-13 19:30:10,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11,115][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.7872459888458252, acc: 0.8399999737739563)
[2025-02-13 19:30:11,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11,497][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.5712527632713318, acc: 0.8888888955116272)
[2025-02-13 19:30:11,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11,914][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.40059396624565125, acc: 0.9133333563804626)
[2025-02-13 19:30:12,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12,326][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.13517753779888153, acc: 0.9636363387107849)
[2025-02-13 19:30:12,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12,692][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.13357263803482056, acc: 0.976190447807312)
[2025-02-13 19:30:12,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13,035][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.4705609977245331, acc: 0.8999999761581421)
[2025-02-13 19:30:13,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13,411][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.4468323886394501, acc: 0.9136690497398376)
[2025-02-13 19:30:13,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13,786][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.28854691982269287, acc: 0.9349112510681152)
[2025-02-13 19:30:13,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14,157][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.6420618295669556, acc: 0.8500000238418579)
[2025-02-13 19:30:14,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14,579][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.22992543876171112, acc: 0.9354838728904724)
[2025-02-13 19:30:14,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14,973][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.1271827071905136, acc: 0.9764705896377563)
[2025-02-13 19:30:15,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15,369][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.16697603464126587, acc: 0.9435897469520569)
[2025-02-13 19:30:15,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15,733][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.31213000416755676, acc: 0.9162561297416687)
[2025-02-13 19:30:15,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16,142][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.3557714819908142, acc: 0.9230769276618958)
[2025-02-13 19:30:16,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16,556][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.775547981262207, acc: 0.8616352081298828)
[2025-02-13 19:30:16,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16,968][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.2568477392196655, acc: 0.9497487545013428)
[2025-02-13 19:30:17,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17,349][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.12835998833179474, acc: 0.9729729890823364)
[2025-02-13 19:30:17,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17,726][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.23260226845741272, acc: 0.9333333373069763)
[2025-02-13 19:30:17,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18,110][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.1996341347694397, acc: 0.9324324131011963)
[2025-02-13 19:30:18,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18,464][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.4179958403110504, acc: 0.8823529481887817)
[2025-02-13 19:30:18,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18,875][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.19892434775829315, acc: 0.9477611780166626)
[2025-02-13 19:30:19,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19,263][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.31540074944496155, acc: 0.9389312863349915)
[2025-02-13 19:30:19,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19,647][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.4874922037124634, acc: 0.925000011920929)
[2025-02-13 19:30:19,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20,011][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.2031223475933075, acc: 0.9677419066429138)
[2025-02-13 19:30:20,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20,411][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.12579354643821716, acc: 0.9599999785423279)
[2025-02-13 19:30:20,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20,809][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.08727636933326721, acc: 0.9784172773361206)
[2025-02-13 19:30:20,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21,165][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.1531822830438614, acc: 0.9644970297813416)
[2025-02-13 19:30:21,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21,542][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.09613718837499619, acc: 0.9750000238418579)
[2025-02-13 19:30:21,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21,887][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.05861286446452141, acc: 0.9876543283462524)
[2025-02-13 19:30:22,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22,243][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.36608943343162537, acc: 0.9308176040649414)
[2025-02-13 19:30:22,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22,623][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.15206994116306305, acc: 0.9617834687232971)
[2025-02-13 19:30:22,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23,048][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.1765485256910324, acc: 0.9634146094322205)
[2025-02-13 19:30:23,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23,454][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.2803606390953064, acc: 0.9186046719551086)
[2025-02-13 19:30:23,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23,863][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.3712726831436157, acc: 0.8793103694915771)
[2025-02-13 19:30:23,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24,226][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.16901297867298126, acc: 0.9801324605941772)
[2025-02-13 19:30:24,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24,591][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.25025811791419983, acc: 0.887417197227478)
[2025-02-13 19:30:24,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24,973][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.16037540137767792, acc: 0.9653179049491882)
[2025-02-13 19:30:25,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25,344][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.2156570851802826, acc: 0.9532163739204407)
[2025-02-13 19:30:25,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25,735][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.3451739251613617, acc: 0.931034505367279)
[2025-02-13 19:30:25,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26,138][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.25072190165519714, acc: 0.9338235259056091)
[2025-02-13 19:30:26,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26,531][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.26325416564941406, acc: 0.95652174949646)
[2025-02-13 19:30:26,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26,898][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.5332346558570862, acc: 0.893081784248352)
[2025-02-13 19:30:27,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27,300][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.2743891775608063, acc: 0.9308176040649414)
[2025-02-13 19:30:27,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27,699][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.37266501784324646, acc: 0.9166666865348816)
[2025-02-13 19:30:27,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28,081][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.29100480675697327, acc: 0.9103448390960693)
[2025-02-13 19:30:28,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28,450][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.23088490962982178, acc: 0.949999988079071)
[2025-02-13 19:30:28,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28,820][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.4621327519416809, acc: 0.8876404762268066)
[2025-02-13 19:30:28,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29,237][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.4222502112388611, acc: 0.9055555462837219)
[2025-02-13 19:30:29,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29,589][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.28799745440483093, acc: 0.9071428775787354)
[2025-02-13 19:30:29,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29,971][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.5821194052696228, acc: 0.8787878751754761)
[2025-02-13 19:30:30,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30,367][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.3074316084384918, acc: 0.9078947305679321)
[2025-02-13 19:30:30,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30,764][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.13129036128520966, acc: 0.9816513657569885)
[2025-02-13 19:30:30,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31,163][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.31157827377319336, acc: 0.9305555820465088)
[2025-02-13 19:30:31,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31,545][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.17175012826919556, acc: 0.961240291595459)
[2025-02-13 19:30:31,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31,909][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.15543943643569946, acc: 0.9534883499145508)
[2025-02-13 19:30:32,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32,285][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.3039223253726959, acc: 0.9230769276618958)
[2025-02-13 19:30:32,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32,695][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.17031607031822205, acc: 0.9360465407371521)
[2025-02-13 19:30:32,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33,124][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.22770896553993225, acc: 0.9430894255638123)
[2025-02-13 19:30:33,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33,531][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.13026653230190277, acc: 0.9634146094322205)
[2025-02-13 19:30:33,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33,921][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.29816004633903503, acc: 0.9254658222198486)
[2025-02-13 19:30:34,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34,293][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.33820950984954834, acc: 0.9230769276618958)
[2025-02-13 19:30:34,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34,670][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.06242544576525688, acc: 0.9870129823684692)
[2025-02-13 19:30:34,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35,080][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.30485448241233826, acc: 0.9395604133605957)
[2025-02-13 19:30:35,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35,470][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.35600197315216064, acc: 0.9435483813285828)
[2025-02-13 19:30:35,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35,865][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.14467264711856842, acc: 0.9700000286102295)
[2025-02-13 19:30:36,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36,276][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.11600539833307266, acc: 0.9779005646705627)
[2025-02-13 19:30:36,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36,638][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.07903243601322174, acc: 0.978723406791687)
[2025-02-13 19:30:36,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37,027][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.1508202850818634, acc: 0.9411764740943909)
[2025-02-13 19:30:37,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37,403][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.15823635458946228, acc: 0.9695122241973877)
[2025-02-13 19:30:37,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37,788][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.4615178406238556, acc: 0.9175823926925659)
[2025-02-13 19:30:37,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38,157][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.16020916402339935, acc: 0.9668874144554138)
[2025-02-13 19:30:38,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38,503][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.20912306010723114, acc: 0.969072163105011)
[2025-02-13 19:30:38,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38,869][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.3325052857398987, acc: 0.9599999785423279)
[2025-02-13 19:30:39,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39,236][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.16548609733581543, acc: 0.9666666388511658)
[2025-02-13 19:30:39,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39,590][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.1557852327823639, acc: 0.9693877696990967)
[2025-02-13 19:30:39,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39,975][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.11220750212669373, acc: 0.970588207244873)
[2025-02-13 19:30:40,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40,342][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.24272072315216064, acc: 0.9539473652839661)
[2025-02-13 19:30:40,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40,691][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.08837681263685226, acc: 0.9679999947547913)
[2025-02-13 19:30:40,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41,036][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.39105600118637085, acc: 0.9202454090118408)
[2025-02-13 19:30:41,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41,425][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.2876809537410736, acc: 0.9419354796409607)
[2025-02-13 19:30:41,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41,801][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.11517376452684402, acc: 0.9679487347602844)
[2025-02-13 19:30:41,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42,163][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.18301735818386078, acc: 0.976331353187561)
[2025-02-13 19:30:42,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42,519][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.23564592003822327, acc: 0.955974817276001)
[2025-02-13 19:30:42,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42,886][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.3753313720226288, acc: 0.908450722694397)
[2025-02-13 19:30:43,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43,259][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.23687751591205597, acc: 0.9520547986030579)
[2025-02-13 19:30:43,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43,629][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.2611640393733978, acc: 0.9285714030265808)
[2025-02-13 19:30:43,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44,039][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.1068878322839737, acc: 0.9777777791023254)
[2025-02-13 19:30:44,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44,427][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.06222565472126007, acc: 0.9743589758872986)
[2025-02-13 19:30:44,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44,821][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.08705565333366394, acc: 0.9838709831237793)
[2025-02-13 19:30:44,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45,226][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.18115010857582092, acc: 0.96875)
[2025-02-13 19:30:45,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45,660][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.4541175365447998, acc: 0.8985507488250732)
[2025-02-13 19:30:45,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46,066][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.1386292576789856, acc: 0.9777777791023254)
[2025-02-13 19:30:46,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46,443][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.11564850062131882, acc: 0.9850746393203735)
[2025-02-13 19:30:46,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46,868][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.12894687056541443, acc: 0.9691358208656311)
[2025-02-13 19:30:47,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47,256][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.05873497202992439, acc: 0.9937106966972351)
[2025-02-13 19:30:47,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47,647][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.025247227400541306, acc: 0.9923076629638672)
[2025-02-13 19:30:47,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48,012][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.23744189739227295, acc: 0.9551281929016113)
[2025-02-13 19:30:48,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48,389][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.14779417216777802, acc: 0.966292142868042)
[2025-02-13 19:30:48,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48,776][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.05475075915455818, acc: 0.9850746393203735)
[2025-02-13 19:30:48,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49,153][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.04283144697546959, acc: 1.0)
[2025-02-13 19:30:49,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49,545][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.38320192694664, acc: 0.9052631855010986)
[2025-02-13 19:30:49,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49,913][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.34845778346061707, acc: 0.9219858050346375)
[2025-02-13 19:30:50,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50,305][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.16229043900966644, acc: 0.9447852969169617)
[2025-02-13 19:30:50,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50,704][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.25982072949409485, acc: 0.9407894611358643)
[2025-02-13 19:30:50,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51,083][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.10254698246717453, acc: 0.9781420826911926)
[2025-02-13 19:30:51,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51,478][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.16999787092208862, acc: 0.970059871673584)
[2025-02-13 19:30:51,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51,875][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.25478097796440125, acc: 0.9375)
[2025-02-13 19:30:52,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52,265][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.14623667299747467, acc: 0.9671052694320679)
[2025-02-13 19:30:52,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52,642][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.09705314040184021, acc: 0.9744898080825806)
[2025-02-13 19:30:52,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53,028][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.27346140146255493, acc: 0.9175257682800293)
[2025-02-13 19:30:53,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53,428][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.2160937488079071, acc: 0.9454545378684998)
[2025-02-13 19:30:53,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53,838][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.30054181814193726, acc: 0.9156626462936401)
[2025-02-13 19:30:53,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54,237][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.25984716415405273, acc: 0.9735099077224731)
[2025-02-13 19:30:54,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54,617][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.20558270812034607, acc: 0.9510489702224731)
[2025-02-13 19:30:54,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54,981][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.21802303194999695, acc: 0.949367105960846)
[2025-02-13 19:30:55,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55,384][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.17346103489398956, acc: 0.939393937587738)
[2025-02-13 19:30:55,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55,753][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.13037484884262085, acc: 0.9724137783050537)
[2025-02-13 19:30:55,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56,142][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.19229911267757416, acc: 0.9470587968826294)
[2025-02-13 19:30:56,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56,503][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.195580393075943, acc: 0.9508196711540222)
[2025-02-13 19:30:56,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56,887][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.2332918643951416, acc: 0.9661017060279846)
[2025-02-13 19:30:56,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57,174][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.13513028621673584, acc: 0.96875)
[2025-02-13 19:30:57,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57,521][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.15288355946540833, acc: 0.9239130616188049)
[2025-02-13 19:30:57,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57,880][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.025458991527557373, acc: 1.0)
[2025-02-13 19:30:58,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58,314][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.0446988008916378, acc: 0.9899497628211975)
[2025-02-13 19:30:58,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58,685][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.10317748785018921, acc: 0.9797979593276978)
[2025-02-13 19:30:58,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59,071][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.10624702274799347, acc: 0.9727891087532043)
[2025-02-13 19:30:59,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59,475][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.049862392246723175, acc: 0.9916666746139526)
[2025-02-13 19:30:59,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59,911][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.07088223844766617, acc: 0.9786096215248108)
[2025-02-13 19:31:00,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00,308][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.07249104231595993, acc: 0.9884393215179443)
[2025-02-13 19:31:00,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00,698][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.07637622207403183, acc: 0.9852941036224365)
[2025-02-13 19:31:00,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01,089][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.26618313789367676, acc: 0.9308176040649414)
[2025-02-13 19:31:01,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01,413][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.3106459975242615, acc: 0.9322034120559692)
[2025-02-13 19:31:01,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01,788][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.06783362478017807, acc: 0.9852941036224365)
[2025-02-13 19:31:01,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02,169][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.1435573399066925, acc: 0.9504132270812988)
[2025-02-13 19:31:02,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02,525][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.34385010600090027, acc: 0.9548386931419373)
[2025-02-13 19:31:02,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02,894][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.08833485841751099, acc: 0.9693877696990967)
[2025-02-13 19:31:03,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03,316][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.051637981086969376, acc: 0.9900990128517151)
[2025-02-13 19:31:03,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03,704][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.08392166346311569, acc: 0.9696969985961914)
[2025-02-13 19:31:03,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04,081][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.02580355852842331, acc: 1.0)
[2025-02-13 19:31:04,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04,496][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.0619790181517601, acc: 0.9803921580314636)
[2025-02-13 19:31:04,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04,893][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.09463559836149216, acc: 0.9644669890403748)
[2025-02-13 19:31:05,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05,319][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.11434280127286911, acc: 0.9720930457115173)
[2025-02-13 19:31:05,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05,696][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.10798650979995728, acc: 0.964102566242218)
[2025-02-13 19:31:05,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06,098][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.06809123605489731, acc: 0.9938271641731262)
[2025-02-13 19:31:06,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06,513][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.3865707814693451, acc: 0.9214285612106323)
[2025-02-13 19:31:06,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06,897][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.36597493290901184, acc: 0.8992805480957031)
[2025-02-13 19:31:07,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07,245][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.27671369910240173, acc: 0.9224806427955627)
[2025-02-13 19:31:07,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07,616][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.18636967241764069, acc: 0.9342105388641357)
[2025-02-13 19:31:07,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07,970][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.12223716825246811, acc: 0.9583333134651184)
[2025-02-13 19:31:08,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08,390][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.14111168682575226, acc: 0.9722222089767456)
[2025-02-13 19:31:08,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08,781][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.2727178633213043, acc: 0.9308176040649414)
[2025-02-13 19:31:08,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09,200][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.17797401547431946, acc: 0.9384615421295166)
[2025-02-13 19:31:09,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09,630][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.3061130940914154, acc: 0.9375)
[2025-02-13 19:31:09,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10,058][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.2667553424835205, acc: 0.9375)
[2025-02-13 19:31:10,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10,446][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.13938499987125397, acc: 0.9704142212867737)
[2025-02-13 19:31:10,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10,821][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.19981791079044342, acc: 0.9629629850387573)
[2025-02-13 19:31:10,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11,208][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.17300166189670563, acc: 0.965753436088562)
[2025-02-13 19:31:11,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11,648][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.23449666798114777, acc: 0.9441340565681458)
[2025-02-13 19:31:11,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12,055][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.2169150859117508, acc: 0.9459459185600281)
[2025-02-13 19:31:12,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12,449][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.35325369238853455, acc: 0.9069767594337463)
[2025-02-13 19:31:12,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12,877][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.5571279525756836, acc: 0.8682634830474854)
[2025-02-13 19:31:12,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13,282][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.12886296212673187, acc: 0.9798657894134521)
[2025-02-13 19:31:13,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13,658][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.19431224465370178, acc: 0.954023003578186)
[2025-02-13 19:31:13,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14,048][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.14436770975589752, acc: 0.9642857313156128)
[2025-02-13 19:31:14,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14,404][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.14108583331108093, acc: 0.9567901492118835)
[2025-02-13 19:31:14,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14,809][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.2973460555076599, acc: 0.9135802388191223)
[2025-02-13 19:31:14,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15,189][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.1376160830259323, acc: 0.9702380895614624)
[2025-02-13 19:31:15,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15,563][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.17264918982982635, acc: 0.9707602262496948)
[2025-02-13 19:31:15,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15,958][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.15888433158397675, acc: 0.9822485446929932)
[2025-02-13 19:31:16,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16,338][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.17063260078430176, acc: 0.9671052694320679)
[2025-02-13 19:31:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16,713][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.19735924899578094, acc: 0.961240291595459)
[2025-02-13 19:31:16,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17,100][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.2030791938304901, acc: 0.9617834687232971)
[2025-02-13 19:31:17,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17,477][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.253905326128006, acc: 0.9301075339317322)
[2025-02-13 19:31:17,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17,858][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.187447652220726, acc: 0.9418604373931885)
[2025-02-13 19:31:18,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18,247][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.19615550339221954, acc: 0.9497206807136536)
[2025-02-13 19:31:18,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18,647][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.36018067598342896, acc: 0.9327731132507324)
[2025-02-13 19:31:18,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19,028][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.21575967967510223, acc: 0.9285714030265808)
[2025-02-13 19:31:19,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19,430][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.10080718249082565, acc: 0.9783783555030823)
[2025-02-13 19:31:19,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19,818][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.11911063641309738, acc: 0.9671052694320679)
[2025-02-13 19:31:19,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20,206][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.039133280515670776, acc: 0.9940476417541504)
[2025-02-13 19:31:20,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20,577][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.12350182235240936, acc: 0.9726775884628296)
[2025-02-13 19:31:20,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20,955][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.18334320187568665, acc: 0.9658536314964294)
[2025-02-13 19:31:21,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21,374][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.10705570131540298, acc: 0.9813664555549622)
[2025-02-13 19:31:21,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21,780][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.1940232366323471, acc: 0.9351851940155029)
[2025-02-13 19:31:21,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22,147][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.22604522109031677, acc: 0.964102566242218)
[2025-02-13 19:31:22,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22,503][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.1567421406507492, acc: 0.9740259647369385)
[2025-02-13 19:31:22,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22,873][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.17537179589271545, acc: 0.9551281929016113)
[2025-02-13 19:31:23,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23,280][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.07917491346597672, acc: 0.9882352948188782)
[2025-02-13 19:31:23,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23,657][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.2428627461194992, acc: 0.9463087320327759)
[2025-02-13 19:31:23,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24,051][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.17881256341934204, acc: 0.9548386931419373)
[2025-02-13 19:31:24,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24,432][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.12158051878213882, acc: 0.9666666388511658)
[2025-02-13 19:31:24,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24,821][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.09887564182281494, acc: 0.9607843160629272)
[2025-02-13 19:31:24,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25,215][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.10505198687314987, acc: 0.9677419066429138)
[2025-02-13 19:31:25,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25,595][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.2092342972755432, acc: 0.9519230723381042)
[2025-02-13 19:31:25,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25,989][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.07456953078508377, acc: 0.9842932224273682)
[2025-02-13 19:31:26,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26,381][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.10249795764684677, acc: 0.9780219793319702)
[2025-02-13 19:31:26,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26,773][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.04989136382937431, acc: 0.9893048405647278)
[2025-02-13 19:31:26,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27,228][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.18220491707324982, acc: 0.9622641801834106)
[2025-02-13 19:31:27,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27,601][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.19993560016155243, acc: 0.954081654548645)
[2025-02-13 19:31:27,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27,978][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.15211020410060883, acc: 0.9675324559211731)
[2025-02-13 19:31:28,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28,365][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.7432698011398315, acc: 0.8448275923728943)
[2025-02-13 19:31:28,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28,702][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.13331551849842072, acc: 0.9588235020637512)
[2025-02-13 19:31:28,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29,077][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.07319750636816025, acc: 0.9744898080825806)
[2025-02-13 19:31:29,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29,419][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.12614649534225464, acc: 0.9664804339408875)
[2025-02-13 19:31:29,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29,834][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.09288642555475235, acc: 0.9852216839790344)
[2025-02-13 19:31:29,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30,251][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.07961194217205048, acc: 0.9892473220825195)
[2025-02-13 19:31:30,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30,648][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.13282646238803864, acc: 0.9756097793579102)
[2025-02-13 19:31:30,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31,009][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.11972946673631668, acc: 0.9839572310447693)
[2025-02-13 19:31:31,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31,377][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.09479431062936783, acc: 0.9695122241973877)
[2025-02-13 19:31:31,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31,752][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.0715930387377739, acc: 0.9794871807098389)
[2025-02-13 19:31:31,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32,121][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.1006767749786377, acc: 0.9781022071838379)
[2025-02-13 19:31:32,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32,489][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.03905363380908966, acc: 0.9941176176071167)
[2025-02-13 19:31:32,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32,900][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.060785554349422455, acc: 0.9803921580314636)
[2025-02-13 19:31:33,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33,311][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.03541512414813042, acc: 1.0)
[2025-02-13 19:31:33,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33,683][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.09185989201068878, acc: 0.9863945841789246)
[2025-02-13 19:31:33,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34,056][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.05355241149663925, acc: 0.9870129823684692)
[2025-02-13 19:31:34,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34,427][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.0367126502096653, acc: 0.9932885766029358)
[2025-02-13 19:31:34,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34,807][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.16502277553081512, acc: 0.9677419066429138)
[2025-02-13 19:31:34,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35,189][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.07468216866254807, acc: 0.9839572310447693)
[2025-02-13 19:31:35,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35,562][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.06309828907251358, acc: 0.9850746393203735)
[2025-02-13 19:31:35,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35,991][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.16073094308376312, acc: 0.9510489702224731)
[2025-02-13 19:31:36,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36,403][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.10952683538198471, acc: 0.9806451797485352)
[2025-02-13 19:31:36,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36,770][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.015052271075546741, acc: 1.0)
[2025-02-13 19:31:36,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37,176][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.2703624367713928, acc: 0.9347826242446899)
[2025-02-13 19:31:37,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37,540][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.20430317521095276, acc: 0.9554139971733093)
[2025-02-13 19:31:37,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37,907][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.04141104221343994, acc: 0.9938271641731262)
[2025-02-13 19:31:38,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38,316][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.13886788487434387, acc: 0.9830508232116699)
[2025-02-13 19:31:38,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38,710][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.2556559443473816, acc: 0.9440993666648865)
[2025-02-13 19:31:38,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39,110][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.1971503347158432, acc: 0.9343065619468689)
[2025-02-13 19:31:39,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39,534][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.11191871762275696, acc: 0.9813664555549622)
[2025-02-13 19:31:39,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39,915][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.08468521386384964, acc: 0.97826087474823)
[2025-02-13 19:31:40,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40,268][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.09602658450603485, acc: 0.9805825352668762)
[2025-02-13 19:31:40,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40,655][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.15188299119472504, acc: 0.9463087320327759)
[2025-02-13 19:31:40,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41,037][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.14614862203598022, acc: 0.9731543660163879)
[2025-02-13 19:31:41,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41,426][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.07256778329610825, acc: 0.9870967864990234)
[2025-02-13 19:31:41,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41,819][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.05282524973154068, acc: 0.9894737005233765)
[2025-02-13 19:31:41,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42,232][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.08243061602115631, acc: 0.9868420958518982)
[2025-02-13 19:31:42,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42,621][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.13475194573402405, acc: 0.9797297120094299)
[2025-02-13 19:31:42,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42,996][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.13392196595668793, acc: 0.9693251252174377)
[2025-02-13 19:31:43,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43,364][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.11239992827177048, acc: 0.970588207244873)
[2025-02-13 19:31:43,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43,747][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.2099192887544632, acc: 0.9371727705001831)
[2025-02-13 19:31:43,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44,123][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.061773210763931274, acc: 0.9882352948188782)
[2025-02-13 19:31:44,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44,490][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.11185692995786667, acc: 0.9588235020637512)
[2025-02-13 19:31:44,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44,874][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.05239550396800041, acc: 0.9863945841789246)
[2025-02-13 19:31:45,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45,264][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.15411266684532166, acc: 0.9860140085220337)
[2025-02-13 19:31:45,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45,649][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.04181747883558273, acc: 0.9938271641731262)
[2025-02-13 19:31:45,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46,030][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.19439104199409485, acc: 0.949999988079071)
[2025-02-13 19:31:46,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46,397][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.1983267217874527, acc: 0.9457364082336426)
[2025-02-13 19:31:46,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46,812][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.1863396316766739, acc: 0.9520958065986633)
[2025-02-13 19:31:46,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47,202][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.13068154454231262, acc: 0.9556962251663208)
[2025-02-13 19:31:47,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47,584][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.20007693767547607, acc: 0.9437500238418579)
[2025-02-13 19:31:47,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47,972][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.3344041705131531, acc: 0.9327731132507324)
[2025-02-13 19:31:48,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48,384][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.33375313878059387, acc: 0.9318181872367859)
[2025-02-13 19:31:48,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48,794][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.26340070366859436, acc: 0.9375)
[2025-02-13 19:31:48,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49,173][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.2447417676448822, acc: 0.9304347634315491)
[2025-02-13 19:31:49,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49,563][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.20716509222984314, acc: 0.9333333373069763)
[2025-02-13 19:31:49,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49,944][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.0775415375828743, acc: 0.9876543283462524)
[2025-02-13 19:31:50,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50,340][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.06132647395133972, acc: 0.9931972622871399)
[2025-02-13 19:31:50,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50,749][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.11955507844686508, acc: 0.966292142868042)
[2025-02-13 19:31:50,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51,172][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.07880806922912598, acc: 0.9662162065505981)
[2025-02-13 19:31:51,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51,528][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.10313005745410919, acc: 0.9819819927215576)
[2025-02-13 19:31:51,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51,929][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.17836138606071472, acc: 0.9387755393981934)
[2025-02-13 19:31:52,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52,309][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.15271605551242828, acc: 0.9583333134651184)
[2025-02-13 19:31:52,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52,681][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.32483020424842834, acc: 0.9529411792755127)
[2025-02-13 19:31:52,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53,068][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.20261971652507782, acc: 0.957446813583374)
[2025-02-13 19:31:53,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53,491][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.1403619796037674, acc: 0.9433962106704712)
[2025-02-13 19:31:53,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53,890][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.1954975575208664, acc: 0.9578313231468201)
[2025-02-13 19:31:54,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54,267][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.19345980882644653, acc: 0.9632353186607361)
[2025-02-13 19:31:54,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54,684][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.15681329369544983, acc: 0.96875)
[2025-02-13 19:31:54,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55,069][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.10538260638713837, acc: 0.9756097793579102)
[2025-02-13 19:31:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55,470][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.07693013548851013, acc: 0.9677419066429138)
[2025-02-13 19:31:55,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55,822][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.21494948863983154, acc: 0.9496402740478516)
[2025-02-13 19:31:55,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56,192][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.046911053359508514, acc: 0.991304337978363)
[2025-02-13 19:31:56,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56,565][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.23959071934223175, acc: 0.9340659379959106)
[2025-02-13 19:31:56,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56,951][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.11482540518045425, acc: 0.9844961166381836)
[2025-02-13 19:31:57,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57,327][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.4000066816806793, acc: 0.8652482032775879)
[2025-02-13 19:31:57,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57,701][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.1423666775226593, acc: 0.951724112033844)
[2025-02-13 19:31:57,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58,084][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.13040436804294586, acc: 0.9793103337287903)
[2025-02-13 19:31:58,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58,453][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.09429992735385895, acc: 0.9735099077224731)
[2025-02-13 19:31:58,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58,842][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.2585410475730896, acc: 0.9437500238418579)
[2025-02-13 19:31:58,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59,233][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.13127364218235016, acc: 0.9696969985961914)
[2025-02-13 19:31:59,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59,624][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.1454416960477829, acc: 0.9621211886405945)
[2025-02-13 19:31:59,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59,991][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.40867361426353455, acc: 0.9103448390960693)
[2025-02-13 19:32:00,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00,317][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.1867101788520813, acc: 0.9684210419654846)
[2025-02-13 19:32:00,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00,668][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.3073328733444214, acc: 0.9417475461959839)
[2025-02-13 19:32:00,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01,062][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.10408040881156921, acc: 0.977011501789093)
[2025-02-13 19:32:01,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01,443][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.3778955638408661, acc: 0.9230769276618958)
[2025-02-13 19:32:01,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01,827][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.25309139490127563, acc: 0.9357143044471741)
[2025-02-13 19:32:01,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02,211][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.19256894290447235, acc: 0.9357143044471741)
[2025-02-13 19:32:02,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02,593][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.23450955748558044, acc: 0.954954981803894)
[2025-02-13 19:32:02,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02,985][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.1860538125038147, acc: 0.9640287756919861)
[2025-02-13 19:32:03,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03,380][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.3495774269104004, acc: 0.9245283007621765)
[2025-02-13 19:32:03,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03,762][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.28492268919944763, acc: 0.9459459185600281)
[2025-02-13 19:32:03,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04,138][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.11500059813261032, acc: 0.9790209531784058)
[2025-02-13 19:32:04,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04,507][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.08869647979736328, acc: 0.9716981053352356)
[2025-02-13 19:32:04,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04,877][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.0959387868642807, acc: 0.982758641242981)
[2025-02-13 19:32:05,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05,299][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.1374814510345459, acc: 0.9647058844566345)
[2025-02-13 19:32:05,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05,659][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.11354978382587433, acc: 0.9743589758872986)
[2025-02-13 19:32:05,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06,014][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.062283508479595184, acc: 0.9924812316894531)
[2025-02-13 19:32:06,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06,410][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.17907162010669708, acc: 0.9741379022598267)
[2025-02-13 19:32:06,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06,768][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.04388829693198204, acc: 1.0)
[2025-02-13 19:32:06,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07,174][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.06610926240682602, acc: 0.9867549538612366)
[2025-02-13 19:32:07,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07,567][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.18773400783538818, acc: 0.9814814925193787)
[2025-02-13 19:32:07,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07,924][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.08231721818447113, acc: 0.9916666746139526)
[2025-02-13 19:32:08,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08,314][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.10091499984264374, acc: 0.9694656729698181)
[2025-02-13 19:32:08,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08,707][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.17712821066379547, acc: 0.9466666579246521)
[2025-02-13 19:32:08,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09,069][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.13261516392230988, acc: 0.9763779640197754)
[2025-02-13 19:32:09,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09,493][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.11214887350797653, acc: 0.9587628841400146)
[2025-02-13 19:32:09,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09,888][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.1974838674068451, acc: 0.9659863710403442)
[2025-02-13 19:32:10,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10,279][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.08541149646043777, acc: 0.9734513163566589)
[2025-02-13 19:32:10,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10,676][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.06721855700016022, acc: 0.9777777791023254)
[2025-02-13 19:32:10,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11,041][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.03547004610300064, acc: 1.0)
[2025-02-13 19:32:11,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11,431][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.18911583721637726, acc: 0.9670329689979553)
[2025-02-13 19:32:11,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11,803][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.1052747294306755, acc: 0.9671052694320679)
[2025-02-13 19:32:11,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12,157][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.07184607535600662, acc: 0.9876543283462524)
[2025-02-13 19:32:12,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12,579][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.2723807096481323, acc: 0.9583333134651184)
[2025-02-13 19:32:12,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12,988][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.2945787012577057, acc: 0.9175257682800293)
[2025-02-13 19:32:13,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13,361][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.15797318518161774, acc: 0.9487179517745972)
[2025-02-13 19:32:13,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13,777][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.14765208959579468, acc: 0.9655172228813171)
[2025-02-13 19:32:13,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14,156][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.053688496351242065, acc: 1.0)
[2025-02-13 19:32:14,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14,545][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.06902103126049042, acc: 0.9829059839248657)
[2025-02-13 19:32:14,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14,922][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.1514529585838318, acc: 0.9807692170143127)
[2025-02-13 19:32:15,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15,326][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.20887687802314758, acc: 0.9509202241897583)
[2025-02-13 19:32:15,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15,654][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.3528474271297455, acc: 0.9389312863349915)
[2025-02-13 19:32:15,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16,027][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.12457691878080368, acc: 0.9772727489471436)
[2025-02-13 19:32:16,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16,433][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.2762162983417511, acc: 0.9248120188713074)
[2025-02-13 19:32:16,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16,816][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.45426514744758606, acc: 0.909604549407959)
[2025-02-13 19:32:16,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17,224][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.396151602268219, acc: 0.9047619104385376)
[2025-02-13 19:32:17,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17,602][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.14826428890228271, acc: 0.9809523820877075)
[2025-02-13 19:32:17,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18,001][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.08591951429843903, acc: 0.9927536249160767)
[2025-02-13 19:32:18,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18,388][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.12808802723884583, acc: 0.9677419066429138)
[2025-02-13 19:32:18,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18,823][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.07060060650110245, acc: 0.9819276928901672)
[2025-02-13 19:32:18,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19,226][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.18461713194847107, acc: 0.9615384340286255)
[2025-02-13 19:32:19,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19,624][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.12449855357408524, acc: 0.9694656729698181)
[2025-02-13 19:32:19,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19,994][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.09711367636919022, acc: 0.9767441749572754)
[2025-02-13 19:32:20,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20,364][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.11341801285743713, acc: 0.9718309640884399)
[2025-02-13 19:32:20,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20,738][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.08968909084796906, acc: 0.9769230484962463)
[2025-02-13 19:32:20,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21,134][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.14764131605625153, acc: 0.9640718698501587)
[2025-02-13 19:32:21,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21,523][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.3831365406513214, acc: 0.9420289993286133)
[2025-02-13 19:32:21,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21,910][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.08764441311359406, acc: 0.98591548204422)
[2025-02-13 19:32:22,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22,335][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.05029461905360222, acc: 0.98591548204422)
[2025-02-13 19:32:22,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22,762][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.0647122785449028, acc: 0.9924812316894531)
[2025-02-13 19:32:22,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23,136][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.01424302440136671, acc: 1.0)
[2025-02-13 19:32:23,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23,483][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.22234898805618286, acc: 0.9615384340286255)
[2025-02-13 19:32:23,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23,877][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.124130979180336, acc: 0.9647058844566345)
[2025-02-13 19:32:23,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24,233][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.5798291563987732, acc: 0.895652174949646)
[2025-02-13 19:32:24,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24,577][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.1520785242319107, acc: 0.9645389914512634)
[2025-02-13 19:32:24,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24,937][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.1661217212677002, acc: 0.9750000238418579)
[2025-02-13 19:32:25,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25,307][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.041228149086236954, acc: 0.9941860437393188)
[2025-02-13 19:32:25,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25,706][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.3499131202697754, acc: 0.9417475461959839)
[2025-02-13 19:32:25,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26,055][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.21338345110416412, acc: 0.9203540086746216)
[2025-02-13 19:32:26,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26,431][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.13835081458091736, acc: 0.9617834687232971)
[2025-02-13 19:32:26,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26,817][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.2294570654630661, acc: 0.9513513445854187)
[2025-02-13 19:32:26,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27,174][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.29894837737083435, acc: 0.9516128897666931)
[2025-02-13 19:32:27,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27,543][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.18248283863067627, acc: 0.9489051103591919)
[2025-02-13 19:32:27,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27,913][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.22202925384044647, acc: 0.9642857313156128)
[2025-02-13 19:32:28,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28,263][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.263519823551178, acc: 0.9235668778419495)
[2025-02-13 19:32:28,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28,670][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.29882416129112244, acc: 0.96875)
[2025-02-13 19:32:28,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29,035][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.24996371567249298, acc: 0.9387755393981934)
[2025-02-13 19:32:29,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29,451][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.21804305911064148, acc: 0.942307710647583)
[2025-02-13 19:32:29,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29,873][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.36871030926704407, acc: 0.925000011920929)
[2025-02-13 19:32:30,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30,293][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.23155906796455383, acc: 0.9259259104728699)
[2025-02-13 19:32:30,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30,720][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.2864018976688385, acc: 0.9292035102844238)
[2025-02-13 19:32:30,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31,138][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.23053810000419617, acc: 0.9395973086357117)
[2025-02-13 19:32:31,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31,532][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.3188660740852356, acc: 0.9363057613372803)
[2025-02-13 19:32:31,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31,966][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.12509936094284058, acc: 0.9634146094322205)
[2025-02-13 19:32:32,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32,359][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.14664383232593536, acc: 0.9580838084220886)
[2025-02-13 19:32:32,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32,765][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.170597642660141, acc: 0.9733333587646484)
[2025-02-13 19:32:32,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33,129][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.294178307056427, acc: 0.9238095283508301)
[2025-02-13 19:32:33,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33,491][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.20023313164710999, acc: 0.9407407641410828)
[2025-02-13 19:32:33,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33,860][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.1533571183681488, acc: 0.9580419659614563)
[2025-02-13 19:32:34,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34,231][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.11415358632802963, acc: 0.9642857313156128)
[2025-02-13 19:32:34,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34,618][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.2071666568517685, acc: 0.9496855139732361)
[2025-02-13 19:32:34,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35,001][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.17867009341716766, acc: 0.9655172228813171)
[2025-02-13 19:32:35,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35,411][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.07738002389669418, acc: 0.9918032884597778)
[2025-02-13 19:32:35,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35,810][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.21733559668064117, acc: 0.9366196990013123)
[2025-02-13 19:32:35,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36,205][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.1083933487534523, acc: 0.9797297120094299)
[2025-02-13 19:32:36,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36,589][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.04029393568634987, acc: 0.9878048896789551)
[2025-02-13 19:32:36,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36,954][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.1851808726787567, acc: 0.9588235020637512)
[2025-02-13 19:32:37,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37,372][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.17983411252498627, acc: 0.9450549483299255)
[2025-02-13 19:32:37,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37,752][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.1610325574874878, acc: 0.9465649127960205)
[2025-02-13 19:32:37,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38,147][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.16636402904987335, acc: 0.9457364082336426)
[2025-02-13 19:32:38,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38,524][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.18503132462501526, acc: 0.939130425453186)
[2025-02-13 19:32:38,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38,917][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.17418649792671204, acc: 0.9569892287254333)
[2025-02-13 19:32:39,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39,300][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.12446563690900803, acc: 0.9632353186607361)
[2025-02-13 19:32:39,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39,701][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.35043323040008545, acc: 0.925000011920929)
[2025-02-13 19:32:39,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40,082][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.14879511296749115, acc: 0.949999988079071)
[2025-02-13 19:32:40,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40,476][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.08851303160190582, acc: 0.9656862616539001)
[2025-02-13 19:32:40,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40,860][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.10753325372934341, acc: 0.9653179049491882)
[2025-02-13 19:32:41,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41,246][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.12928597629070282, acc: 0.9655172228813171)
[2025-02-13 19:32:41,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41,607][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.19142544269561768, acc: 0.9585798978805542)
[2025-02-13 19:32:41,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41,988][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.10239089280366898, acc: 0.9653465151786804)
[2025-02-13 19:32:42,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42,374][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.1548854559659958, acc: 0.9484536051750183)
[2025-02-13 19:32:42,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42,719][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.1256304532289505, acc: 0.9613259434700012)
[2025-02-13 19:32:42,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43,083][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.07964435964822769, acc: 0.9783783555030823)
[2025-02-13 19:32:43,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43,481][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.02983754128217697, acc: 1.0)
[2025-02-13 19:32:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43,847][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.10179910808801651, acc: 0.9846938848495483)
[2025-02-13 19:32:43,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44,216][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.074999138712883, acc: 0.9805825352668762)
[2025-02-13 19:32:44,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44,582][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.13588660955429077, acc: 0.9670329689979553)
[2025-02-13 19:32:44,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45,005][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.11475528031587601, acc: 0.97826087474823)
[2025-02-13 19:32:45,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45,421][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.13950587809085846, acc: 0.9602272510528564)
[2025-02-13 19:32:45,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45,829][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.08690577000379562, acc: 0.9923076629638672)
[2025-02-13 19:32:45,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46,230][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.36641445755958557, acc: 0.9044585824012756)
[2025-02-13 19:32:46,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46,608][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.18788129091262817, acc: 0.9627659320831299)
[2025-02-13 19:32:46,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46,982][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.13069717586040497, acc: 0.9625668525695801)
[2025-02-13 19:32:47,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47,357][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.22653095424175262, acc: 0.9438775777816772)
[2025-02-13 19:32:47,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47,771][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.15791670978069305, acc: 0.9523809552192688)
[2025-02-13 19:32:47,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48,182][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.14346538484096527, acc: 0.9558823704719543)
[2025-02-13 19:32:48,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48,575][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.08725661784410477, acc: 0.9842932224273682)
[2025-02-13 19:32:48,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48,958][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.18214255571365356, acc: 0.9512194991111755)
[2025-02-13 19:32:49,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49,373][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.15275144577026367, acc: 0.961904764175415)
[2025-02-13 19:32:49,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49,771][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.13054785132408142, acc: 0.9736841917037964)
[2025-02-13 19:32:49,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50,153][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.21374092996120453, acc: 0.9470198750495911)
[2025-02-13 19:32:50,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50,515][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.34790557622909546, acc: 0.931506872177124)
[2025-02-13 19:32:50,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50,926][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.39787012338638306, acc: 0.918367326259613)
[2025-02-13 19:32:51,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51,329][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.5051538944244385, acc: 0.8794326186180115)
[2025-02-13 19:32:51,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51,764][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.5071943998336792, acc: 0.8894472122192383)
[2025-02-13 19:32:51,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52,185][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.280307799577713, acc: 0.9395604133605957)
[2025-02-13 19:32:52,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52,600][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.23678885400295258, acc: 0.9367815852165222)
[2025-02-13 19:32:52,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52,979][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.2975583076477051, acc: 0.9243243336677551)
[2025-02-13 19:32:53,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53,359][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.2277785986661911, acc: 0.9470198750495911)
[2025-02-13 19:32:53,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53,752][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.2375389039516449, acc: 0.9337748289108276)
[2025-02-13 19:32:53,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54,125][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.1308591365814209, acc: 0.9523809552192688)
[2025-02-13 19:32:54,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54,501][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.13608191907405853, acc: 0.9576719403266907)
[2025-02-13 19:32:54,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54,867][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.11906246840953827, acc: 0.9779005646705627)
[2025-02-13 19:32:54,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55,210][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.18333111703395844, acc: 0.9457831382751465)
[2025-02-13 19:32:55,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55,560][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.13787966966629028, acc: 0.982758641242981)
[2025-02-13 19:32:55,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55,928][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.1962788701057434, acc: 0.9576719403266907)
[2025-02-13 19:32:56,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56,295][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.13230888545513153, acc: 0.9757575988769531)
[2025-02-13 19:32:56,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56,664][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.26757222414016724, acc: 0.9418604373931885)
[2025-02-13 19:32:56,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57,033][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.11488749831914902, acc: 0.9752475023269653)
[2025-02-13 19:32:57,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57,434][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.2440836876630783, acc: 0.935960590839386)
[2025-02-13 19:32:57,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57,824][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.09576544910669327, acc: 0.9803921580314636)
[2025-02-13 19:32:57,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58,221][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.1799992471933365, acc: 0.9379310607910156)
[2025-02-13 19:32:58,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58,585][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.38830965757369995, acc: 0.9252873659133911)
[2025-02-13 19:32:58,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58,990][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.12206947058439255, acc: 0.9784482717514038)
[2025-02-13 19:32:59,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59,383][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.31311431527137756, acc: 0.9457364082336426)
[2025-02-13 19:32:59,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59,750][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.1460759937763214, acc: 0.9675324559211731)
[2025-02-13 19:32:59,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00,129][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.34800249338150024, acc: 0.9294871687889099)
[2025-02-13 19:33:00,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00,460][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.24681782722473145, acc: 0.9375)
[2025-02-13 19:33:00,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00,855][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.2864055633544922, acc: 0.9268292784690857)
[2025-02-13 19:33:00,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01,236][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.07295700907707214, acc: 0.9855072498321533)
[2025-02-13 19:33:01,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01,636][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.22199556231498718, acc: 0.9768785834312439)
[2025-02-13 19:33:01,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01,999][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.15347926318645477, acc: 0.9623655676841736)
[2025-02-13 19:33:02,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02,357][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.37068429589271545, acc: 0.9347826242446899)
[2025-02-13 19:33:02,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02,736][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.3058421313762665, acc: 0.936170220375061)
[2025-02-13 19:33:02,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03,132][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.18301044404506683, acc: 0.9588235020637512)
[2025-02-13 19:33:03,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03,514][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.22333122789859772, acc: 0.9541666507720947)
[2025-02-13 19:33:03,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03,915][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.22541002929210663, acc: 0.9672130942344666)
[2025-02-13 19:33:04,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04,286][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.09580957889556885, acc: 0.9850746393203735)
[2025-02-13 19:33:04,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04,719][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.16030889749526978, acc: 0.9502487778663635)
[2025-02-13 19:33:04,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05,110][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.11908909678459167, acc: 0.9682539701461792)
[2025-02-13 19:33:05,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05,502][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.12389383465051651, acc: 0.96875)
[2025-02-13 19:33:05,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05,873][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.24386373162269592, acc: 0.9453551769256592)
[2025-02-13 19:33:06,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06,300][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.10006802529096603, acc: 0.9679487347602844)
[2025-02-13 19:33:06,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06,666][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.2986641824245453, acc: 0.9375)
[2025-02-13 19:33:06,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07,041][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.14775343239307404, acc: 0.959276020526886)
[2025-02-13 19:33:07,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07,424][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.2472531646490097, acc: 0.9341317415237427)
[2025-02-13 19:33:07,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07,805][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.18987871706485748, acc: 0.9599999785423279)
[2025-02-13 19:33:07,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08,193][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.13209445774555206, acc: 0.9647058844566345)
[2025-02-13 19:33:08,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08,614][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.14805428683757782, acc: 0.9673202633857727)
[2025-02-13 19:33:08,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09,031][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.07754631340503693, acc: 0.9750000238418579)
[2025-02-13 19:33:09,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09,393][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.23248660564422607, acc: 0.9489051103591919)
[2025-02-13 19:33:09,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09,802][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.1525457203388214, acc: 0.9768518805503845)
[2025-02-13 19:33:09,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10,194][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.20086072385311127, acc: 0.9488636255264282)
[2025-02-13 19:33:10,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10,586][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.336478590965271, acc: 0.8974359035491943)
[2025-02-13 19:33:10,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10,972][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.2859056890010834, acc: 0.9242424368858337)
[2025-02-13 19:33:11,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11,361][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.2620031237602234, acc: 0.9222221970558167)
[2025-02-13 19:33:11,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11,743][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.4227740466594696, acc: 0.9086021780967712)
[2025-02-13 19:33:11,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12,135][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.29824694991111755, acc: 0.9257143139839172)
[2025-02-13 19:33:12,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12,500][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.2239713817834854, acc: 0.9569892287254333)
[2025-02-13 19:33:12,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12,860][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.10459187626838684, acc: 0.9806451797485352)
[2025-02-13 19:33:13,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13,268][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.17957843840122223, acc: 0.9575757384300232)
[2025-02-13 19:33:13,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13,674][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.2109660804271698, acc: 0.9259259104728699)
[2025-02-13 19:33:13,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14,038][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.28187182545661926, acc: 0.939226508140564)
[2025-02-13 19:33:14,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14,398][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.3729008436203003, acc: 0.9244186282157898)
[2025-02-13 19:33:14,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14,812][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.27679678797721863, acc: 0.9451219439506531)
[2025-02-13 19:33:14,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15,180][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.42948949337005615, acc: 0.9067357778549194)
[2025-02-13 19:33:15,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15,534][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.21890580654144287, acc: 0.9430894255638123)
[2025-02-13 19:33:15,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15,922][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.5906054377555847, acc: 0.8658536672592163)
[2025-02-13 19:33:16,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16,282][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.18183815479278564, acc: 0.9570552110671997)
[2025-02-13 19:33:16,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16,651][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.15336784720420837, acc: 0.9595375657081604)
[2025-02-13 19:33:16,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17,031][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.203853577375412, acc: 0.946107804775238)
[2025-02-13 19:33:17,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17,434][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.1692247837781906, acc: 0.9473684430122375)
[2025-02-13 19:33:17,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17,844][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.3099916875362396, acc: 0.9466666579246521)
[2025-02-13 19:33:17,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18,240][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.3887769281864166, acc: 0.9266666769981384)
[2025-02-13 19:33:18,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18,609][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.27939772605895996, acc: 0.9285714030265808)
[2025-02-13 19:33:18,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19,007][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.3141483962535858, acc: 0.9078013896942139)
[2025-02-13 19:33:19,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19,402][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.6008960008621216, acc: 0.898876428604126)
[2025-02-13 19:33:19,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19,782][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.8550878763198853, acc: 0.8267716765403748)
[2025-02-13 19:33:19,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20,193][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.29717475175857544, acc: 0.9558823704719543)
[2025-02-13 19:33:20,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20,600][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.12555530667304993, acc: 0.9711538553237915)
[2025-02-13 19:33:20,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21,000][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.19380329549312592, acc: 0.9702970385551453)
[2025-02-13 19:33:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21,410][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.19132007658481598, acc: 0.960629940032959)
[2025-02-13 19:33:21,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21,793][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.07179256528615952, acc: 0.9919999837875366)
[2025-02-13 19:33:21,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22,203][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.3126317858695984, acc: 0.9130434989929199)
[2025-02-13 19:33:22,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22,630][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.26086950302124023, acc: 0.9603960514068604)
[2025-02-13 19:33:22,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22,976][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.10949191451072693, acc: 0.9727272987365723)
[2025-02-13 19:33:23,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23,379][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.3462370038032532, acc: 0.9135802388191223)
[2025-02-13 19:33:23,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23,762][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.21733002364635468, acc: 0.9696969985961914)
[2025-02-13 19:33:23,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24,116][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.11620991677045822, acc: 0.9750000238418579)
[2025-02-13 19:33:24,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24,551][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.3449900448322296, acc: 0.9108280539512634)
[2025-02-13 19:33:24,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24,954][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.37412458658218384, acc: 0.9074074029922485)
[2025-02-13 19:33:25,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25,356][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.34386155009269714, acc: 0.949999988079071)
[2025-02-13 19:33:25,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25,710][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.18791262805461884, acc: 0.9365079402923584)
[2025-02-13 19:33:25,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26,055][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.23598742485046387, acc: 0.9426229596138)
[2025-02-13 19:33:26,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26,434][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.4083813428878784, acc: 0.8837209343910217)
[2025-02-13 19:33:26,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26,794][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.16583940386772156, acc: 0.9444444179534912)
[2025-02-13 19:33:26,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27,183][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.33371254801750183, acc: 0.9275362491607666)
[2025-02-13 19:33:27,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27,544][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.27587538957595825, acc: 0.9375)
[2025-02-13 19:33:27,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27,956][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.11352285742759705, acc: 0.9725274443626404)
[2025-02-13 19:33:28,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28,327][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.06864036619663239, acc: 1.0)
[2025-02-13 19:33:28,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28,738][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.16959567368030548, acc: 0.9638554453849792)
[2025-02-13 19:33:28,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29,146][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.138326495885849, acc: 0.9766082167625427)
[2025-02-13 19:33:29,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29,523][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.6758878827095032, acc: 0.8809523582458496)
[2025-02-13 19:33:29,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29,934][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.19054915010929108, acc: 0.959770143032074)
[2025-02-13 19:33:30,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30,286][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.23252247273921967, acc: 0.932330846786499)
[2025-02-13 19:33:30,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30,666][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.1072167158126831, acc: 0.9704142212867737)
[2025-02-13 19:33:30,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31,067][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.170606330037117, acc: 0.9603174328804016)
[2025-02-13 19:33:31,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31,498][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.3095455765724182, acc: 0.9220778942108154)
[2025-02-13 19:33:31,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31,874][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.20975901186466217, acc: 0.9629629850387573)
[2025-02-13 19:33:32,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32,263][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.4113743305206299, acc: 0.9159663915634155)
[2025-02-13 19:33:32,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32,635][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.44258710741996765, acc: 0.9108911156654358)
[2025-02-13 19:33:32,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32,983][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.310278058052063, acc: 0.9027777910232544)
[2025-02-13 19:33:33,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33,367][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.41698697209358215, acc: 0.9021739363670349)
[2025-02-13 19:33:33,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33,711][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.22902077436447144, acc: 0.9405940771102905)
[2025-02-13 19:33:33,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34,118][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.22415149211883545, acc: 0.9537037014961243)
[2025-02-13 19:33:34,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34,514][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.1869070678949356, acc: 0.931506872177124)
[2025-02-13 19:33:34,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34,915][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.15502993762493134, acc: 0.9753086566925049)
[2025-02-13 19:33:35,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35,286][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.17624062299728394, acc: 0.9404761791229248)
[2025-02-13 19:33:35,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35,696][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.139150932431221, acc: 0.9646017551422119)
[2025-02-13 19:33:35,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36,038][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.22741059958934784, acc: 0.9407407641410828)
[2025-02-13 19:33:36,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36,404][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.19767561554908752, acc: 0.9671052694320679)
[2025-02-13 19:33:36,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36,754][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.25088801980018616, acc: 0.9551281929016113)
[2025-02-13 19:33:36,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37,194][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.20410308241844177, acc: 0.9435897469520569)
[2025-02-13 19:33:37,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37,559][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.11101555079221725, acc: 0.9714285731315613)
[2025-02-13 19:33:37,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37,931][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.05803285911679268, acc: 1.0)
[2025-02-13 19:33:38,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38,330][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.06324706971645355, acc: 1.0)
[2025-02-13 19:33:38,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38,706][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.10344278067350388, acc: 0.9793814420700073)
[2025-02-13 19:33:38,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39,098][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.10597110539674759, acc: 0.9717513918876648)
[2025-02-13 19:33:39,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39,478][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.13450872898101807, acc: 0.9770641922950745)
[2025-02-13 19:33:39,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39,939][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.11094452440738678, acc: 0.9736841917037964)
[2025-02-13 19:33:40,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40,327][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.07343762367963791, acc: 0.9797297120094299)
[2025-02-13 19:33:40,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40,706][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.06280911713838577, acc: 0.985401451587677)
[2025-02-13 19:33:40,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41,132][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.05850612744688988, acc: 0.9929577708244324)
[2025-02-13 19:33:41,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41,505][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.10844870656728745, acc: 0.9776119589805603)
[2025-02-13 19:33:41,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41,849][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.18947875499725342, acc: 0.9327731132507324)
[2025-02-13 19:33:41,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42,243][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.13212651014328003, acc: 0.9651162624359131)
[2025-02-13 19:33:42,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42,627][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.23562905192375183, acc: 0.9741935729980469)
[2025-02-13 19:33:42,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43,026][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.09257223457098007, acc: 0.9802631735801697)
[2025-02-13 19:33:43,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43,394][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.034768763929605484, acc: 0.9928571581840515)
[2025-02-13 19:33:43,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43,797][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.18459685146808624, acc: 0.9593023061752319)
[2025-02-13 19:33:43,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44,229][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.1299269050359726, acc: 0.9603174328804016)
[2025-02-13 19:33:44,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44,633][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.1264985054731369, acc: 0.9620253443717957)
[2025-02-13 19:33:44,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45,019][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.14415454864501953, acc: 0.9555555582046509)
[2025-02-13 19:33:45,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45,403][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.13465489447116852, acc: 0.9770992398262024)
[2025-02-13 19:33:45,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45,781][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.10313498228788376, acc: 0.9745222926139832)
[2025-02-13 19:33:45,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46,231][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.127994105219841, acc: 0.967391312122345)
[2025-02-13 19:33:46,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46,614][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.12777237594127655, acc: 0.96875)
[2025-02-13 19:33:46,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46,973][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.24395382404327393, acc: 0.9380530714988708)
[2025-02-13 19:33:47,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47,337][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.1028827354311943, acc: 0.9791666865348816)
[2025-02-13 19:33:47,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47,695][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.3185853362083435, acc: 0.9300699234008789)
[2025-02-13 19:33:47,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48,061][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.17465896904468536, acc: 0.954023003578186)
[2025-02-13 19:33:48,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48,438][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.27703940868377686, acc: 0.95333331823349)
[2025-02-13 19:33:48,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48,834][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.36085787415504456, acc: 0.9281437397003174)
[2025-02-13 19:33:48,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49,211][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.2576969861984253, acc: 0.9312169551849365)
[2025-02-13 19:33:49,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49,622][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.2949904501438141, acc: 0.9424460530281067)
[2025-02-13 19:33:49,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50,053][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.16269004344940186, acc: 0.9682539701461792)
[2025-02-13 19:33:50,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50,462][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.3061390817165375, acc: 0.9230769276618958)
[2025-02-13 19:33:50,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50,881][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.17330165207386017, acc: 0.9578313231468201)
[2025-02-13 19:33:51,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51,303][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.16704264283180237, acc: 0.9436619877815247)
[2025-02-13 19:33:51,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51,703][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.17748285830020905, acc: 0.9489796161651611)
[2025-02-13 19:33:51,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52,092][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.26648223400115967, acc: 0.9453551769256592)
[2025-02-13 19:33:52,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52,476][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.06903520971536636, acc: 0.9876543283462524)
[2025-02-13 19:33:52,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52,850][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.4251071810722351, acc: 0.8888888955116272)
[2025-02-13 19:33:52,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53,235][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.27165889739990234, acc: 0.930232584476471)
[2025-02-13 19:33:53,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53,633][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.2616373896598816, acc: 0.9571428298950195)
[2025-02-13 19:33:53,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54,029][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.22921060025691986, acc: 0.959770143032074)
[2025-02-13 19:33:54,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54,437][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.21867074072360992, acc: 0.9424460530281067)
[2025-02-13 19:33:54,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54,825][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.1525188535451889, acc: 0.9595375657081604)
[2025-02-13 19:33:54,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55,204][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.24458906054496765, acc: 0.9375)
[2025-02-13 19:33:55,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55,572][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.2523992955684662, acc: 0.942307710647583)
[2025-02-13 19:33:55,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55,947][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.10475027561187744, acc: 0.9837398529052734)
[2025-02-13 19:33:56,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56,332][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.2741677761077881, acc: 0.9407894611358643)
[2025-02-13 19:33:56,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56,691][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.09629581868648529, acc: 0.9808917045593262)
[2025-02-13 19:33:56,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57,051][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.39848849177360535, acc: 0.9212121367454529)
[2025-02-13 19:33:57,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57,416][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.05489229038357735, acc: 0.9918699264526367)
[2025-02-13 19:33:57,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57,787][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.15428677201271057, acc: 0.949999988079071)
[2025-02-13 19:33:57,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58,149][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.2064625322818756, acc: 0.9492753744125366)
[2025-02-13 19:33:58,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58,500][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.37760061025619507, acc: 0.9418604373931885)
[2025-02-13 19:33:58,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58,863][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.15288816392421722, acc: 0.9562841653823853)
[2025-02-13 19:33:59,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59,227][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.19572623074054718, acc: 0.9750000238418579)
[2025-02-13 19:33:59,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59,590][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.24884375929832458, acc: 0.9408602118492126)
[2025-02-13 19:33:59,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59,962][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.17097477614879608, acc: 0.9798657894134521)
[2025-02-13 19:34:00,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00,367][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.2593388557434082, acc: 0.9444444179534912)
[2025-02-13 19:34:00,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00,747][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.17290781438350677, acc: 0.96875)
[2025-02-13 19:34:00,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01,135][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.16004455089569092, acc: 0.9602272510528564)
[2025-02-13 19:34:01,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01,510][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.2568422853946686, acc: 0.9312169551849365)
[2025-02-13 19:34:01,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01,888][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.08208305388689041, acc: 0.9729729890823364)
[2025-02-13 19:34:02,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02,255][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.08718682080507278, acc: 0.9887005686759949)
[2025-02-13 19:34:02,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02,622][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.12403416633605957, acc: 0.9617834687232971)
[2025-02-13 19:34:02,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02,989][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.1726725846529007, acc: 0.9467455744743347)
[2025-02-13 19:34:03,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03,363][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.11224395781755447, acc: 0.961240291595459)
[2025-02-13 19:34:03,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03,736][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.12236259877681732, acc: 0.9661017060279846)
[2025-02-13 19:34:03,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04,127][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.14404384791851044, acc: 0.9578313231468201)
[2025-02-13 19:34:04,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04,528][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.22092723846435547, acc: 0.9432623982429504)
[2025-02-13 19:34:04,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04,883][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.07884841412305832, acc: 0.9794520735740662)
[2025-02-13 19:34:05,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05,292][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.0588284395635128, acc: 0.9759036302566528)
[2025-02-13 19:34:05,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05,666][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.09332746267318726, acc: 0.9878048896789551)
[2025-02-13 19:34:05,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06,049][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.1812090277671814, acc: 0.9608938694000244)
[2025-02-13 19:34:06,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06,433][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.47627368569374084, acc: 0.8787878751754761)
[2025-02-13 19:34:06,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06,844][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.5428644418716431, acc: 0.8835616707801819)
[2025-02-13 19:34:06,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07,218][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.24733392894268036, acc: 0.9312499761581421)
[2025-02-13 19:34:07,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07,590][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.1668236255645752, acc: 0.9555555582046509)
[2025-02-13 19:34:07,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07,986][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.4110414683818817, acc: 0.9298245906829834)
[2025-02-13 19:34:08,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08,379][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.2050396203994751, acc: 0.9567901492118835)
[2025-02-13 19:34:08,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08,781][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.5424007773399353, acc: 0.8881118893623352)
[2025-02-13 19:34:08,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09,176][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.4091617465019226, acc: 0.8863636255264282)
[2025-02-13 19:34:09,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09,563][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.388816773891449, acc: 0.9006211161613464)
[2025-02-13 19:34:09,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09,975][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.17311203479766846, acc: 0.9603174328804016)
[2025-02-13 19:34:10,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10,389][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.2543792128562927, acc: 0.9508196711540222)
[2025-02-13 19:34:10,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10,780][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.2562081813812256, acc: 0.9214285612106323)
[2025-02-13 19:34:10,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11,158][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.11973731219768524, acc: 0.971222996711731)
[2025-02-13 19:34:11,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11,531][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.14854256808757782, acc: 0.95333331823349)
[2025-02-13 19:34:11,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11,926][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.1880628913640976, acc: 0.9523809552192688)
[2025-02-13 19:34:12,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12,283][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.05715586245059967, acc: 0.987261176109314)
[2025-02-13 19:34:12,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12,681][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.20685438811779022, acc: 0.9473684430122375)
[2025-02-13 19:34:12,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13,106][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.35854241251945496, acc: 0.9182389974594116)
[2025-02-13 19:34:13,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13,497][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.2170885056257248, acc: 0.9415584206581116)
[2025-02-13 19:34:13,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13,871][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.13118745386600494, acc: 0.9466666579246521)
[2025-02-13 19:34:14,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14,263][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.24516573548316956, acc: 0.9127516746520996)
[2025-02-13 19:34:14,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14,622][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.11602339893579483, acc: 0.9736841917037964)
[2025-02-13 19:34:14,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15,006][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.15328221023082733, acc: 0.9637681245803833)
[2025-02-13 19:34:15,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15,393][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.2826118767261505, acc: 0.931034505367279)
[2025-02-13 19:34:15,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15,744][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.16718249022960663, acc: 0.948387086391449)
[2025-02-13 19:34:15,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16,116][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.14737868309020996, acc: 0.969924807548523)
[2025-02-13 19:34:16,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16,491][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.177859827876091, acc: 0.9599999785423279)
[2025-02-13 19:34:16,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16,914][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.29444870352745056, acc: 0.9367088675498962)
[2025-02-13 19:34:17,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17,305][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.2944471836090088, acc: 0.9111111164093018)
[2025-02-13 19:34:17,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17,717][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.1888635903596878, acc: 0.9520547986030579)
[2025-02-13 19:34:17,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18,110][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.08091197162866592, acc: 0.9868420958518982)
[2025-02-13 19:34:18,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18,489][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.2790188193321228, acc: 0.9214285612106323)
[2025-02-13 19:34:18,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18,859][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.14284265041351318, acc: 0.9798657894134521)
[2025-02-13 19:34:19,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19,253][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.2386535257101059, acc: 0.9436619877815247)
[2025-02-13 19:34:19,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19,615][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.20836330950260162, acc: 0.9439252614974976)
[2025-02-13 19:34:19,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20,030][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.1490689367055893, acc: 0.9478672742843628)
[2025-02-13 19:34:20,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20,415][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.18507462739944458, acc: 0.9505494236946106)
[2025-02-13 19:34:20,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20,798][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.3263917565345764, acc: 0.9127516746520996)
[2025-02-13 19:34:20,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21,188][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.2107086181640625, acc: 0.949999988079071)
[2025-02-13 19:34:21,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21,567][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.5185458660125732, acc: 0.8894736766815186)
[2025-02-13 19:34:21,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21,938][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.27869051694869995, acc: 0.929729700088501)
[2025-02-13 19:34:22,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22,350][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.2627750635147095, acc: 0.9210526347160339)
[2025-02-13 19:34:22,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22,727][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.25801101326942444, acc: 0.9242424368858337)
[2025-02-13 19:34:22,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23,124][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.1837797909975052, acc: 0.9587156176567078)
[2025-02-13 19:34:23,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23,506][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.1849544644355774, acc: 0.9462365508079529)
[2025-02-13 19:34:23,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23,924][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.23306125402450562, acc: 0.9337349534034729)
[2025-02-13 19:34:24,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:39,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:33,531][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3111, device='cuda:0') eval_epoch_loss=tensor(0.2708, device='cuda:0') eval_epoch_acc=tensor(0.9377, device='cuda:0')
[2025-02-13 19:38:33,534][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:38:33,535][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:38:33,894][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_5349_loss_0.2708382308483124/model.pt
[2025-02-13 19:38:33,903][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:38:33,904][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.2708382308483124
[2025-02-13 19:38:33,905][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9377016425132751
[2025-02-13 19:38:34,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34,309][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.42448508739471436, acc: 0.8914285898208618)
[2025-02-13 19:38:34,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34,706][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.14349909126758575, acc: 0.9529411792755127)
[2025-02-13 19:38:34,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35,078][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.2601259648799896, acc: 0.9405405521392822)
[2025-02-13 19:38:35,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35,449][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.2274029701948166, acc: 0.946107804775238)
[2025-02-13 19:38:35,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35,835][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.24481192231178284, acc: 0.939393937587738)
[2025-02-13 19:38:35,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36,240][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.2111603021621704, acc: 0.9490740895271301)
[2025-02-13 19:38:36,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36,585][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.25990644097328186, acc: 0.953125)
[2025-02-13 19:38:36,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36,980][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.267905592918396, acc: 0.9308510422706604)
[2025-02-13 19:38:37,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37,372][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.16129857301712036, acc: 0.9432623982429504)
[2025-02-13 19:38:37,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37,809][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.5433454513549805, acc: 0.8681318759918213)
[2025-02-13 19:38:37,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38,297][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.14764390885829926, acc: 0.9753694534301758)
[2025-02-13 19:38:38,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38,697][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.14564532041549683, acc: 0.9655172228813171)
[2025-02-13 19:38:38,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39,104][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.3427368700504303, acc: 0.9238578677177429)
[2025-02-13 19:38:39,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39,490][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.28174033761024475, acc: 0.9391891956329346)
[2025-02-13 19:38:39,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39,854][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.2532947361469269, acc: 0.9390243887901306)
[2025-02-13 19:38:39,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40,216][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.18210065364837646, acc: 0.9537572264671326)
[2025-02-13 19:38:40,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40,619][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.21504688262939453, acc: 0.9318181872367859)
[2025-02-13 19:38:40,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40,980][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.295502632856369, acc: 0.9701492786407471)
[2025-02-13 19:38:41,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41,369][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.24786031246185303, acc: 0.9402173757553101)
[2025-02-13 19:38:41,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41,756][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.15126517415046692, acc: 0.9534883499145508)
[2025-02-13 19:38:41,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42,150][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.26867353916168213, acc: 0.9491525292396545)
[2025-02-13 19:38:42,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42,557][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.27563369274139404, acc: 0.9415584206581116)
[2025-02-13 19:38:42,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42,932][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.40497708320617676, acc: 0.9200000166893005)
[2025-02-13 19:38:43,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43,321][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.11315524578094482, acc: 0.9622641801834106)
[2025-02-13 19:38:43,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43,687][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.1576322764158249, acc: 0.976190447807312)
[2025-02-13 19:38:43,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44,046][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.11640386283397675, acc: 0.976190447807312)
[2025-02-13 19:38:44,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44,438][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.10213744640350342, acc: 0.9844961166381836)
[2025-02-13 19:38:44,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44,817][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.08414994180202484, acc: 0.9797297120094299)
[2025-02-13 19:38:44,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45,169][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.0850379467010498, acc: 0.9876543283462524)
[2025-02-13 19:38:45,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45,540][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.056725919246673584, acc: 0.9947368502616882)
[2025-02-13 19:38:45,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45,912][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.07799556106328964, acc: 0.984375)
[2025-02-13 19:38:46,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46,325][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.10794582217931747, acc: 0.9851852059364319)
[2025-02-13 19:38:46,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46,721][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.10509859770536423, acc: 0.9661017060279846)
[2025-02-13 19:38:46,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47,137][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.07807514071464539, acc: 0.9710144996643066)
[2025-02-13 19:38:47,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47,577][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.09075913578271866, acc: 0.975806474685669)
[2025-02-13 19:38:47,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48,015][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.100165955722332, acc: 0.989847719669342)
[2025-02-13 19:38:48,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48,396][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.12363526225090027, acc: 0.9767441749572754)
[2025-02-13 19:38:48,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48,791][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.08261473476886749, acc: 0.9890109896659851)
[2025-02-13 19:38:48,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49,176][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.1554015725851059, acc: 0.9836065769195557)
[2025-02-13 19:38:49,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49,624][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.06721503287553787, acc: 0.9888268113136292)
[2025-02-13 19:38:49,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50,067][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.07876282930374146, acc: 0.9795918464660645)
[2025-02-13 19:38:50,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50,529][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.21274514496326447, acc: 0.9622641801834106)
[2025-02-13 19:38:50,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50,919][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.21162927150726318, acc: 0.9424460530281067)
[2025-02-13 19:38:51,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51,302][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.23043641448020935, acc: 0.9488636255264282)
[2025-02-13 19:38:51,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51,783][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.21757546067237854, acc: 0.9333333373069763)
[2025-02-13 19:38:51,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52,277][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.4308054745197296, acc: 0.875)
[2025-02-13 19:38:52,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52,737][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.06990452855825424, acc: 0.9932432174682617)
[2025-02-13 19:38:52,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53,220][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.1314421147108078, acc: 0.9736841917037964)
[2025-02-13 19:38:53,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53,639][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.10219988971948624, acc: 0.9716312289237976)
[2025-02-13 19:38:53,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54,034][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.1563108265399933, acc: 0.9698795080184937)
[2025-02-13 19:38:54,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54,455][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.1458316296339035, acc: 0.9485714435577393)
[2025-02-13 19:38:54,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54,861][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.3085123300552368, acc: 0.9379310607910156)
[2025-02-13 19:38:55,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55,238][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.18801186978816986, acc: 0.9652777910232544)
[2025-02-13 19:38:55,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55,604][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.15425582230091095, acc: 0.9520547986030579)
[2025-02-13 19:38:55,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55,950][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.26166418194770813, acc: 0.9262295365333557)
[2025-02-13 19:38:56,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56,355][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.3489205837249756, acc: 0.9194630980491638)
[2025-02-13 19:38:56,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56,775][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.13613516092300415, acc: 0.9826086759567261)
[2025-02-13 19:38:56,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57,222][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.2465466856956482, acc: 0.9279279112815857)
[2025-02-13 19:38:57,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57,637][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.1546383798122406, acc: 0.9780219793319702)
[2025-02-13 19:38:57,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58,060][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.3423722982406616, acc: 0.9078013896942139)
[2025-02-13 19:38:58,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58,460][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.33026123046875, acc: 0.9440559148788452)
[2025-02-13 19:38:58,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58,837][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.0902426466345787, acc: 0.9819276928901672)
[2025-02-13 19:38:58,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59,186][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.21313828229904175, acc: 0.9507042169570923)
[2025-02-13 19:38:59,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59,590][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.3555903732776642, acc: 0.9557521939277649)
[2025-02-13 19:38:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59,989][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.32501399517059326, acc: 0.9027777910232544)
[2025-02-13 19:39:00,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00,377][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.5574327111244202, acc: 0.8622754216194153)
[2025-02-13 19:39:00,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00,824][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.3256462514400482, acc: 0.9285714030265808)
[2025-02-13 19:39:00,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01,263][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.34101977944374084, acc: 0.9090909361839294)
[2025-02-13 19:39:01,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01,648][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.28480905294418335, acc: 0.9102563858032227)
[2025-02-13 19:39:01,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02,014][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.37724217772483826, acc: 0.9097222089767456)
[2025-02-13 19:39:02,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02,377][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.7540991306304932, acc: 0.8175182342529297)
[2025-02-13 19:39:02,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02,758][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.6094194054603577, acc: 0.8421052694320679)
[2025-02-13 19:39:02,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03,136][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.5671136975288391, acc: 0.8687499761581421)
[2025-02-13 19:39:03,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03,533][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.2298279106616974, acc: 0.9513888955116272)
[2025-02-13 19:39:03,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03,891][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.20849406719207764, acc: 0.949999988079071)
[2025-02-13 19:39:04,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04,311][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.2785990536212921, acc: 0.9307692050933838)
[2025-02-13 19:39:04,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04,706][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.18697981536388397, acc: 0.9453125)
[2025-02-13 19:39:04,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05,124][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.2109973281621933, acc: 0.9754098653793335)
[2025-02-13 19:39:05,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05,538][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.14911456406116486, acc: 0.9477124214172363)
[2025-02-13 19:39:05,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05,914][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.1965101957321167, acc: 0.9448275566101074)
[2025-02-13 19:39:06,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06,352][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.27990421652793884, acc: 0.926174521446228)
[2025-02-13 19:39:06,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06,768][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.26052212715148926, acc: 0.9545454382896423)
[2025-02-13 19:39:06,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07,149][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.26868483424186707, acc: 0.9172932505607605)
[2025-02-13 19:39:07,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07,536][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.150973379611969, acc: 0.9453125)
[2025-02-13 19:39:07,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07,943][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.2673752009868622, acc: 0.924369752407074)
[2025-02-13 19:39:08,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08,334][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.283704549074173, acc: 0.9596773982048035)
[2025-02-13 19:39:08,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08,694][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.21692052483558655, acc: 0.9520000219345093)
[2025-02-13 19:39:08,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09,043][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.18244245648384094, acc: 0.9677419066429138)
[2025-02-13 19:39:09,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09,443][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.21267150342464447, acc: 0.9548872113227844)
[2025-02-13 19:39:09,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09,887][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.2799150049686432, acc: 0.9111111164093018)
[2025-02-13 19:39:10,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10,310][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.43315577507019043, acc: 0.8787878751754761)
[2025-02-13 19:39:10,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10,699][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.27878430485725403, acc: 0.9166666865348816)
[2025-02-13 19:39:10,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11,088][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.33643460273742676, acc: 0.908108115196228)
[2025-02-13 19:39:11,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11,497][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.16150815784931183, acc: 0.9583333134651184)
[2025-02-13 19:39:11,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11,838][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.11650611460208893, acc: 0.9817073345184326)
[2025-02-13 19:39:11,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12,217][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.1707010418176651, acc: 0.9485294222831726)
[2025-02-13 19:39:12,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12,585][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.25223618745803833, acc: 0.9338235259056091)
[2025-02-13 19:39:12,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12,956][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.13153792917728424, acc: 0.9509202241897583)
[2025-02-13 19:39:13,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13,313][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.20923489332199097, acc: 0.9383561611175537)
[2025-02-13 19:39:13,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13,692][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.2370183765888214, acc: 0.9450549483299255)
[2025-02-13 19:39:13,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14,048][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.2772113084793091, acc: 0.9344262480735779)
[2025-02-13 19:39:14,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14,460][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.2183680236339569, acc: 0.9555555582046509)
[2025-02-13 19:39:14,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14,866][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.1099839061498642, acc: 0.976331353187561)
[2025-02-13 19:39:15,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15,235][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.09765543788671494, acc: 0.9822485446929932)
[2025-02-13 19:39:15,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15,600][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.2793828248977661, acc: 0.949999988079071)
[2025-02-13 19:39:15,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15,972][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.19235500693321228, acc: 0.967391312122345)
[2025-02-13 19:39:16,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16,343][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.19146877527236938, acc: 0.9572192430496216)
[2025-02-13 19:39:16,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16,716][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.1969859004020691, acc: 0.9408602118492126)
[2025-02-13 19:39:16,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17,138][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.206441268324852, acc: 0.9425287246704102)
[2025-02-13 19:39:17,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17,512][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.13684777915477753, acc: 0.9558011293411255)
[2025-02-13 19:39:17,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17,862][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.050804611295461655, acc: 0.9815950989723206)
[2025-02-13 19:39:17,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18,223][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.12245754152536392, acc: 0.9526627063751221)
[2025-02-13 19:39:18,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18,631][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.3455384373664856, acc: 0.9174311757087708)
[2025-02-13 19:39:18,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19,028][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.09344805777072906, acc: 0.9754601120948792)
[2025-02-13 19:39:19,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19,417][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.258769154548645, acc: 0.9320987462997437)
[2025-02-13 19:39:19,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19,808][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.2873660624027252, acc: 0.9234972596168518)
[2025-02-13 19:39:19,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20,197][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.11009404808282852, acc: 0.987500011920929)
[2025-02-13 19:39:20,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20,569][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.12181242555379868, acc: 0.9502487778663635)
[2025-02-13 19:39:20,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20,954][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.08959642797708511, acc: 0.9751243591308594)
[2025-02-13 19:39:21,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21,320][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.0855356752872467, acc: 0.9738219976425171)
[2025-02-13 19:39:21,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21,764][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.2051302045583725, acc: 0.9417989253997803)
[2025-02-13 19:39:21,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22,174][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.06151224672794342, acc: 0.9800994992256165)
[2025-02-13 19:39:22,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22,600][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.0599510632455349, acc: 0.9941176176071167)
[2025-02-13 19:39:22,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22,989][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.10933871567249298, acc: 0.9888268113136292)
[2025-02-13 19:39:23,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23,374][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.05683675408363342, acc: 0.9898989796638489)
[2025-02-13 19:39:23,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23,762][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.13111767172813416, acc: 0.9818181991577148)
[2025-02-13 19:39:23,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24,153][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.08001081645488739, acc: 0.9814814925193787)
[2025-02-13 19:39:24,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24,542][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.046270180493593216, acc: 0.9870129823684692)
[2025-02-13 19:39:24,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24,932][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.061900537461042404, acc: 0.9795918464660645)
[2025-02-13 19:39:25,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25,357][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.14385537803173065, acc: 0.970370352268219)
[2025-02-13 19:39:25,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25,743][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.336087167263031, acc: 0.912162184715271)
[2025-02-13 19:39:25,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26,151][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.22126011550426483, acc: 0.9345238208770752)
[2025-02-13 19:39:26,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26,561][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.17380771040916443, acc: 0.9672130942344666)
[2025-02-13 19:39:26,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27,183][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.30821189284324646, acc: 0.9432989954948425)
[2025-02-13 19:39:27,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27,630][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.236319899559021, acc: 0.903030276298523)
[2025-02-13 19:39:27,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28,034][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.22852057218551636, acc: 0.9197530746459961)
[2025-02-13 19:39:28,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28,416][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.336797833442688, acc: 0.9112903475761414)
[2025-02-13 19:39:28,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28,834][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.41457951068878174, acc: 0.8863636255264282)
[2025-02-13 19:39:28,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29,231][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.19293312728405, acc: 0.9455782175064087)
[2025-02-13 19:39:29,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29,645][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.3038322627544403, acc: 0.910179615020752)
[2025-02-13 19:39:29,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30,042][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.5716214776039124, acc: 0.8658536672592163)
[2025-02-13 19:39:30,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30,419][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.19023415446281433, acc: 0.939393937587738)
[2025-02-13 19:39:30,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30,788][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.16544973850250244, acc: 0.9590643048286438)
[2025-02-13 19:39:30,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31,198][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.3422669768333435, acc: 0.9328358173370361)
[2025-02-13 19:39:31,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31,612][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.34420791268348694, acc: 0.90625)
[2025-02-13 19:39:31,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32,005][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.18708369135856628, acc: 0.9685534834861755)
[2025-02-13 19:39:32,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32,452][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.22206076979637146, acc: 0.932330846786499)
[2025-02-13 19:39:32,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32,801][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.35682961344718933, acc: 0.9365079402923584)
[2025-02-13 19:39:32,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33,192][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.32455170154571533, acc: 0.9117646813392639)
[2025-02-13 19:39:33,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33,569][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.3026478588581085, acc: 0.9308510422706604)
[2025-02-13 19:39:33,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34,007][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.3067719042301178, acc: 0.9009009003639221)
[2025-02-13 19:39:34,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34,395][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.26773178577423096, acc: 0.9430052042007446)
[2025-02-13 19:39:34,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34,789][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.2442900538444519, acc: 0.9444444179534912)
[2025-02-13 19:39:34,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35,171][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.1719958484172821, acc: 0.9604519605636597)
[2025-02-13 19:39:35,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35,543][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.5383158922195435, acc: 0.892307698726654)
[2025-02-13 19:39:35,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35,923][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.3249674141407013, acc: 0.9036144614219666)
[2025-02-13 19:39:36,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36,305][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.4681874215602875, acc: 0.8636363744735718)
[2025-02-13 19:39:36,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36,700][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.35506191849708557, acc: 0.9040403962135315)
[2025-02-13 19:39:36,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37,062][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.26265037059783936, acc: 0.9181286692619324)
[2025-02-13 19:39:37,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37,432][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.30277058482170105, acc: 0.9425837397575378)
[2025-02-13 19:39:37,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37,811][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.31623557209968567, acc: 0.900473952293396)
[2025-02-13 19:39:37,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38,193][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.28203466534614563, acc: 0.9247311949729919)
[2025-02-13 19:39:38,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38,579][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.34755024313926697, acc: 0.9126213788986206)
[2025-02-13 19:39:38,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39,004][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.36882826685905457, acc: 0.9175823926925659)
[2025-02-13 19:39:39,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39,401][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.3473077714443207, acc: 0.8875739574432373)
[2025-02-13 19:39:39,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39,793][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.4046972393989563, acc: 0.8935185074806213)
[2025-02-13 19:39:39,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40,176][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.2990523874759674, acc: 0.9285714030265808)
[2025-02-13 19:39:40,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40,565][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.1576480269432068, acc: 0.9655172228813171)
[2025-02-13 19:39:40,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40,929][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.18958090245723724, acc: 0.9553072452545166)
[2025-02-13 19:39:41,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41,344][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.26091697812080383, acc: 0.9333333373069763)
[2025-02-13 19:39:41,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41,725][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.17347507178783417, acc: 0.9629629850387573)
[2025-02-13 19:39:41,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42,106][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.19637858867645264, acc: 0.9431279897689819)
[2025-02-13 19:39:42,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42,522][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.2717262804508209, acc: 0.9162303805351257)
[2025-02-13 19:39:42,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42,934][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.3803979754447937, acc: 0.8795811533927917)
[2025-02-13 19:39:43,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43,308][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.32406747341156006, acc: 0.9230769276618958)
[2025-02-13 19:39:43,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43,702][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.33874621987342834, acc: 0.8860759735107422)
[2025-02-13 19:39:43,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44,113][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.19745701551437378, acc: 0.9451219439506531)
[2025-02-13 19:39:44,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44,482][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.16714191436767578, acc: 0.9503105878829956)
[2025-02-13 19:39:44,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44,864][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.17140847444534302, acc: 0.9650349617004395)
[2025-02-13 19:39:45,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45,280][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.19356034696102142, acc: 0.9444444179534912)
[2025-02-13 19:39:45,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45,702][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.18383580446243286, acc: 0.9590908885002136)
[2025-02-13 19:39:45,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46,140][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.16017669439315796, acc: 0.9659863710403442)
[2025-02-13 19:39:46,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46,561][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.1388210952281952, acc: 0.9638554453849792)
[2025-02-13 19:39:46,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46,979][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.2925698757171631, acc: 0.9378530979156494)
[2025-02-13 19:39:47,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47,419][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.07452470809221268, acc: 0.9888888597488403)
[2025-02-13 19:39:47,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47,893][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.28691181540489197, acc: 0.932584285736084)
[2025-02-13 19:39:48,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48,335][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.17821291089057922, acc: 0.9740932583808899)
[2025-02-13 19:39:48,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48,752][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.1503438651561737, acc: 0.9672130942344666)
[2025-02-13 19:39:48,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49,172][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.26261791586875916, acc: 0.9482758641242981)
[2025-02-13 19:39:49,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49,583][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.18775606155395508, acc: 0.9738219976425171)
[2025-02-13 19:39:49,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50,033][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.16917754709720612, acc: 0.9672130942344666)
[2025-02-13 19:39:50,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50,442][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.247815802693367, acc: 0.9337349534034729)
[2025-02-13 19:39:50,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50,875][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.1273401826620102, acc: 0.9534883499145508)
[2025-02-13 19:39:51,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51,306][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.07697396725416183, acc: 0.9806451797485352)
[2025-02-13 19:39:51,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51,765][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.15252292156219482, acc: 0.9839572310447693)
[2025-02-13 19:39:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52,205][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.07889025658369064, acc: 0.9890710115432739)
[2025-02-13 19:39:52,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52,624][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.16929404437541962, acc: 0.9583333134651184)
[2025-02-13 19:39:52,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53,015][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.08942343294620514, acc: 0.9880239367485046)
[2025-02-13 19:39:53,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53,389][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.1466776430606842, acc: 0.9723756909370422)
[2025-02-13 19:39:53,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53,755][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.1453738659620285, acc: 0.970588207244873)
[2025-02-13 19:39:53,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54,142][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.16393066942691803, acc: 0.9731543660163879)
[2025-02-13 19:39:54,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54,613][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.11381924897432327, acc: 0.983146071434021)
[2025-02-13 19:39:54,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55,030][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.44273272156715393, acc: 0.9290322661399841)
[2025-02-13 19:39:55,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55,436][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.10537785291671753, acc: 0.9693251252174377)
[2025-02-13 19:39:55,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55,846][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.2095150649547577, acc: 0.9585798978805542)
[2025-02-13 19:39:56,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56,296][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.10104541480541229, acc: 0.9808917045593262)
[2025-02-13 19:39:56,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56,660][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.2208775281906128, acc: 0.9464285969734192)
[2025-02-13 19:39:56,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57,032][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.3005477786064148, acc: 0.9385474920272827)
[2025-02-13 19:39:57,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57,408][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.19488832354545593, acc: 0.95652174949646)
[2025-02-13 19:39:57,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57,784][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.16281874477863312, acc: 0.9611111283302307)
[2025-02-13 19:39:57,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58,172][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.2782856523990631, acc: 0.9553072452545166)
[2025-02-13 19:39:58,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58,630][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.3522748351097107, acc: 0.9111111164093018)
[2025-02-13 19:39:58,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59,006][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.295261412858963, acc: 0.9590163826942444)
[2025-02-13 19:39:59,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59,410][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.29179951548576355, acc: 0.935251772403717)
[2025-02-13 19:39:59,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59,831][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.13649755716323853, acc: 0.9652777910232544)
[2025-02-13 19:39:59,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00,249][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.1290658712387085, acc: 0.96875)
[2025-02-13 19:40:00,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00,663][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.054678644984960556, acc: 0.9866666793823242)
[2025-02-13 19:40:00,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01,120][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.06921415030956268, acc: 0.989847719669342)
[2025-02-13 19:40:01,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01,540][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.06723500788211823, acc: 0.9850746393203735)
[2025-02-13 19:40:01,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01,901][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.06717096269130707, acc: 0.9890109896659851)
[2025-02-13 19:40:02,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02,292][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.051929280161857605, acc: 0.9947916865348816)
[2025-02-13 19:40:02,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02,681][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.07820289582014084, acc: 0.9728260636329651)
[2025-02-13 19:40:02,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03,055][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.10320315510034561, acc: 0.9810126423835754)
[2025-02-13 19:40:03,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03,448][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.15774564445018768, acc: 0.9693251252174377)
[2025-02-13 19:40:03,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03,885][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.10533208400011063, acc: 0.9803921580314636)
[2025-02-13 19:40:04,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04,261][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.10753685981035233, acc: 0.9791666865348816)
[2025-02-13 19:40:04,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04,638][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.22930091619491577, acc: 0.9599999785423279)
[2025-02-13 19:40:04,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05,023][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.07216598838567734, acc: 0.9940476417541504)
[2025-02-13 19:40:05,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05,421][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.2843111455440521, acc: 0.9333333373069763)
[2025-02-13 19:40:05,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05,818][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.2715720534324646, acc: 0.9303797483444214)
[2025-02-13 19:40:05,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06,231][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.22824199497699738, acc: 0.9386503100395203)
[2025-02-13 19:40:06,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06,623][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.09222768247127533, acc: 0.9833333492279053)
[2025-02-13 19:40:06,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07,023][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.15401121973991394, acc: 0.9696969985961914)
[2025-02-13 19:40:07,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07,438][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.12315832078456879, acc: 0.9723756909370422)
[2025-02-13 19:40:07,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07,840][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.08718711882829666, acc: 0.9748743772506714)
[2025-02-13 19:40:07,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08,215][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.16070245206356049, acc: 0.9556962251663208)
[2025-02-13 19:40:08,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08,631][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.16337822377681732, acc: 0.9609755873680115)
[2025-02-13 19:40:08,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09,085][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.06266815960407257, acc: 1.0)
[2025-02-13 19:40:09,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09,516][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.11207632720470428, acc: 0.9714285731315613)
[2025-02-13 19:40:09,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09,956][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.06369554251432419, acc: 0.9938271641731262)
[2025-02-13 19:40:10,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10,372][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.22438029944896698, acc: 0.9503105878829956)
[2025-02-13 19:40:10,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10,743][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.16152897477149963, acc: 0.9595375657081604)
[2025-02-13 19:40:10,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11,188][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.07272114604711533, acc: 0.9826589822769165)
[2025-02-13 19:40:11,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11,623][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.09233979880809784, acc: 0.9941860437393188)
[2025-02-13 19:40:11,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12,061][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.13421465456485748, acc: 0.9619565010070801)
[2025-02-13 19:40:12,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12,460][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.08411381393671036, acc: 0.9772727489471436)
[2025-02-13 19:40:12,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12,865][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.11387834697961807, acc: 0.9772727489471436)
[2025-02-13 19:40:12,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13,239][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.07087439298629761, acc: 0.9820359349250793)
[2025-02-13 19:40:13,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13,624][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.15320947766304016, acc: 0.9638554453849792)
[2025-02-13 19:40:13,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14,089][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.16533297300338745, acc: 0.9653179049491882)
[2025-02-13 19:40:14,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14,564][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.2473776638507843, acc: 0.9455782175064087)
[2025-02-13 19:40:14,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14,971][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.13485093414783478, acc: 0.9642857313156128)
[2025-02-13 19:40:15,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15,307][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.11840871721506119, acc: 0.9618320465087891)
[2025-02-13 19:40:15,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15,707][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.2512681484222412, acc: 0.9295774698257446)
[2025-02-13 19:40:15,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16,125][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.227101668715477, acc: 0.9426229596138)
[2025-02-13 19:40:16,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16,537][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.1808471828699112, acc: 0.9593495726585388)
[2025-02-13 19:40:16,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16,938][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.19565895199775696, acc: 0.9448819160461426)
[2025-02-13 19:40:17,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17,359][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.2751028835773468, acc: 0.9532710313796997)
[2025-02-13 19:40:17,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17,723][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.08021552860736847, acc: 0.9784172773361206)
[2025-02-13 19:40:17,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18,191][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.14097820222377777, acc: 0.9767441749572754)
[2025-02-13 19:40:18,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18,646][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.21080507338047028, acc: 0.9416666626930237)
[2025-02-13 19:40:18,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19,007][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.06637288630008698, acc: 0.9818181991577148)
[2025-02-13 19:40:19,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19,447][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.16524365544319153, acc: 0.9626865386962891)
[2025-02-13 19:40:19,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19,845][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.24966011941432953, acc: 0.9436619877815247)
[2025-02-13 19:40:19,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20,234][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.23968885838985443, acc: 0.9285714030265808)
[2025-02-13 19:40:20,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20,639][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.2565244436264038, acc: 0.9463087320327759)
[2025-02-13 19:40:20,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21,078][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.12171825766563416, acc: 0.9809523820877075)
[2025-02-13 19:40:21,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21,496][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.09319397062063217, acc: 0.9851852059364319)
[2025-02-13 19:40:21,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21,903][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.11289755254983902, acc: 0.9689922332763672)
[2025-02-13 19:40:22,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22,308][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.18536114692687988, acc: 0.9338235259056091)
[2025-02-13 19:40:22,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22,687][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.1918015033006668, acc: 0.9663865566253662)
[2025-02-13 19:40:22,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23,140][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.13394705951213837, acc: 0.9652174115180969)
[2025-02-13 19:40:23,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23,584][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.15129241347312927, acc: 0.9419354796409607)
[2025-02-13 19:40:23,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24,037][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.20617151260375977, acc: 0.9572192430496216)
[2025-02-13 19:40:24,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24,453][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.23886793851852417, acc: 0.9444444179534912)
[2025-02-13 19:40:24,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24,879][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.13557882606983185, acc: 0.9738562107086182)
[2025-02-13 19:40:25,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25,247][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.1105601117014885, acc: 0.970588207244873)
[2025-02-13 19:40:25,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25,656][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.11809060722589493, acc: 0.9655172228813171)
[2025-02-13 19:40:25,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26,094][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.4704347550868988, acc: 0.8999999761581421)
[2025-02-13 19:40:26,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26,515][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.1894114762544632, acc: 0.9642857313156128)
[2025-02-13 19:40:26,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26,890][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.07552804797887802, acc: 0.9836065769195557)
[2025-02-13 19:40:27,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27,328][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.25361618399620056, acc: 0.9607843160629272)
[2025-02-13 19:40:27,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27,700][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.16122981905937195, acc: 0.9430894255638123)
[2025-02-13 19:40:27,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28,076][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.17731262743473053, acc: 0.9711538553237915)
[2025-02-13 19:40:28,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28,466][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.11452214419841766, acc: 0.9821428656578064)
[2025-02-13 19:40:28,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28,898][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.140997514128685, acc: 0.9646017551422119)
[2025-02-13 19:40:29,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29,301][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.10380801558494568, acc: 0.9609375)
[2025-02-13 19:40:29,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29,707][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.13465939462184906, acc: 0.9615384340286255)
[2025-02-13 19:40:29,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30,119][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.15242727100849152, acc: 0.9545454382896423)
[2025-02-13 19:40:30,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30,504][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.0962371751666069, acc: 0.9736841917037964)
[2025-02-13 19:40:30,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30,881][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.14428545534610748, acc: 0.9545454382896423)
[2025-02-13 19:40:31,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31,308][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.12024323642253876, acc: 0.978723406791687)
[2025-02-13 19:40:31,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31,695][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.18707290291786194, acc: 0.966292142868042)
[2025-02-13 19:40:31,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32,068][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.20306271314620972, acc: 0.9510489702224731)
[2025-02-13 19:40:32,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32,484][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.24019823968410492, acc: 0.9368420839309692)
[2025-02-13 19:40:32,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32,876][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.15773612260818481, acc: 0.9809523820877075)
[2025-02-13 19:40:33,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33,307][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.12265376001596451, acc: 0.9652777910232544)
[2025-02-13 19:40:33,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33,698][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.15786020457744598, acc: 0.9357143044471741)
[2025-02-13 19:40:33,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34,097][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.1825462281703949, acc: 0.9513888955116272)
[2025-02-13 19:40:34,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34,467][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.22682543098926544, acc: 0.9466666579246521)
[2025-02-13 19:40:34,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34,832][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.29924994707107544, acc: 0.9054054021835327)
[2025-02-13 19:40:34,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35,199][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.2178449034690857, acc: 0.9274193644523621)
[2025-02-13 19:40:35,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35,575][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.12670141458511353, acc: 0.9727891087532043)
[2025-02-13 19:40:35,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35,976][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.3210049867630005, acc: 0.9208633303642273)
[2025-02-13 19:40:36,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36,387][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.2764189541339874, acc: 0.8961039185523987)
[2025-02-13 19:40:36,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36,764][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.10472548753023148, acc: 0.96875)
[2025-02-13 19:40:36,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37,152][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.207218736410141, acc: 0.9439252614974976)
[2025-02-13 19:40:37,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37,556][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.16713224351406097, acc: 0.9508196711540222)
[2025-02-13 19:40:37,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37,955][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.36442551016807556, acc: 0.9072847962379456)
[2025-02-13 19:40:38,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38,376][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.21329087018966675, acc: 0.9477124214172363)
[2025-02-13 19:40:38,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38,782][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.1419752836227417, acc: 0.949999988079071)
[2025-02-13 19:40:38,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39,182][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.16850745677947998, acc: 0.953125)
[2025-02-13 19:40:39,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39,540][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.14550702273845673, acc: 0.9663865566253662)
[2025-02-13 19:40:39,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39,957][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.14992870390415192, acc: 0.9568345546722412)
[2025-02-13 19:40:40,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40,398][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.06693437695503235, acc: 0.9855072498321533)
[2025-02-13 19:40:40,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40,788][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.3201638460159302, acc: 0.9383561611175537)
[2025-02-13 19:40:40,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41,159][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.10336898267269135, acc: 0.9795918464660645)
[2025-02-13 19:40:41,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41,530][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.18501797318458557, acc: 0.9738562107086182)
[2025-02-13 19:40:41,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41,955][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.20517022907733917, acc: 0.9674796462059021)
[2025-02-13 19:40:42,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42,363][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.23536482453346252, acc: 0.9555555582046509)
[2025-02-13 19:40:42,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42,788][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.18441033363342285, acc: 0.9490445852279663)
[2025-02-13 19:40:42,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43,165][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.08901463449001312, acc: 0.9841269850730896)
[2025-02-13 19:40:43,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43,606][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.1794627159833908, acc: 0.966292142868042)
[2025-02-13 19:40:43,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43,987][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.1364159882068634, acc: 0.9777777791023254)
[2025-02-13 19:40:44,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44,380][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.14326190948486328, acc: 0.965753436088562)
[2025-02-13 19:40:44,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44,768][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.20976829528808594, acc: 0.9455782175064087)
[2025-02-13 19:40:44,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45,186][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.3437223434448242, acc: 0.9066666960716248)
[2025-02-13 19:40:45,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45,633][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.38105836510658264, acc: 0.8793103694915771)
[2025-02-13 19:40:45,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46,026][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.4729146957397461, acc: 0.8901098966598511)
[2025-02-13 19:40:46,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46,422][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.1800239235162735, acc: 0.9506173133850098)
[2025-02-13 19:40:46,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46,809][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.518086850643158, acc: 0.892405092716217)
[2025-02-13 19:40:46,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47,200][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.5560427904129028, acc: 0.8695651888847351)
[2025-02-13 19:40:47,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47,582][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.40873658657073975, acc: 0.903030276298523)
[2025-02-13 19:40:47,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47,973][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.2635428309440613, acc: 0.9337349534034729)
[2025-02-13 19:40:48,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48,359][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.26665207743644714, acc: 0.9444444179534912)
[2025-02-13 19:40:48,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48,750][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.5534080862998962, acc: 0.9024389982223511)
[2025-02-13 19:40:48,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49,122][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.37276971340179443, acc: 0.9055117964744568)
[2025-02-13 19:40:49,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49,530][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.21736016869544983, acc: 0.9424460530281067)
[2025-02-13 19:40:49,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49,887][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.36350953578948975, acc: 0.9090909361839294)
[2025-02-13 19:40:50,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50,282][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.3801926076412201, acc: 0.9111111164093018)
[2025-02-13 19:40:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50,643][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.20324918627738953, acc: 0.9634146094322205)
[2025-02-13 19:40:50,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51,058][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.18172989785671234, acc: 0.9278350472450256)
[2025-02-13 19:40:51,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51,478][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.2990894317626953, acc: 0.9289617538452148)
[2025-02-13 19:40:51,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51,858][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.20857268571853638, acc: 0.9428571462631226)
[2025-02-13 19:40:51,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52,217][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.04375534504652023, acc: 0.9811320900917053)
[2025-02-13 19:40:52,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52,545][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.1151251345872879, acc: 0.9709302186965942)
[2025-02-13 19:40:52,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52,919][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.10768492519855499, acc: 0.9693251252174377)
[2025-02-13 19:40:53,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53,347][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.11846722662448883, acc: 0.9752475023269653)
[2025-02-13 19:40:53,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53,728][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.18332639336585999, acc: 0.9696969985961914)
[2025-02-13 19:40:53,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54,084][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.13597916066646576, acc: 0.9715909361839294)
[2025-02-13 19:40:54,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54,469][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.27185648679733276, acc: 0.9482758641242981)
[2025-02-13 19:40:54,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54,848][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.1636466532945633, acc: 0.9681528806686401)
[2025-02-13 19:40:55,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55,323][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.13181667029857635, acc: 0.9675675630569458)
[2025-02-13 19:40:55,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55,764][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.14737378060817719, acc: 0.9767441749572754)
[2025-02-13 19:40:55,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56,175][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.06846829503774643, acc: 0.9874213933944702)
[2025-02-13 19:40:56,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56,543][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.2002575546503067, acc: 0.9529411792755127)
[2025-02-13 19:40:56,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56,948][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.07489807903766632, acc: 0.9852941036224365)
[2025-02-13 19:40:57,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57,359][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.1561720073223114, acc: 0.9603960514068604)
[2025-02-13 19:40:57,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57,747][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.19734139740467072, acc: 0.9599999785423279)
[2025-02-13 19:40:57,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58,149][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.0417436882853508, acc: 0.9935897588729858)
[2025-02-13 19:40:58,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58,528][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.24912039935588837, acc: 0.95652174949646)
[2025-02-13 19:40:58,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58,930][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.178156316280365, acc: 0.954285740852356)
[2025-02-13 19:40:59,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59,324][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.25340619683265686, acc: 0.9462365508079529)
[2025-02-13 19:40:59,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59,724][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.09032364189624786, acc: 0.975806474685669)
[2025-02-13 19:40:59,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00,161][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.11276896297931671, acc: 0.983146071434021)
[2025-02-13 19:41:00,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00,546][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.06236867979168892, acc: 0.989130437374115)
[2025-02-13 19:41:00,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00,950][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.34107422828674316, acc: 0.9411764740943909)
[2025-02-13 19:41:01,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01,338][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.44907325506210327, acc: 0.9027777910232544)
[2025-02-13 19:41:01,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01,740][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.3580833077430725, acc: 0.9181286692619324)
[2025-02-13 19:41:01,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02,103][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.15472348034381866, acc: 0.9539473652839661)
[2025-02-13 19:41:02,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02,529][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.22188256680965424, acc: 0.9235293865203857)
[2025-02-13 19:41:02,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02,921][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.30122271180152893, acc: 0.9230769276618958)
[2025-02-13 19:41:03,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03,284][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.25023216009140015, acc: 0.9430052042007446)
[2025-02-13 19:41:03,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03,620][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.10243266075849533, acc: 0.9727272987365723)
[2025-02-13 19:41:03,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03,976][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.20907656848430634, acc: 0.9312169551849365)
[2025-02-13 19:41:04,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04,358][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.24208006262779236, acc: 0.9333333373069763)
[2025-02-13 19:41:04,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04,773][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.1647651642560959, acc: 0.9599999785423279)
[2025-02-13 19:41:04,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05,125][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.2522110342979431, acc: 0.9341317415237427)
[2025-02-13 19:41:05,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05,531][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.09140841662883759, acc: 0.9748743772506714)
[2025-02-13 19:41:05,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05,926][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.05186961963772774, acc: 0.9852941036224365)
[2025-02-13 19:41:06,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06,301][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.11439014971256256, acc: 0.9740259647369385)
[2025-02-13 19:41:06,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06,717][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.18167723715305328, acc: 0.9481481313705444)
[2025-02-13 19:41:06,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07,118][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.18470178544521332, acc: 0.9437500238418579)
[2025-02-13 19:41:07,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07,509][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.9202825427055359, acc: 0.7831325531005859)
[2025-02-13 19:41:07,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07,873][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.34180644154548645, acc: 0.9113923907279968)
[2025-02-13 19:41:08,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08,246][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.29011791944503784, acc: 0.9285714030265808)
[2025-02-13 19:41:08,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08,629][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.32030045986175537, acc: 0.9139072895050049)
[2025-02-13 19:41:08,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09,037][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.3776739239692688, acc: 0.9171270728111267)
[2025-02-13 19:41:09,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09,429][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.18790574371814728, acc: 0.9532163739204407)
[2025-02-13 19:41:09,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09,834][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.16518697142601013, acc: 0.9545454382896423)
[2025-02-13 19:41:09,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10,248][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.21421456336975098, acc: 0.9583333134651184)
[2025-02-13 19:41:10,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10,650][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.13095895946025848, acc: 0.96875)
[2025-02-13 19:41:10,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11,047][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.13782216608524323, acc: 0.9715909361839294)
[2025-02-13 19:41:11,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11,455][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.1105261892080307, acc: 0.9775280952453613)
[2025-02-13 19:41:11,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11,872][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.09212377667427063, acc: 0.9739583134651184)
[2025-02-13 19:41:12,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12,235][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.07812713831663132, acc: 0.982300877571106)
[2025-02-13 19:41:12,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12,622][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.29168301820755005, acc: 0.9382022619247437)
[2025-02-13 19:41:12,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13,018][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.15451140701770782, acc: 0.9629629850387573)
[2025-02-13 19:41:13,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13,414][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.18934166431427002, acc: 0.9575757384300232)
[2025-02-13 19:41:13,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13,836][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.12476163357496262, acc: 0.960629940032959)
[2025-02-13 19:41:13,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14,232][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.22226105630397797, acc: 0.9585798978805542)
[2025-02-13 19:41:14,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14,664][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.34227487444877625, acc: 0.9385474920272827)
[2025-02-13 19:41:14,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15,067][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.22846540808677673, acc: 0.940397322177887)
[2025-02-13 19:41:15,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15,517][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 1.3497732877731323, acc: 0.7251908183097839)
[2025-02-13 19:41:15,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15,907][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.28994959592819214, acc: 0.9395973086357117)
[2025-02-13 19:41:16,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16,363][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.3146182894706726, acc: 0.9482758641242981)
[2025-02-13 19:41:16,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16,762][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.28920724987983704, acc: 0.948051929473877)
[2025-02-13 19:41:16,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17,197][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.31900331377983093, acc: 0.9011628031730652)
[2025-02-13 19:41:17,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17,585][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.34093284606933594, acc: 0.9044944047927856)
[2025-02-13 19:41:17,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17,925][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.4616434872150421, acc: 0.9197530746459961)
[2025-02-13 19:41:18,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18,343][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.6280930042266846, acc: 0.8640776872634888)
[2025-02-13 19:41:18,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18,713][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.20347514748573303, acc: 0.9350649118423462)
[2025-02-13 19:41:18,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19,130][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.22972621023654938, acc: 0.9279999732971191)
[2025-02-13 19:41:19,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19,503][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.22347204387187958, acc: 0.9689922332763672)
[2025-02-13 19:41:19,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19,862][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.13153871893882751, acc: 0.9672130942344666)
[2025-02-13 19:41:20,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20,251][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.07886935770511627, acc: 0.9770992398262024)
[2025-02-13 19:41:20,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20,623][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.20839136838912964, acc: 0.9496855139732361)
[2025-02-13 19:41:20,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20,995][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.10728777199983597, acc: 0.9806451797485352)
[2025-02-13 19:41:21,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21,342][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.14656904339790344, acc: 0.9751552939414978)
[2025-02-13 19:41:21,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21,705][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.08434052020311356, acc: 0.981249988079071)
[2025-02-13 19:41:21,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22,105][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.050946541130542755, acc: 0.9857142567634583)
[2025-02-13 19:41:22,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22,510][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.06971875578165054, acc: 0.9837837815284729)
[2025-02-13 19:41:22,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22,894][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.21867482364177704, acc: 0.9570552110671997)
[2025-02-13 19:41:23,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23,294][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.08968470990657806, acc: 0.9750000238418579)
[2025-02-13 19:41:23,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23,706][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.1441449671983719, acc: 0.9624060392379761)
[2025-02-13 19:41:23,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24,110][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.13822181522846222, acc: 0.9629629850387573)
[2025-02-13 19:41:24,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24,463][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.12256975471973419, acc: 0.9466666579246521)
[2025-02-13 19:41:24,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24,833][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.26622697710990906, acc: 0.9213483333587646)
[2025-02-13 19:41:24,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25,237][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.20443055033683777, acc: 0.9719626307487488)
[2025-02-13 19:41:25,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25,694][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.39015665650367737, acc: 0.9146341681480408)
[2025-02-13 19:41:25,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26,072][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.15962952375411987, acc: 0.9588235020637512)
[2025-02-13 19:41:26,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26,471][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.13392126560211182, acc: 0.9601989984512329)
[2025-02-13 19:41:26,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26,848][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.361855685710907, acc: 0.9017341136932373)
[2025-02-13 19:41:26,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27,231][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.2292121946811676, acc: 0.9558011293411255)
[2025-02-13 19:41:27,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27,606][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.19197197258472443, acc: 0.9653179049491882)
[2025-02-13 19:41:27,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28,039][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.19002340734004974, acc: 0.9435028433799744)
[2025-02-13 19:41:28,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28,423][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.23804441094398499, acc: 0.9567567706108093)
[2025-02-13 19:41:28,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28,807][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.14265593886375427, acc: 0.9444444179534912)
[2025-02-13 19:41:28,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29,260][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.14880046248435974, acc: 0.9581151604652405)
[2025-02-13 19:41:29,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29,696][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.08278634399175644, acc: 0.9802631735801697)
[2025-02-13 19:41:29,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30,163][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.12618358433246613, acc: 0.9747474789619446)
[2025-02-13 19:41:30,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30,553][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.2009379118680954, acc: 0.9583333134651184)
[2025-02-13 19:41:30,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31,001][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.09459646791219711, acc: 0.9776119589805603)
[2025-02-13 19:41:31,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31,409][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.5528941750526428, acc: 0.8724831938743591)
[2025-02-13 19:41:31,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31,815][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.379006564617157, acc: 0.9141104221343994)
[2025-02-13 19:41:31,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32,194][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.508788526058197, acc: 0.8662420511245728)
[2025-02-13 19:41:32,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32,582][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.6231468915939331, acc: 0.8796992301940918)
[2025-02-13 19:41:32,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32,951][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.4143778681755066, acc: 0.9007633328437805)
[2025-02-13 19:41:33,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33,371][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.26426681876182556, acc: 0.926174521446228)
[2025-02-13 19:41:33,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33,777][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 1.1092994213104248, acc: 0.8372092843055725)
[2025-02-13 19:41:33,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34,170][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.3879780173301697, acc: 0.9291338324546814)
[2025-02-13 19:41:34,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34,586][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.4505667984485626, acc: 0.9208633303642273)
[2025-02-13 19:41:34,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34,948][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.4918949007987976, acc: 0.895061731338501)
[2025-02-13 19:41:35,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35,372][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.20025792717933655, acc: 0.9558823704719543)
[2025-02-13 19:41:35,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35,786][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.18596011400222778, acc: 0.959770143032074)
[2025-02-13 19:41:35,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36,245][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.2970968782901764, acc: 0.8980891704559326)
[2025-02-13 19:41:36,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36,676][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.27551788091659546, acc: 0.940397322177887)
[2025-02-13 19:41:36,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37,140][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.22070099413394928, acc: 0.9454545378684998)
[2025-02-13 19:41:37,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37,594][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.280836820602417, acc: 0.9295774698257446)
[2025-02-13 19:41:37,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38,028][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.15970462560653687, acc: 0.9620253443717957)
[2025-02-13 19:41:38,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38,418][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.13944366574287415, acc: 0.970588207244873)
[2025-02-13 19:41:38,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38,812][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.11153607070446014, acc: 0.9586777091026306)
[2025-02-13 19:41:38,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39,174][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.16119174659252167, acc: 0.9450549483299255)
[2025-02-13 19:41:39,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39,531][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.103462815284729, acc: 0.9852941036224365)
[2025-02-13 19:41:39,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39,917][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.1816561073064804, acc: 0.9754098653793335)
[2025-02-13 19:41:40,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40,351][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.11887917667627335, acc: 0.9599999785423279)
[2025-02-13 19:41:40,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40,751][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.1595226228237152, acc: 0.9482758641242981)
[2025-02-13 19:41:40,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41,119][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.04541755095124245, acc: 0.9893048405647278)
[2025-02-13 19:41:41,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41,479][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.21903261542320251, acc: 0.961904764175415)
[2025-02-13 19:41:41,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41,903][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.14661410450935364, acc: 0.9477124214172363)
[2025-02-13 19:41:42,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42,288][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.10390366613864899, acc: 0.98591548204422)
[2025-02-13 19:41:42,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42,652][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.11849012225866318, acc: 0.9752066135406494)
[2025-02-13 19:41:42,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43,010][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.06122499704360962, acc: 0.9862068891525269)
[2025-02-13 19:41:43,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43,392][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.09451374411582947, acc: 0.9731543660163879)
[2025-02-13 19:41:43,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43,849][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.08575452119112015, acc: 0.9863945841789246)
[2025-02-13 19:41:43,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44,239][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.07922736555337906, acc: 0.9818181991577148)
[2025-02-13 19:41:44,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44,614][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.21130387485027313, acc: 0.9465649127960205)
[2025-02-13 19:41:44,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44,988][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.055609650909900665, acc: 0.9922480583190918)
[2025-02-13 19:41:45,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45,386][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.16827817261219025, acc: 0.9677419066429138)
[2025-02-13 19:41:45,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45,784][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.1446082592010498, acc: 0.9542483687400818)
[2025-02-13 19:41:45,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46,174][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.07076404243707657, acc: 0.9878048896789551)
[2025-02-13 19:41:46,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46,530][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.1103738471865654, acc: 0.9760000109672546)
[2025-02-13 19:41:46,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46,931][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.08229215443134308, acc: 0.9795918464660645)
[2025-02-13 19:41:47,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47,318][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.23020829260349274, acc: 0.9541984796524048)
[2025-02-13 19:41:47,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47,686][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.09589734673500061, acc: 0.969924807548523)
[2025-02-13 19:41:47,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48,086][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.1713697761297226, acc: 0.9532163739204407)
[2025-02-13 19:41:48,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48,487][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.16221994161605835, acc: 0.9599999785423279)
[2025-02-13 19:41:48,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48,916][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.34696874022483826, acc: 0.9108280539512634)
[2025-02-13 19:41:49,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49,347][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.5859056711196899, acc: 0.9127516746520996)
[2025-02-13 19:41:49,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49,766][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.20060253143310547, acc: 0.971222996711731)
[2025-02-13 19:41:49,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50,149][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.4302879571914673, acc: 0.9202898740768433)
[2025-02-13 19:41:50,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50,516][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.04821831360459328, acc: 0.9860140085220337)
[2025-02-13 19:41:50,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50,930][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.41616159677505493, acc: 0.8812500238418579)
[2025-02-13 19:41:51,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51,295][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.1657792031764984, acc: 0.942148745059967)
[2025-02-13 19:41:51,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51,671][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.23941271007061005, acc: 0.9583333134651184)
[2025-02-13 19:41:51,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52,061][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.18320254981517792, acc: 0.9397590160369873)
[2025-02-13 19:41:52,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52,443][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.23756861686706543, acc: 0.9415584206581116)
[2025-02-13 19:41:52,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52,827][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.15797042846679688, acc: 0.9404761791229248)
[2025-02-13 19:41:52,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53,223][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.10859944671392441, acc: 0.9759036302566528)
[2025-02-13 19:41:53,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53,611][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.20744197070598602, acc: 0.9391891956329346)
[2025-02-13 19:41:53,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54,021][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.28780385851860046, acc: 0.9285714030265808)
[2025-02-13 19:41:54,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54,394][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.18853390216827393, acc: 0.9507042169570923)
[2025-02-13 19:41:54,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54,821][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.25063538551330566, acc: 0.9246575236320496)
[2025-02-13 19:41:54,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55,219][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.1909557580947876, acc: 0.9510489702224731)
[2025-02-13 19:41:55,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55,624][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.2564813792705536, acc: 0.9382022619247437)
[2025-02-13 19:41:55,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55,996][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.30333706736564636, acc: 0.9307692050933838)
[2025-02-13 19:41:56,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56,387][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.21786418557167053, acc: 0.9491525292396545)
[2025-02-13 19:41:56,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56,780][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.34426531195640564, acc: 0.9154929518699646)
[2025-02-13 19:41:56,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57,172][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.2446814477443695, acc: 0.9398496150970459)
[2025-02-13 19:41:57,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57,523][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.16633136570453644, acc: 0.9449541568756104)
[2025-02-13 19:41:57,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57,951][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.13977785408496857, acc: 0.9741379022598267)
[2025-02-13 19:41:58,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58,377][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.2570059597492218, acc: 0.9318181872367859)
[2025-02-13 19:41:58,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58,767][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.12208288908004761, acc: 0.9714285731315613)
[2025-02-13 19:41:58,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59,149][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.2142341434955597, acc: 0.9406779408454895)
[2025-02-13 19:41:59,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59,551][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.28341665863990784, acc: 0.9354838728904724)
[2025-02-13 19:41:59,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59,921][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.3959417939186096, acc: 0.9181286692619324)
[2025-02-13 19:42:00,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00,375][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.20327532291412354, acc: 0.9513888955116272)
[2025-02-13 19:42:00,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00,783][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.08232451230287552, acc: 0.9780219793319702)
[2025-02-13 19:42:00,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01,182][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.16437871754169464, acc: 0.9527027010917664)
[2025-02-13 19:42:01,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01,606][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.19588886201381683, acc: 0.9542483687400818)
[2025-02-13 19:42:01,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01,970][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.14804492890834808, acc: 0.9640718698501587)
[2025-02-13 19:42:02,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02,356][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.14053227007389069, acc: 0.9629629850387573)
[2025-02-13 19:42:02,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02,723][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.07478772848844528, acc: 0.963302731513977)
[2025-02-13 19:42:02,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03,140][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.23718084394931793, acc: 0.9440993666648865)
[2025-02-13 19:42:03,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03,586][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.2644505798816681, acc: 0.9465240836143494)
[2025-02-13 19:42:03,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03,939][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.11805824935436249, acc: 0.976190447807312)
[2025-02-13 19:42:04,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04,359][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.19179949164390564, acc: 0.939393937587738)
[2025-02-13 19:42:04,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04,738][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.09055102616548538, acc: 0.9550561904907227)
[2025-02-13 19:42:04,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05,084][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.12272795289754868, acc: 0.9720670580863953)
[2025-02-13 19:42:05,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05,471][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.14442172646522522, acc: 0.9727272987365723)
[2025-02-13 19:42:05,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05,901][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.0585404597222805, acc: 0.9888888597488403)
[2025-02-13 19:42:06,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06,329][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.23242805898189545, acc: 0.9642857313156128)
[2025-02-13 19:42:06,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06,748][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.12945209443569183, acc: 0.9487179517745972)
[2025-02-13 19:42:06,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07,211][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.053679946810007095, acc: 0.9803921580314636)
[2025-02-13 19:42:07,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07,666][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.07595781981945038, acc: 0.9769230484962463)
[2025-02-13 19:42:07,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08,094][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.174137145280838, acc: 0.9523809552192688)
[2025-02-13 19:42:08,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08,565][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.4004729390144348, acc: 0.9215686321258545)
[2025-02-13 19:42:08,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08,963][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.11011207848787308, acc: 0.976190447807312)
[2025-02-13 19:42:09,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09,329][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.09072530269622803, acc: 0.9750000238418579)
[2025-02-13 19:42:09,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09,727][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.3875807821750641, acc: 0.9219858050346375)
[2025-02-13 19:42:09,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10,152][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.11887006461620331, acc: 0.9807692170143127)
[2025-02-13 19:42:10,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10,537][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.07215499132871628, acc: 0.9864864945411682)
[2025-02-13 19:42:10,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10,921][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.20405888557434082, acc: 0.9642857313156128)
[2025-02-13 19:42:11,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11,306][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.17537422478199005, acc: 0.9651162624359131)
[2025-02-13 19:42:11,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11,691][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.09491530060768127, acc: 0.9815950989723206)
[2025-02-13 19:42:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12,095][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.08040130883455276, acc: 0.9864864945411682)
[2025-02-13 19:42:12,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12,519][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.10657874494791031, acc: 0.96875)
[2025-02-13 19:42:12,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12,892][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.2571351230144501, acc: 0.9496855139732361)
[2025-02-13 19:42:13,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13,270][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.07524576783180237, acc: 0.9772727489471436)
[2025-02-13 19:42:13,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13,658][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.15596355497837067, acc: 0.9732142686843872)
[2025-02-13 19:42:13,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14,082][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.14802929759025574, acc: 0.9743589758872986)
[2025-02-13 19:42:14,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14,447][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.14146572351455688, acc: 0.9506173133850098)
[2025-02-13 19:42:14,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14,890][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.32594048976898193, acc: 0.9175257682800293)
[2025-02-13 19:42:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15,279][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.306026428937912, acc: 0.9489796161651611)
[2025-02-13 19:42:15,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15,664][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.15756505727767944, acc: 0.9557521939277649)
[2025-02-13 19:42:15,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16,092][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.19769150018692017, acc: 0.9436619877815247)
[2025-02-13 19:42:16,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16,490][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.21078433096408844, acc: 0.9459459185600281)
[2025-02-13 19:42:16,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16,950][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.22585523128509521, acc: 0.9346405267715454)
[2025-02-13 19:42:17,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17,395][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.15995050966739655, acc: 0.9652777910232544)
[2025-02-13 19:42:17,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17,820][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.17858344316482544, acc: 0.9545454382896423)
[2025-02-13 19:42:18,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18,358][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.23228169977664948, acc: 0.9489051103591919)
[2025-02-13 19:42:18,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18,777][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.32718825340270996, acc: 0.9271523356437683)
[2025-02-13 19:42:18,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19,165][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.18007096648216248, acc: 0.9652174115180969)
[2025-02-13 19:42:19,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19,523][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.10761343687772751, acc: 0.984375)
[2025-02-13 19:42:19,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19,981][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.22620703279972076, acc: 0.9526627063751221)
[2025-02-13 19:42:20,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20,380][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.27673161029815674, acc: 0.949999988079071)
[2025-02-13 19:42:20,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20,770][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.14439135789871216, acc: 0.9770992398262024)
[2025-02-13 19:42:20,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21,178][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.1722612828016281, acc: 0.9602649211883545)
[2025-02-13 19:42:21,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21,548][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.07618539780378342, acc: 0.984000027179718)
[2025-02-13 19:42:21,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21,996][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.2574882507324219, acc: 0.9370629191398621)
[2025-02-13 19:42:22,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22,428][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.16754759848117828, acc: 0.9776119589805603)
[2025-02-13 19:42:22,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22,814][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.12297137826681137, acc: 0.9626865386962891)
[2025-02-13 19:42:22,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23,192][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.24992889165878296, acc: 0.9396551847457886)
[2025-02-13 19:42:23,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23,573][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.8470004200935364, acc: 0.807692289352417)
[2025-02-13 19:42:23,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23,949][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.2848544716835022, acc: 0.9430379867553711)
[2025-02-13 19:42:24,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24,357][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.23601895570755005, acc: 0.932692289352417)
[2025-02-13 19:42:24,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24,740][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.9639351963996887, acc: 0.8222222328186035)
[2025-02-13 19:42:24,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25,151][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.39462780952453613, acc: 0.9144737124443054)
[2025-02-13 19:42:25,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25,579][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.29910895228385925, acc: 0.9464285969734192)
[2025-02-13 19:42:25,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25,972][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 1.0251848697662354, acc: 0.7972972989082336)
[2025-02-13 19:42:26,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26,385][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.36200031638145447, acc: 0.9032257795333862)
[2025-02-13 19:42:26,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26,730][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.7158090472221375, acc: 0.8429751992225647)
[2025-02-13 19:42:26,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27,100][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.26633861660957336, acc: 0.9658119678497314)
[2025-02-13 19:42:27,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27,488][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.6302492022514343, acc: 0.8705036044120789)
[2025-02-13 19:42:27,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27,838][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.5861411690711975, acc: 0.8823529481887817)
[2025-02-13 19:42:27,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28,245][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.3284469246864319, acc: 0.9285714030265808)
[2025-02-13 19:42:28,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28,625][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.1481924206018448, acc: 0.9672130942344666)
[2025-02-13 19:42:28,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29,055][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.4904683530330658, acc: 0.8895705342292786)
[2025-02-13 19:42:29,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29,489][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.23508624732494354, acc: 0.9507042169570923)
[2025-02-13 19:42:29,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29,925][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.14662054181098938, acc: 0.9774436354637146)
[2025-02-13 19:42:30,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30,303][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 0.9023134708404541, acc: 0.8275862336158752)
[2025-02-13 19:42:30,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30,742][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.3013172447681427, acc: 0.9192546606063843)
[2025-02-13 19:42:30,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31,192][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.2839667797088623, acc: 0.9285714030265808)
[2025-02-13 19:42:31,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31,591][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.11608105897903442, acc: 0.984000027179718)
[2025-02-13 19:42:31,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31,978][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.09731899946928024, acc: 0.9754601120948792)
[2025-02-13 19:42:32,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32,369][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.22395026683807373, acc: 0.9438775777816772)
[2025-02-13 19:42:32,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32,790][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.09706411510705948, acc: 0.9822485446929932)
[2025-02-13 19:42:32,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33,157][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.23902927339076996, acc: 0.9583333134651184)
[2025-02-13 19:42:33,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33,531][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.26085346937179565, acc: 0.932692289352417)
[2025-02-13 19:42:33,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33,938][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.3748311400413513, acc: 0.9186992049217224)
[2025-02-13 19:42:34,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34,373][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.09282349795103073, acc: 0.9833333492279053)
[2025-02-13 19:42:34,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34,794][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.2822006642818451, acc: 0.9207921028137207)
[2025-02-13 19:42:34,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35,161][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.34338101744651794, acc: 0.9345238208770752)
[2025-02-13 19:42:35,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35,559][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.7102090120315552, acc: 0.8881579041481018)
[2025-02-13 19:42:35,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35,943][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.34397435188293457, acc: 0.8962264060974121)
[2025-02-13 19:42:36,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36,379][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.15892931818962097, acc: 0.9702970385551453)
[2025-02-13 19:42:36,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36,787][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.2067991942167282, acc: 0.9814814925193787)
[2025-02-13 19:42:36,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37,157][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.34435078501701355, acc: 0.9333333373069763)
[2025-02-13 19:42:37,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37,566][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.171246737241745, acc: 0.9634146094322205)
[2025-02-13 19:42:37,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37,969][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.2211136370897293, acc: 0.9371428489685059)
[2025-02-13 19:42:38,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38,372][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.24465113878250122, acc: 0.936170220375061)
[2025-02-13 19:42:38,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38,795][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.055659692734479904, acc: 0.988095223903656)
[2025-02-13 19:42:38,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39,243][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.2569407522678375, acc: 0.9259259104728699)
[2025-02-13 19:42:39,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39,645][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.13902433216571808, acc: 0.9724137783050537)
[2025-02-13 19:42:39,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40,078][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.10545177757740021, acc: 0.977142870426178)
[2025-02-13 19:42:40,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40,511][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.02580123022198677, acc: 1.0)
[2025-02-13 19:42:40,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40,917][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.1253720223903656, acc: 0.9743589758872986)
[2025-02-13 19:42:41,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41,311][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.14622549712657928, acc: 0.967391312122345)
[2025-02-13 19:42:41,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41,745][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.14348195493221283, acc: 0.9583333134651184)
[2025-02-13 19:42:41,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42,118][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.28246864676475525, acc: 0.9599999785423279)
[2025-02-13 19:42:42,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42,551][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.19670340418815613, acc: 0.948387086391449)
[2025-02-13 19:42:42,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42,966][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.13559384644031525, acc: 0.9765625)
[2025-02-13 19:42:43,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43,338][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.07912563532590866, acc: 0.9789473414421082)
[2025-02-13 19:42:43,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43,711][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.278138667345047, acc: 0.9263803958892822)
[2025-02-13 19:42:43,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44,106][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.20500828325748444, acc: 0.9657142758369446)
[2025-02-13 19:42:44,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44,550][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.09414126724004745, acc: 0.9852941036224365)
[2025-02-13 19:42:44,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44,946][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.0882301777601242, acc: 0.9756097793579102)
[2025-02-13 19:42:45,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45,358][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.1669362187385559, acc: 0.9738562107086182)
[2025-02-13 19:42:45,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45,818][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.13190895318984985, acc: 0.9700000286102295)
[2025-02-13 19:42:45,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46,206][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.19217367470264435, acc: 0.9627329111099243)
[2025-02-13 19:42:46,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46,574][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.19705675542354584, acc: 0.9684210419654846)
[2025-02-13 19:42:46,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46,946][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.09466993063688278, acc: 0.97826087474823)
[2025-02-13 19:42:47,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47,382][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.13756556808948517, acc: 0.9796954393386841)
[2025-02-13 19:42:47,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47,755][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.09637293219566345, acc: 0.9838709831237793)
[2025-02-13 19:42:47,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48,190][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.18038082122802734, acc: 0.959770143032074)
[2025-02-13 19:42:48,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48,570][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.11597993969917297, acc: 0.9757575988769531)
[2025-02-13 19:42:48,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48,945][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.1587391346693039, acc: 0.9550561904907227)
[2025-02-13 19:42:49,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49,331][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.20337842404842377, acc: 0.9629629850387573)
[2025-02-13 19:42:49,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49,713][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.17774036526679993, acc: 0.9560975432395935)
[2025-02-13 19:42:49,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50,156][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.06488918513059616, acc: 0.9888888597488403)
[2025-02-13 19:42:50,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50,531][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.1783963143825531, acc: 0.9615384340286255)
[2025-02-13 19:42:50,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50,963][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.22228577733039856, acc: 0.9668508172035217)
[2025-02-13 19:42:51,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51,353][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.2547619044780731, acc: 0.9545454382896423)
[2025-02-13 19:42:51,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51,797][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.06671151518821716, acc: 0.9897959232330322)
[2025-02-13 19:42:51,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52,196][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.11513540893793106, acc: 0.966292142868042)
[2025-02-13 19:42:52,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52,559][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.062357861548662186, acc: 0.9917355179786682)
[2025-02-13 19:42:52,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52,940][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.10619246959686279, acc: 0.9826589822769165)
[2025-02-13 19:42:53,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53,365][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.13051526248455048, acc: 0.9866666793823242)
[2025-02-13 19:42:53,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53,787][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.18920888006687164, acc: 0.9602272510528564)
[2025-02-13 19:42:53,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54,166][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.14651772379875183, acc: 0.9470899701118469)
[2025-02-13 19:42:54,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54,502][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.08184587955474854, acc: 0.990338146686554)
[2025-02-13 19:42:54,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54,899][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.0998338907957077, acc: 0.98591548204422)
[2025-02-13 19:42:55,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55,274][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.08235000818967819, acc: 0.987730085849762)
[2025-02-13 19:42:55,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55,676][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.1838347464799881, acc: 0.9669421315193176)
[2025-02-13 19:42:55,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56,123][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.21528464555740356, acc: 0.948051929473877)
[2025-02-13 19:42:56,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56,561][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.47832441329956055, acc: 0.9190751314163208)
[2025-02-13 19:42:56,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56,966][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.23173275589942932, acc: 0.9470198750495911)
[2025-02-13 19:42:57,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57,370][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.2918131649494171, acc: 0.9279999732971191)
[2025-02-13 19:42:57,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57,793][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.1698121428489685, acc: 0.9558823704719543)
[2025-02-13 19:42:57,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58,195][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.23423638939857483, acc: 0.9300699234008789)
[2025-02-13 19:42:58,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58,552][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.22827348113059998, acc: 0.9398496150970459)
[2025-02-13 19:42:58,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58,964][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.2792351543903351, acc: 0.9303797483444214)
[2025-02-13 19:42:59,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59,347][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.3527337610721588, acc: 0.9285714030265808)
[2025-02-13 19:42:59,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59,751][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.19067619740962982, acc: 0.9515151381492615)
[2025-02-13 19:42:59,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00,118][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.4070715606212616, acc: 0.915730357170105)
[2025-02-13 19:43:00,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00,474][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.32383012771606445, acc: 0.913385808467865)
[2025-02-13 19:43:00,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00,901][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.3477495312690735, acc: 0.9189189076423645)
[2025-02-13 19:43:01,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01,285][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.4919237196445465, acc: 0.8918918967247009)
[2025-02-13 19:43:01,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01,638][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.4106366038322449, acc: 0.9038461446762085)
[2025-02-13 19:43:01,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02,007][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.3103887140750885, acc: 0.9230769276618958)
[2025-02-13 19:43:02,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02,388][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.33217525482177734, acc: 0.9166666865348816)
[2025-02-13 19:43:02,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02,773][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.3598077893257141, acc: 0.9365079402923584)
[2025-02-13 19:43:02,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03,129][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.14115352928638458, acc: 0.9512194991111755)
[2025-02-13 19:43:03,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03,473][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.19663332402706146, acc: 0.9473684430122375)
[2025-02-13 19:43:03,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03,836][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.29170069098472595, acc: 0.9545454382896423)
[2025-02-13 19:43:03,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04,208][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.03454551473259926, acc: 1.0)
[2025-02-13 19:43:04,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04,629][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.2718631625175476, acc: 0.9344262480735779)
[2025-02-13 19:43:04,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05,023][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 1.003635048866272, acc: 0.8135592937469482)
[2025-02-13 19:43:05,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05,455][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.08912306278944016, acc: 0.966292142868042)
[2025-02-13 19:43:05,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05,804][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.2253427654504776, acc: 0.949999988079071)
[2025-02-13 19:43:05,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06,243][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.12374810874462128, acc: 0.9718309640884399)
[2025-02-13 19:43:06,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06,626][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.1059470921754837, acc: 0.9696969985961914)
[2025-02-13 19:43:06,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07,024][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.1539178043603897, acc: 0.9534883499145508)
[2025-02-13 19:43:07,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07,465][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.09427089244127274, acc: 0.9702970385551453)
[2025-02-13 19:43:07,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07,903][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.1787468045949936, acc: 0.9545454382896423)
[2025-02-13 19:43:08,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08,340][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.4204859733581543, acc: 0.9253731369972229)
[2025-02-13 19:43:08,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08,739][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.6658303141593933, acc: 0.8544601202011108)
[2025-02-13 19:43:08,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09,150][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.48750171065330505, acc: 0.8876404762268066)
[2025-02-13 19:43:09,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09,554][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.27715978026390076, acc: 0.914893627166748)
[2025-02-13 19:43:09,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09,953][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.31900423765182495, acc: 0.9215686321258545)
[2025-02-13 19:43:10,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10,340][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.3219611942768097, acc: 0.9219512343406677)
[2025-02-13 19:43:10,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10,791][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.20485329627990723, acc: 0.933920681476593)
[2025-02-13 19:43:10,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11,215][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.3973644971847534, acc: 0.8817204236984253)
[2025-02-13 19:43:11,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11,633][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.13767634332180023, acc: 0.963302731513977)
[2025-02-13 19:43:11,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12,010][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.249016672372818, acc: 0.9462365508079529)
[2025-02-13 19:43:12,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12,443][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.16355091333389282, acc: 0.9476190209388733)
[2025-02-13 19:43:12,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12,882][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.23501773178577423, acc: 0.9330143332481384)
[2025-02-13 19:43:13,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13,275][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.4890328049659729, acc: 0.8972602486610413)
[2025-02-13 19:43:13,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13,675][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.4393494725227356, acc: 0.918367326259613)
[2025-02-13 19:43:13,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14,072][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.2994106113910675, acc: 0.8943089246749878)
[2025-02-13 19:43:14,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14,506][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.24845653772354126, acc: 0.9230769276618958)
[2025-02-13 19:43:14,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14,909][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.37274226546287537, acc: 0.8786126971244812)
[2025-02-13 19:43:15,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15,341][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.4875582456588745, acc: 0.8806818127632141)
[2025-02-13 19:43:15,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15,749][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.3796916604042053, acc: 0.8733333349227905)
[2025-02-13 19:43:15,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16,166][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.2827492356300354, acc: 0.9159291982650757)
[2025-02-13 19:43:16,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16,549][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.22291645407676697, acc: 0.9378238320350647)
[2025-02-13 19:43:16,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16,938][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.3402647376060486, acc: 0.9415584206581116)
[2025-02-13 19:43:17,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17,321][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.6497551202774048, acc: 0.8558558821678162)
[2025-02-13 19:43:17,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17,700][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.3101220428943634, acc: 0.9220778942108154)
[2025-02-13 19:43:17,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18,086][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.24969901144504547, acc: 0.9322034120559692)
[2025-02-13 19:43:18,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18,486][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.18847692012786865, acc: 0.931506872177124)
[2025-02-13 19:43:18,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18,955][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.2082575559616089, acc: 0.9629629850387573)
[2025-02-13 19:43:19,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19,333][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.28854379057884216, acc: 0.9154929518699646)
[2025-02-13 19:43:19,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19,723][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.35042762756347656, acc: 0.9047619104385376)
[2025-02-13 19:43:19,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20,135][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.2371366024017334, acc: 0.9406779408454895)
[2025-02-13 19:43:20,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20,546][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.48024147748947144, acc: 0.888198733329773)
[2025-02-13 19:43:20,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20,942][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.19755730032920837, acc: 0.9593908786773682)
[2025-02-13 19:43:21,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21,324][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.5120463371276855, acc: 0.8296296000480652)
[2025-02-13 19:43:21,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21,733][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.46599093079566956, acc: 0.8815789222717285)
[2025-02-13 19:43:21,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22,226][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.6273396611213684, acc: 0.8486486673355103)
[2025-02-13 19:43:22,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22,605][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.3745880722999573, acc: 0.8764705657958984)
[2025-02-13 19:43:22,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22,969][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.34652504324913025, acc: 0.9142857193946838)
[2025-02-13 19:43:23,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23,363][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.35218361020088196, acc: 0.8875739574432373)
[2025-02-13 19:43:23,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23,735][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.6009275317192078, acc: 0.8583333492279053)
[2025-02-13 19:43:23,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24,140][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.28378644585609436, acc: 0.9171974658966064)
[2025-02-13 19:43:24,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24,583][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.500507652759552, acc: 0.9096774458885193)
[2025-02-13 19:43:24,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25,032][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.2804562449455261, acc: 0.9315789341926575)
[2025-02-13 19:43:25,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25,447][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.4948526918888092, acc: 0.866310179233551)
[2025-02-13 19:43:25,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25,832][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.18586687743663788, acc: 0.9526627063751221)
[2025-02-13 19:43:25,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26,226][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.34979090094566345, acc: 0.909604549407959)
[2025-02-13 19:43:26,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26,661][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.21603406965732574, acc: 0.9469696879386902)
[2025-02-13 19:43:26,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27,078][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.1718219667673111, acc: 0.959770143032074)
[2025-02-13 19:43:27,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27,497][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.3798263370990753, acc: 0.9324324131011963)
[2025-02-13 19:43:27,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27,911][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.12354112416505814, acc: 0.9664429426193237)
[2025-02-13 19:43:28,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28,331][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.12209407240152359, acc: 0.9740932583808899)
[2025-02-13 19:43:28,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28,726][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.22094810009002686, acc: 0.95333331823349)
[2025-02-13 19:43:28,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29,144][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.1764257550239563, acc: 0.9583333134651184)
[2025-02-13 19:43:29,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29,580][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.10043758153915405, acc: 0.9562841653823853)
[2025-02-13 19:43:29,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29,956][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.26889052987098694, acc: 0.9411764740943909)
[2025-02-13 19:43:30,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30,362][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.3771509826183319, acc: 0.9135135412216187)
[2025-02-13 19:43:30,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30,768][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.16360431909561157, acc: 0.954285740852356)
[2025-02-13 19:43:30,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31,162][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.1314929574728012, acc: 0.9679144620895386)
[2025-02-13 19:43:31,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31,558][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.17966774106025696, acc: 0.9617834687232971)
[2025-02-13 19:43:31,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31,990][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.1495191603899002, acc: 0.939393937587738)
[2025-02-13 19:43:32,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32,412][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.2291574478149414, acc: 0.9477611780166626)
[2025-02-13 19:43:32,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32,830][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.4049363136291504, acc: 0.8977272510528564)
[2025-02-13 19:43:32,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33,313][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.19590188562870026, acc: 0.9509202241897583)
[2025-02-13 19:43:33,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33,765][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.33112505078315735, acc: 0.8984771370887756)
[2025-02-13 19:43:33,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34,190][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.3049406111240387, acc: 0.9130434989929199)
[2025-02-13 19:43:34,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34,600][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.2728795111179352, acc: 0.9298245906829834)
[2025-02-13 19:43:34,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35,015][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.23792757093906403, acc: 0.9455782175064087)
[2025-02-13 19:43:35,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35,459][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.45934194326400757, acc: 0.9358974099159241)
[2025-02-13 19:43:35,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35,817][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.07374531030654907, acc: 0.9722222089767456)
[2025-02-13 19:43:35,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36,176][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.2880845069885254, acc: 0.9296875)
[2025-02-13 19:43:36,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36,587][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.20251473784446716, acc: 0.9545454382896423)
[2025-02-13 19:43:36,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37,000][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.41855382919311523, acc: 0.8851351141929626)
[2025-02-13 19:43:37,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37,403][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.6574767827987671, acc: 0.8484848737716675)
[2025-02-13 19:43:37,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37,807][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.2527627646923065, acc: 0.9527559280395508)
[2025-02-13 19:43:37,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38,309][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.2015843540430069, acc: 0.9487179517745972)
[2025-02-13 19:43:38,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38,792][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.21387240290641785, acc: 0.9504132270812988)
[2025-02-13 19:43:38,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39,245][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.16098114848136902, acc: 0.947826087474823)
[2025-02-13 19:43:39,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39,645][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.506244421005249, acc: 0.8828125)
[2025-02-13 19:43:39,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40,014][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.31478551030158997, acc: 0.9347826242446899)
[2025-02-13 19:43:40,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40,412][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.34229591488838196, acc: 0.9346405267715454)
[2025-02-13 19:43:40,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40,803][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.2925110459327698, acc: 0.9390243887901306)
[2025-02-13 19:43:40,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41,208][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.14264991879463196, acc: 0.9720279574394226)
[2025-02-13 19:43:41,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41,578][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.15384015440940857, acc: 0.9760000109672546)
[2025-02-13 19:43:41,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41,960][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.3981207013130188, acc: 0.9193548560142517)
[2025-02-13 19:43:42,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42,349][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.338397353887558, acc: 0.940119743347168)
[2025-02-13 19:43:42,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42,755][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.14204053580760956, acc: 0.9523809552192688)
[2025-02-13 19:43:42,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43,177][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.16306494176387787, acc: 0.9513513445854187)
[2025-02-13 19:43:43,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43,606][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.14785760641098022, acc: 0.9772727489471436)
[2025-02-13 19:43:43,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44,015][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.26694050431251526, acc: 0.9370078444480896)
[2025-02-13 19:43:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44,390][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.12022272497415543, acc: 0.975806474685669)
[2025-02-13 19:43:44,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44,772][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.34215882420539856, acc: 0.9487179517745972)
[2025-02-13 19:43:44,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45,186][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.13812324404716492, acc: 0.9663865566253662)
[2025-02-13 19:43:45,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45,608][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.5210539698600769, acc: 0.9439252614974976)
[2025-02-13 19:43:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46,025][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.19262927770614624, acc: 0.9668874144554138)
[2025-02-13 19:43:46,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46,430][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.06840956956148148, acc: 0.9803921580314636)
[2025-02-13 19:43:46,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46,816][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.2173091024160385, acc: 0.9495798349380493)
[2025-02-13 19:43:46,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47,221][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.470443457365036, acc: 0.88165682554245)
[2025-02-13 19:43:47,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47,624][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.3201678991317749, acc: 0.9029850959777832)
[2025-02-13 19:43:47,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48,039][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.40635159611701965, acc: 0.9029126167297363)
[2025-02-13 19:43:48,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48,495][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.23004058003425598, acc: 0.9551281929016113)
[2025-02-13 19:43:48,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48,966][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.3679237365722656, acc: 0.9230769276618958)
[2025-02-13 19:43:49,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49,345][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.31924375891685486, acc: 0.9075144529342651)
[2025-02-13 19:43:49,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49,778][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.17423370480537415, acc: 0.961240291595459)
[2025-02-13 19:43:49,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50,172][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.28044939041137695, acc: 0.9580838084220886)
[2025-02-13 19:43:50,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50,570][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.21639595925807953, acc: 0.9610389471054077)
[2025-02-13 19:43:50,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50,967][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.2525342106819153, acc: 0.9570552110671997)
[2025-02-13 19:43:51,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51,398][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.33052027225494385, acc: 0.8979591727256775)
[2025-02-13 19:43:51,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51,842][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.20802506804466248, acc: 0.9692307710647583)
[2025-02-13 19:43:51,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52,240][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.48424893617630005, acc: 0.8840579986572266)
[2025-02-13 19:43:52,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52,661][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.23085078597068787, acc: 0.9428571462631226)
[2025-02-13 19:43:52,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53,061][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.20021694898605347, acc: 0.9696969985961914)
[2025-02-13 19:43:53,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53,429][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.22679762542247772, acc: 0.9452054500579834)
[2025-02-13 19:43:53,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53,816][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.35853371024131775, acc: 0.9289617538452148)
[2025-02-13 19:43:53,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54,200][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.3194057047367096, acc: 0.9216867685317993)
[2025-02-13 19:43:54,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54,610][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.25934427976608276, acc: 0.9370078444480896)
[2025-02-13 19:43:54,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54,999][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.22119122743606567, acc: 0.951724112033844)
[2025-02-13 19:43:55,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55,381][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.2638777196407318, acc: 0.9470198750495911)
[2025-02-13 19:43:55,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55,766][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.17522381246089935, acc: 0.9580419659614563)
[2025-02-13 19:43:55,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56,148][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.12270393967628479, acc: 0.9681528806686401)
[2025-02-13 19:43:56,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56,538][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.25233903527259827, acc: 0.9657142758369446)
[2025-02-13 19:43:56,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56,952][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.14122186601161957, acc: 0.9753086566925049)
[2025-02-13 19:43:57,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57,349][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.21265095472335815, acc: 0.9613259434700012)
[2025-02-13 19:43:57,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57,745][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.15031826496124268, acc: 0.9585798978805542)
[2025-02-13 19:43:57,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58,103][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.15601788461208344, acc: 0.9556962251663208)
[2025-02-13 19:43:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58,511][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.17292717099189758, acc: 0.9556962251663208)
[2025-02-13 19:43:58,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58,957][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.2997002601623535, acc: 0.9298245906829834)
[2025-02-13 19:43:59,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59,341][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.28118059039115906, acc: 0.9345238208770752)
[2025-02-13 19:43:59,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59,782][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.4379046559333801, acc: 0.8980891704559326)
[2025-02-13 19:43:59,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00,240][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.42182186245918274, acc: 0.913385808467865)
[2025-02-13 19:44:00,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00,625][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.5609838366508484, acc: 0.8782051205635071)
[2025-02-13 19:44:00,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01,099][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.37375399470329285, acc: 0.930232584476471)
[2025-02-13 19:44:01,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01,498][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.33027705550193787, acc: 0.9207317233085632)
[2025-02-13 19:44:01,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01,929][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.2953893840312958, acc: 0.9312977194786072)
[2025-02-13 19:44:02,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02,351][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.17006710171699524, acc: 0.956204354763031)
[2025-02-13 19:44:02,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02,717][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.10992018133401871, acc: 0.9767441749572754)
[2025-02-13 19:44:02,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03,110][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.12216930091381073, acc: 0.9784946441650391)
[2025-02-13 19:44:03,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03,495][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.18541012704372406, acc: 0.9759036302566528)
[2025-02-13 19:44:03,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03,884][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.14208319783210754, acc: 0.9568345546722412)
[2025-02-13 19:44:04,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04,261][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.0853780210018158, acc: 0.9890109896659851)
[2025-02-13 19:44:04,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04,601][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.11723364144563675, acc: 0.9609375)
[2025-02-13 19:44:04,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04,988][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.1478758603334427, acc: 0.9551281929016113)
[2025-02-13 19:44:05,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05,407][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.06442739814519882, acc: 0.9757575988769531)
[2025-02-13 19:44:05,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05,806][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.12642639875411987, acc: 0.9691358208656311)
[2025-02-13 19:44:05,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06,195][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.10870921611785889, acc: 0.9817073345184326)
[2025-02-13 19:44:06,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06,622][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.12605439126491547, acc: 0.9670329689979553)
[2025-02-13 19:44:06,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07,055][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.1499742567539215, acc: 0.9673202633857727)
[2025-02-13 19:44:07,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07,433][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.30723994970321655, acc: 0.9333333373069763)
[2025-02-13 19:44:07,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07,786][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.8363087177276611, acc: 0.8297872543334961)
[2025-02-13 19:44:07,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08,181][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 1.1204458475112915, acc: 0.75)
[2025-02-13 19:44:08,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08,617][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.5260940194129944, acc: 0.9069767594337463)
[2025-02-13 19:44:08,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09,008][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.7338055968284607, acc: 0.8209876418113708)
[2025-02-13 19:44:09,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09,403][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.6317936778068542, acc: 0.8608695864677429)
[2025-02-13 19:44:09,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09,868][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.23708707094192505, acc: 0.9596773982048035)
[2025-02-13 19:44:10,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10,307][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.17668932676315308, acc: 0.9694656729698181)
[2025-02-13 19:44:10,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10,710][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.24637693166732788, acc: 0.9415204524993896)
[2025-02-13 19:44:10,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11,086][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.22819149494171143, acc: 0.9411764740943909)
[2025-02-13 19:44:11,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11,512][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.35208114981651306, acc: 0.9090909361839294)
[2025-02-13 19:44:11,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11,882][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.6314757466316223, acc: 0.8857142925262451)
[2025-02-13 19:44:12,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12,259][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.2428428679704666, acc: 0.934959352016449)
[2025-02-13 19:44:12,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12,687][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.19529005885124207, acc: 0.9463414549827576)
[2025-02-13 19:44:12,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13,102][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.3044045865535736, acc: 0.9090909361839294)
[2025-02-13 19:44:13,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13,503][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.24863462150096893, acc: 0.9180327653884888)
[2025-02-13 19:44:13,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13,914][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.17439313232898712, acc: 0.9622641801834106)
[2025-02-13 19:44:14,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14,293][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.19480127096176147, acc: 0.9320987462997437)
[2025-02-13 19:44:14,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14,729][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.3180480897426605, acc: 0.9488636255264282)
[2025-02-13 19:44:14,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15,152][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.13773216307163239, acc: 0.9646017551422119)
[2025-02-13 19:44:15,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15,547][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.16003213822841644, acc: 0.9576719403266907)
[2025-02-13 19:44:15,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15,917][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.09752058237791061, acc: 0.9692307710647583)
[2025-02-13 19:44:16,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16,311][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.14102275669574738, acc: 0.9569377899169922)
[2025-02-13 19:44:16,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16,678][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.30985790491104126, acc: 0.9238095283508301)
[2025-02-13 19:44:16,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17,037][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.13838817179203033, acc: 0.9593023061752319)
[2025-02-13 19:44:17,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17,403][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.051089685410261154, acc: 0.9916666746139526)
[2025-02-13 19:44:17,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17,775][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.14010660350322723, acc: 0.9640287756919861)
[2025-02-13 19:44:17,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18,136][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.07492737472057343, acc: 0.9813664555549622)
[2025-02-13 19:44:18,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18,565][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.5185205340385437, acc: 0.8909090757369995)
[2025-02-13 19:44:18,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18,960][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.194415882229805, acc: 0.9322916865348816)
[2025-02-13 19:44:19,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19,344][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.1355050802230835, acc: 0.976190447807312)
[2025-02-13 19:44:19,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19,748][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.21201345324516296, acc: 0.9411764740943909)
[2025-02-13 19:44:19,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20,114][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.20411035418510437, acc: 0.9454545378684998)
[2025-02-13 19:44:20,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20,500][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.19522297382354736, acc: 0.9572649598121643)
[2025-02-13 19:44:20,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20,916][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.14987437427043915, acc: 0.9636363387107849)
[2025-02-13 19:44:21,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21,333][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.09217890352010727, acc: 0.9830508232116699)
[2025-02-13 19:44:21,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21,752][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.2634636461734772, acc: 0.9371980428695679)
[2025-02-13 19:44:21,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22,146][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.08009187132120132, acc: 0.9795918464660645)
[2025-02-13 19:44:22,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22,542][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.1180403009057045, acc: 0.9556962251663208)
[2025-02-13 19:44:22,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22,929][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.08782054483890533, acc: 0.9806451797485352)
[2025-02-13 19:44:23,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23,319][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.14935222268104553, acc: 0.9673202633857727)
[2025-02-13 19:44:23,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23,682][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.08897789567708969, acc: 0.9885057210922241)
[2025-02-13 19:44:23,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24,067][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.04171174019575119, acc: 0.9925373196601868)
[2025-02-13 19:44:24,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24,412][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.33340102434158325, acc: 0.9277108311653137)
[2025-02-13 19:44:24,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24,788][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.35055720806121826, acc: 0.921875)
[2025-02-13 19:44:24,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25,150][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.11516666412353516, acc: 0.9662162065505981)
[2025-02-13 19:44:25,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25,529][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.18828339874744415, acc: 0.9608938694000244)
[2025-02-13 19:44:25,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25,886][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.3028438091278076, acc: 0.9239130616188049)
[2025-02-13 19:44:26,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26,264][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.10035821050405502, acc: 0.9624999761581421)
[2025-02-13 19:44:26,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26,647][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.17068900167942047, acc: 0.957446813583374)
[2025-02-13 19:44:26,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27,043][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.39489689469337463, acc: 0.8975609540939331)
[2025-02-13 19:44:27,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27,424][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.33521705865859985, acc: 0.9095744490623474)
[2025-02-13 19:44:27,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27,801][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.1412300318479538, acc: 0.9649122953414917)
[2025-02-13 19:44:27,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28,258][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.23546351492404938, acc: 0.9340659379959106)
[2025-02-13 19:44:28,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28,737][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.1358473002910614, acc: 0.959770143032074)
[2025-02-13 19:44:28,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29,163][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.3579421639442444, acc: 0.9333333373069763)
[2025-02-13 19:44:29,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29,569][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.13236916065216064, acc: 0.9583333134651184)
[2025-02-13 19:44:29,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29,980][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.1881822943687439, acc: 0.9553072452545166)
[2025-02-13 19:44:30,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30,408][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.291185587644577, acc: 0.9265536665916443)
[2025-02-13 19:44:30,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30,817][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.15773622691631317, acc: 0.9735099077224731)
[2025-02-13 19:44:30,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31,267][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.14365996420383453, acc: 0.9689119458198547)
[2025-02-13 19:44:31,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31,659][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.04942047595977783, acc: 0.9879518151283264)
[2025-02-13 19:44:31,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32,079][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.0936591625213623, acc: 0.9733333587646484)
[2025-02-13 19:44:32,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32,560][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.06948557496070862, acc: 0.9863945841789246)
[2025-02-13 19:44:32,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32,930][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.21607157588005066, acc: 0.9463087320327759)
[2025-02-13 19:44:33,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33,298][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.18451711535453796, acc: 0.9580838084220886)
[2025-02-13 19:44:33,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33,722][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.15782268345355988, acc: 0.9585798978805542)
[2025-02-13 19:44:33,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34,147][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.09508222341537476, acc: 0.9732620120048523)
[2025-02-13 19:44:34,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34,639][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.18669484555721283, acc: 0.945652186870575)
[2025-02-13 19:44:34,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35,099][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.4314827024936676, acc: 0.9041916131973267)
[2025-02-13 19:44:35,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35,503][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.07635392248630524, acc: 0.9735099077224731)
[2025-02-13 19:44:35,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35,879][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.023029804229736328, acc: 1.0)
[2025-02-13 19:44:36,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36,297][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.12634976208209991, acc: 0.9801324605941772)
[2025-02-13 19:44:36,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36,680][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.03445926308631897, acc: 0.9928571581840515)
[2025-02-13 19:44:36,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37,095][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.12392067909240723, acc: 0.9720279574394226)
[2025-02-13 19:44:37,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37,473][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.19664393365383148, acc: 0.9626865386962891)
[2025-02-13 19:44:37,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37,854][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.1286424696445465, acc: 0.9715909361839294)
[2025-02-13 19:44:37,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38,252][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.11508118361234665, acc: 0.9661017060279846)
[2025-02-13 19:44:38,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38,636][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.10188527405261993, acc: 0.9822485446929932)
[2025-02-13 19:44:38,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38,995][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.09491728246212006, acc: 0.9733333587646484)
[2025-02-13 19:44:39,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39,441][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.07183198630809784, acc: 0.9813664555549622)
[2025-02-13 19:44:39,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39,842][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.08005285263061523, acc: 0.987261176109314)
[2025-02-13 19:44:40,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40,309][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.13876333832740784, acc: 0.9627329111099243)
[2025-02-13 19:44:40,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40,739][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.052980318665504456, acc: 0.9876543283462524)
[2025-02-13 19:44:40,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41,118][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.03603459149599075, acc: 0.9871794581413269)
[2025-02-13 19:44:41,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41,489][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.05149426311254501, acc: 0.9726027250289917)
[2025-02-13 19:44:41,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41,890][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.09847865998744965, acc: 0.987730085849762)
[2025-02-13 19:44:42,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42,269][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.03967313840985298, acc: 0.9923076629638672)
[2025-02-13 19:44:42,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42,672][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.05339506268501282, acc: 0.9868420958518982)
[2025-02-13 19:44:42,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43,121][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.08474106341600418, acc: 0.9875776171684265)
[2025-02-13 19:44:43,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43,538][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.03079044818878174, acc: 0.9910714030265808)
[2025-02-13 19:44:43,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43,917][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.021887563169002533, acc: 1.0)
[2025-02-13 19:44:44,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44,330][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.08946637809276581, acc: 0.9886363744735718)
[2025-02-13 19:44:44,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44,736][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.02623387798666954, acc: 0.9938650131225586)
[2025-02-13 19:44:44,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45,155][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.01865667663514614, acc: 1.0)
[2025-02-13 19:44:45,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45,555][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.020491255447268486, acc: 1.0)
[2025-02-13 19:44:45,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45,943][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.10625691711902618, acc: 0.9731543660163879)
[2025-02-13 19:44:46,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46,334][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.13689613342285156, acc: 0.9674796462059021)
[2025-02-13 19:44:46,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46,750][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.3466985523700714, acc: 0.9056603908538818)
[2025-02-13 19:44:46,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47,155][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.11291604489088058, acc: 0.9779411554336548)
[2025-02-13 19:44:47,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47,521][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.07411129027605057, acc: 0.9846153855323792)
[2025-02-13 19:44:47,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47,954][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.1204032301902771, acc: 0.9701492786407471)
[2025-02-13 19:44:48,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48,344][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.1252618134021759, acc: 0.9632353186607361)
[2025-02-13 19:44:48,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48,726][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.11110422760248184, acc: 0.9583333134651184)
[2025-02-13 19:44:48,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49,106][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.11652468144893646, acc: 0.9826086759567261)
[2025-02-13 19:44:49,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49,475][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.30095845460891724, acc: 0.9363636374473572)
[2025-02-13 19:44:49,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49,855][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.2756922245025635, acc: 0.8947368264198303)
[2025-02-13 19:44:49,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50,269][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.06639062613248825, acc: 0.9861111044883728)
[2025-02-13 19:44:50,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50,704][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.16495120525360107, acc: 0.9545454382896423)
[2025-02-13 19:44:50,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51,103][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.054755229502916336, acc: 0.9858155846595764)
[2025-02-13 19:44:51,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51,479][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.10026893764734268, acc: 0.9696969985961914)
[2025-02-13 19:44:51,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51,886][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.13052533566951752, acc: 0.9552238583564758)
[2025-02-13 19:44:52,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52,307][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.05016421154141426, acc: 0.9848484992980957)
[2025-02-13 19:44:52,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52,744][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.26479050517082214, acc: 0.9642857313156128)
[2025-02-13 19:44:52,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53,172][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.2517394423484802, acc: 0.9454545378684998)
[2025-02-13 19:44:53,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53,560][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.21547769010066986, acc: 0.9473684430122375)
[2025-02-13 19:44:53,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53,925][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.1583409309387207, acc: 0.9696969985961914)
[2025-02-13 19:44:54,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54,305][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.33595436811447144, acc: 0.9280575513839722)
[2025-02-13 19:44:54,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54,726][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.1880936175584793, acc: 0.9248120188713074)
[2025-02-13 19:44:54,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55,122][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.22443005442619324, acc: 0.9513888955116272)
[2025-02-13 19:44:55,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55,495][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.1699979305267334, acc: 0.9539473652839661)
[2025-02-13 19:44:55,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55,844][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.20943616330623627, acc: 0.9539473652839661)
[2025-02-13 19:44:55,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56,252][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.06121113523840904, acc: 0.9675925970077515)
[2025-02-13 19:44:56,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56,628][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.048435717821121216, acc: 0.9888268113136292)
[2025-02-13 19:44:56,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57,012][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.06104569509625435, acc: 0.9910314083099365)
[2025-02-13 19:44:57,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57,432][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.09304609149694443, acc: 0.9715909361839294)
[2025-02-13 19:44:57,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57,884][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.15589550137519836, acc: 0.9726027250289917)
[2025-02-13 19:44:58,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58,287][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.11719589680433273, acc: 0.971563994884491)
[2025-02-13 19:44:58,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58,731][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.2621065378189087, acc: 0.9465649127960205)
[2025-02-13 19:44:58,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59,108][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.1178029328584671, acc: 0.9792746305465698)
[2025-02-13 19:44:59,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59,524][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.09202968329191208, acc: 0.9822485446929932)
[2025-02-13 19:44:59,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59,972][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.2928391695022583, acc: 0.9298245906829834)
[2025-02-13 19:45:00,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00,346][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.11690253019332886, acc: 0.9583333134651184)
[2025-02-13 19:45:00,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00,768][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.08806444704532623, acc: 0.9808917045593262)
[2025-02-13 19:45:00,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01,186][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.05701812356710434, acc: 0.9891892075538635)
[2025-02-13 19:45:01,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01,639][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.13624712824821472, acc: 0.9569892287254333)
[2025-02-13 19:45:01,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02,034][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.19358748197555542, acc: 0.9664429426193237)
[2025-02-13 19:45:02,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02,410][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.07975011318922043, acc: 0.9862068891525269)
[2025-02-13 19:45:02,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02,788][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.07077101618051529, acc: 0.9864864945411682)
[2025-02-13 19:45:02,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03,161][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.06980608403682709, acc: 0.9802631735801697)
[2025-02-13 19:45:03,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03,570][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.1107201874256134, acc: 0.9688888788223267)
[2025-02-13 19:45:03,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03,975][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.07341939210891724, acc: 0.9864864945411682)
[2025-02-13 19:45:04,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04,324][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.044781848788261414, acc: 0.9904761910438538)
[2025-02-13 19:45:04,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04,717][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.1562110334634781, acc: 0.9490445852279663)
[2025-02-13 19:45:04,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05,148][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.13067027926445007, acc: 0.9624999761581421)
[2025-02-13 19:45:05,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05,530][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.18182162940502167, acc: 0.9617486596107483)
[2025-02-13 19:45:05,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05,897][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.1491369754076004, acc: 0.9596773982048035)
[2025-02-13 19:45:06,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06,329][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.24995218217372894, acc: 0.9308176040649414)
[2025-02-13 19:45:06,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06,723][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.07998455315828323, acc: 0.9890109896659851)
[2025-02-13 19:45:06,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07,117][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.2373504936695099, acc: 0.9503105878829956)
[2025-02-13 19:45:07,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07,510][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.11347188800573349, acc: 0.9679487347602844)
[2025-02-13 19:45:07,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07,961][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.16390426456928253, acc: 0.9716981053352356)
[2025-02-13 19:45:08,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08,341][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.05124210566282272, acc: 0.9850746393203735)
[2025-02-13 19:45:08,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08,777][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.30711662769317627, acc: 0.936170220375061)
[2025-02-13 19:45:08,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09,157][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.17951421439647675, acc: 0.931034505367279)
[2025-02-13 19:45:09,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09,512][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.07111353427171707, acc: 0.970588207244873)
[2025-02-13 19:45:09,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09,894][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.06290696561336517, acc: 0.9808917045593262)
[2025-02-13 19:45:10,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10,303][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.13978037238121033, acc: 0.9561403393745422)
[2025-02-13 19:45:10,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10,742][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.05872446298599243, acc: 0.9720279574394226)
[2025-02-13 19:45:10,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11,183][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.08928897976875305, acc: 0.9937106966972351)
[2025-02-13 19:45:11,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11,618][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.06352028250694275, acc: 0.9869281053543091)
[2025-02-13 19:45:11,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12,004][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.06526194512844086, acc: 0.9754601120948792)
[2025-02-13 19:45:12,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12,428][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.12159708142280579, acc: 0.9685039520263672)
[2025-02-13 19:45:12,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12,833][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.09017068147659302, acc: 0.9826589822769165)
[2025-02-13 19:45:13,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13,281][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.08941053599119186, acc: 0.9726027250289917)
[2025-02-13 19:45:13,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13,715][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.18449662625789642, acc: 0.9668508172035217)
[2025-02-13 19:45:13,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14,171][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.16278915107250214, acc: 0.967391312122345)
[2025-02-13 19:45:14,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14,602][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.35548752546310425, acc: 0.9360465407371521)
[2025-02-13 19:45:14,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15,009][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.36422887444496155, acc: 0.9411764740943909)
[2025-02-13 19:45:15,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15,443][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.16184310615062714, acc: 0.9305555820465088)
[2025-02-13 19:45:15,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15,848][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.1314704716205597, acc: 0.9621211886405945)
[2025-02-13 19:45:15,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16,287][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.26827144622802734, acc: 0.9259259104728699)
[2025-02-13 19:45:16,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16,724][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.22100788354873657, acc: 0.946107804775238)
[2025-02-13 19:45:16,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17,126][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.3320162296295166, acc: 0.913294792175293)
[2025-02-13 19:45:17,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17,535][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.23870934545993805, acc: 0.9447852969169617)
[2025-02-13 19:45:17,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17,911][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.1102714091539383, acc: 0.977011501789093)
[2025-02-13 19:45:18,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18,283][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.05282318592071533, acc: 0.9933333396911621)
[2025-02-13 19:45:18,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18,657][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.09761299192905426, acc: 0.9681528806686401)
[2025-02-13 19:45:18,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19,043][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.12411379814147949, acc: 0.9696969985961914)
[2025-02-13 19:45:19,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19,471][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.159793421626091, acc: 0.9649122953414917)
[2025-02-13 19:45:19,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19,852][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.19767852127552032, acc: 0.9608938694000244)
[2025-02-13 19:45:20,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20,240][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.1690712422132492, acc: 0.9523809552192688)
[2025-02-13 19:45:20,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20,615][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.18179233372211456, acc: 0.9568345546722412)
[2025-02-13 19:45:20,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21,016][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.12254124134778976, acc: 0.9677419066429138)
[2025-02-13 19:45:21,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21,452][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.16001388430595398, acc: 0.9585798978805542)
[2025-02-13 19:45:21,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21,871][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.1331712156534195, acc: 0.9363057613372803)
[2025-02-13 19:45:22,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22,270][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.12087009847164154, acc: 0.9661017060279846)
[2025-02-13 19:45:22,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22,680][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.2018532007932663, acc: 0.9526627063751221)
[2025-02-13 19:45:22,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23,070][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.149992898106575, acc: 0.9534883499145508)
[2025-02-13 19:45:23,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23,484][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.18591096997261047, acc: 0.9518072009086609)
[2025-02-13 19:45:23,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23,869][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.2855188846588135, acc: 0.9464285969734192)
[2025-02-13 19:45:24,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24,253][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.5819825530052185, acc: 0.8720930218696594)
[2025-02-13 19:45:24,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24,621][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.30802008509635925, acc: 0.9215686321258545)
[2025-02-13 19:45:24,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25,017][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.3595740497112274, acc: 0.8706896305084229)
[2025-02-13 19:45:25,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25,425][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.33499786257743835, acc: 0.9032257795333862)
[2025-02-13 19:45:25,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25,817][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.14551375806331635, acc: 0.9747899174690247)
[2025-02-13 19:45:25,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26,231][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.3847390413284302, acc: 0.9171270728111267)
[2025-02-13 19:45:26,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26,639][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.1829635053873062, acc: 0.934959352016449)
[2025-02-13 19:45:26,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27,048][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.19675394892692566, acc: 0.9279999732971191)
[2025-02-13 19:45:27,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27,467][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.4035833179950714, acc: 0.869767427444458)
[2025-02-13 19:45:27,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27,869][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.2507176399230957, acc: 0.9368420839309692)
[2025-02-13 19:45:28,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28,276][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.23211833834648132, acc: 0.956250011920929)
[2025-02-13 19:45:28,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28,668][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.07911594212055206, acc: 0.9864864945411682)
[2025-02-13 19:45:28,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29,064][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.09665048122406006, acc: 0.9670329689979553)
[2025-02-13 19:45:29,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29,452][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.2485845983028412, acc: 0.9653179049491882)
[2025-02-13 19:45:29,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29,825][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.16380180418491364, acc: 0.9746835231781006)
[2025-02-13 19:45:29,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30,211][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.2616099715232849, acc: 0.9365853667259216)
[2025-02-13 19:45:30,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30,603][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.12427149713039398, acc: 0.9731183052062988)
[2025-02-13 19:45:30,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30,992][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.17020002007484436, acc: 0.9696969985961914)
[2025-02-13 19:45:31,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31,449][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.20086392760276794, acc: 0.9556962251663208)
[2025-02-13 19:45:31,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31,919][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.17625641822814941, acc: 0.9471153616905212)
[2025-02-13 19:45:32,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32,353][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.17213019728660583, acc: 0.95652174949646)
[2025-02-13 19:45:32,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32,780][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.3078308403491974, acc: 0.9244186282157898)
[2025-02-13 19:45:32,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33,149][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.2785131335258484, acc: 0.926086962223053)
[2025-02-13 19:45:33,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33,559][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.12226978689432144, acc: 0.9675925970077515)
[2025-02-13 19:45:33,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33,991][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.28342774510383606, acc: 0.949999988079071)
[2025-02-13 19:45:34,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34,393][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.1580747365951538, acc: 0.9653465151786804)
[2025-02-13 19:45:34,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34,775][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.11962241679430008, acc: 0.9712918400764465)
[2025-02-13 19:45:34,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35,197][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.10459515452384949, acc: 0.9751243591308594)
[2025-02-13 19:45:35,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35,600][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.14576692879199982, acc: 0.9586206674575806)
[2025-02-13 19:45:35,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36,045][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.21943840384483337, acc: 0.9550561904907227)
[2025-02-13 19:45:36,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36,476][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.10557089745998383, acc: 0.9759036302566528)
[2025-02-13 19:45:36,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36,871][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.11050198972225189, acc: 0.976190447807312)
[2025-02-13 19:45:37,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37,290][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.22832795977592468, acc: 0.9595959782600403)
[2025-02-13 19:45:37,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37,716][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.06950969249010086, acc: 0.9896373152732849)
[2025-02-13 19:45:37,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:38,121][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.07967730611562729, acc: 0.9704142212867737)
[2025-02-13 19:45:38,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:38,614][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.13393117487430573, acc: 0.9759615659713745)
[2025-02-13 19:45:38,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39,033][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.25816676020622253, acc: 0.9350000023841858)
[2025-02-13 19:45:39,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39,445][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.12437117844820023, acc: 0.9666666388511658)
[2025-02-13 19:45:39,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39,834][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.2868891954421997, acc: 0.9038461446762085)
[2025-02-13 19:45:39,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40,221][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.16804596781730652, acc: 0.9817073345184326)
[2025-02-13 19:45:40,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40,624][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.4678212106227875, acc: 0.8999999761581421)
[2025-02-13 19:45:40,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41,026][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.3075357973575592, acc: 0.9379844665527344)
[2025-02-13 19:45:41,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41,584][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.07919532060623169, acc: 0.9741935729980469)
[2025-02-13 19:45:41,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41,990][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.27439889311790466, acc: 0.9395973086357117)
[2025-02-13 19:45:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42,377][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.13531510531902313, acc: 0.9605262875556946)
[2025-02-13 19:45:42,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42,779][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.10690409690141678, acc: 0.9772727489471436)
[2025-02-13 19:45:42,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43,185][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.18558228015899658, acc: 0.95652174949646)
[2025-02-13 19:45:43,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43,599][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.3941306471824646, acc: 0.930232584476471)
[2025-02-13 19:45:43,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43,985][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.6003955602645874, acc: 0.8724831938743591)
[2025-02-13 19:45:44,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44,462][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.42873555421829224, acc: 0.8896104097366333)
[2025-02-13 19:45:44,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44,888][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.3467799425125122, acc: 0.9177215099334717)
[2025-02-13 19:45:45,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45,298][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.2582905888557434, acc: 0.9457831382751465)
[2025-02-13 19:45:45,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45,748][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.30254700779914856, acc: 0.916201114654541)
[2025-02-13 19:45:45,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46,168][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.28154903650283813, acc: 0.9358974099159241)
[2025-02-13 19:45:46,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46,571][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.19677110016345978, acc: 0.957446813583374)
[2025-02-13 19:45:46,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46,995][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.160940021276474, acc: 0.9664804339408875)
[2025-02-13 19:45:47,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47,415][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.22671186923980713, acc: 0.9441340565681458)
[2025-02-13 19:45:47,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47,798][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.18820726871490479, acc: 0.9554139971733093)
[2025-02-13 19:45:47,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48,209][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.14014525711536407, acc: 0.9554139971733093)
[2025-02-13 19:45:48,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48,581][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.12967602908611298, acc: 0.9612902998924255)
[2025-02-13 19:45:48,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48,972][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.28316548466682434, acc: 0.949999988079071)
[2025-02-13 19:45:49,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49,333][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.35441136360168457, acc: 0.9135135412216187)
[2025-02-13 19:45:49,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49,688][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.9583936333656311, acc: 0.8193548321723938)
[2025-02-13 19:45:49,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50,074][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.3305128812789917, acc: 0.9261363744735718)
[2025-02-13 19:45:50,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50,485][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.06763995438814163, acc: 0.9884393215179443)
[2025-02-13 19:45:50,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50,916][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.15030239522457123, acc: 0.97826087474823)
[2025-02-13 19:45:51,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51,326][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.23765026032924652, acc: 0.9395973086357117)
[2025-02-13 19:45:51,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51,755][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.5039466619491577, acc: 0.9276315569877625)
[2025-02-13 19:45:51,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52,155][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.3752696216106415, acc: 0.9271844625473022)
[2025-02-13 19:45:52,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52,529][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.2649460434913635, acc: 0.9378882050514221)
[2025-02-13 19:45:52,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52,917][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.303058922290802, acc: 0.9471153616905212)
[2025-02-13 19:45:53,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53,357][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.17926499247550964, acc: 0.9622641801834106)
[2025-02-13 19:45:53,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53,810][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.21285539865493774, acc: 0.9563106894493103)
[2025-02-13 19:45:53,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54,194][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.09205316007137299, acc: 0.9901960492134094)
[2025-02-13 19:45:54,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54,613][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.21093249320983887, acc: 0.949999988079071)
[2025-02-13 19:45:54,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55,030][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.38586747646331787, acc: 0.9458128213882446)
[2025-02-13 19:45:55,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55,440][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.11529785394668579, acc: 0.9754902124404907)
[2025-02-13 19:45:55,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55,848][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.29190734028816223, acc: 0.9583333134651184)
[2025-02-13 19:45:55,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56,239][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.5553306341171265, acc: 0.896774172782898)
[2025-02-13 19:45:56,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56,640][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.22974258661270142, acc: 0.9430052042007446)
[2025-02-13 19:45:56,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57,001][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.2116561233997345, acc: 0.9364162087440491)
[2025-02-13 19:45:57,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57,471][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.2253846377134323, acc: 0.9453781247138977)
[2025-02-13 19:45:57,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57,827][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.08085326105356216, acc: 0.9846938848495483)
[2025-02-13 19:45:57,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58,211][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.12539328634738922, acc: 0.9570552110671997)
[2025-02-13 19:45:58,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58,600][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.2436816543340683, acc: 0.9398906826972961)
[2025-02-13 19:45:58,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58,948][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.3439510464668274, acc: 0.9333333373069763)
[2025-02-13 19:45:59,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59,329][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.5993325114250183, acc: 0.8682634830474854)
[2025-02-13 19:45:59,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59,739][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.19128578901290894, acc: 0.9449541568756104)
[2025-02-13 19:45:59,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00,147][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.19852030277252197, acc: 0.9459459185600281)
[2025-02-13 19:46:00,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00,528][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.027895549312233925, acc: 0.9938650131225586)
[2025-02-13 19:46:00,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00,902][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.11039271950721741, acc: 0.9744898080825806)
[2025-02-13 19:46:01,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01,268][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.06964121758937836, acc: 0.9884393215179443)
[2025-02-13 19:46:01,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01,676][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.08729971945285797, acc: 0.9893617033958435)
[2025-02-13 19:46:01,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02,093][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.16669495403766632, acc: 0.9514563083648682)
[2025-02-13 19:46:02,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02,491][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.13107948005199432, acc: 0.9608938694000244)
[2025-02-13 19:46:02,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02,955][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.058595210313797, acc: 0.9836065769195557)
[2025-02-13 19:46:03,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03,375][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.23366239666938782, acc: 0.9312169551849365)
[2025-02-13 19:46:03,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03,792][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.24220789968967438, acc: 0.9457831382751465)
[2025-02-13 19:46:03,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04,189][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.11501277983188629, acc: 0.9852941036224365)
[2025-02-13 19:46:04,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04,655][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.24813951551914215, acc: 0.9392523169517517)
[2025-02-13 19:46:04,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05,008][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.2231510728597641, acc: 0.9117646813392639)
[2025-02-13 19:46:05,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05,423][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.06828299909830093, acc: 0.9856114983558655)
[2025-02-13 19:46:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05,887][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.2533019483089447, acc: 0.9399999976158142)
[2025-02-13 19:46:06,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06,268][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.08335884660482407, acc: 0.9722222089767456)
[2025-02-13 19:46:06,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06,623][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.1414901614189148, acc: 0.9569892287254333)
[2025-02-13 19:46:06,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07,013][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.13778385519981384, acc: 0.9642857313156128)
[2025-02-13 19:46:07,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07,380][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.151719331741333, acc: 0.9599999785423279)
[2025-02-13 19:46:07,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07,705][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.14472848176956177, acc: 0.95652174949646)
[2025-02-13 19:46:07,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08,070][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.24543452262878418, acc: 0.95652174949646)
[2025-02-13 19:46:08,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08,471][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.11990746855735779, acc: 0.9597315192222595)
[2025-02-13 19:46:08,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08,853][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.11939400434494019, acc: 0.9714285731315613)
[2025-02-13 19:46:09,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09,304][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.054143358021974564, acc: 0.9928571581840515)
[2025-02-13 19:46:09,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09,712][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.01671133004128933, acc: 1.0)
[2025-02-13 19:46:09,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10,117][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.2096557766199112, acc: 0.9513888955116272)
[2025-02-13 19:46:10,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10,534][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.114157535135746, acc: 0.9766082167625427)
[2025-02-13 19:46:10,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10,932][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.10470233112573624, acc: 0.9663865566253662)
[2025-02-13 19:46:11,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11,335][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.47664138674736023, acc: 0.8985507488250732)
[2025-02-13 19:46:11,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11,715][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.5721572041511536, acc: 0.8602941036224365)
[2025-02-13 19:46:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12,106][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.3526015281677246, acc: 0.8939393758773804)
[2025-02-13 19:46:12,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12,487][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.47002363204956055, acc: 0.9059829115867615)
[2025-02-13 19:46:12,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12,887][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.5044348239898682, acc: 0.888059675693512)
[2025-02-13 19:46:13,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13,293][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.2772452235221863, acc: 0.9350649118423462)
[2025-02-13 19:46:13,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13,690][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.22003820538520813, acc: 0.9290322661399841)
[2025-02-13 19:46:13,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14,055][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.3471747636795044, acc: 0.9285714030265808)
[2025-02-13 19:46:14,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14,479][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.3679489195346832, acc: 0.9292035102844238)
[2025-02-13 19:46:14,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14,865][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.32188522815704346, acc: 0.9382715821266174)
[2025-02-13 19:46:15,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15,302][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.4195839762687683, acc: 0.9075144529342651)
[2025-02-13 19:46:15,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15,750][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.2820383310317993, acc: 0.9444444179534912)
[2025-02-13 19:46:15,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16,209][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.25822713971138, acc: 0.9352940917015076)
[2025-02-13 19:46:16,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16,657][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.35995885729789734, acc: 0.9255319237709045)
[2025-02-13 19:46:16,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17,079][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.4651179611682892, acc: 0.8908045887947083)
[2025-02-13 19:46:17,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17,507][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.6690288186073303, acc: 0.8350515365600586)
[2025-02-13 19:46:17,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17,970][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.1966230273246765, acc: 0.9444444179534912)
[2025-02-13 19:46:18,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18,413][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.31361186504364014, acc: 0.9192546606063843)
[2025-02-13 19:46:18,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18,787][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.2704068422317505, acc: 0.9109588861465454)
[2025-02-13 19:46:18,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19,196][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.3611491918563843, acc: 0.9133333563804626)
[2025-02-13 19:46:19,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19,621][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.2747223377227783, acc: 0.9484536051750183)
[2025-02-13 19:46:19,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20,009][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.49843549728393555, acc: 0.8650306463241577)
[2025-02-13 19:46:20,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20,419][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.1668124943971634, acc: 0.949999988079071)
[2025-02-13 19:46:20,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20,798][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.14415138959884644, acc: 0.9587628841400146)
[2025-02-13 19:46:20,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21,222][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.2894509732723236, acc: 0.9611650705337524)
[2025-02-13 19:46:21,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21,595][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.169800266623497, acc: 0.9626865386962891)
[2025-02-13 19:46:21,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22,023][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.2507683336734772, acc: 0.9272727370262146)
[2025-02-13 19:46:22,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22,393][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.1201670914888382, acc: 0.970370352268219)
[2025-02-13 19:46:22,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22,790][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.3840724229812622, acc: 0.9320987462997437)
[2025-02-13 19:46:22,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23,204][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.17731530964374542, acc: 0.9670329689979553)
[2025-02-13 19:46:23,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23,565][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.24645152688026428, acc: 0.9411764740943909)
[2025-02-13 19:46:23,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23,931][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.11838550120592117, acc: 0.9751552939414978)
[2025-02-13 19:46:24,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24,320][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.08307944238185883, acc: 0.970059871673584)
[2025-02-13 19:46:24,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24,679][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.17018331587314606, acc: 0.9685039520263672)
[2025-02-13 19:46:24,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25,048][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.1432960331439972, acc: 0.9805194735527039)
[2025-02-13 19:46:25,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25,435][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.1412455141544342, acc: 0.9459459185600281)
[2025-02-13 19:46:25,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25,805][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.21572332084178925, acc: 0.9542483687400818)
[2025-02-13 19:46:25,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26,186][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.4188960790634155, acc: 0.9166666865348816)
[2025-02-13 19:46:26,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26,557][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.2152186632156372, acc: 0.9668508172035217)
[2025-02-13 19:46:26,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26,901][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.07586511969566345, acc: 0.9917355179786682)
[2025-02-13 19:46:27,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27,322][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.21150434017181396, acc: 0.9644669890403748)
[2025-02-13 19:46:27,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27,719][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.21442094445228577, acc: 0.9556962251663208)
[2025-02-13 19:46:27,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28,148][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.24667441844940186, acc: 0.9492753744125366)
[2025-02-13 19:46:28,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28,532][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.4399164021015167, acc: 0.9236640930175781)
[2025-02-13 19:46:28,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28,908][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.22454127669334412, acc: 0.939226508140564)
[2025-02-13 19:46:29,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29,279][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.14010407030582428, acc: 0.957317054271698)
[2025-02-13 19:46:29,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29,655][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.30341142416000366, acc: 0.9505494236946106)
[2025-02-13 19:46:29,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30,041][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.07202868163585663, acc: 0.9740259647369385)
[2025-02-13 19:46:30,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30,397][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.3040813207626343, acc: 0.9421965479850769)
[2025-02-13 19:46:30,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30,753][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.15572862327098846, acc: 0.9433962106704712)
[2025-02-13 19:46:30,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31,132][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.13193844258785248, acc: 0.9652174115180969)
[2025-02-13 19:46:31,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31,457][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.33766576647758484, acc: 0.9107142686843872)
[2025-02-13 19:46:31,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31,816][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.2381087988615036, acc: 0.9520547986030579)
[2025-02-13 19:46:31,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32,183][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.10980929434299469, acc: 0.9870129823684692)
[2025-02-13 19:46:32,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32,556][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.9761770367622375, acc: 0.875)
[2025-02-13 19:46:32,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33,005][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.17380453646183014, acc: 0.9613259434700012)
[2025-02-13 19:46:33,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33,416][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.19516734778881073, acc: 0.9571428298950195)
[2025-02-13 19:46:33,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33,829][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.11380288749933243, acc: 0.9736841917037964)
[2025-02-13 19:46:33,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34,241][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.2911340892314911, acc: 0.9459459185600281)
[2025-02-13 19:46:34,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34,635][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.0581546425819397, acc: 0.9917355179786682)
[2025-02-13 19:46:34,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35,031][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.19274863600730896, acc: 0.97826087474823)
[2025-02-13 19:46:35,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35,411][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.30480045080184937, acc: 0.9275362491607666)
[2025-02-13 19:46:35,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35,761][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.10196075588464737, acc: 0.9809523820877075)
[2025-02-13 19:46:35,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36,132][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.40652668476104736, acc: 0.9007633328437805)
[2025-02-13 19:46:36,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36,529][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.2856297194957733, acc: 0.9108280539512634)
[2025-02-13 19:46:36,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36,904][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.1616823673248291, acc: 0.9593908786773682)
[2025-02-13 19:46:37,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37,329][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.31582000851631165, acc: 0.9251101613044739)
[2025-02-13 19:46:37,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37,717][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.21654845774173737, acc: 0.9494949579238892)
[2025-02-13 19:46:37,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38,144][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.18115364015102386, acc: 0.9672897458076477)
[2025-02-13 19:46:38,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38,568][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.26396507024765015, acc: 0.9353448152542114)
[2025-02-13 19:46:38,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38,972][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.2660391330718994, acc: 0.9502262473106384)
[2025-02-13 19:46:39,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39,343][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.32825741171836853, acc: 0.9369369149208069)
[2025-02-13 19:46:39,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39,728][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.22385744750499725, acc: 0.9579439163208008)
[2025-02-13 19:46:39,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40,083][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.2657139301300049, acc: 0.9397590160369873)
[2025-02-13 19:46:40,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40,509][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.10401768982410431, acc: 0.9738219976425171)
[2025-02-13 19:46:40,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40,917][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.1416092813014984, acc: 0.9581151604652405)
[2025-02-13 19:46:41,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41,292][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.2621438503265381, acc: 0.9308176040649414)
[2025-02-13 19:46:41,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41,691][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.17917650938034058, acc: 0.9545454382896423)
[2025-02-13 19:46:41,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42,060][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.14332425594329834, acc: 0.9819276928901672)
[2025-02-13 19:46:42,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42,435][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.3753087818622589, acc: 0.8963963985443115)
[2025-02-13 19:46:42,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42,834][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.21574029326438904, acc: 0.9516128897666931)
[2025-02-13 19:46:42,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43,220][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.24367378652095795, acc: 0.9288889169692993)
[2025-02-13 19:46:43,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43,596][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.16213704645633698, acc: 0.9560439586639404)
[2025-02-13 19:46:43,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44,049][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.14465981721878052, acc: 0.9593908786773682)
[2025-02-13 19:46:44,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44,424][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.15320123732089996, acc: 0.9655172228813171)
[2025-02-13 19:46:44,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44,801][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.0882921814918518, acc: 0.9696969985961914)
[2025-02-13 19:46:44,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45,184][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.14699630439281464, acc: 0.9609755873680115)
[2025-02-13 19:46:45,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45,568][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.08220728486776352, acc: 0.9805825352668762)
[2025-02-13 19:46:45,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45,962][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.08755617588758469, acc: 0.9784482717514038)
[2025-02-13 19:46:46,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46,406][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.19051286578178406, acc: 0.9530516266822815)
[2025-02-13 19:46:46,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46,792][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.12024758011102676, acc: 0.9724770784378052)
[2025-02-13 19:46:46,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47,131][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.21962599456310272, acc: 0.9454545378684998)
[2025-02-13 19:46:47,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47,582][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.182824969291687, acc: 0.9631147384643555)
[2025-02-13 19:46:47,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47,929][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.4019717574119568, acc: 0.9172932505607605)
[2025-02-13 19:46:48,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48,318][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.17228427529335022, acc: 0.9631578922271729)
[2025-02-13 19:46:48,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48,697][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.19577232003211975, acc: 0.9675675630569458)
[2025-02-13 19:46:48,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49,030][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.3726974129676819, acc: 0.9078947305679321)
[2025-02-13 19:46:49,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49,403][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.22065360844135284, acc: 0.9547738432884216)
[2025-02-13 19:46:49,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49,793][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.2130793035030365, acc: 0.9677419066429138)
[2025-02-13 19:46:49,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50,174][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.17877483367919922, acc: 0.9391891956329346)
[2025-02-13 19:46:50,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50,566][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.24948637187480927, acc: 0.9570552110671997)
[2025-02-13 19:46:50,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50,951][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.25726187229156494, acc: 0.9536082744598389)
[2025-02-13 19:46:51,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51,344][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.21655578911304474, acc: 0.9476743936538696)
[2025-02-13 19:46:51,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51,738][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.19905370473861694, acc: 0.9440993666648865)
[2025-02-13 19:46:51,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52,092][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.2504463195800781, acc: 0.9308510422706604)
[2025-02-13 19:46:52,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52,486][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.20430980622768402, acc: 0.9532163739204407)
[2025-02-13 19:46:52,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52,898][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.21181617677211761, acc: 0.935960590839386)
[2025-02-13 19:46:53,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53,292][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.2626309096813202, acc: 0.9399999976158142)
[2025-02-13 19:46:53,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53,672][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.1736004650592804, acc: 0.9655172228813171)
[2025-02-13 19:46:53,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54,034][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.1815754771232605, acc: 0.9506173133850098)
[2025-02-13 19:46:54,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54,405][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.27877581119537354, acc: 0.9216867685317993)
[2025-02-13 19:46:54,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54,763][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.15875692665576935, acc: 0.9685863852500916)
[2025-02-13 19:46:54,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55,183][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.2844972014427185, acc: 0.9180327653884888)
[2025-02-13 19:46:55,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55,578][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.25047993659973145, acc: 0.9365853667259216)
[2025-02-13 19:46:55,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55,962][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.3537473976612091, acc: 0.9044944047927856)
[2025-02-13 19:46:56,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56,341][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.38949713110923767, acc: 0.9215686321258545)
[2025-02-13 19:46:56,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56,737][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.19665154814720154, acc: 0.9455445408821106)
[2025-02-13 19:46:56,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57,118][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.18365952372550964, acc: 0.9748427867889404)
[2025-02-13 19:46:57,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57,558][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.4102626442909241, acc: 0.9139785170555115)
[2025-02-13 19:46:57,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57,976][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.4617089033126831, acc: 0.9032257795333862)
[2025-02-13 19:46:58,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58,360][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.23153294622898102, acc: 0.9385474920272827)
[2025-02-13 19:46:58,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58,744][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.3733847439289093, acc: 0.9354838728904724)
[2025-02-13 19:46:58,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59,119][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.14846745133399963, acc: 0.965753436088562)
[2025-02-13 19:46:59,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59,515][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.23364248871803284, acc: 0.9371069073677063)
[2025-02-13 19:46:59,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59,855][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.3077627122402191, acc: 0.9151515364646912)
[2025-02-13 19:46:59,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00,219][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.33166831731796265, acc: 0.9178082346916199)
[2025-02-13 19:47:00,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00,608][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.41540655493736267, acc: 0.8974359035491943)
[2025-02-13 19:47:00,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01,000][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.18286344408988953, acc: 0.9395973086357117)
[2025-02-13 19:47:01,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01,357][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.19025327265262604, acc: 0.9615384340286255)
[2025-02-13 19:47:01,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01,700][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.26500147581100464, acc: 0.9391891956329346)
[2025-02-13 19:47:01,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02,071][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.15232950448989868, acc: 0.9696969985961914)
[2025-02-13 19:47:02,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02,399][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.18983475863933563, acc: 0.9777777791023254)
[2025-02-13 19:47:02,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02,758][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.07799261808395386, acc: 0.9855072498321533)
[2025-02-13 19:47:02,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03,140][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.09329597651958466, acc: 0.9717513918876648)
[2025-02-13 19:47:03,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03,506][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.21293166279792786, acc: 0.9305555820465088)
[2025-02-13 19:47:03,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03,880][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.1742166429758072, acc: 0.9314285516738892)
[2025-02-13 19:47:04,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04,257][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.18359558284282684, acc: 0.9358974099159241)
[2025-02-13 19:47:04,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04,623][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.19239580631256104, acc: 0.946107804775238)
[2025-02-13 19:47:04,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04,963][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.0875454768538475, acc: 0.987261176109314)
[2025-02-13 19:47:05,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05,412][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.15816709399223328, acc: 0.9536423683166504)
[2025-02-13 19:47:05,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05,788][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.1617259830236435, acc: 0.9657142758369446)
[2025-02-13 19:47:05,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06,165][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.15177975594997406, acc: 0.9677419066429138)
[2025-02-13 19:47:06,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06,556][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.15767672657966614, acc: 0.9523809552192688)
[2025-02-13 19:47:06,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06,924][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.26138588786125183, acc: 0.956250011920929)
[2025-02-13 19:47:07,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07,325][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.21522939205169678, acc: 0.953125)
[2025-02-13 19:47:07,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07,672][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.11926285177469254, acc: 0.9813664555549622)
[2025-02-13 19:47:07,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08,090][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.12531043589115143, acc: 0.9695431590080261)
[2025-02-13 19:47:08,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08,434][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.18040749430656433, acc: 0.9670329689979553)
[2025-02-13 19:47:08,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08,810][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.11863187700510025, acc: 0.9774436354637146)
[2025-02-13 19:47:08,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09,163][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.4182451665401459, acc: 0.9150000214576721)
[2025-02-13 19:47:09,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09,557][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.23067866265773773, acc: 0.9365853667259216)
[2025-02-13 19:47:09,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09,922][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.17949607968330383, acc: 0.956250011920929)
[2025-02-13 19:47:10,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10,298][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.15102452039718628, acc: 0.9560439586639404)
[2025-02-13 19:47:10,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10,669][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.27117958664894104, acc: 0.9281437397003174)
[2025-02-13 19:47:10,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11,032][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.0919361487030983, acc: 0.9808917045593262)
[2025-02-13 19:47:11,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11,400][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.37992143630981445, acc: 0.9203540086746216)
[2025-02-13 19:47:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11,774][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.21733281016349792, acc: 0.9277108311653137)
[2025-02-13 19:47:11,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12,162][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.18071430921554565, acc: 0.9516128897666931)
[2025-02-13 19:47:12,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12,567][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.29672491550445557, acc: 0.9419087171554565)
[2025-02-13 19:47:12,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12,953][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.16382168233394623, acc: 0.9629629850387573)
[2025-02-13 19:47:13,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13,345][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.20842061936855316, acc: 0.9563318490982056)
[2025-02-13 19:47:13,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13,705][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.10122891515493393, acc: 0.9823529124259949)
[2025-02-13 19:47:13,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14,073][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.19331148266792297, acc: 0.948387086391449)
[2025-02-13 19:47:14,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14,455][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.20308686792850494, acc: 0.9516907930374146)
[2025-02-13 19:47:14,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14,862][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.19806741178035736, acc: 0.9409090876579285)
[2025-02-13 19:47:15,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15,246][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.24977898597717285, acc: 0.9375)
[2025-02-13 19:47:15,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15,677][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.3292001485824585, acc: 0.9313725233078003)
[2025-02-13 19:47:15,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16,098][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.41288772225379944, acc: 0.8727272748947144)
[2025-02-13 19:47:16,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16,498][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.5187354683876038, acc: 0.8860759735107422)
[2025-02-13 19:47:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16,856][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.1803659349679947, acc: 0.9523809552192688)
[2025-02-13 19:47:16,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17,221][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.2938917279243469, acc: 0.9105263352394104)
[2025-02-13 19:47:17,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17,619][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.3458107113838196, acc: 0.9095744490623474)
[2025-02-13 19:47:17,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18,002][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.169134259223938, acc: 0.9695431590080261)
[2025-02-13 19:47:18,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18,379][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.1854247897863388, acc: 0.9617486596107483)
[2025-02-13 19:47:18,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18,750][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.10745050758123398, acc: 0.9713114500045776)
[2025-02-13 19:47:18,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19,124][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.2212994545698166, acc: 0.93034827709198)
[2025-02-13 19:47:19,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19,531][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.5133988857269287, acc: 0.8705036044120789)
[2025-02-13 19:47:19,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19,933][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.1693006008863449, acc: 0.9653679728507996)
[2025-02-13 19:47:20,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20,276][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.21666847169399261, acc: 0.9428571462631226)
[2025-02-13 19:47:20,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20,659][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.2704993784427643, acc: 0.9433962106704712)
[2025-02-13 19:47:20,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21,033][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.14578476548194885, acc: 0.9515151381492615)
[2025-02-13 19:47:21,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21,404][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.39822763204574585, acc: 0.8911564350128174)
[2025-02-13 19:47:21,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21,778][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.07309188693761826, acc: 0.9781022071838379)
[2025-02-13 19:47:21,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22,145][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.11170017719268799, acc: 0.9791666865348816)
[2025-02-13 19:47:22,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22,534][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.1355542093515396, acc: 0.9568345546722412)
[2025-02-13 19:47:22,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22,971][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.0811871588230133, acc: 0.9756097793579102)
[2025-02-13 19:47:23,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23,366][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.0768294408917427, acc: 0.9919999837875366)
[2025-02-13 19:47:23,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23,807][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.11119132488965988, acc: 0.9622641801834106)
[2025-02-13 19:47:23,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24,174][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.04839995503425598, acc: 0.991525411605835)
[2025-02-13 19:47:24,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24,612][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.34443235397338867, acc: 0.9313725233078003)
[2025-02-13 19:47:24,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25,011][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.0909334346652031, acc: 0.9800000190734863)
[2025-02-13 19:47:25,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25,367][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.14286492764949799, acc: 0.9694656729698181)
[2025-02-13 19:47:25,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25,764][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.1143481656908989, acc: 0.9629629850387573)
[2025-02-13 19:47:25,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26,170][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.15296564996242523, acc: 0.9646017551422119)
[2025-02-13 19:47:26,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26,547][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.03565632551908493, acc: 1.0)
[2025-02-13 19:47:26,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26,965][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.01760290563106537, acc: 1.0)
[2025-02-13 19:47:27,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27,418][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.24242767691612244, acc: 0.9514563083648682)
[2025-02-13 19:47:27,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27,796][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.29858386516571045, acc: 0.931034505367279)
[2025-02-13 19:47:27,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28,152][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.25252237915992737, acc: 0.9527559280395508)
[2025-02-13 19:47:28,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28,526][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.16101162135601044, acc: 0.9440000057220459)
[2025-02-13 19:47:28,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28,916][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.2578151226043701, acc: 0.9396551847457886)
[2025-02-13 19:47:29,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29,295][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.16382522881031036, acc: 0.9651162624359131)
[2025-02-13 19:47:29,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29,682][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.4344539940357208, acc: 0.8920863270759583)
[2025-02-13 19:47:29,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30,071][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.28190380334854126, acc: 0.9166666865348816)
[2025-02-13 19:47:30,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30,498][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.38149580359458923, acc: 0.9117646813392639)
[2025-02-13 19:47:30,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30,922][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.12953364849090576, acc: 0.9652777910232544)
[2025-02-13 19:47:31,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31,293][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.5687183141708374, acc: 0.8785046935081482)
[2025-02-13 19:47:31,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31,638][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.25220605731010437, acc: 0.9333333373069763)
[2025-02-13 19:47:31,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32,010][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.16808538138866425, acc: 0.9536423683166504)
[2025-02-13 19:47:32,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32,384][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.14371329545974731, acc: 0.9715909361839294)
[2025-02-13 19:47:32,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32,780][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.23981796205043793, acc: 0.9375)
[2025-02-13 19:47:32,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33,165][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.4331636130809784, acc: 0.8928571343421936)
[2025-02-13 19:47:33,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33,536][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.1638835221529007, acc: 0.9530201554298401)
[2025-02-13 19:47:33,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33,896][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.2254132181406021, acc: 0.961904764175415)
[2025-02-13 19:47:34,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34,271][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.3976258635520935, acc: 0.8814814686775208)
[2025-02-13 19:47:34,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34,661][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.7504629492759705, acc: 0.8059701323509216)
[2025-02-13 19:47:34,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35,032][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.4467505216598511, acc: 0.9130434989929199)
[2025-02-13 19:47:35,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35,438][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.30850499868392944, acc: 0.9420289993286133)
[2025-02-13 19:47:35,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35,852][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.30356431007385254, acc: 0.9080459475517273)
[2025-02-13 19:47:35,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36,218][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.14971520006656647, acc: 0.9768785834312439)
[2025-02-13 19:47:36,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36,593][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.16176879405975342, acc: 0.9545454382896423)
[2025-02-13 19:47:36,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37,001][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.2076244056224823, acc: 0.94017094373703)
[2025-02-13 19:47:37,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37,379][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.23402076959609985, acc: 0.9312977194786072)
[2025-02-13 19:47:37,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37,765][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.12376601248979568, acc: 0.9451219439506531)
[2025-02-13 19:47:37,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38,118][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.3179817199707031, acc: 0.932330846786499)
[2025-02-13 19:47:38,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38,497][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.27583014965057373, acc: 0.949367105960846)
[2025-02-13 19:47:38,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38,885][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.3752749264240265, acc: 0.9172413945198059)
[2025-02-13 19:47:39,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39,411][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.5534729361534119, acc: 0.8834356069564819)
[2025-02-13 19:47:39,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39,800][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.3060556650161743, acc: 0.9190751314163208)
[2025-02-13 19:47:39,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40,201][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.5107210874557495, acc: 0.8986486196517944)
[2025-02-13 19:47:40,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40,578][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.2297113537788391, acc: 0.9452054500579834)
[2025-02-13 19:47:40,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40,944][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.354569673538208, acc: 0.9391891956329346)
[2025-02-13 19:47:41,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41,313][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.2533126473426819, acc: 0.9448275566101074)
[2025-02-13 19:47:41,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41,700][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.18023595213890076, acc: 0.9530201554298401)
[2025-02-13 19:47:41,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42,082][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.35302475094795227, acc: 0.9252873659133911)
[2025-02-13 19:47:42,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42,454][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.32199549674987793, acc: 0.9235668778419495)
[2025-02-13 19:47:42,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42,790][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.46106570959091187, acc: 0.8881118893623352)
[2025-02-13 19:47:42,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43,179][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.2197791188955307, acc: 0.9453551769256592)
[2025-02-13 19:47:43,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43,569][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.3587949872016907, acc: 0.9433962106704712)
[2025-02-13 19:47:43,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43,933][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.21524052321910858, acc: 0.9605262875556946)
[2025-02-13 19:47:44,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44,301][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.45404237508773804, acc: 0.9504132270812988)
[2025-02-13 19:47:44,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44,672][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.5114014148712158, acc: 0.9064748287200928)
[2025-02-13 19:47:44,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45,052][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.3058304488658905, acc: 0.9161290526390076)
[2025-02-13 19:47:45,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45,416][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.11757934838533401, acc: 0.9683544039726257)
[2025-02-13 19:47:45,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45,815][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.3112817108631134, acc: 0.9195402264595032)
[2025-02-13 19:47:45,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46,212][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.26131361722946167, acc: 0.9397590160369873)
[2025-02-13 19:47:46,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46,618][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.18238003551959991, acc: 0.9487179517745972)
[2025-02-13 19:47:46,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47,000][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.18816353380680084, acc: 0.9568345546722412)
[2025-02-13 19:47:47,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47,343][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.28241321444511414, acc: 0.9300699234008789)
[2025-02-13 19:47:47,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47,672][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.2963171601295471, acc: 0.9230769276618958)
[2025-02-13 19:47:47,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48,066][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.22780735790729523, acc: 0.9333333373069763)
[2025-02-13 19:47:48,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48,439][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.40347009897232056, acc: 0.9205297827720642)
[2025-02-13 19:47:48,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48,832][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.14890523254871368, acc: 0.9558823704719543)
[2025-02-13 19:47:48,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49,217][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.2486458122730255, acc: 0.9642857313156128)
[2025-02-13 19:47:49,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49,620][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.22342610359191895, acc: 0.9189189076423645)
[2025-02-13 19:47:49,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50,015][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.10805891454219818, acc: 0.9759036302566528)
[2025-02-13 19:47:50,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50,412][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.06380143761634827, acc: 0.9823529124259949)
[2025-02-13 19:47:50,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50,865][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.0866960734128952, acc: 0.9774011373519897)
[2025-02-13 19:47:51,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51,250][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.09262969344854355, acc: 0.9662162065505981)
[2025-02-13 19:47:51,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51,615][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.05915353447198868, acc: 0.9825581312179565)
[2025-02-13 19:47:51,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51,977][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.07255770266056061, acc: 0.9776119589805603)
[2025-02-13 19:47:52,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52,349][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.09690120816230774, acc: 0.9738219976425171)
[2025-02-13 19:47:52,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52,737][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.14181110262870789, acc: 0.9803921580314636)
[2025-02-13 19:47:52,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53,167][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.15669235587120056, acc: 0.9599999785423279)
[2025-02-13 19:47:53,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53,546][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.13008086383342743, acc: 0.9752475023269653)
[2025-02-13 19:47:53,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53,935][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.09425864368677139, acc: 0.9850746393203735)
[2025-02-13 19:47:54,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54,307][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.18383285403251648, acc: 0.9509202241897583)
[2025-02-13 19:47:54,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54,689][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.06533251702785492, acc: 0.9747474789619446)
[2025-02-13 19:47:54,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55,069][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.07150866091251373, acc: 0.9895287752151489)
[2025-02-13 19:47:55,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55,465][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.13705606758594513, acc: 0.9738219976425171)
[2025-02-13 19:47:55,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55,880][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.04104999080300331, acc: 0.9887640476226807)
[2025-02-13 19:47:56,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56,279][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.058187901973724365, acc: 0.9806451797485352)
[2025-02-13 19:47:56,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56,659][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.07371937483549118, acc: 0.9792746305465698)
[2025-02-13 19:47:56,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57,031][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.10333151370286942, acc: 0.9732142686843872)
[2025-02-13 19:47:57,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57,403][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.4171925485134125, acc: 0.9468085169792175)
[2025-02-13 19:47:57,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57,755][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.23417708277702332, acc: 0.9666666388511658)
[2025-02-13 19:47:57,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58,136][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.07008122652769089, acc: 0.9850000143051147)
[2025-02-13 19:47:58,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58,524][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.04220995306968689, acc: 0.9893048405647278)
[2025-02-13 19:47:58,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58,896][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.10908973962068558, acc: 0.9823529124259949)
[2025-02-13 19:47:59,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59,289][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.0501646064221859, acc: 0.9890109896659851)
[2025-02-13 19:47:59,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59,647][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.12687276303768158, acc: 0.9714285731315613)
[2025-02-13 19:47:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59,934][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.23710419237613678, acc: 0.9375)
[2025-02-13 19:48:00,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00,317][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.12362765520811081, acc: 0.9841269850730896)
[2025-02-13 19:48:00,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00,675][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.5514857172966003, acc: 0.8974359035491943)
[2025-02-13 19:48:00,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01,089][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.1503489911556244, acc: 0.9684210419654846)
[2025-02-13 19:48:01,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01,465][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.07648225128650665, acc: 0.9824561476707458)
[2025-02-13 19:48:01,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01,859][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.37405091524124146, acc: 0.9370629191398621)
[2025-02-13 19:48:01,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02,245][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.1701602041721344, acc: 0.9753086566925049)
[2025-02-13 19:48:02,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02,635][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.2655318081378937, acc: 0.9289940595626831)
[2025-02-13 19:48:02,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03,004][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.12322025001049042, acc: 0.9586206674575806)
[2025-02-13 19:48:03,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03,371][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.12853728234767914, acc: 0.9679999947547913)
[2025-02-13 19:48:03,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03,771][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.16861853003501892, acc: 0.9828571677207947)
[2025-02-13 19:48:03,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04,172][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.06438714265823364, acc: 0.9715909361839294)
[2025-02-13 19:48:04,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04,546][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.11239586770534515, acc: 0.9702380895614624)
[2025-02-13 19:48:04,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04,943][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.06309272348880768, acc: 0.9880239367485046)
[2025-02-13 19:48:05,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05,350][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.08335728943347931, acc: 0.9766082167625427)
[2025-02-13 19:48:05,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05,798][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.025671806186437607, acc: 1.0)
[2025-02-13 19:48:05,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06,167][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.18415164947509766, acc: 0.9586206674575806)
[2025-02-13 19:48:06,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06,589][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.09533050656318665, acc: 0.9746835231781006)
[2025-02-13 19:48:06,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07,004][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.07565981894731522, acc: 0.9673202633857727)
[2025-02-13 19:48:07,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07,423][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.04051413759589195, acc: 0.9887005686759949)
[2025-02-13 19:48:07,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07,807][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.07646309584379196, acc: 0.9836065769195557)
[2025-02-13 19:48:07,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08,186][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.11988189816474915, acc: 0.971222996711731)
[2025-02-13 19:48:08,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08,567][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.07869578152894974, acc: 0.978723406791687)
[2025-02-13 19:48:08,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08,957][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.19747163355350494, acc: 0.9659090638160706)
[2025-02-13 19:48:09,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09,361][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.13102582097053528, acc: 0.9615384340286255)
[2025-02-13 19:48:09,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09,753][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.04341251403093338, acc: 1.0)
[2025-02-13 19:48:09,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10,166][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.06831246614456177, acc: 0.9824561476707458)
[2025-02-13 19:48:10,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10,549][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.08784002810716629, acc: 0.9931972622871399)
[2025-02-13 19:48:10,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10,934][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.055729806423187256, acc: 0.9942857027053833)
[2025-02-13 19:48:11,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11,316][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.019206933677196503, acc: 1.0)
[2025-02-13 19:48:11,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11,717][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.19477427005767822, acc: 0.9512194991111755)
[2025-02-13 19:48:11,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12,081][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.14195369184017181, acc: 0.9578313231468201)
[2025-02-13 19:48:12,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12,434][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.08992953598499298, acc: 0.9820359349250793)
[2025-02-13 19:48:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12,791][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.10714589804410934, acc: 0.9683544039726257)
[2025-02-13 19:48:12,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13,185][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.13365302979946136, acc: 0.9640287756919861)
[2025-02-13 19:48:13,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13,544][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.08190411329269409, acc: 0.9860140085220337)
[2025-02-13 19:48:13,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13,930][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.10607034713029861, acc: 0.9722222089767456)
[2025-02-13 19:48:14,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14,304][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.1899852305650711, acc: 0.9530201554298401)
[2025-02-13 19:48:14,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14,687][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.25968414545059204, acc: 0.9555555582046509)
[2025-02-13 19:48:14,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15,048][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.21867042779922485, acc: 0.966292142868042)
[2025-02-13 19:48:15,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15,404][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.09637057781219482, acc: 0.981249988079071)
[2025-02-13 19:48:15,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15,757][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.12616640329360962, acc: 0.9597315192222595)
[2025-02-13 19:48:15,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16,152][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.16269034147262573, acc: 0.9679487347602844)
[2025-02-13 19:48:16,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16,526][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.13500267267227173, acc: 0.9318181872367859)
[2025-02-13 19:48:16,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16,905][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.24006520211696625, acc: 0.9709302186965942)
[2025-02-13 19:48:17,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17,292][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.12681126594543457, acc: 0.9777777791023254)
[2025-02-13 19:48:17,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17,669][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.09716831147670746, acc: 0.981249988079071)
[2025-02-13 19:48:17,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18,061][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.09348064661026001, acc: 0.9828571677207947)
[2025-02-13 19:48:18,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18,463][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.06216346472501755, acc: 0.9941176176071167)
[2025-02-13 19:48:18,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18,842][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.16297291219234467, acc: 0.9652777910232544)
[2025-02-13 19:48:18,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19,223][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.2752591073513031, acc: 0.9436619877815247)
[2025-02-13 19:48:19,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19,623][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.0782892256975174, acc: 0.9931972622871399)
[2025-02-13 19:48:19,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20,051][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.15285497903823853, acc: 0.956204354763031)
[2025-02-13 19:48:20,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20,446][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.11111310869455338, acc: 0.970802903175354)
[2025-02-13 19:48:20,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20,813][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.026887042447924614, acc: 1.0)
[2025-02-13 19:48:20,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21,210][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.28802651166915894, acc: 0.9379844665527344)
[2025-02-13 19:48:21,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21,580][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.19279517233371735, acc: 0.9638554453849792)
[2025-02-13 19:48:21,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21,986][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.3210863471031189, acc: 0.9186992049217224)
[2025-02-13 19:48:22,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22,390][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.274445116519928, acc: 0.9139785170555115)
[2025-02-13 19:48:22,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22,758][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.2818778157234192, acc: 0.9275362491607666)
[2025-02-13 19:48:22,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23,157][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.5823891758918762, acc: 0.890625)
[2025-02-13 19:48:23,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23,564][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.43270838260650635, acc: 0.8943662047386169)
[2025-02-13 19:48:23,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23,953][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.3467368483543396, acc: 0.9349112510681152)
[2025-02-13 19:48:24,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24,331][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.19347718358039856, acc: 0.9459459185600281)
[2025-02-13 19:48:24,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24,695][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.19095991551876068, acc: 0.9497206807136536)
[2025-02-13 19:48:24,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25,087][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.28931450843811035, acc: 0.9375)
[2025-02-13 19:48:25,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25,468][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.2981703579425812, acc: 0.9171974658966064)
[2025-02-13 19:48:25,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25,842][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.2658364176750183, acc: 0.9463087320327759)
[2025-02-13 19:48:25,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26,197][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.2757890820503235, acc: 0.9477124214172363)
[2025-02-13 19:48:26,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26,538][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.18475739657878876, acc: 0.9620853066444397)
[2025-02-13 19:48:26,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26,920][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.18433374166488647, acc: 0.9646464586257935)
[2025-02-13 19:48:27,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27,290][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.29177722334861755, acc: 0.9337349534034729)
[2025-02-13 19:48:27,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27,622][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.428989976644516, acc: 0.9200000166893005)
[2025-02-13 19:48:27,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27,978][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.3140449523925781, acc: 0.9299362897872925)
[2025-02-13 19:48:28,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28,333][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.1622769832611084, acc: 0.9451219439506531)
[2025-02-13 19:48:28,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28,709][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.250519335269928, acc: 0.9450549483299255)
[2025-02-13 19:48:28,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29,095][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.18623417615890503, acc: 0.9585492014884949)
[2025-02-13 19:48:29,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29,458][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.24469813704490662, acc: 0.9328858852386475)
[2025-02-13 19:48:29,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29,853][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.21711473166942596, acc: 0.9615384340286255)
[2025-02-13 19:48:29,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30,243][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.1927463859319687, acc: 0.9567901492118835)
[2025-02-13 19:48:30,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30,654][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.16815152764320374, acc: 0.9624999761581421)
[2025-02-13 19:48:30,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31,078][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.2620967924594879, acc: 0.932584285736084)
[2025-02-13 19:48:31,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31,454][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.1757550984621048, acc: 0.9571428298950195)
[2025-02-13 19:48:31,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31,821][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.1852908879518509, acc: 0.9428571462631226)
[2025-02-13 19:48:31,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32,178][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.24114805459976196, acc: 0.950276255607605)
[2025-02-13 19:48:32,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32,580][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.16858232021331787, acc: 0.9642857313156128)
[2025-02-13 19:48:32,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32,987][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.18263311684131622, acc: 0.9479768872261047)
[2025-02-13 19:48:33,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:33,371][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.13828206062316895, acc: 0.9745762944221497)
[2025-02-13 19:48:33,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:33,768][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.08350882679224014, acc: 0.9813084006309509)
[2025-02-13 19:48:33,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34,145][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.04307880997657776, acc: 0.9879518151283264)
[2025-02-13 19:48:34,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34,536][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.05619033798575401, acc: 0.988095223903656)
[2025-02-13 19:48:34,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34,838][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.0189548060297966, acc: 1.0)
[2025-02-13 19:48:34,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35,219][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.04964263737201691, acc: 0.9887640476226807)
[2025-02-13 19:48:35,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35,624][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.09558911621570587, acc: 0.9862068891525269)
[2025-02-13 19:48:35,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35,995][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.0698070377111435, acc: 0.9790209531784058)
[2025-02-13 19:48:36,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36,408][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.13693223893642426, acc: 0.9621211886405945)
[2025-02-13 19:48:36,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36,793][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.08945496380329132, acc: 0.9764705896377563)
[2025-02-13 19:48:36,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37,182][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.07419009506702423, acc: 0.9858155846595764)
[2025-02-13 19:48:37,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37,548][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.14273808896541595, acc: 0.9593495726585388)
[2025-02-13 19:48:37,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37,919][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.03038383647799492, acc: 0.9934640526771545)
[2025-02-13 19:48:38,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38,313][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.024297792464494705, acc: 0.9924812316894531)
[2025-02-13 19:48:38,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38,706][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.01900339685380459, acc: 1.0)
[2025-02-13 19:48:38,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39,059][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.02657226100564003, acc: 1.0)
[2025-02-13 19:48:39,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39,413][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.10817039012908936, acc: 0.9683544039726257)
[2025-02-13 19:48:39,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39,787][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.03799214959144592, acc: 0.9920634627342224)
[2025-02-13 19:48:39,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40,176][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.05925370380282402, acc: 0.9943181872367859)
[2025-02-13 19:48:40,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40,586][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.11069809645414352, acc: 0.9823529124259949)
[2025-02-13 19:48:40,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40,942][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.03423989191651344, acc: 0.9921875)
[2025-02-13 19:48:41,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41,348][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.14206218719482422, acc: 0.9664429426193237)
[2025-02-13 19:48:41,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41,724][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.0774138867855072, acc: 0.9847328066825867)
[2025-02-13 19:48:41,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42,120][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.07292630523443222, acc: 0.9882352948188782)
[2025-02-13 19:48:42,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42,492][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.13245730102062225, acc: 0.9607843160629272)
[2025-02-13 19:48:42,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42,816][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.1310705840587616, acc: 0.9741379022598267)
[2025-02-13 19:48:42,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43,195][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.06644003838300705, acc: 0.9847715497016907)
[2025-02-13 19:48:43,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43,565][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.15400128066539764, acc: 0.9627659320831299)
[2025-02-13 19:48:43,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43,947][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.2971319258213043, acc: 0.9436619877815247)
[2025-02-13 19:48:44,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44,326][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.16226185858249664, acc: 0.9632353186607361)
[2025-02-13 19:48:44,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44,712][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.18791450560092926, acc: 0.9545454382896423)
[2025-02-13 19:48:44,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45,084][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.05360584333539009, acc: 1.0)
[2025-02-13 19:48:45,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45,451][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.14461290836334229, acc: 0.9594594836235046)
[2025-02-13 19:48:45,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45,795][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.22909227013587952, acc: 0.939130425453186)
[2025-02-13 19:48:45,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46,167][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.1345367431640625, acc: 0.9733333587646484)
[2025-02-13 19:48:46,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46,535][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.11604032665491104, acc: 0.9673202633857727)
[2025-02-13 19:48:46,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46,904][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.24618998169898987, acc: 0.9462365508079529)
[2025-02-13 19:48:47,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47,267][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.174202099442482, acc: 0.9567901492118835)
[2025-02-13 19:48:47,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47,645][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.18156537413597107, acc: 0.9677419066429138)
[2025-02-13 19:48:47,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48,029][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.23873212933540344, acc: 0.9375)
[2025-02-13 19:48:48,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48,431][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.16547586023807526, acc: 0.9635416865348816)
[2025-02-13 19:48:48,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48,827][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.13214465975761414, acc: 0.9753694534301758)
[2025-02-13 19:48:48,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49,203][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.2467917501926422, acc: 0.9306358098983765)
[2025-02-13 19:48:49,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49,568][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.1481751799583435, acc: 0.9554455280303955)
[2025-02-13 19:48:49,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49,958][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.16795644164085388, acc: 0.9595375657081604)
[2025-02-13 19:48:50,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50,320][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.1336853802204132, acc: 0.9680851101875305)
[2025-02-13 19:48:50,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50,672][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.3446231782436371, acc: 0.9248554706573486)
[2025-02-13 19:48:50,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51,063][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.1557362675666809, acc: 0.9621621370315552)
[2025-02-13 19:48:51,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51,447][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.057673100382089615, acc: 0.9887640476226807)
[2025-02-13 19:48:51,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51,823][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.08000575006008148, acc: 0.9902439117431641)
[2025-02-13 19:48:51,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52,203][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.11940320581197739, acc: 0.9740932583808899)
[2025-02-13 19:48:52,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52,583][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.16459910571575165, acc: 0.9601989984512329)
[2025-02-13 19:48:52,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52,964][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.18011188507080078, acc: 0.9754902124404907)
[2025-02-13 19:48:53,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53,336][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.15590894222259521, acc: 0.9689119458198547)
[2025-02-13 19:48:53,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53,710][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.1767420768737793, acc: 0.9666666388511658)
[2025-02-13 19:48:53,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54,083][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.13356076180934906, acc: 0.9672130942344666)
[2025-02-13 19:48:54,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54,472][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.0916610136628151, acc: 0.9791666865348816)
[2025-02-13 19:48:54,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54,848][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.36438342928886414, acc: 0.9281768202781677)
[2025-02-13 19:48:54,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55,222][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.1850324273109436, acc: 0.9609755873680115)
[2025-02-13 19:48:55,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55,638][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.16130895912647247, acc: 0.966183602809906)
[2025-02-13 19:48:55,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55,993][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.37704718112945557, acc: 0.9215686321258545)
[2025-02-13 19:48:56,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56,346][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.18364426493644714, acc: 0.9567567706108093)
[2025-02-13 19:48:56,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56,719][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.201409712433815, acc: 0.9414634108543396)
[2025-02-13 19:48:56,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57,092][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.17795422673225403, acc: 0.9484536051750183)
[2025-02-13 19:48:57,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57,449][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.12697356939315796, acc: 0.969072163105011)
[2025-02-13 19:48:57,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57,829][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.2126665860414505, acc: 0.9593908786773682)
[2025-02-13 19:48:57,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58,216][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.08320952951908112, acc: 0.970059871673584)
[2025-02-13 19:48:58,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58,601][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.16508235037326813, acc: 0.9656862616539001)
[2025-02-13 19:48:58,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58,954][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.18538522720336914, acc: 0.9487179517745972)
[2025-02-13 19:48:59,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59,323][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.17444594204425812, acc: 0.9561403393745422)
[2025-02-13 19:48:59,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59,726][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.1762673556804657, acc: 0.9515151381492615)
[2025-02-13 19:48:59,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00,144][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.1768263876438141, acc: 0.9411764740943909)
[2025-02-13 19:49:00,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00,527][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.209882915019989, acc: 0.9444444179534912)
[2025-02-13 19:49:00,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00,921][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.21683497726917267, acc: 0.9554455280303955)
[2025-02-13 19:49:01,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01,287][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.31650346517562866, acc: 0.953125)
[2025-02-13 19:49:01,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01,678][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.13055956363677979, acc: 0.9659090638160706)
[2025-02-13 19:49:01,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02,026][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.15587997436523438, acc: 0.9583333134651184)
[2025-02-13 19:49:02,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02,425][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.2002231478691101, acc: 0.9576719403266907)
[2025-02-13 19:49:02,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02,808][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.23483924567699432, acc: 0.940119743347168)
[2025-02-13 19:49:02,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03,187][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.2980649769306183, acc: 0.940119743347168)
[2025-02-13 19:49:03,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03,555][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.20295710861682892, acc: 0.9551281929016113)
[2025-02-13 19:49:03,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03,948][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.14872217178344727, acc: 0.9734042286872864)
[2025-02-13 19:49:04,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04,345][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.11352658271789551, acc: 0.9623655676841736)
[2025-02-13 19:49:04,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04,725][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.2548842132091522, acc: 0.9589040875434875)
[2025-02-13 19:49:04,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05,113][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.5229774117469788, acc: 0.8678160905838013)
[2025-02-13 19:49:05,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05,481][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.27461639046669006, acc: 0.9226190447807312)
[2025-02-13 19:49:05,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05,848][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.4996768832206726, acc: 0.9034482836723328)
[2025-02-13 19:49:05,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06,215][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.09536530822515488, acc: 0.9779411554336548)
[2025-02-13 19:49:06,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06,599][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.22802384197711945, acc: 0.9554139971733093)
[2025-02-13 19:49:06,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06,979][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.21676412224769592, acc: 0.9469026327133179)
[2025-02-13 19:49:07,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07,355][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.20654873549938202, acc: 0.951724112033844)
[2025-02-13 19:49:07,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07,693][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.33570048213005066, acc: 0.953125)
[2025-02-13 19:49:07,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08,070][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.3830730617046356, acc: 0.9139785170555115)
[2025-02-13 19:49:08,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08,423][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.23491869866847992, acc: 0.9557521939277649)
[2025-02-13 19:49:08,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08,788][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.3672120273113251, acc: 0.9142857193946838)
[2025-02-13 19:49:08,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09,181][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.22694136202335358, acc: 0.939130425453186)
[2025-02-13 19:49:09,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09,569][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.14548024535179138, acc: 0.9620253443717957)
[2025-02-13 19:49:09,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09,958][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.27854034304618835, acc: 0.9166666865348816)
[2025-02-13 19:49:10,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10,315][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.231816366314888, acc: 0.9428571462631226)
[2025-02-13 19:49:10,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10,691][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.2664143145084381, acc: 0.9398496150970459)
[2025-02-13 19:49:10,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11,082][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.4194607436656952, acc: 0.9396551847457886)
[2025-02-13 19:49:11,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11,472][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.17204241454601288, acc: 0.925000011920929)
[2025-02-13 19:49:11,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11,879][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.16178198158740997, acc: 0.956204354763031)
[2025-02-13 19:49:12,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12,246][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.16789565980434418, acc: 0.9679999947547913)
[2025-02-13 19:49:12,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12,611][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.4226873219013214, acc: 0.8803418874740601)
[2025-02-13 19:49:12,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12,993][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.43070200085639954, acc: 0.9174311757087708)
[2025-02-13 19:49:13,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13,380][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.24929577112197876, acc: 0.9236111044883728)
[2025-02-13 19:49:13,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13,761][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.1862790584564209, acc: 0.9487179517745972)
[2025-02-13 19:49:13,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14,142][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.19255052506923676, acc: 0.95652174949646)
[2025-02-13 19:49:14,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14,512][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.157741978764534, acc: 0.9470198750495911)
[2025-02-13 19:49:14,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14,884][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.06811688095331192, acc: 0.9848484992980957)
[2025-02-13 19:49:15,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15,313][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.04260962828993797, acc: 1.0)
[2025-02-13 19:49:15,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15,687][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.12099780887365341, acc: 0.9523809552192688)
[2025-02-13 19:49:15,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16,061][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.07965903729200363, acc: 0.9795918464660645)
[2025-02-13 19:49:16,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16,422][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.11971604079008102, acc: 0.9798657894134521)
[2025-02-13 19:49:16,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16,796][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.2913607656955719, acc: 0.9274193644523621)
[2025-02-13 19:49:16,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17,175][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.29366880655288696, acc: 0.9396551847457886)
[2025-02-13 19:49:17,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17,560][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.29545941948890686, acc: 0.9236111044883728)
[2025-02-13 19:49:17,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17,947][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.18358214199543, acc: 0.9534883499145508)
[2025-02-13 19:49:18,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18,302][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.21628202497959137, acc: 0.982758641242981)
[2025-02-13 19:49:18,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18,680][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.1872764676809311, acc: 0.9542483687400818)
[2025-02-13 19:49:18,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19,057][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.20107820630073547, acc: 0.9487179517745972)
[2025-02-13 19:49:19,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19,423][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.18216954171657562, acc: 0.9642857313156128)
[2025-02-13 19:49:19,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19,786][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.24568213522434235, acc: 0.9236640930175781)
[2025-02-13 19:49:19,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20,163][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.13582906126976013, acc: 0.9556962251663208)
[2025-02-13 19:49:20,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20,526][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.21143318712711334, acc: 0.9470198750495911)
[2025-02-13 19:49:20,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20,913][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.544774055480957, acc: 0.887005627155304)
[2025-02-13 19:49:21,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21,305][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.3384867012500763, acc: 0.907216489315033)
[2025-02-13 19:49:21,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21,659][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.3557206690311432, acc: 0.9319371581077576)
[2025-02-13 19:49:21,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22,048][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.48613348603248596, acc: 0.8653846383094788)
[2025-02-13 19:49:22,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22,414][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.23697492480278015, acc: 0.9069767594337463)
[2025-02-13 19:49:22,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22,805][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.4039919972419739, acc: 0.9012875556945801)
[2025-02-13 19:49:22,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23,181][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.2098115086555481, acc: 0.9320987462997437)
[2025-02-13 19:49:23,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23,573][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.23996523022651672, acc: 0.93388432264328)
[2025-02-13 19:49:23,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23,930][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.12402019649744034, acc: 0.9842519760131836)
[2025-02-13 19:49:24,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24,291][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.4223124086856842, acc: 0.8918918967247009)
[2025-02-13 19:49:24,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24,672][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.04425954446196556, acc: 0.9814814925193787)
[2025-02-13 19:49:24,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25,066][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.4203238785266876, acc: 0.9108911156654358)
[2025-02-13 19:49:25,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25,452][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.21514493227005005, acc: 0.9279279112815857)
[2025-02-13 19:49:25,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25,806][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.3567463457584381, acc: 0.8917197585105896)
[2025-02-13 19:49:25,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26,180][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.5180500149726868, acc: 0.8999999761581421)
[2025-02-13 19:49:26,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26,569][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.150242879986763, acc: 0.9709302186965942)
[2025-02-13 19:49:26,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26,940][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.19101710617542267, acc: 0.931506872177124)
[2025-02-13 19:49:27,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27,309][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.1389404684305191, acc: 0.948387086391449)
[2025-02-13 19:49:27,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27,698][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.348554402589798, acc: 0.9052631855010986)
[2025-02-13 19:49:27,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28,083][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.07718753069639206, acc: 0.9774011373519897)
[2025-02-13 19:49:28,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28,426][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.08787401020526886, acc: 0.9909090995788574)
[2025-02-13 19:49:28,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28,790][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.3392013907432556, acc: 0.9610389471054077)
[2025-02-13 19:49:28,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29,151][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.10897666215896606, acc: 0.9747899174690247)
[2025-02-13 19:49:29,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29,511][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.06063346192240715, acc: 0.9948186278343201)
[2025-02-13 19:49:29,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29,890][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.266256183385849, acc: 0.9329268336296082)
[2025-02-13 19:49:30,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30,255][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.16966086626052856, acc: 0.9578313231468201)
[2025-02-13 19:49:30,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30,602][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.10419365763664246, acc: 0.9689440727233887)
[2025-02-13 19:49:30,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30,969][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.19095009565353394, acc: 0.9548386931419373)
[2025-02-13 19:49:31,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31,304][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.1943221539258957, acc: 0.9779005646705627)
[2025-02-13 19:49:31,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31,678][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.23050910234451294, acc: 0.953125)
[2025-02-13 19:49:31,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32,062][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.07688603550195694, acc: 0.9806451797485352)
[2025-02-13 19:49:32,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32,407][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.20116864144802094, acc: 0.9476439952850342)
[2025-02-13 19:49:32,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32,759][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.08851350098848343, acc: 1.0)
[2025-02-13 19:49:32,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33,145][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.06474421173334122, acc: 0.9808917045593262)
[2025-02-13 19:49:33,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33,540][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.1684059351682663, acc: 0.9527027010917664)
[2025-02-13 19:49:33,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33,873][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.10161659121513367, acc: 0.9776119589805603)
[2025-02-13 19:49:34,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34,274][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.2762329876422882, acc: 0.9551281929016113)
[2025-02-13 19:49:34,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34,658][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.22941787540912628, acc: 0.9380530714988708)
[2025-02-13 19:49:34,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35,036][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.23952297866344452, acc: 0.9567901492118835)
[2025-02-13 19:49:35,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35,435][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.42254093289375305, acc: 0.905063271522522)
[2025-02-13 19:49:35,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35,848][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.1150866225361824, acc: 0.9819276928901672)
[2025-02-13 19:49:36,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36,260][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.13124634325504303, acc: 0.9629629850387573)
[2025-02-13 19:49:36,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36,625][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.3736710548400879, acc: 0.8611111044883728)
[2025-02-13 19:49:36,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37,015][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.4421903192996979, acc: 0.9397590160369873)
[2025-02-13 19:49:37,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37,403][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.3104090392589569, acc: 0.9420289993286133)
[2025-02-13 19:49:37,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37,787][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.391683429479599, acc: 0.9142857193946838)
[2025-02-13 19:49:37,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38,136][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.5455751419067383, acc: 0.859375)
[2025-02-13 19:49:38,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38,517][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.3872511088848114, acc: 0.9154929518699646)
[2025-02-13 19:49:38,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38,882][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.1025775671005249, acc: 0.9780219793319702)
[2025-02-13 19:49:39,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39,259][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.16688378155231476, acc: 0.9550561904907227)
[2025-02-13 19:49:39,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39,630][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.360650897026062, acc: 0.9130434989929199)
[2025-02-13 19:49:39,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40,022][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.3750044107437134, acc: 0.9259259104728699)
[2025-02-13 19:49:40,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40,374][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.44295889139175415, acc: 0.8970588445663452)
[2025-02-13 19:49:40,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40,761][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.3802095651626587, acc: 0.9382715821266174)
[2025-02-13 19:49:40,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41,127][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.36814871430397034, acc: 0.9375)
[2025-02-13 19:49:41,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41,558][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.3657558560371399, acc: 0.9365079402923584)
[2025-02-13 19:49:41,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41,953][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.3921761214733124, acc: 0.8730158805847168)
[2025-02-13 19:49:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42,381][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.2926666736602783, acc: 0.9104477763175964)
[2025-02-13 19:49:42,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42,769][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.37898239493370056, acc: 0.9367088675498962)
[2025-02-13 19:49:42,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43,182][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.3397286832332611, acc: 0.9111111164093018)
[2025-02-13 19:49:43,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43,521][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.460899293422699, acc: 0.8799999952316284)
[2025-02-13 19:49:43,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43,878][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.38196685910224915, acc: 0.8999999761581421)
[2025-02-13 19:49:44,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44,273][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.40887266397476196, acc: 0.9154929518699646)
[2025-02-13 19:49:44,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44,640][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.4384081959724426, acc: 0.9333333373069763)
[2025-02-13 19:49:44,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45,016][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.37346357107162476, acc: 0.8873239159584045)
[2025-02-13 19:49:45,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45,358][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.2710838317871094, acc: 0.9241379499435425)
[2025-02-13 19:49:45,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45,745][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.38819077610969543, acc: 0.910179615020752)
[2025-02-13 19:49:45,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46,155][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.31657177209854126, acc: 0.9248826503753662)
[2025-02-13 19:49:46,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46,567][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.04192887246608734, acc: 0.9896907210350037)
[2025-02-13 19:49:46,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46,942][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.16201837360858917, acc: 0.9537572264671326)
[2025-02-13 19:49:47,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47,320][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.08279497176408768, acc: 0.9885714054107666)
[2025-02-13 19:49:47,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47,706][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.09384920448064804, acc: 0.9619565010070801)
[2025-02-13 19:49:47,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48,091][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.1220521554350853, acc: 0.9768785834312439)
[2025-02-13 19:49:48,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48,470][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.10611694306135178, acc: 0.9781420826911926)
[2025-02-13 19:49:48,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48,886][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.26106712222099304, acc: 0.948387086391449)
[2025-02-13 19:49:49,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49,274][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.26587915420532227, acc: 0.9395604133605957)
[2025-02-13 19:49:49,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49,709][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.2128443866968155, acc: 0.9388889074325562)
[2025-02-13 19:49:49,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50,101][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.5849291086196899, acc: 0.9060773253440857)
[2025-02-13 19:49:50,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50,481][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.2544845640659332, acc: 0.9534883499145508)
[2025-02-13 19:49:50,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50,862][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.13325081765651703, acc: 0.9551281929016113)
[2025-02-13 19:49:50,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51,262][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.1891852170228958, acc: 0.9718309640884399)
[2025-02-13 19:49:51,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51,669][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.2556397318840027, acc: 0.9247311949729919)
[2025-02-13 19:49:51,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52,048][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.2095697820186615, acc: 0.9491525292396545)
[2025-02-13 19:49:52,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52,392][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.4205218255519867, acc: 0.9351351261138916)
[2025-02-13 19:49:52,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52,776][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.24120284616947174, acc: 0.9638554453849792)
[2025-02-13 19:49:52,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53,161][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.08496041595935822, acc: 0.9898989796638489)
[2025-02-13 19:49:53,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53,537][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.15773780643939972, acc: 0.963350772857666)
[2025-02-13 19:49:53,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53,935][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.11317317932844162, acc: 0.9629629850387573)
[2025-02-13 19:49:54,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54,379][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.04871527478098869, acc: 0.9902439117431641)
[2025-02-13 19:49:54,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54,760][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.09051340818405151, acc: 0.9826839566230774)
[2025-02-13 19:49:54,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55,145][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.10521294176578522, acc: 0.9768785834312439)
[2025-02-13 19:49:55,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55,527][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.18082889914512634, acc: 0.9747899174690247)
[2025-02-13 19:49:55,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55,914][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.11110421270132065, acc: 0.9646464586257935)
[2025-02-13 19:49:56,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56,302][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.12975259125232697, acc: 0.9648241400718689)
[2025-02-13 19:49:56,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56,683][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.525359034538269, acc: 0.8780487775802612)
[2025-02-13 19:49:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57,082][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.2702906131744385, acc: 0.9299362897872925)
[2025-02-13 19:49:57,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57,454][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.388490229845047, acc: 0.932584285736084)
[2025-02-13 19:49:57,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57,839][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.3961867094039917, acc: 0.8904109597206116)
[2025-02-13 19:49:57,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58,229][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.21290989220142365, acc: 0.9448275566101074)
[2025-02-13 19:49:58,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58,594][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.13628613948822021, acc: 0.9605262875556946)
[2025-02-13 19:49:58,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58,979][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.21697084605693817, acc: 0.9455782175064087)
[2025-02-13 19:49:59,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59,371][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.20260679721832275, acc: 0.9653179049491882)
[2025-02-13 19:49:59,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59,746][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.2737761437892914, acc: 0.9407894611358643)
[2025-02-13 19:49:59,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00,145][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.11501985788345337, acc: 0.9716312289237976)
[2025-02-13 19:50:00,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00,501][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.1816360056400299, acc: 0.954023003578186)
[2025-02-13 19:50:00,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00,883][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.2121652364730835, acc: 0.95333331823349)
[2025-02-13 19:50:01,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01,269][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.15084971487522125, acc: 0.9548386931419373)
[2025-02-13 19:50:01,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01,641][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.18076816201210022, acc: 0.9844961166381836)
[2025-02-13 19:50:01,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02,054][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.17456288635730743, acc: 0.9432623982429504)
[2025-02-13 19:50:02,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02,421][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.27987074851989746, acc: 0.9290322661399841)
[2025-02-13 19:50:02,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02,815][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.18407809734344482, acc: 0.9702380895614624)
[2025-02-13 19:50:02,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03,211][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.11214295774698257, acc: 0.9696969985961914)
[2025-02-13 19:50:03,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03,594][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.17646969854831696, acc: 0.969924807548523)
[2025-02-13 19:50:03,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03,945][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.2230236977338791, acc: 0.94017094373703)
[2025-02-13 19:50:04,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04,321][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.2091054916381836, acc: 0.9583333134651184)
[2025-02-13 19:50:04,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04,706][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.2842733860015869, acc: 0.9322034120559692)
[2025-02-13 19:50:04,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05,060][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.20585715770721436, acc: 0.9436619877815247)
[2025-02-13 19:50:05,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05,456][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.3200252950191498, acc: 0.8974359035491943)
[2025-02-13 19:50:05,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05,830][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.12645375728607178, acc: 0.9790209531784058)
[2025-02-13 19:50:05,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06,200][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.30032268166542053, acc: 0.9130434989929199)
[2025-02-13 19:50:06,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06,575][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.22535623610019684, acc: 0.946107804775238)
[2025-02-13 19:50:06,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06,957][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.2370714694261551, acc: 0.9424460530281067)
[2025-02-13 19:50:07,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07,343][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.17536480724811554, acc: 0.9576271176338196)
[2025-02-13 19:50:07,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07,690][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.22957128286361694, acc: 0.921875)
[2025-02-13 19:50:07,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08,029][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.15067730844020844, acc: 0.9561403393745422)
[2025-02-13 19:50:08,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08,383][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.11692118644714355, acc: 0.9652777910232544)
[2025-02-13 19:50:08,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08,754][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.08941614627838135, acc: 0.9851852059364319)
[2025-02-13 19:50:08,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09,134][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.13175180554389954, acc: 0.95652174949646)
[2025-02-13 19:50:09,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09,581][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.15223659574985504, acc: 0.9635036587715149)
[2025-02-13 19:50:09,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09,969][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.24849502742290497, acc: 0.9185185432434082)
[2025-02-13 19:50:10,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10,340][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.06629382818937302, acc: 0.9849624037742615)
[2025-02-13 19:50:10,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10,694][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.2208349108695984, acc: 0.9537037014961243)
[2025-02-13 19:50:10,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11,061][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.23046739399433136, acc: 0.9279279112815857)
[2025-02-13 19:50:11,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11,443][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.41129326820373535, acc: 0.9044585824012756)
[2025-02-13 19:50:11,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11,813][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.286990761756897, acc: 0.9266055226325989)
[2025-02-13 19:50:11,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12,182][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.27760377526283264, acc: 0.9465649127960205)
[2025-02-13 19:50:12,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12,555][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.2854044437408447, acc: 0.9411764740943909)
[2025-02-13 19:50:12,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12,965][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.283679723739624, acc: 0.913385808467865)
[2025-02-13 19:50:13,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13,351][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.22796566784381866, acc: 0.9357143044471741)
[2025-02-13 19:50:13,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13,723][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.09680032730102539, acc: 0.9793103337287903)
[2025-02-13 19:50:13,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14,101][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.28536197543144226, acc: 0.9514563083648682)
[2025-02-13 19:50:14,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14,466][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.030843881890177727, acc: 1.0)
[2025-02-13 19:50:14,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14,824][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.23033148050308228, acc: 0.9569892287254333)
[2025-02-13 19:50:14,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15,198][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.22748826444149017, acc: 0.9494949579238892)
[2025-02-13 19:50:15,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15,605][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.1985911875963211, acc: 0.9590163826942444)
[2025-02-13 19:50:15,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15,959][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.23869316279888153, acc: 0.9541984796524048)
[2025-02-13 19:50:16,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16,300][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.3525595963001251, acc: 0.9124087691307068)
[2025-02-13 19:50:16,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16,660][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.14538060128688812, acc: 0.9506173133850098)
[2025-02-13 19:50:16,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17,027][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.1658875197172165, acc: 0.965753436088562)
[2025-02-13 19:50:17,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17,426][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.67940753698349, acc: 0.8372092843055725)
[2025-02-13 19:50:17,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17,805][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.7539271116256714, acc: 0.8266666531562805)
[2025-02-13 19:50:17,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18,173][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.25535619258880615, acc: 0.9339622855186462)
[2025-02-13 19:50:19,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21,312][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3100, device='cuda:0') eval_epoch_loss=tensor(0.2700, device='cuda:0') eval_epoch_acc=tensor(0.9410, device='cuda:0')
[2025-02-13 19:54:21,314][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:54:21,314][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:54:21,784][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_7132_loss_0.27001067996025085/model.pt
[2025-02-13 19:54:21,796][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:54:21,798][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.27001067996025085
[2025-02-13 19:54:21,799][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9410226941108704
[2025-02-13 19:54:21,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22,244][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.11102339625358582, acc: 0.9558823704719543)
[2025-02-13 19:54:22,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22,602][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.2014123797416687, acc: 0.9624060392379761)
[2025-02-13 19:54:22,988][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.3516, train_epoch_loss=0.3013, epoch time 3798.5576279982924s
[2025-02-13 19:54:22,988][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 19:54:22,988][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 19:54:22,988][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 19:54:22,988][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 19:54:22,988][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 19:54:23,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24,195][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.42045846581459045, acc: 0.9230769276618958)
[2025-02-13 19:54:24,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24,578][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.17341351509094238, acc: 0.9681528806686401)
[2025-02-13 19:54:24,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24,938][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.11855478584766388, acc: 0.9772727489471436)
[2025-02-13 19:54:25,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25,316][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.10787203907966614, acc: 0.9593023061752319)
[2025-02-13 19:54:25,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25,695][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.15587562322616577, acc: 0.955974817276001)
[2025-02-13 19:54:25,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26,075][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.07715434581041336, acc: 0.9832402467727661)
[2025-02-13 19:54:26,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26,430][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.021950161084532738, acc: 1.0)
[2025-02-13 19:54:26,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26,816][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.08020659536123276, acc: 0.9784946441650391)
[2025-02-13 19:54:26,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27,302][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.1885693520307541, acc: 0.957317054271698)
[2025-02-13 19:54:27,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27,698][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.06169845163822174, acc: 0.9801324605941772)
[2025-02-13 19:54:27,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28,151][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.24802859127521515, acc: 0.9171597361564636)
[2025-02-13 19:54:28,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28,554][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.10032636672258377, acc: 0.9750000238418579)
[2025-02-13 19:54:28,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29,001][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.0663817822933197, acc: 0.9942196607589722)
[2025-02-13 19:54:29,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29,412][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.1386018991470337, acc: 0.949438214302063)
[2025-02-13 19:54:29,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29,807][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.02631833776831627, acc: 1.0)
[2025-02-13 19:54:29,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30,215][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.041635435074567795, acc: 0.9927536249160767)
[2025-02-13 19:54:30,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30,604][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.04778539016842842, acc: 0.9912280440330505)
[2025-02-13 19:54:30,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30,979][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.2406567484140396, acc: 0.9468085169792175)
[2025-02-13 19:54:31,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31,371][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.016555676236748695, acc: 1.0)
[2025-02-13 19:54:31,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31,752][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.08366432785987854, acc: 0.9653179049491882)
[2025-02-13 19:54:31,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32,126][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.03924392908811569, acc: 0.9943181872367859)
[2025-02-13 19:54:32,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32,528][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.08962980657815933, acc: 0.9772727489471436)
[2025-02-13 19:54:32,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32,903][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.005895962007343769, acc: 1.0)
[2025-02-13 19:54:33,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33,259][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.022851327434182167, acc: 1.0)
[2025-02-13 19:54:33,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33,638][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.03090597875416279, acc: 0.9944444298744202)
[2025-02-13 19:54:33,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34,026][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.10244466364383698, acc: 0.976331353187561)
[2025-02-13 19:54:34,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34,423][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.16997800767421722, acc: 0.9570552110671997)
[2025-02-13 19:54:34,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34,820][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.02725272625684738, acc: 0.9935897588729858)
[2025-02-13 19:54:34,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35,219][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.2962417006492615, acc: 0.9340101480484009)
[2025-02-13 19:54:35,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35,619][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.1530340015888214, acc: 0.9822485446929932)
[2025-02-13 19:54:35,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36,009][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.1517627090215683, acc: 0.9710144996643066)
[2025-02-13 19:54:36,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36,419][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.18311373889446259, acc: 0.9627906680107117)
[2025-02-13 19:54:36,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36,780][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.2589147686958313, acc: 0.945652186870575)
[2025-02-13 19:54:36,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37,129][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.2641151547431946, acc: 0.9461538195610046)
[2025-02-13 19:54:37,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37,653][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.1926598846912384, acc: 0.961904764175415)
[2025-02-13 19:54:37,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38,087][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.1597607582807541, acc: 0.95686274766922)
[2025-02-13 19:54:38,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38,499][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.20369189977645874, acc: 0.9677419066429138)
[2025-02-13 19:54:38,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38,884][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.17985717952251434, acc: 0.9576719403266907)
[2025-02-13 19:54:39,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39,262][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.16128285229206085, acc: 0.9622641801834106)
[2025-02-13 19:54:39,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39,637][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.07400552928447723, acc: 0.9766082167625427)
[2025-02-13 19:54:39,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40,014][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.15795503556728363, acc: 0.9821428656578064)
[2025-02-13 19:54:40,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40,391][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.18996456265449524, acc: 0.9734042286872864)
[2025-02-13 19:54:40,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40,759][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.10333026945590973, acc: 0.9753086566925049)
[2025-02-13 19:54:40,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41,178][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.05890282988548279, acc: 0.9888268113136292)
[2025-02-13 19:54:41,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41,592][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.11739103496074677, acc: 0.9786096215248108)
[2025-02-13 19:54:41,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41,985][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.157060906291008, acc: 0.9669811129570007)
[2025-02-13 19:54:42,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42,349][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.16641072928905487, acc: 0.9534883499145508)
[2025-02-13 19:54:42,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42,743][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.1324767917394638, acc: 0.9720670580863953)
[2025-02-13 19:54:42,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43,159][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.0920271947979927, acc: 0.9855072498321533)
[2025-02-13 19:54:43,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43,554][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.10465452820062637, acc: 0.970588207244873)
[2025-02-13 19:54:43,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43,927][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.12023552507162094, acc: 0.9708737730979919)
[2025-02-13 19:54:44,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44,297][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.09069894999265671, acc: 0.9744898080825806)
[2025-02-13 19:54:44,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44,635][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.07697410136461258, acc: 0.9735099077224731)
[2025-02-13 19:54:44,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45,017][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.08714167773723602, acc: 0.9891892075538635)
[2025-02-13 19:54:45,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45,395][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.042686980217695236, acc: 0.9952380657196045)
[2025-02-13 19:54:45,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45,765][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.17597457766532898, acc: 0.945652186870575)
[2025-02-13 19:54:45,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46,151][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.05869856849312782, acc: 0.9846153855323792)
[2025-02-13 19:54:46,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46,522][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.2928500473499298, acc: 0.9230769276618958)
[2025-02-13 19:54:46,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46,918][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.23311690986156464, acc: 0.9441340565681458)
[2025-02-13 19:54:47,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47,303][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.07661867141723633, acc: 0.9839572310447693)
[2025-02-13 19:54:47,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47,683][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.15125644207000732, acc: 0.949367105960846)
[2025-02-13 19:54:47,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48,047][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.12216787785291672, acc: 0.9720670580863953)
[2025-02-13 19:54:48,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48,431][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.18768194317817688, acc: 0.9606741666793823)
[2025-02-13 19:54:48,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48,814][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.2593581974506378, acc: 0.9395604133605957)
[2025-02-13 19:54:48,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49,203][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.22771583497524261, acc: 0.9411764740943909)
[2025-02-13 19:54:49,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49,604][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.2671048939228058, acc: 0.936170220375061)
[2025-02-13 19:54:49,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50,002][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.1499919295310974, acc: 0.9621211886405945)
[2025-02-13 19:54:50,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50,406][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.26551002264022827, acc: 0.931034505367279)
[2025-02-13 19:54:50,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50,784][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.25431811809539795, acc: 0.9269663095474243)
[2025-02-13 19:54:50,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51,168][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.19398309290409088, acc: 0.9470899701118469)
[2025-02-13 19:54:51,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51,553][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.18240000307559967, acc: 0.953125)
[2025-02-13 19:54:51,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51,924][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.15969012677669525, acc: 0.9454545378684998)
[2025-02-13 19:54:52,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52,284][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.08366702497005463, acc: 0.976331353187561)
[2025-02-13 19:54:52,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52,658][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.13358202576637268, acc: 0.9710982441902161)
[2025-02-13 19:54:52,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53,042][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.11634030193090439, acc: 0.9709302186965942)
[2025-02-13 19:54:53,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53,443][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.27027183771133423, acc: 0.9583333134651184)
[2025-02-13 19:54:53,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53,824][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.13316987454891205, acc: 0.97826087474823)
[2025-02-13 19:54:53,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54,178][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.17651031911373138, acc: 0.9668508172035217)
[2025-02-13 19:54:54,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54,589][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.2622409462928772, acc: 0.9337748289108276)
[2025-02-13 19:54:54,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55,003][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.29811206459999084, acc: 0.9333333373069763)
[2025-02-13 19:54:55,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55,412][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.1256784200668335, acc: 0.9775280952453613)
[2025-02-13 19:54:55,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55,827][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.15901020169258118, acc: 0.9682539701461792)
[2025-02-13 19:54:55,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56,234][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.21190324425697327, acc: 0.9518072009086609)
[2025-02-13 19:54:56,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56,650][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.12628190219402313, acc: 0.978723406791687)
[2025-02-13 19:54:56,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57,038][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.18832392990589142, acc: 0.9424460530281067)
[2025-02-13 19:54:57,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57,449][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.25689589977264404, acc: 0.9399999976158142)
[2025-02-13 19:54:57,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57,823][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.24157556891441345, acc: 0.9285714030265808)
[2025-02-13 19:54:57,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58,288][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.3093867599964142, acc: 0.9220778942108154)
[2025-02-13 19:54:58,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58,689][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.24066157639026642, acc: 0.9457831382751465)
[2025-02-13 19:54:58,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59,082][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.17779329419136047, acc: 0.9657142758369446)
[2025-02-13 19:54:59,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59,458][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.12926003336906433, acc: 0.9663865566253662)
[2025-02-13 19:54:59,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59,795][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.38683614134788513, acc: 0.9173553586006165)
[2025-02-13 19:54:59,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00,176][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.14082913100719452, acc: 0.9675324559211731)
[2025-02-13 19:55:00,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00,583][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.24749656021595, acc: 0.9485714435577393)
[2025-02-13 19:55:00,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01,012][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.3767452836036682, acc: 0.9245283007621765)
[2025-02-13 19:55:01,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01,389][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.1361001580953598, acc: 0.9695122241973877)
[2025-02-13 19:55:01,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01,771][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.07320433855056763, acc: 0.9906542301177979)
[2025-02-13 19:55:01,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02,155][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.8676871061325073, acc: 0.8095238208770752)
[2025-02-13 19:55:02,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02,557][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.2778501808643341, acc: 0.922535240650177)
[2025-02-13 19:55:02,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02,999][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.2143559753894806, acc: 0.9384615421295166)
[2025-02-13 19:55:03,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03,407][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.32690325379371643, acc: 0.9337016344070435)
[2025-02-13 19:55:03,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03,797][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.19416776299476624, acc: 0.953125)
[2025-02-13 19:55:03,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04,194][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.10078302025794983, acc: 0.9726775884628296)
[2025-02-13 19:55:04,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04,655][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.21939638257026672, acc: 0.9520547986030579)
[2025-02-13 19:55:04,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05,037][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.09886030852794647, acc: 0.9733333587646484)
[2025-02-13 19:55:05,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05,415][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.1190405860543251, acc: 0.9776119589805603)
[2025-02-13 19:55:05,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05,785][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.1031893864274025, acc: 0.9751552939414978)
[2025-02-13 19:55:05,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06,175][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.561623752117157, acc: 0.8662790656089783)
[2025-02-13 19:55:06,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06,561][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.2108810991048813, acc: 0.9621621370315552)
[2025-02-13 19:55:06,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06,924][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.4106570780277252, acc: 0.9126983880996704)
[2025-02-13 19:55:07,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07,311][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.13562405109405518, acc: 0.9608938694000244)
[2025-02-13 19:55:07,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07,668][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.18211476504802704, acc: 0.9545454382896423)
[2025-02-13 19:55:07,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08,020][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.22599560022354126, acc: 0.9444444179534912)
[2025-02-13 19:55:08,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08,391][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.08594910055398941, acc: 0.9832402467727661)
[2025-02-13 19:55:08,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08,772][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.2049562782049179, acc: 0.9485714435577393)
[2025-02-13 19:55:08,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09,130][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.3890860080718994, acc: 0.9009901285171509)
[2025-02-13 19:55:09,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09,521][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.19671641290187836, acc: 0.959770143032074)
[2025-02-13 19:55:09,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09,931][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.1556146889925003, acc: 0.9515151381492615)
[2025-02-13 19:55:10,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10,330][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.2137453258037567, acc: 0.9611111283302307)
[2025-02-13 19:55:10,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10,695][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.13612058758735657, acc: 0.978723406791687)
[2025-02-13 19:55:10,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11,087][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.11427982896566391, acc: 0.9702380895614624)
[2025-02-13 19:55:11,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11,462][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.1882402002811432, acc: 0.9553072452545166)
[2025-02-13 19:55:11,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11,857][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.14503443241119385, acc: 0.9725274443626404)
[2025-02-13 19:55:11,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12,236][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.2002629190683365, acc: 0.9622641801834106)
[2025-02-13 19:55:12,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12,600][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.20590545237064362, acc: 0.9722222089767456)
[2025-02-13 19:55:12,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12,984][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.25250229239463806, acc: 0.9313725233078003)
[2025-02-13 19:55:13,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13,364][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.2387378215789795, acc: 0.936170220375061)
[2025-02-13 19:55:13,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13,723][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.27200961112976074, acc: 0.9270833134651184)
[2025-02-13 19:55:13,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14,125][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.1891797035932541, acc: 0.9569377899169922)
[2025-02-13 19:55:14,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14,559][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.2623783349990845, acc: 0.9273743033409119)
[2025-02-13 19:55:14,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14,926][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.1326364427804947, acc: 0.9588235020637512)
[2025-02-13 19:55:15,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15,294][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.06381832808256149, acc: 0.988304078578949)
[2025-02-13 19:55:15,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15,661][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.24982526898384094, acc: 0.9371069073677063)
[2025-02-13 19:55:15,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16,053][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.14046888053417206, acc: 0.970588207244873)
[2025-02-13 19:55:16,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16,501][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.12406472861766815, acc: 0.9624060392379761)
[2025-02-13 19:55:16,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16,877][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.15891651809215546, acc: 0.9523809552192688)
[2025-02-13 19:55:17,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17,278][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.1716354340314865, acc: 0.9510869383811951)
[2025-02-13 19:55:17,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17,669][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.17118880152702332, acc: 0.9547738432884216)
[2025-02-13 19:55:17,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18,049][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.1751244068145752, acc: 0.9408602118492126)
[2025-02-13 19:55:18,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18,438][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.1090615838766098, acc: 0.9627659320831299)
[2025-02-13 19:55:18,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18,799][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.11664249002933502, acc: 0.9631901979446411)
[2025-02-13 19:55:18,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19,156][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.06753045320510864, acc: 0.9850000143051147)
[2025-02-13 19:55:19,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19,538][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.17267557978630066, acc: 0.9661017060279846)
[2025-02-13 19:55:19,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19,914][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.32854175567626953, acc: 0.9226519465446472)
[2025-02-13 19:55:20,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20,289][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.2692181169986725, acc: 0.9189189076423645)
[2025-02-13 19:55:20,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20,657][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.701123833656311, acc: 0.8393782377243042)
[2025-02-13 19:55:20,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21,038][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.3631313443183899, acc: 0.8700565099716187)
[2025-02-13 19:55:21,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21,392][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.5801424384117126, acc: 0.9034482836723328)
[2025-02-13 19:55:21,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21,772][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.7400906085968018, acc: 0.7983871102333069)
[2025-02-13 19:55:21,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22,164][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.5562562942504883, acc: 0.8883248567581177)
[2025-02-13 19:55:22,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22,596][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.4676031172275543, acc: 0.8848167657852173)
[2025-02-13 19:55:22,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23,020][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.1982656866312027, acc: 0.9462365508079529)
[2025-02-13 19:55:23,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23,404][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.19480504095554352, acc: 0.9473684430122375)
[2025-02-13 19:55:23,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23,782][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.5216879844665527, acc: 0.9060773253440857)
[2025-02-13 19:55:23,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24,147][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.4767993092536926, acc: 0.915032684803009)
[2025-02-13 19:55:24,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24,532][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.43992123007774353, acc: 0.9071038365364075)
[2025-02-13 19:55:24,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24,916][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.2028464674949646, acc: 0.9408866763114929)
[2025-02-13 19:55:25,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25,298][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.40820005536079407, acc: 0.9259259104728699)
[2025-02-13 19:55:25,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25,676][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.12592144310474396, acc: 0.9824561476707458)
[2025-02-13 19:55:25,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26,057][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.24967233836650848, acc: 0.9479768872261047)
[2025-02-13 19:55:26,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26,514][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.1261185109615326, acc: 0.9830508232116699)
[2025-02-13 19:55:26,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26,920][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.04024340584874153, acc: 1.0)
[2025-02-13 19:55:27,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27,343][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.15027160942554474, acc: 0.9506173133850098)
[2025-02-13 19:55:27,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27,748][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.07921842485666275, acc: 0.9784172773361206)
[2025-02-13 19:55:27,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28,197][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.19552916288375854, acc: 0.939393937587738)
[2025-02-13 19:55:28,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28,566][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.24265219271183014, acc: 0.9399999976158142)
[2025-02-13 19:55:28,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28,952][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.16587451100349426, acc: 0.9736841917037964)
[2025-02-13 19:55:29,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29,334][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 1.4387377500534058, acc: 0.6927710771560669)
[2025-02-13 19:55:29,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29,743][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 3.2405788898468018, acc: 0.4363636374473572)
[2025-02-13 19:55:29,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30,148][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 2.9392588138580322, acc: 0.4609375)
[2025-02-13 19:55:30,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30,579][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 1.7223222255706787, acc: 0.6904761791229248)
[2025-02-13 19:55:30,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31,035][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 1.8096612691879272, acc: 0.6534653306007385)
[2025-02-13 19:55:31,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31,447][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.7979004383087158, acc: 0.8294573426246643)
[2025-02-13 19:55:31,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31,815][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.6758773326873779, acc: 0.8240000009536743)
[2025-02-13 19:55:31,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32,209][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.9201528429985046, acc: 0.8289473652839661)
[2025-02-13 19:55:32,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32,598][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.5639265775680542, acc: 0.8476821184158325)
[2025-02-13 19:55:32,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33,007][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.6606487035751343, acc: 0.8633880019187927)
[2025-02-13 19:55:33,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33,376][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 1.0135222673416138, acc: 0.826347291469574)
[2025-02-13 19:55:33,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33,707][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.6023558378219604, acc: 0.8831169009208679)
[2025-02-13 19:55:33,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34,076][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.20422790944576263, acc: 0.9416058659553528)
[2025-02-13 19:55:34,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34,469][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.19444499909877777, acc: 0.9664429426193237)
[2025-02-13 19:55:34,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34,836][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.09239280968904495, acc: 0.9862068891525269)
[2025-02-13 19:55:34,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35,247][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.12818345427513123, acc: 0.9689440727233887)
[2025-02-13 19:55:35,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35,664][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.2614232301712036, acc: 0.931506872177124)
[2025-02-13 19:55:35,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36,066][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.2472737729549408, acc: 0.953125)
[2025-02-13 19:55:36,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36,472][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.1264120489358902, acc: 0.9795918464660645)
[2025-02-13 19:55:36,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36,875][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.18452318012714386, acc: 0.9776119589805603)
[2025-02-13 19:55:37,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37,221][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.29728299379348755, acc: 0.915032684803009)
[2025-02-13 19:55:37,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37,598][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.41642776131629944, acc: 0.8963414430618286)
[2025-02-13 19:55:37,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37,953][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.3855958580970764, acc: 0.9098360538482666)
[2025-02-13 19:55:38,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38,312][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.5598011016845703, acc: 0.9029850959777832)
[2025-02-13 19:55:38,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38,708][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.38861921429634094, acc: 0.9038461446762085)
[2025-02-13 19:55:38,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39,133][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.14023254811763763, acc: 0.9709302186965942)
[2025-02-13 19:55:39,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39,569][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.30561363697052, acc: 0.9281437397003174)
[2025-02-13 19:55:39,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39,982][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.45410943031311035, acc: 0.8742138147354126)
[2025-02-13 19:55:40,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40,367][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.327147901058197, acc: 0.9140625)
[2025-02-13 19:55:40,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40,725][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.5106184482574463, acc: 0.8826815485954285)
[2025-02-13 19:55:40,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41,108][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.15168482065200806, acc: 0.9594594836235046)
[2025-02-13 19:55:41,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41,491][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.15106356143951416, acc: 0.9554139971733093)
[2025-02-13 19:55:41,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41,902][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.410754919052124, acc: 0.9191176295280457)
[2025-02-13 19:55:42,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42,335][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.10167139023542404, acc: 0.9863945841789246)
[2025-02-13 19:55:42,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42,753][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.06662560254335403, acc: 0.9858155846595764)
[2025-02-13 19:55:42,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43,100][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.30225318670272827, acc: 0.9419354796409607)
[2025-02-13 19:55:43,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43,477][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.4054539203643799, acc: 0.9112426042556763)
[2025-02-13 19:55:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43,846][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.452707439661026, acc: 0.893081784248352)
[2025-02-13 19:55:43,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44,233][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.5345426797866821, acc: 0.8650000095367432)
[2025-02-13 19:55:44,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44,642][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.22404029965400696, acc: 0.9513888955116272)
[2025-02-13 19:55:44,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45,012][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.3559480309486389, acc: 0.918367326259613)
[2025-02-13 19:55:45,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45,398][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.5836999416351318, acc: 0.8357142806053162)
[2025-02-13 19:55:45,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45,766][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.4715809226036072, acc: 0.910614550113678)
[2025-02-13 19:55:45,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46,133][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.4508063495159149, acc: 0.9030836820602417)
[2025-02-13 19:55:46,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46,469][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.3510177731513977, acc: 0.9054054021835327)
[2025-02-13 19:55:46,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46,878][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.5291436314582825, acc: 0.9008264541625977)
[2025-02-13 19:55:47,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47,297][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.5435678958892822, acc: 0.9200000166893005)
[2025-02-13 19:55:47,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47,629][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.1512426733970642, acc: 0.9603960514068604)
[2025-02-13 19:55:47,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47,996][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.20734179019927979, acc: 0.949999988079071)
[2025-02-13 19:55:48,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48,357][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.09468375146389008, acc: 0.983146071434021)
[2025-02-13 19:55:48,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48,741][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.1507038027048111, acc: 0.9791666865348816)
[2025-02-13 19:55:48,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49,128][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.07413159310817719, acc: 0.9839572310447693)
[2025-02-13 19:55:49,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49,520][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.1291847825050354, acc: 0.9561403393745422)
[2025-02-13 19:55:49,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49,915][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.10031729191541672, acc: 0.9826086759567261)
[2025-02-13 19:55:50,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50,291][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.12026961147785187, acc: 0.96875)
[2025-02-13 19:55:50,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50,685][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.1559007316827774, acc: 0.9537572264671326)
[2025-02-13 19:55:50,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51,060][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.3015153110027313, acc: 0.929729700088501)
[2025-02-13 19:55:51,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51,409][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.6360348463058472, acc: 0.8715083599090576)
[2025-02-13 19:55:51,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51,773][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.41041380167007446, acc: 0.9112426042556763)
[2025-02-13 19:55:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52,149][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.25726690888404846, acc: 0.9428571462631226)
[2025-02-13 19:55:52,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52,525][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.29693251848220825, acc: 0.9202127456665039)
[2025-02-13 19:55:52,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52,910][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.39654740691185, acc: 0.916167676448822)
[2025-02-13 19:55:53,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53,293][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.33110910654067993, acc: 0.9200000166893005)
[2025-02-13 19:55:53,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53,667][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.22088679671287537, acc: 0.939226508140564)
[2025-02-13 19:55:53,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54,045][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.16924792528152466, acc: 0.9385964870452881)
[2025-02-13 19:55:54,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54,420][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.2758091986179352, acc: 0.9370629191398621)
[2025-02-13 19:55:54,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54,806][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.29599863290786743, acc: 0.9051094651222229)
[2025-02-13 19:55:54,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55,200][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.2826661765575409, acc: 0.9507042169570923)
[2025-02-13 19:55:55,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55,585][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.286582887172699, acc: 0.9390243887901306)
[2025-02-13 19:55:55,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55,932][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.16425400972366333, acc: 0.9470198750495911)
[2025-02-13 19:55:56,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56,340][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.3025658428668976, acc: 0.9426751732826233)
[2025-02-13 19:55:56,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56,738][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.39000165462493896, acc: 0.9160305261611938)
[2025-02-13 19:55:56,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57,158][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.20946043729782104, acc: 0.9622641801834106)
[2025-02-13 19:55:57,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57,576][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.29989108443260193, acc: 0.9514563083648682)
[2025-02-13 19:55:57,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57,965][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.3627215027809143, acc: 0.9179104566574097)
[2025-02-13 19:55:58,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58,322][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.4702892005443573, acc: 0.8859060406684875)
[2025-02-13 19:55:58,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58,690][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.5775965452194214, acc: 0.8689655065536499)
[2025-02-13 19:55:58,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59,040][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.21958453953266144, acc: 0.9640287756919861)
[2025-02-13 19:55:59,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59,431][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.21317000687122345, acc: 0.9344262480735779)
[2025-02-13 19:55:59,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59,821][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.15889260172843933, acc: 0.95652174949646)
[2025-02-13 19:55:59,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00,203][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.2791942358016968, acc: 0.9444444179534912)
[2025-02-13 19:56:00,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00,592][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.22365057468414307, acc: 0.934959352016449)
[2025-02-13 19:56:00,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00,936][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.10660181939601898, acc: 0.989130437374115)
[2025-02-13 19:56:01,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01,287][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.3037010729312897, acc: 0.9271523356437683)
[2025-02-13 19:56:01,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01,632][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.25063008069992065, acc: 0.935251772403717)
[2025-02-13 19:56:01,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02,011][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.11207898706197739, acc: 0.9640287756919861)
[2025-02-13 19:56:02,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02,410][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.10763068497180939, acc: 0.984000027179718)
[2025-02-13 19:56:02,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02,800][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.1779070794582367, acc: 0.9473684430122375)
[2025-02-13 19:56:02,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03,178][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.1423485428094864, acc: 0.9615384340286255)
[2025-02-13 19:56:03,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03,557][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.5662214159965515, acc: 0.8648648858070374)
[2025-02-13 19:56:03,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03,945][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.3316700756549835, acc: 0.9390243887901306)
[2025-02-13 19:56:04,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04,322][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.11194618046283722, acc: 0.9838709831237793)
[2025-02-13 19:56:04,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04,753][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.14896874129772186, acc: 0.9696969985961914)
[2025-02-13 19:56:04,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05,170][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.17640380561351776, acc: 0.9548872113227844)
[2025-02-13 19:56:05,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05,551][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.2796134948730469, acc: 0.9351851940155029)
[2025-02-13 19:56:05,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05,928][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.06629206240177155, acc: 0.9918699264526367)
[2025-02-13 19:56:06,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06,330][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.16008725762367249, acc: 0.9629629850387573)
[2025-02-13 19:56:06,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06,743][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.07023726403713226, acc: 0.9882352948188782)
[2025-02-13 19:56:06,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07,157][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.2015615552663803, acc: 0.9550561904907227)
[2025-02-13 19:56:07,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07,508][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.2484954595565796, acc: 0.921875)
[2025-02-13 19:56:07,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07,870][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.21634207665920258, acc: 0.9308510422706604)
[2025-02-13 19:56:08,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08,230][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.19226151704788208, acc: 0.9454545378684998)
[2025-02-13 19:56:08,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08,620][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.27663654088974, acc: 0.9404761791229248)
[2025-02-13 19:56:08,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08,966][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.2083919495344162, acc: 0.956250011920929)
[2025-02-13 19:56:09,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09,330][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.12225359678268433, acc: 0.9777777791023254)
[2025-02-13 19:56:09,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09,706][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.28024864196777344, acc: 0.9385474920272827)
[2025-02-13 19:56:09,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10,083][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.16697758436203003, acc: 0.9521276354789734)
[2025-02-13 19:56:10,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10,433][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.2626414895057678, acc: 0.9415204524993896)
[2025-02-13 19:56:10,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10,790][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.16260391473770142, acc: 0.9695431590080261)
[2025-02-13 19:56:10,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11,171][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.11834269762039185, acc: 0.9639175534248352)
[2025-02-13 19:56:11,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11,540][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.23449061810970306, acc: 0.9513513445854187)
[2025-02-13 19:56:11,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11,913][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.23347651958465576, acc: 0.953125)
[2025-02-13 19:56:12,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12,287][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.21781669557094574, acc: 0.9459459185600281)
[2025-02-13 19:56:12,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12,664][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.2847946286201477, acc: 0.9440000057220459)
[2025-02-13 19:56:12,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13,039][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.14125196635723114, acc: 0.9745762944221497)
[2025-02-13 19:56:13,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13,423][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.3044905960559845, acc: 0.9303797483444214)
[2025-02-13 19:56:13,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13,805][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.12937963008880615, acc: 0.9694656729698181)
[2025-02-13 19:56:13,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14,175][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.07605605572462082, acc: 0.9720279574394226)
[2025-02-13 19:56:14,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14,539][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.25578171014785767, acc: 0.9333333373069763)
[2025-02-13 19:56:14,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14,921][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.13183999061584473, acc: 0.9583333134651184)
[2025-02-13 19:56:15,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15,308][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.21134568750858307, acc: 0.9597315192222595)
[2025-02-13 19:56:15,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15,708][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.12948229908943176, acc: 0.9685039520263672)
[2025-02-13 19:56:15,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16,104][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.12131630629301071, acc: 0.9767441749572754)
[2025-02-13 19:56:16,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16,472][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.32528239488601685, acc: 0.9155844449996948)
[2025-02-13 19:56:16,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16,852][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.32490333914756775, acc: 0.9319728016853333)
[2025-02-13 19:56:16,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17,236][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.604883074760437, acc: 0.8888888955116272)
[2025-02-13 19:56:17,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17,603][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.3561680316925049, acc: 0.8938053250312805)
[2025-02-13 19:56:17,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18,010][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.3076351284980774, acc: 0.9322034120559692)
[2025-02-13 19:56:18,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18,427][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.5097469091415405, acc: 0.8910256624221802)
[2025-02-13 19:56:18,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18,856][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.12396000325679779, acc: 0.9696969985961914)
[2025-02-13 19:56:19,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19,256][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.2919920086860657, acc: 0.9504132270812988)
[2025-02-13 19:56:19,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19,644][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.1145174503326416, acc: 0.9615384340286255)
[2025-02-13 19:56:19,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20,018][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.06933443993330002, acc: 0.9927007555961609)
[2025-02-13 19:56:20,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20,397][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.047402217984199524, acc: 0.9927536249160767)
[2025-02-13 19:56:20,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20,816][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.12153919041156769, acc: 0.9720279574394226)
[2025-02-13 19:56:20,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21,161][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.11273656785488129, acc: 0.9801980257034302)
[2025-02-13 19:56:21,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21,541][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.1527072936296463, acc: 0.9639639854431152)
[2025-02-13 19:56:21,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21,928][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.09935085475444794, acc: 0.9801980257034302)
[2025-02-13 19:56:22,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22,342][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.23749202489852905, acc: 0.96875)
[2025-02-13 19:56:22,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22,758][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.07272830605506897, acc: 0.991304337978363)
[2025-02-13 19:56:22,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23,175][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.14135907590389252, acc: 0.9626168012619019)
[2025-02-13 19:56:23,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23,574][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.11952874809503555, acc: 0.960629940032959)
[2025-02-13 19:56:23,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23,984][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.1037268117070198, acc: 0.9734513163566589)
[2025-02-13 19:56:24,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24,371][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.08779974281787872, acc: 0.9844961166381836)
[2025-02-13 19:56:24,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24,764][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.13501985371112823, acc: 0.969924807548523)
[2025-02-13 19:56:24,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25,133][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.033652935177087784, acc: 1.0)
[2025-02-13 19:56:25,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25,583][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.03850875049829483, acc: 1.0)
[2025-02-13 19:56:25,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25,964][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.09399721026420593, acc: 0.970370352268219)
[2025-02-13 19:56:26,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26,369][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.23002247512340546, acc: 0.949999988079071)
[2025-02-13 19:56:26,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26,710][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.16648650169372559, acc: 0.9729729890823364)
[2025-02-13 19:56:26,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27,114][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.07972291111946106, acc: 0.9873417615890503)
[2025-02-13 19:56:27,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27,519][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.15484315156936646, acc: 0.9408602118492126)
[2025-02-13 19:56:27,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27,908][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.18785704672336578, acc: 0.9503105878829956)
[2025-02-13 19:56:28,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28,295][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.1615157425403595, acc: 0.9642857313156128)
[2025-02-13 19:56:28,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28,713][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.14347264170646667, acc: 0.9712643623352051)
[2025-02-13 19:56:28,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29,155][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.08896633237600327, acc: 0.9934640526771545)
[2025-02-13 19:56:29,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29,554][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.31665483117103577, acc: 0.918367326259613)
[2025-02-13 19:56:29,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29,937][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.24435338377952576, acc: 0.9398148059844971)
[2025-02-13 19:56:30,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30,321][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.0951685830950737, acc: 0.9772727489471436)
[2025-02-13 19:56:30,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30,696][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.10903942584991455, acc: 0.9550561904907227)
[2025-02-13 19:56:30,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31,090][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.13570570945739746, acc: 0.9696969985961914)
[2025-02-13 19:56:31,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31,502][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.09519658237695694, acc: 0.981249988079071)
[2025-02-13 19:56:31,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31,869][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.07289339601993561, acc: 0.9850746393203735)
[2025-02-13 19:56:32,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32,256][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.1080380231142044, acc: 0.9780701994895935)
[2025-02-13 19:56:32,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32,654][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.0690430998802185, acc: 0.9895833134651184)
[2025-02-13 19:56:32,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33,052][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.022777676582336426, acc: 0.9900497794151306)
[2025-02-13 19:56:33,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33,425][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.19337229430675507, acc: 0.9462365508079529)
[2025-02-13 19:56:33,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33,798][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.15320564806461334, acc: 0.9725274443626404)
[2025-02-13 19:56:33,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34,180][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.06951861083507538, acc: 0.9797297120094299)
[2025-02-13 19:56:34,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34,586][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.35275793075561523, acc: 0.9481865167617798)
[2025-02-13 19:56:34,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34,977][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.16264446079730988, acc: 0.9627329111099243)
[2025-02-13 19:56:35,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35,367][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.19555801153182983, acc: 0.9528796076774597)
[2025-02-13 19:56:35,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35,750][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.14400318264961243, acc: 0.9679144620895386)
[2025-02-13 19:56:35,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36,130][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.21487247943878174, acc: 0.939393937587738)
[2025-02-13 19:56:36,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36,503][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.13874714076519012, acc: 0.9591836929321289)
[2025-02-13 19:56:36,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36,895][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.1384771764278412, acc: 0.9685534834861755)
[2025-02-13 19:56:37,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37,268][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.23725414276123047, acc: 0.9461538195610046)
[2025-02-13 19:56:37,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37,625][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.11199440807104111, acc: 0.961240291595459)
[2025-02-13 19:56:37,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37,997][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.17048661410808563, acc: 0.9645389914512634)
[2025-02-13 19:56:38,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38,365][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.06098613142967224, acc: 0.9868420958518982)
[2025-02-13 19:56:38,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38,740][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.2173726111650467, acc: 0.949999988079071)
[2025-02-13 19:56:38,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39,125][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.15102313458919525, acc: 0.9526627063751221)
[2025-02-13 19:56:39,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39,529][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.21121124923229218, acc: 0.9644970297813416)
[2025-02-13 19:56:39,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39,988][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.1708272248506546, acc: 0.942105233669281)
[2025-02-13 19:56:40,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40,441][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.13124492764472961, acc: 0.9836956262588501)
[2025-02-13 19:56:40,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40,851][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.1090303286910057, acc: 0.9707602262496948)
[2025-02-13 19:56:41,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41,268][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.1226736530661583, acc: 0.9631901979446411)
[2025-02-13 19:56:41,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41,657][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.17487242817878723, acc: 0.9547738432884216)
[2025-02-13 19:56:41,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42,024][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.2344828099012375, acc: 0.9467455744743347)
[2025-02-13 19:56:42,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42,409][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.08768501877784729, acc: 0.970059871673584)
[2025-02-13 19:56:42,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42,811][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.07448852807283401, acc: 0.9825581312179565)
[2025-02-13 19:56:42,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43,192][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.12450587004423141, acc: 0.9602272510528564)
[2025-02-13 19:56:43,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43,596][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.0769016295671463, acc: 0.9882352948188782)
[2025-02-13 19:56:43,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43,984][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.10787730664014816, acc: 0.9542483687400818)
[2025-02-13 19:56:44,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44,363][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.07737982273101807, acc: 0.9815950989723206)
[2025-02-13 19:56:44,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44,746][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.09693503379821777, acc: 0.9556962251663208)
[2025-02-13 19:56:44,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45,150][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.13358691334724426, acc: 0.9726775884628296)
[2025-02-13 19:56:45,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45,518][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.11011207103729248, acc: 0.9611650705337524)
[2025-02-13 19:56:45,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45,911][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.05831426382064819, acc: 0.9873417615890503)
[2025-02-13 19:56:46,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46,308][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.056342657655477524, acc: 0.9808917045593262)
[2025-02-13 19:56:46,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46,761][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.035446520894765854, acc: 0.9948453903198242)
[2025-02-13 19:56:46,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47,160][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.04040517285466194, acc: 0.9881656765937805)
[2025-02-13 19:56:47,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47,554][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.1507990062236786, acc: 0.9509202241897583)
[2025-02-13 19:56:47,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47,947][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.1351340264081955, acc: 0.9620253443717957)
[2025-02-13 19:56:48,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48,331][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.08645717799663544, acc: 0.9751552939414978)
[2025-02-13 19:56:48,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48,729][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.07695480436086655, acc: 0.9811320900917053)
[2025-02-13 19:56:48,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49,136][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.05409405753016472, acc: 0.9869281053543091)
[2025-02-13 19:56:49,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49,535][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.10722466558218002, acc: 0.9797297120094299)
[2025-02-13 19:56:49,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49,904][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.181194007396698, acc: 0.957446813583374)
[2025-02-13 19:56:50,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50,291][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.2737722098827362, acc: 0.9277777671813965)
[2025-02-13 19:56:50,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50,685][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.1364196091890335, acc: 0.9720670580863953)
[2025-02-13 19:56:50,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51,034][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.1998608410358429, acc: 0.9378530979156494)
[2025-02-13 19:56:51,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51,407][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.05339138209819794, acc: 0.9852941036224365)
[2025-02-13 19:56:51,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51,797][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.3685586750507355, acc: 0.9226804375648499)
[2025-02-13 19:56:51,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52,186][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.2044437974691391, acc: 0.9736841917037964)
[2025-02-13 19:56:52,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52,615][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.1004282608628273, acc: 0.9692307710647583)
[2025-02-13 19:56:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53,036][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.1198527067899704, acc: 0.9689922332763672)
[2025-02-13 19:56:53,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53,469][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.22577928006649017, acc: 0.9668508172035217)
[2025-02-13 19:56:53,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53,839][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.09607324004173279, acc: 0.9738562107086182)
[2025-02-13 19:56:53,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54,191][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.045743800699710846, acc: 0.9909909963607788)
[2025-02-13 19:56:54,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54,571][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.09865157306194305, acc: 0.9585798978805542)
[2025-02-13 19:56:54,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54,945][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.10931748151779175, acc: 0.9760765433311462)
[2025-02-13 19:56:55,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55,321][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.15787307918071747, acc: 0.9714285731315613)
[2025-02-13 19:56:55,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55,678][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.07924193143844604, acc: 0.9932432174682617)
[2025-02-13 19:56:55,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56,039][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.09383416175842285, acc: 0.9800000190734863)
[2025-02-13 19:56:56,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56,418][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.13377709686756134, acc: 0.9651162624359131)
[2025-02-13 19:56:56,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56,764][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.10106799751520157, acc: 0.9679144620895386)
[2025-02-13 19:56:56,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57,127][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.08459439128637314, acc: 0.9923664331436157)
[2025-02-13 19:56:57,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57,488][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.2293299436569214, acc: 0.9627659320831299)
[2025-02-13 19:56:57,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57,890][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.15118619799613953, acc: 0.9741379022598267)
[2025-02-13 19:56:58,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58,305][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.23920473456382751, acc: 0.9510869383811951)
[2025-02-13 19:56:58,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58,728][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.06326141953468323, acc: 0.9898989796638489)
[2025-02-13 19:56:58,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59,117][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.08199197798967361, acc: 0.9683544039726257)
[2025-02-13 19:56:59,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59,523][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.29979416728019714, acc: 0.905063271522522)
[2025-02-13 19:56:59,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59,927][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.16272851824760437, acc: 0.9509803652763367)
[2025-02-13 19:57:00,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00,304][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.20419251918792725, acc: 0.9597989916801453)
[2025-02-13 19:57:00,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00,696][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.09169048815965652, acc: 0.9694322943687439)
[2025-02-13 19:57:00,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01,039][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.10105820000171661, acc: 0.9731183052062988)
[2025-02-13 19:57:01,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01,392][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.34317857027053833, acc: 0.9011628031730652)
[2025-02-13 19:57:01,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01,792][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.35290977358818054, acc: 0.9405405521392822)
[2025-02-13 19:57:01,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02,150][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.3163156807422638, acc: 0.9299362897872925)
[2025-02-13 19:57:02,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02,522][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.2365676462650299, acc: 0.9329608678817749)
[2025-02-13 19:57:02,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02,891][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.1650850772857666, acc: 0.9631578922271729)
[2025-02-13 19:57:03,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03,274][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.26318126916885376, acc: 0.939393937587738)
[2025-02-13 19:57:03,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03,667][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.11917640268802643, acc: 0.9639639854431152)
[2025-02-13 19:57:03,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04,041][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.11278550326824188, acc: 0.9730941653251648)
[2025-02-13 19:57:04,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04,426][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.15229982137680054, acc: 0.9476439952850342)
[2025-02-13 19:57:04,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04,836][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.07654907554388046, acc: 0.9826589822769165)
[2025-02-13 19:57:04,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05,266][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.08710679411888123, acc: 0.976190447807312)
[2025-02-13 19:57:05,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05,658][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.0744917243719101, acc: 0.9711538553237915)
[2025-02-13 19:57:05,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06,038][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.18339140713214874, acc: 0.9450549483299255)
[2025-02-13 19:57:06,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06,423][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.11753633618354797, acc: 0.9613526463508606)
[2025-02-13 19:57:06,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06,808][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.07864080369472504, acc: 0.9801980257034302)
[2025-02-13 19:57:06,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07,191][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.14312699437141418, acc: 0.9571428298950195)
[2025-02-13 19:57:07,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07,577][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.10371120274066925, acc: 0.9748427867889404)
[2025-02-13 19:57:07,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07,968][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.09617596864700317, acc: 0.9585798978805542)
[2025-02-13 19:57:08,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08,353][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.03902630880475044, acc: 0.9943820238113403)
[2025-02-13 19:57:08,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08,713][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.06358572840690613, acc: 0.9876543283462524)
[2025-02-13 19:57:08,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09,079][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.016595903784036636, acc: 0.9943181872367859)
[2025-02-13 19:57:09,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09,453][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.030930398032069206, acc: 0.9941176176071167)
[2025-02-13 19:57:09,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09,828][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.03922730311751366, acc: 0.9874213933944702)
[2025-02-13 19:57:09,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10,233][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.029264966025948524, acc: 1.0)
[2025-02-13 19:57:10,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10,636][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.29459676146507263, acc: 0.9210526347160339)
[2025-02-13 19:57:10,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10,998][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.06329971551895142, acc: 0.9882352948188782)
[2025-02-13 19:57:11,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11,360][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.07725867629051208, acc: 0.9881656765937805)
[2025-02-13 19:57:11,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11,763][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.08866994082927704, acc: 0.9848484992980957)
[2025-02-13 19:57:11,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12,147][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.11516762524843216, acc: 0.9680851101875305)
[2025-02-13 19:57:12,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12,516][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.13022343814373016, acc: 0.9567901492118835)
[2025-02-13 19:57:12,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12,935][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.1337074339389801, acc: 0.9611111283302307)
[2025-02-13 19:57:13,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13,325][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.051534514874219894, acc: 1.0)
[2025-02-13 19:57:13,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13,700][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.16419537365436554, acc: 0.9647887349128723)
[2025-02-13 19:57:13,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14,053][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.042786404490470886, acc: 0.9921259880065918)
[2025-02-13 19:57:14,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14,437][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.1361333727836609, acc: 0.959770143032074)
[2025-02-13 19:57:14,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14,774][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.11151254177093506, acc: 0.9692307710647583)
[2025-02-13 19:57:14,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15,155][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.05423658713698387, acc: 0.9863013625144958)
[2025-02-13 19:57:15,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15,530][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.019881803542375565, acc: 0.9922480583190918)
[2025-02-13 19:57:15,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15,903][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.025035666301846504, acc: 0.9931972622871399)
[2025-02-13 19:57:16,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16,266][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.018202301114797592, acc: 1.0)
[2025-02-13 19:57:16,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16,639][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.13731342554092407, acc: 0.9622641801834106)
[2025-02-13 19:57:16,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17,029][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.052038200199604034, acc: 0.9822485446929932)
[2025-02-13 19:57:17,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17,389][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.0483492910861969, acc: 0.9938650131225586)
[2025-02-13 19:57:17,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17,782][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.23830118775367737, acc: 0.9470899701118469)
[2025-02-13 19:57:17,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18,167][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.17163914442062378, acc: 0.9468085169792175)
[2025-02-13 19:57:18,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18,569][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.13333410024642944, acc: 0.9622641801834106)
[2025-02-13 19:57:18,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18,968][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.2854679822921753, acc: 0.9279661178588867)
[2025-02-13 19:57:19,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19,370][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.2513054311275482, acc: 0.9438202381134033)
[2025-02-13 19:57:19,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19,729][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.14506256580352783, acc: 0.9503546357154846)
[2025-02-13 19:57:19,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20,110][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.24823860824108124, acc: 0.9409282803535461)
[2025-02-13 19:57:20,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20,504][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.19008393585681915, acc: 0.9487179517745972)
[2025-02-13 19:57:20,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20,892][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.07535593956708908, acc: 0.987730085849762)
[2025-02-13 19:57:21,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21,282][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.24376341700553894, acc: 0.9586206674575806)
[2025-02-13 19:57:21,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21,697][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.1435602307319641, acc: 0.9435483813285828)
[2025-02-13 19:57:21,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22,092][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.1685539036989212, acc: 0.9518072009086609)
[2025-02-13 19:57:22,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22,524][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.13658346235752106, acc: 0.9666666388511658)
[2025-02-13 19:57:22,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22,894][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.132165789604187, acc: 0.9529411792755127)
[2025-02-13 19:57:23,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23,301][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.16738109290599823, acc: 0.9675925970077515)
[2025-02-13 19:57:23,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23,692][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.3548928499221802, acc: 0.9353233575820923)
[2025-02-13 19:57:23,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24,096][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.14981523156166077, acc: 0.9659090638160706)
[2025-02-13 19:57:24,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24,483][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.30013608932495117, acc: 0.9553072452545166)
[2025-02-13 19:57:24,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24,851][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.1977057158946991, acc: 0.9473684430122375)
[2025-02-13 19:57:24,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25,217][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.2646007239818573, acc: 0.9523809552192688)
[2025-02-13 19:57:25,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25,564][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.20612646639347076, acc: 0.9399999976158142)
[2025-02-13 19:57:25,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25,930][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.21650844812393188, acc: 0.9513888955116272)
[2025-02-13 19:57:26,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26,314][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.2213934361934662, acc: 0.9459459185600281)
[2025-02-13 19:57:26,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26,673][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.2223498523235321, acc: 0.9560439586639404)
[2025-02-13 19:57:26,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27,041][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.19334273040294647, acc: 0.9666666388511658)
[2025-02-13 19:57:27,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27,407][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.22409658133983612, acc: 0.9469696879386902)
[2025-02-13 19:57:27,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27,802][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.2732762396335602, acc: 0.9575757384300232)
[2025-02-13 19:57:27,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28,171][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.2755224406719208, acc: 0.9580419659614563)
[2025-02-13 19:57:28,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28,550][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.25393879413604736, acc: 0.949999988079071)
[2025-02-13 19:57:28,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28,946][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.2097073346376419, acc: 0.9411764740943909)
[2025-02-13 19:57:29,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29,313][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.3008657991886139, acc: 0.9217391014099121)
[2025-02-13 19:57:29,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29,674][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.21312347054481506, acc: 0.9271523356437683)
[2025-02-13 19:57:29,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30,061][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.15250974893569946, acc: 0.949999988079071)
[2025-02-13 19:57:30,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30,435][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.13677139580249786, acc: 0.9813664555549622)
[2025-02-13 19:57:30,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30,815][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.13030961155891418, acc: 0.955974817276001)
[2025-02-13 19:57:30,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31,202][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.09804431349039078, acc: 0.9725274443626404)
[2025-02-13 19:57:31,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31,578][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.23778952658176422, acc: 0.9285714030265808)
[2025-02-13 19:57:31,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31,953][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.2871086597442627, acc: 0.9192546606063843)
[2025-02-13 19:57:32,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32,332][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.20014159381389618, acc: 0.948387086391449)
[2025-02-13 19:57:32,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32,708][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.14321781694889069, acc: 0.9691358208656311)
[2025-02-13 19:57:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33,073][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.300883024930954, acc: 0.9552238583564758)
[2025-02-13 19:57:33,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33,462][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.4180999994277954, acc: 0.9496402740478516)
[2025-02-13 19:57:33,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33,835][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.12743200361728668, acc: 0.9810126423835754)
[2025-02-13 19:57:33,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34,200][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.16567537188529968, acc: 0.9513888955116272)
[2025-02-13 19:57:34,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34,567][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.07886447757482529, acc: 0.9806451797485352)
[2025-02-13 19:57:34,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34,965][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.08892982453107834, acc: 0.9704142212867737)
[2025-02-13 19:57:35,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35,351][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.03840544819831848, acc: 0.9933333396911621)
[2025-02-13 19:57:35,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35,727][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.12506455183029175, acc: 0.9752066135406494)
[2025-02-13 19:57:35,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36,131][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.08984193205833435, acc: 0.9684210419654846)
[2025-02-13 19:57:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36,510][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.14031726121902466, acc: 0.9615384340286255)
[2025-02-13 19:57:36,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36,888][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.1289551705121994, acc: 0.9731543660163879)
[2025-02-13 19:57:37,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37,279][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.0679595023393631, acc: 0.9823529124259949)
[2025-02-13 19:57:37,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37,679][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.08733685314655304, acc: 0.9767441749572754)
[2025-02-13 19:57:37,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38,062][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.06423213332891464, acc: 0.9884393215179443)
[2025-02-13 19:57:38,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38,485][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.06918586790561676, acc: 0.9805194735527039)
[2025-02-13 19:57:38,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38,863][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.15812614560127258, acc: 0.9659090638160706)
[2025-02-13 19:57:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39,273][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.11335530132055283, acc: 0.9751552939414978)
[2025-02-13 19:57:39,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39,666][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.09006790071725845, acc: 0.988095223903656)
[2025-02-13 19:57:39,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40,055][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.16143016517162323, acc: 0.9772727489471436)
[2025-02-13 19:57:40,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40,439][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.09251832962036133, acc: 0.9726775884628296)
[2025-02-13 19:57:40,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40,841][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.12137646228075027, acc: 0.9647058844566345)
[2025-02-13 19:57:40,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41,230][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.23308806121349335, acc: 0.9256756901741028)
[2025-02-13 19:57:41,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41,605][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.31244900822639465, acc: 0.9268292784690857)
[2025-02-13 19:57:41,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41,964][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.18181753158569336, acc: 0.9523809552192688)
[2025-02-13 19:57:42,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42,330][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.2840760350227356, acc: 0.9083969593048096)
[2025-02-13 19:57:42,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42,686][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.28720414638519287, acc: 0.9395973086357117)
[2025-02-13 19:57:42,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43,011][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.33577415347099304, acc: 0.9351851940155029)
[2025-02-13 19:57:43,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43,397][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.14805914461612701, acc: 0.9642857313156128)
[2025-02-13 19:57:43,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43,727][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.19106563925743103, acc: 0.9576271176338196)
[2025-02-13 19:57:43,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44,106][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.3226626515388489, acc: 0.9420289993286133)
[2025-02-13 19:57:44,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44,490][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.13841944932937622, acc: 0.9801324605941772)
[2025-02-13 19:57:44,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44,850][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.11610652506351471, acc: 0.9739130139350891)
[2025-02-13 19:57:44,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45,222][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.06689202040433884, acc: 0.9868420958518982)
[2025-02-13 19:57:45,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45,606][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.17293089628219604, acc: 0.9707602262496948)
[2025-02-13 19:57:45,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46,033][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.11769679188728333, acc: 0.9719101190567017)
[2025-02-13 19:57:46,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46,421][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.12211967259645462, acc: 0.9709302186965942)
[2025-02-13 19:57:46,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46,803][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.15945346653461456, acc: 0.9717513918876648)
[2025-02-13 19:57:46,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47,165][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.17582182586193085, acc: 0.9754601120948792)
[2025-02-13 19:57:47,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47,585][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.11909856647253036, acc: 0.9659090638160706)
[2025-02-13 19:57:47,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47,955][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.14461661875247955, acc: 0.96875)
[2025-02-13 19:57:48,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48,336][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.07936938107013702, acc: 0.9741379022598267)
[2025-02-13 19:57:48,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48,752][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.19041751325130463, acc: 0.9426751732826233)
[2025-02-13 19:57:48,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49,165][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.1594693958759308, acc: 0.9461538195610046)
[2025-02-13 19:57:49,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49,544][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.3102378845214844, acc: 0.9383561611175537)
[2025-02-13 19:57:49,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49,957][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.049396637827157974, acc: 0.991150438785553)
[2025-02-13 19:57:50,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50,350][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.30015185475349426, acc: 0.918367326259613)
[2025-02-13 19:57:50,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50,744][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.09282344579696655, acc: 0.976047933101654)
[2025-02-13 19:57:50,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51,158][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.186459481716156, acc: 0.956204354763031)
[2025-02-13 19:57:51,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51,548][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.08055298775434494, acc: 0.9878048896789551)
[2025-02-13 19:57:51,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51,942][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.1448134183883667, acc: 0.9653179049491882)
[2025-02-13 19:57:52,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52,306][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.11304234713315964, acc: 0.9759036302566528)
[2025-02-13 19:57:52,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52,689][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.1421593725681305, acc: 0.9694656729698181)
[2025-02-13 19:57:52,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53,072][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.19247488677501678, acc: 0.9433962106704712)
[2025-02-13 19:57:53,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53,471][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.2136038988828659, acc: 0.9523809552192688)
[2025-02-13 19:57:53,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53,821][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.3249116837978363, acc: 0.9399999976158142)
[2025-02-13 19:57:53,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54,214][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.38054001331329346, acc: 0.8815789222717285)
[2025-02-13 19:57:54,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54,612][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.1969948261976242, acc: 0.949999988079071)
[2025-02-13 19:57:54,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55,011][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.30055972933769226, acc: 0.9398496150970459)
[2025-02-13 19:57:55,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55,405][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.3922966718673706, acc: 0.8796296119689941)
[2025-02-13 19:57:55,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55,788][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.1726980209350586, acc: 0.9735099077224731)
[2025-02-13 19:57:55,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56,165][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.1378994882106781, acc: 0.9605262875556946)
[2025-02-13 19:57:56,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56,545][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.3600519299507141, acc: 0.8999999761581421)
[2025-02-13 19:57:56,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56,922][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.17695969343185425, acc: 0.960629940032959)
[2025-02-13 19:57:57,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57,274][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.22930502891540527, acc: 0.9375)
[2025-02-13 19:57:57,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57,635][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.1637086570262909, acc: 0.9591836929321289)
[2025-02-13 19:57:57,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57,984][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.16449449956417084, acc: 0.9368420839309692)
[2025-02-13 19:57:58,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58,363][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.3195442259311676, acc: 0.9363057613372803)
[2025-02-13 19:57:58,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58,757][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.3652602732181549, acc: 0.9155844449996948)
[2025-02-13 19:57:58,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59,130][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.19508302211761475, acc: 0.949999988079071)
[2025-02-13 19:57:59,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59,505][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.17226846516132355, acc: 0.9503546357154846)
[2025-02-13 19:57:59,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59,874][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.22339287400245667, acc: 0.9551281929016113)
[2025-02-13 19:58:00,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00,244][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.2976917028427124, acc: 0.926174521446228)
[2025-02-13 19:58:00,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00,612][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.0996747687458992, acc: 0.9756097793579102)
[2025-02-13 19:58:00,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00,977][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.20637452602386475, acc: 0.949999988079071)
[2025-02-13 19:58:01,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01,368][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.1793275624513626, acc: 0.9285714030265808)
[2025-02-13 19:58:01,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01,764][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.38022637367248535, acc: 0.9230769276618958)
[2025-02-13 19:58:01,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02,179][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.3408384621143341, acc: 0.9532163739204407)
[2025-02-13 19:58:02,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02,572][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.2098069190979004, acc: 0.9527559280395508)
[2025-02-13 19:58:02,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02,979][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.30200067162513733, acc: 0.9189189076423645)
[2025-02-13 19:58:03,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03,405][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.26266559958457947, acc: 0.9281437397003174)
[2025-02-13 19:58:03,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03,861][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.18813934922218323, acc: 0.9736841917037964)
[2025-02-13 19:58:03,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04,256][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.11712430417537689, acc: 0.9776536226272583)
[2025-02-13 19:58:04,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04,634][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.22169487178325653, acc: 0.93034827709198)
[2025-02-13 19:58:04,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05,008][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.0886172279715538, acc: 0.9890109896659851)
[2025-02-13 19:58:05,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05,421][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.0920170471072197, acc: 0.977142870426178)
[2025-02-13 19:58:05,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05,809][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.11261129379272461, acc: 0.9757575988769531)
[2025-02-13 19:58:05,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06,195][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.11409039795398712, acc: 0.9585798978805542)
[2025-02-13 19:58:06,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06,597][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.15012095868587494, acc: 0.9766082167625427)
[2025-02-13 19:58:06,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06,981][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.11475736647844315, acc: 0.9623655676841736)
[2025-02-13 19:58:07,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07,368][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.14013832807540894, acc: 0.9636363387107849)
[2025-02-13 19:58:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07,762][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.13097892701625824, acc: 0.9701492786407471)
[2025-02-13 19:58:07,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08,182][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.15677107870578766, acc: 0.9677419066429138)
[2025-02-13 19:58:08,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08,582][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.48764994740486145, acc: 0.9111111164093018)
[2025-02-13 19:58:08,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08,964][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.06256360560655594, acc: 0.9802631735801697)
[2025-02-13 19:58:09,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09,373][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.11941459774971008, acc: 0.97826087474823)
[2025-02-13 19:58:09,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09,778][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.10604896396398544, acc: 0.9820359349250793)
[2025-02-13 19:58:09,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10,182][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.08890029788017273, acc: 0.9836065769195557)
[2025-02-13 19:58:10,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10,566][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.08618618547916412, acc: 0.9891892075538635)
[2025-02-13 19:58:10,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10,957][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.09193292260169983, acc: 0.9680851101875305)
[2025-02-13 19:58:11,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11,332][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.12452109903097153, acc: 0.9668508172035217)
[2025-02-13 19:58:11,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11,702][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.08562660962343216, acc: 0.9729729890823364)
[2025-02-13 19:58:11,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12,086][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.18691548705101013, acc: 0.9693251252174377)
[2025-02-13 19:58:12,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12,469][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.12150873988866806, acc: 0.9735099077224731)
[2025-02-13 19:58:12,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12,837][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.1232265830039978, acc: 0.9887005686759949)
[2025-02-13 19:58:12,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13,217][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.08304169028997421, acc: 0.9829545617103577)
[2025-02-13 19:58:13,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13,604][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.11935672163963318, acc: 0.9824561476707458)
[2025-02-13 19:58:13,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13,997][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.2238781601190567, acc: 0.9580838084220886)
[2025-02-13 19:58:14,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14,413][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.16448469460010529, acc: 0.9677419066429138)
[2025-02-13 19:58:14,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14,800][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.08762300759553909, acc: 0.9777777791023254)
[2025-02-13 19:58:14,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15,192][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.19989867508411407, acc: 0.9608938694000244)
[2025-02-13 19:58:15,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15,576][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.25159451365470886, acc: 0.95652174949646)
[2025-02-13 19:58:15,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15,956][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.14675776660442352, acc: 0.9731183052062988)
[2025-02-13 19:58:16,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16,337][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.14094693958759308, acc: 0.9508196711540222)
[2025-02-13 19:58:16,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16,742][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.12034719437360764, acc: 0.9659090638160706)
[2025-02-13 19:58:16,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17,139][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.13366714119911194, acc: 0.9621621370315552)
[2025-02-13 19:58:17,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17,522][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.17598506808280945, acc: 0.9523809552192688)
[2025-02-13 19:58:17,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17,874][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.16590352356433868, acc: 0.9696969985961914)
[2025-02-13 19:58:17,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18,237][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.10365685075521469, acc: 0.9786096215248108)
[2025-02-13 19:58:18,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18,625][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.10670546442270279, acc: 0.9700000286102295)
[2025-02-13 19:58:18,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19,013][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.10441052168607712, acc: 0.9712643623352051)
[2025-02-13 19:58:19,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19,398][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.056418873369693756, acc: 0.9837837815284729)
[2025-02-13 19:58:19,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19,789][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.11706886440515518, acc: 0.9743589758872986)
[2025-02-13 19:58:19,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20,225][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.09533847123384476, acc: 0.9901477694511414)
[2025-02-13 19:58:20,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20,604][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.15050749480724335, acc: 0.9661017060279846)
[2025-02-13 19:58:20,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20,966][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.162067711353302, acc: 0.95652174949646)
[2025-02-13 19:58:21,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21,352][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.1558045595884323, acc: 0.9595959782600403)
[2025-02-13 19:58:21,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21,732][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.038891926407814026, acc: 0.989130437374115)
[2025-02-13 19:58:21,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22,110][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.06353588402271271, acc: 0.9894737005233765)
[2025-02-13 19:58:22,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22,498][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.09275511652231216, acc: 0.981566846370697)
[2025-02-13 19:58:22,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22,878][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.09638229757547379, acc: 0.9760765433311462)
[2025-02-13 19:58:23,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23,262][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.06848776340484619, acc: 0.9839572310447693)
[2025-02-13 19:58:23,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23,587][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.15046252310276031, acc: 0.9836065769195557)
[2025-02-13 19:58:23,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23,965][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.09206371009349823, acc: 0.965753436088562)
[2025-02-13 19:58:24,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24,360][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.16418495774269104, acc: 0.9733333587646484)
[2025-02-13 19:58:24,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24,756][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.20144939422607422, acc: 0.9539473652839661)
[2025-02-13 19:58:24,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25,126][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.3384408950805664, acc: 0.9496402740478516)
[2025-02-13 19:58:25,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25,517][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.12420183420181274, acc: 0.985401451587677)
[2025-02-13 19:58:25,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25,911][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.1887495517730713, acc: 0.949999988079071)
[2025-02-13 19:58:26,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26,309][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.1338910460472107, acc: 0.9760000109672546)
[2025-02-13 19:58:26,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26,654][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.10986844450235367, acc: 0.9659863710403442)
[2025-02-13 19:58:26,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27,075][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.08661432564258575, acc: 0.9727891087532043)
[2025-02-13 19:58:27,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27,481][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.19196587800979614, acc: 0.9645389914512634)
[2025-02-13 19:58:27,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27,858][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.18518568575382233, acc: 0.9490445852279663)
[2025-02-13 19:58:27,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28,228][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.1392895132303238, acc: 0.9740259647369385)
[2025-02-13 19:58:28,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28,622][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.08331901580095291, acc: 0.9788732528686523)
[2025-02-13 19:58:28,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29,000][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.13014280796051025, acc: 0.96875)
[2025-02-13 19:58:29,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29,379][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.19930382072925568, acc: 0.9740259647369385)
[2025-02-13 19:58:29,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29,768][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.147284135222435, acc: 0.9545454382896423)
[2025-02-13 19:58:29,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30,163][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.14687363803386688, acc: 0.970059871673584)
[2025-02-13 19:58:30,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30,547][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.10613656789064407, acc: 0.971222996711731)
[2025-02-13 19:58:30,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30,957][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.0868522971868515, acc: 0.9731543660163879)
[2025-02-13 19:58:31,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31,356][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.09921669960021973, acc: 0.9714285731315613)
[2025-02-13 19:58:31,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31,788][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.17643587291240692, acc: 0.95652174949646)
[2025-02-13 19:58:31,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32,158][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.09456134587526321, acc: 0.97826087474823)
[2025-02-13 19:58:32,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32,530][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.10426731407642365, acc: 0.9852941036224365)
[2025-02-13 19:58:32,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32,903][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.046780120581388474, acc: 0.9943820238113403)
[2025-02-13 19:58:33,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33,276][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.057123247534036636, acc: 0.984375)
[2025-02-13 19:58:33,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33,645][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.07768067717552185, acc: 0.9724137783050537)
[2025-02-13 19:58:33,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34,023][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.07721414417028427, acc: 0.9810126423835754)
[2025-02-13 19:58:34,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34,395][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.04667248949408531, acc: 0.9925925731658936)
[2025-02-13 19:58:34,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34,790][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.23616372048854828, acc: 0.9615384340286255)
[2025-02-13 19:58:34,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35,163][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.22547315061092377, acc: 0.9459459185600281)
[2025-02-13 19:58:35,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35,534][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.2796965539455414, acc: 0.9109588861465454)
[2025-02-13 19:58:35,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35,897][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.38221487402915955, acc: 0.9350649118423462)
[2025-02-13 19:58:36,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36,271][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.33292776346206665, acc: 0.9436619877815247)
[2025-02-13 19:58:36,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36,638][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.10331776738166809, acc: 0.9696969985961914)
[2025-02-13 19:58:36,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36,999][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.2499610185623169, acc: 0.9398496150970459)
[2025-02-13 19:58:37,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37,366][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.09039870649576187, acc: 0.9741379022598267)
[2025-02-13 19:58:37,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37,732][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.1690404862165451, acc: 0.9595959782600403)
[2025-02-13 19:58:37,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38,115][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.16821856796741486, acc: 0.9577465057373047)
[2025-02-13 19:58:38,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38,490][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.12219437956809998, acc: 0.9661017060279846)
[2025-02-13 19:58:38,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38,864][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.07769203186035156, acc: 0.9820359349250793)
[2025-02-13 19:58:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39,241][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.18027259409427643, acc: 0.9529411792755127)
[2025-02-13 19:58:39,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39,622][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.18226927518844604, acc: 0.944915235042572)
[2025-02-13 19:58:39,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40,005][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.13926340639591217, acc: 0.9611111283302307)
[2025-02-13 19:58:40,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40,377][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.15376237034797668, acc: 0.9611650705337524)
[2025-02-13 19:58:40,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40,734][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.2090592086315155, acc: 0.9370629191398621)
[2025-02-13 19:58:40,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41,101][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.3212023079395294, acc: 0.9453551769256592)
[2025-02-13 19:58:41,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41,482][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.1360420137643814, acc: 0.9657142758369446)
[2025-02-13 19:58:41,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41,836][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.29788675904273987, acc: 0.940119743347168)
[2025-02-13 19:58:41,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42,229][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.10487992316484451, acc: 0.9726775884628296)
[2025-02-13 19:58:42,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42,638][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.18224695324897766, acc: 0.9664804339408875)
[2025-02-13 19:58:42,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43,047][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.1862419843673706, acc: 0.9589040875434875)
[2025-02-13 19:58:43,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43,464][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.12140851467847824, acc: 0.9683257937431335)
[2025-02-13 19:58:43,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43,877][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.20335045456886292, acc: 0.96875)
[2025-02-13 19:58:44,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44,253][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.24094024300575256, acc: 0.9353233575820923)
[2025-02-13 19:58:44,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44,601][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.09314150363206863, acc: 0.9695122241973877)
[2025-02-13 19:58:44,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44,972][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.14212870597839355, acc: 0.9668508172035217)
[2025-02-13 19:58:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45,348][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.09023762494325638, acc: 0.989130437374115)
[2025-02-13 19:58:45,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45,724][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.11840689927339554, acc: 0.9735449552536011)
[2025-02-13 19:58:45,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46,105][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.12947651743888855, acc: 0.9634146094322205)
[2025-02-13 19:58:46,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46,486][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.05346636474132538, acc: 0.9924812316894531)
[2025-02-13 19:58:46,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46,896][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.14675675332546234, acc: 0.9648241400718689)
[2025-02-13 19:58:47,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47,293][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.15381915867328644, acc: 0.9684210419654846)
[2025-02-13 19:58:47,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47,663][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.05635266751050949, acc: 0.9887005686759949)
[2025-02-13 19:58:47,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48,065][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.08168650418519974, acc: 0.9729729890823364)
[2025-02-13 19:58:48,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48,436][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.12035881727933884, acc: 0.9722222089767456)
[2025-02-13 19:58:48,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48,820][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.10183773189783096, acc: 0.9698492288589478)
[2025-02-13 19:58:48,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49,200][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.022790607064962387, acc: 1.0)
[2025-02-13 19:58:49,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49,579][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.04421519488096237, acc: 0.9921259880065918)
[2025-02-13 19:58:49,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49,936][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.04136745259165764, acc: 0.9936708807945251)
[2025-02-13 19:58:50,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50,290][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.07344945520162582, acc: 0.9851852059364319)
[2025-02-13 19:58:50,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50,671][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.1894429326057434, acc: 0.9677419066429138)
[2025-02-13 19:58:50,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51,054][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.16034820675849915, acc: 0.9610389471054077)
[2025-02-13 19:58:51,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51,461][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.23703432083129883, acc: 0.9340101480484009)
[2025-02-13 19:58:51,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51,845][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.17926187813282013, acc: 0.9503546357154846)
[2025-02-13 19:58:51,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52,257][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.3383837044239044, acc: 0.9515151381492615)
[2025-02-13 19:58:52,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52,662][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.21252146363258362, acc: 0.9328358173370361)
[2025-02-13 19:58:52,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53,054][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.18729735910892487, acc: 0.9496855139732361)
[2025-02-13 19:58:53,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53,434][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.14091458916664124, acc: 0.95652174949646)
[2025-02-13 19:58:53,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53,831][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.05427682027220726, acc: 0.9924242496490479)
[2025-02-13 19:58:53,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54,225][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.1075739935040474, acc: 0.965753436088562)
[2025-02-13 19:58:54,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54,595][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.1607360690832138, acc: 0.9520000219345093)
[2025-02-13 19:58:54,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54,967][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.28440624475479126, acc: 0.9130434989929199)
[2025-02-13 19:58:55,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55,354][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.18167521059513092, acc: 0.940397322177887)
[2025-02-13 19:58:55,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55,793][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.17272593080997467, acc: 0.9527027010917664)
[2025-02-13 19:58:55,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56,174][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.20180034637451172, acc: 0.9545454382896423)
[2025-02-13 19:58:56,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56,564][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.24336549639701843, acc: 0.9311926364898682)
[2025-02-13 19:58:56,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56,919][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.11073622852563858, acc: 0.9716981053352356)
[2025-02-13 19:58:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57,281][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.1243223026394844, acc: 0.9851484894752502)
[2025-02-13 19:58:57,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57,667][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.10111743211746216, acc: 0.9719101190567017)
[2025-02-13 19:58:57,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58,014][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.1350117325782776, acc: 0.9599999785423279)
[2025-02-13 19:58:58,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58,395][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.10125991702079773, acc: 0.984375)
[2025-02-13 19:58:58,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58,770][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.11425802856683731, acc: 0.9558823704719543)
[2025-02-13 19:58:58,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59,135][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.04971885681152344, acc: 0.9895833134651184)
[2025-02-13 19:58:59,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59,515][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.06451111286878586, acc: 0.9849246144294739)
[2025-02-13 19:58:59,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59,878][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.11566528677940369, acc: 0.9800000190734863)
[2025-02-13 19:59:00,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00,264][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.046632181853055954, acc: 0.9836956262588501)
[2025-02-13 19:59:00,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00,638][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.05659094080328941, acc: 0.9852941036224365)
[2025-02-13 19:59:00,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01,017][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.11126247048377991, acc: 0.9732620120048523)
[2025-02-13 19:59:01,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01,404][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.05467674136161804, acc: 0.9788359999656677)
[2025-02-13 19:59:01,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01,807][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.06101316213607788, acc: 0.9929078221321106)
[2025-02-13 19:59:01,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02,220][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.06208045035600662, acc: 0.9837837815284729)
[2025-02-13 19:59:02,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02,605][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.05842076241970062, acc: 0.9894737005233765)
[2025-02-13 19:59:02,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02,996][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.10010629892349243, acc: 0.9696969985961914)
[2025-02-13 19:59:03,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03,386][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.049645014107227325, acc: 0.989130437374115)
[2025-02-13 19:59:03,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03,761][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.09711579233407974, acc: 0.9874213933944702)
[2025-02-13 19:59:03,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04,193][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.02849234640598297, acc: 1.0)
[2025-02-13 19:59:04,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04,583][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.029916470870375633, acc: 0.9888268113136292)
[2025-02-13 19:59:04,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05,051][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.10720707476139069, acc: 0.9826589822769165)
[2025-02-13 19:59:05,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05,444][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.028119929134845734, acc: 0.9942857027053833)
[2025-02-13 19:59:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05,843][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.04958128184080124, acc: 0.9873417615890503)
[2025-02-13 19:59:05,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06,271][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.06532245129346848, acc: 0.9893617033958435)
[2025-02-13 19:59:06,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06,687][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.012519801035523415, acc: 1.0)
[2025-02-13 19:59:06,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07,098][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.20285442471504211, acc: 0.9590163826942444)
[2025-02-13 19:59:07,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07,484][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.1520368754863739, acc: 0.9538461565971375)
[2025-02-13 19:59:07,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07,852][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.08366931974887848, acc: 0.9677419066429138)
[2025-02-13 19:59:07,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08,220][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.0629865750670433, acc: 0.98591548204422)
[2025-02-13 19:59:08,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08,625][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.2855895161628723, acc: 0.9504950642585754)
[2025-02-13 19:59:08,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09,021][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.30540820956230164, acc: 0.9237288236618042)
[2025-02-13 19:59:09,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09,406][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.05694537237286568, acc: 0.9905660152435303)
[2025-02-13 19:59:09,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09,811][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.07666243612766266, acc: 0.9826086759567261)
[2025-02-13 19:59:09,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10,194][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.03376743942499161, acc: 1.0)
[2025-02-13 19:59:10,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10,571][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.07050834596157074, acc: 0.9851852059364319)
[2025-02-13 19:59:10,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10,967][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.11299419403076172, acc: 0.9789473414421082)
[2025-02-13 19:59:11,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11,368][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.26461654901504517, acc: 0.9455782175064087)
[2025-02-13 19:59:11,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11,728][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.06374730914831161, acc: 0.9900000095367432)
[2025-02-13 19:59:11,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12,084][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.11547042429447174, acc: 0.970588207244873)
[2025-02-13 19:59:12,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12,483][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.1259172409772873, acc: 0.9770992398262024)
[2025-02-13 19:59:12,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12,901][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.23857633769512177, acc: 0.9370078444480896)
[2025-02-13 19:59:13,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13,266][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.14714708924293518, acc: 0.9626865386962891)
[2025-02-13 19:59:13,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13,705][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.09009329974651337, acc: 0.9714285731315613)
[2025-02-13 19:59:13,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14,200][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.08377783000469208, acc: 0.9834710955619812)
[2025-02-13 19:59:14,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14,620][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.1722298413515091, acc: 0.9599999785423279)
[2025-02-13 19:59:14,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15,015][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.23192250728607178, acc: 0.9729729890823364)
[2025-02-13 19:59:15,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15,366][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.07086959481239319, acc: 0.984375)
[2025-02-13 19:59:15,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15,765][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.09802613407373428, acc: 0.9710144996643066)
[2025-02-13 19:59:15,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16,154][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.1647222340106964, acc: 0.9679999947547913)
[2025-02-13 19:59:16,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16,587][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.05012141540646553, acc: 0.9910714030265808)
[2025-02-13 19:59:16,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16,993][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.01683632843196392, acc: 1.0)
[2025-02-13 19:59:17,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17,398][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.0465535931289196, acc: 0.9909909963607788)
[2025-02-13 19:59:17,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17,809][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.07795840501785278, acc: 0.9909909963607788)
[2025-02-13 19:59:17,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18,214][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.08165495097637177, acc: 0.9772727489471436)
[2025-02-13 19:59:18,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18,551][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.12720881402492523, acc: 0.9580838084220886)
[2025-02-13 19:59:18,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18,891][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.11993928998708725, acc: 0.9637681245803833)
[2025-02-13 19:59:19,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19,254][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.21258413791656494, acc: 0.9605262875556946)
[2025-02-13 19:59:19,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19,640][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.08500918000936508, acc: 0.976331353187561)
[2025-02-13 19:59:19,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20,016][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.06059842184185982, acc: 0.9748427867889404)
[2025-02-13 19:59:20,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20,378][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.10704243183135986, acc: 0.956250011920929)
[2025-02-13 19:59:20,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20,765][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.081606425344944, acc: 0.9702380895614624)
[2025-02-13 19:59:20,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21,162][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.18021425604820251, acc: 0.9726775884628296)
[2025-02-13 19:59:21,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21,522][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.11534778773784637, acc: 0.9813664555549622)
[2025-02-13 19:59:21,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21,896][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.07832926511764526, acc: 0.9876543283462524)
[2025-02-13 19:59:22,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22,282][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.0510198213160038, acc: 0.9927007555961609)
[2025-02-13 19:59:22,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22,676][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.09432599693536758, acc: 0.9659863710403442)
[2025-02-13 19:59:22,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23,060][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.1537976711988449, acc: 0.9526315927505493)
[2025-02-13 19:59:23,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23,435][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.12801600992679596, acc: 0.9507042169570923)
[2025-02-13 19:59:23,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23,809][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.02502267248928547, acc: 1.0)
[2025-02-13 19:59:23,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24,174][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.02425687201321125, acc: 0.993630588054657)
[2025-02-13 19:59:24,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24,532][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.060398414731025696, acc: 0.9866666793823242)
[2025-02-13 19:59:24,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24,920][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.10818956792354584, acc: 0.9874213933944702)
[2025-02-13 19:59:25,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25,274][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.10614565759897232, acc: 0.9766082167625427)
[2025-02-13 19:59:25,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25,677][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.1636631339788437, acc: 0.9675675630569458)
[2025-02-13 19:59:25,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26,096][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.20432011783123016, acc: 0.9460784196853638)
[2025-02-13 19:59:26,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26,471][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.1765405237674713, acc: 0.949367105960846)
[2025-02-13 19:59:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26,879][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.12615461647510529, acc: 0.9626168012619019)
[2025-02-13 19:59:27,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27,311][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.0532616525888443, acc: 0.9950248599052429)
[2025-02-13 19:59:27,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27,755][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.16683736443519592, acc: 0.9599999785423279)
[2025-02-13 19:59:27,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28,159][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.16511237621307373, acc: 0.9776119589805603)
[2025-02-13 19:59:28,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28,572][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.042718105018138885, acc: 0.9849624037742615)
[2025-02-13 19:59:28,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28,974][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.20745839178562164, acc: 0.9441340565681458)
[2025-02-13 19:59:29,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29,418][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.08870693296194077, acc: 0.9807692170143127)
[2025-02-13 19:59:29,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29,832][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.1879853457212448, acc: 0.9594594836235046)
[2025-02-13 19:59:29,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30,252][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.24688118696212769, acc: 0.9512194991111755)
[2025-02-13 19:59:30,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30,669][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.12741407752037048, acc: 0.9556962251663208)
[2025-02-13 19:59:30,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31,029][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.05897306650876999, acc: 0.987500011920929)
[2025-02-13 19:59:31,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31,387][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.12349694222211838, acc: 0.9655172228813171)
[2025-02-13 19:59:31,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31,756][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.05117994174361229, acc: 1.0)
[2025-02-13 19:59:31,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32,137][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.1640111654996872, acc: 0.9523809552192688)
[2025-02-13 19:59:32,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32,526][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.07163143903017044, acc: 0.9763779640197754)
[2025-02-13 19:59:32,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32,900][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.05163750424981117, acc: 0.9898989796638489)
[2025-02-13 19:59:33,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33,275][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.12569661438465118, acc: 0.9669421315193176)
[2025-02-13 19:59:33,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33,673][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.12650887668132782, acc: 0.9555555582046509)
[2025-02-13 19:59:33,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34,093][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.22994862496852875, acc: 0.9548386931419373)
[2025-02-13 19:59:34,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34,469][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.10529177635908127, acc: 0.9716312289237976)
[2025-02-13 19:59:34,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34,854][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.059914421290159225, acc: 0.9927536249160767)
[2025-02-13 19:59:34,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35,218][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.18393707275390625, acc: 0.961240291595459)
[2025-02-13 19:59:35,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35,593][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.29636380076408386, acc: 0.9090909361839294)
[2025-02-13 19:59:35,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35,931][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.10568393766880035, acc: 0.9691358208656311)
[2025-02-13 19:59:36,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36,319][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.13071928918361664, acc: 0.949999988079071)
[2025-02-13 19:59:36,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36,690][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.07011725008487701, acc: 0.9746192693710327)
[2025-02-13 19:59:36,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37,058][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.0505569726228714, acc: 0.9866666793823242)
[2025-02-13 19:59:37,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37,422][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.16638675332069397, acc: 0.9735449552536011)
[2025-02-13 19:59:37,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37,784][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.16978003084659576, acc: 0.9586206674575806)
[2025-02-13 19:59:37,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38,161][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.08441268652677536, acc: 0.9736841917037964)
[2025-02-13 19:59:38,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38,526][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.1925623118877411, acc: 0.9647058844566345)
[2025-02-13 19:59:38,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38,969][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.06826366484165192, acc: 0.9887005686759949)
[2025-02-13 19:59:39,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39,387][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.08927903324365616, acc: 0.9871794581413269)
[2025-02-13 19:59:39,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39,796][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.18345126509666443, acc: 0.9503546357154846)
[2025-02-13 19:59:39,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40,213][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.20358452200889587, acc: 0.9424460530281067)
[2025-02-13 19:59:40,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40,667][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.3080121576786041, acc: 0.9542483687400818)
[2025-02-13 19:59:40,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41,038][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.31064364314079285, acc: 0.9246575236320496)
[2025-02-13 19:59:41,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41,470][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.2063566893339157, acc: 0.948051929473877)
[2025-02-13 19:59:41,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41,885][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.10105087608098984, acc: 0.9826589822769165)
[2025-02-13 19:59:42,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42,287][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.1188458800315857, acc: 0.9683544039726257)
[2025-02-13 19:59:42,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42,695][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.0943121463060379, acc: 0.9756097793579102)
[2025-02-13 19:59:42,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43,109][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.1649366021156311, acc: 0.9545454382896423)
[2025-02-13 19:59:43,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43,522][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.0918179303407669, acc: 0.9886363744735718)
[2025-02-13 19:59:43,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43,953][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.08029294013977051, acc: 0.9707602262496948)
[2025-02-13 19:59:44,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44,378][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.10489118844270706, acc: 0.9590643048286438)
[2025-02-13 19:59:44,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44,785][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.12062577158212662, acc: 0.9675324559211731)
[2025-02-13 19:59:44,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45,226][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.14659586548805237, acc: 0.9615384340286255)
[2025-02-13 19:59:45,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45,637][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.09378454089164734, acc: 0.971222996711731)
[2025-02-13 19:59:45,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46,039][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.059723082929849625, acc: 0.9879518151283264)
[2025-02-13 19:59:46,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46,443][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.22813457250595093, acc: 0.9523809552192688)
[2025-02-13 19:59:46,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46,844][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.028760656714439392, acc: 1.0)
[2025-02-13 19:59:46,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47,291][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.12461009621620178, acc: 0.970588207244873)
[2025-02-13 19:59:47,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47,703][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.11536470055580139, acc: 0.9707602262496948)
[2025-02-13 19:59:47,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48,099][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.11046497523784637, acc: 0.9543147087097168)
[2025-02-13 19:59:48,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48,485][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.1440420299768448, acc: 0.9717513918876648)
[2025-02-13 19:59:48,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48,917][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.09753935039043427, acc: 0.9803921580314636)
[2025-02-13 19:59:49,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49,292][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.051820307970047, acc: 0.9801980257034302)
[2025-02-13 19:59:49,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49,648][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.07930932939052582, acc: 0.970059871673584)
[2025-02-13 19:59:49,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50,041][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.03467830643057823, acc: 0.9941520690917969)
[2025-02-13 19:59:50,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50,459][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.0697697252035141, acc: 0.9823529124259949)
[2025-02-13 19:59:50,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50,862][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.0487336739897728, acc: 0.9824561476707458)
[2025-02-13 19:59:51,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51,277][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.07463152706623077, acc: 0.9726027250289917)
[2025-02-13 19:59:51,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51,721][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.07338505983352661, acc: 0.9777777791023254)
[2025-02-13 19:59:51,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52,119][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.03384228050708771, acc: 0.9863013625144958)
[2025-02-13 19:59:52,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52,572][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.15908610820770264, acc: 0.9640718698501587)
[2025-02-13 19:59:52,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52,962][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.10302041471004486, acc: 0.9707602262496948)
[2025-02-13 19:59:53,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53,355][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.05315374583005905, acc: 0.9878048896789551)
[2025-02-13 19:59:53,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53,748][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.07041580229997635, acc: 0.9865771532058716)
[2025-02-13 19:59:53,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54,145][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.166956827044487, acc: 0.9476743936538696)
[2025-02-13 19:59:54,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54,534][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.07640638202428818, acc: 0.9842932224273682)
[2025-02-13 19:59:54,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54,928][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.15645158290863037, acc: 0.9790576100349426)
[2025-02-13 19:59:55,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55,319][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.05154748633503914, acc: 0.9878048896789551)
[2025-02-13 19:59:55,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55,700][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.05875829979777336, acc: 0.9942196607589722)
[2025-02-13 19:59:55,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56,108][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.04602210223674774, acc: 0.9890710115432739)
[2025-02-13 19:59:56,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56,484][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.09250064939260483, acc: 0.9642857313156128)
[2025-02-13 19:59:56,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56,855][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.20645354688167572, acc: 0.9583333134651184)
[2025-02-13 19:59:56,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57,227][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.20097622275352478, acc: 0.9378530979156494)
[2025-02-13 19:59:57,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57,605][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.1755618005990982, acc: 0.9536423683166504)
[2025-02-13 19:59:57,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57,991][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.13050296902656555, acc: 0.9462365508079529)
[2025-02-13 19:59:58,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58,363][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.13597063720226288, acc: 0.977142870426178)
[2025-02-13 19:59:58,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58,737][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.17667384445667267, acc: 0.9562841653823853)
[2025-02-13 19:59:58,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59,121][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.1841857135295868, acc: 0.9682539701461792)
[2025-02-13 19:59:59,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59,527][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.24456177651882172, acc: 0.9360465407371521)
[2025-02-13 19:59:59,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59,937][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.21557432413101196, acc: 0.9571428298950195)
[2025-02-13 20:00:00,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00,357][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.21119341254234314, acc: 0.956250011920929)
[2025-02-13 20:00:00,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00,780][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.3549955487251282, acc: 0.930232584476471)
[2025-02-13 20:00:00,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01,203][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.19919021427631378, acc: 0.9518072009086609)
[2025-02-13 20:00:01,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01,635][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.1389458328485489, acc: 0.9649122953414917)
[2025-02-13 20:00:01,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02,022][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.15439629554748535, acc: 0.9520547986030579)
[2025-02-13 20:00:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02,391][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.07843563705682755, acc: 0.9722222089767456)
[2025-02-13 20:00:02,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02,752][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.18684959411621094, acc: 0.9515151381492615)
[2025-02-13 20:00:02,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03,152][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.08737307786941528, acc: 0.9861111044883728)
[2025-02-13 20:00:03,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03,522][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.056423790752887726, acc: 0.9856114983558655)
[2025-02-13 20:00:03,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03,908][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.06922084838151932, acc: 0.9868420958518982)
[2025-02-13 20:00:04,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04,267][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.23838268220424652, acc: 0.9386503100395203)
[2025-02-13 20:00:04,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04,637][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.12467046082019806, acc: 0.96875)
[2025-02-13 20:00:04,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05,009][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.16478337347507477, acc: 0.9554139971733093)
[2025-02-13 20:00:05,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05,377][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.08178317546844482, acc: 0.9693251252174377)
[2025-02-13 20:00:05,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05,753][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.08204497396945953, acc: 0.9756097793579102)
[2025-02-13 20:00:05,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06,135][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.07826019823551178, acc: 0.9838709831237793)
[2025-02-13 20:00:06,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06,492][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.12227673083543777, acc: 0.9605262875556946)
[2025-02-13 20:00:06,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06,855][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.09927346557378769, acc: 0.9820359349250793)
[2025-02-13 20:00:06,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07,220][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.034121546894311905, acc: 1.0)
[2025-02-13 20:00:07,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07,598][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.12019719928503036, acc: 0.9831932783126831)
[2025-02-13 20:00:07,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07,949][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.10511129349470139, acc: 0.9850746393203735)
[2025-02-13 20:00:08,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08,319][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.1709907054901123, acc: 0.9530201554298401)
[2025-02-13 20:00:08,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08,672][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.04642431437969208, acc: 0.9900990128517151)
[2025-02-13 20:00:08,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09,044][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.17539237439632416, acc: 0.9463087320327759)
[2025-02-13 20:00:09,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09,421][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.19042982161045074, acc: 0.9608938694000244)
[2025-02-13 20:00:09,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09,800][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.07725954800844193, acc: 0.9821428656578064)
[2025-02-13 20:00:09,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10,175][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.19734172523021698, acc: 0.9607843160629272)
[2025-02-13 20:00:10,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10,560][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.029248762875795364, acc: 1.0)
[2025-02-13 20:00:10,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10,922][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.09759128838777542, acc: 0.9863013625144958)
[2025-02-13 20:00:11,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11,292][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.05854886397719383, acc: 0.9763779640197754)
[2025-02-13 20:00:11,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11,677][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.11495912075042725, acc: 0.9832402467727661)
[2025-02-13 20:00:11,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12,071][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.1917399913072586, acc: 0.9431818127632141)
[2025-02-13 20:00:12,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12,483][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.11891096830368042, acc: 0.9679144620895386)
[2025-02-13 20:00:12,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12,829][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.10368765890598297, acc: 0.9756097793579102)
[2025-02-13 20:00:12,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13,181][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.16499483585357666, acc: 0.9642857313156128)
[2025-02-13 20:00:13,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13,541][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.07693804800510406, acc: 0.9870129823684692)
[2025-02-13 20:00:13,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13,919][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.0351727120578289, acc: 0.9942196607589722)
[2025-02-13 20:00:14,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14,321][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.07885875552892685, acc: 0.9774011373519897)
[2025-02-13 20:00:14,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14,709][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.03561902791261673, acc: 1.0)
[2025-02-13 20:00:14,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15,082][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.08259890973567963, acc: 0.9779005646705627)
[2025-02-13 20:00:15,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15,461][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.06411612033843994, acc: 0.9892473220825195)
[2025-02-13 20:00:15,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15,830][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.08647160977125168, acc: 0.9833333492279053)
[2025-02-13 20:00:15,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16,190][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.07841219007968903, acc: 0.9736841917037964)
[2025-02-13 20:00:16,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16,578][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.10884024202823639, acc: 0.981249988079071)
[2025-02-13 20:00:16,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16,947][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.07843516021966934, acc: 0.9806451797485352)
[2025-02-13 20:00:17,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17,380][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.1490647792816162, acc: 0.9578947424888611)
[2025-02-13 20:00:17,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17,791][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.08630192279815674, acc: 0.9780219793319702)
[2025-02-13 20:00:17,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18,184][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.11494959890842438, acc: 0.9696969985961914)
[2025-02-13 20:00:18,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18,563][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.07998230308294296, acc: 0.9857142567634583)
[2025-02-13 20:00:18,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18,950][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.09087861329317093, acc: 0.9767441749572754)
[2025-02-13 20:00:19,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19,324][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.1571643054485321, acc: 0.9610389471054077)
[2025-02-13 20:00:19,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19,721][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.09526696056127548, acc: 0.9801324605941772)
[2025-02-13 20:00:19,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20,070][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.04070524871349335, acc: 0.9870129823684692)
[2025-02-13 20:00:20,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20,461][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.0947936624288559, acc: 0.9709302186965942)
[2025-02-13 20:00:20,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20,847][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.04284149408340454, acc: 0.9941520690917969)
[2025-02-13 20:00:20,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21,229][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.030207926407456398, acc: 0.9925373196601868)
[2025-02-13 20:00:21,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21,595][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.06841865926980972, acc: 0.9941520690917969)
[2025-02-13 20:00:21,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21,989][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.05317937955260277, acc: 0.9774011373519897)
[2025-02-13 20:00:22,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22,366][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.29080939292907715, acc: 0.9419354796409607)
[2025-02-13 20:00:22,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22,764][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.14300423860549927, acc: 0.9611111283302307)
[2025-02-13 20:00:22,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23,177][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.19754959642887115, acc: 0.9640287756919861)
[2025-02-13 20:00:23,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23,626][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.008616318926215172, acc: 1.0)
[2025-02-13 20:00:23,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24,073][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.11950210481882095, acc: 0.9802631735801697)
[2025-02-13 20:00:24,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24,496][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.18262454867362976, acc: 0.9354838728904724)
[2025-02-13 20:00:24,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24,883][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.11634403467178345, acc: 0.9779411554336548)
[2025-02-13 20:00:25,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25,307][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.0942976176738739, acc: 0.9702380895614624)
[2025-02-13 20:00:25,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25,728][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.18408401310443878, acc: 0.9580838084220886)
[2025-02-13 20:00:25,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26,176][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.12626294791698456, acc: 0.9825581312179565)
[2025-02-13 20:00:26,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26,574][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.034665267914533615, acc: 0.9922480583190918)
[2025-02-13 20:00:26,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26,957][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.1166125014424324, acc: 0.960629940032959)
[2025-02-13 20:00:27,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27,349][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.06274419277906418, acc: 0.9918699264526367)
[2025-02-13 20:00:27,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27,728][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.09087243676185608, acc: 0.9743589758872986)
[2025-02-13 20:00:27,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28,150][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.1386789232492447, acc: 0.9695122241973877)
[2025-02-13 20:00:28,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28,568][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.08326040208339691, acc: 0.9710982441902161)
[2025-02-13 20:00:28,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29,008][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.0780249759554863, acc: 0.9775280952453613)
[2025-02-13 20:00:29,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29,409][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.18137405812740326, acc: 0.9593023061752319)
[2025-02-13 20:00:29,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29,816][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.09200902283191681, acc: 0.9754601120948792)
[2025-02-13 20:00:29,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30,199][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.10282555222511292, acc: 0.9745222926139832)
[2025-02-13 20:00:30,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30,603][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.11811923235654831, acc: 0.9801324605941772)
[2025-02-13 20:00:30,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31,026][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.0293340552598238, acc: 1.0)
[2025-02-13 20:00:31,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31,398][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.1367557793855667, acc: 0.970588207244873)
[2025-02-13 20:00:31,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31,781][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.0577118881046772, acc: 0.9759036302566528)
[2025-02-13 20:00:31,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32,179][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.04657962545752525, acc: 1.0)
[2025-02-13 20:00:32,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32,565][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.07796822488307953, acc: 0.9805194735527039)
[2025-02-13 20:00:32,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32,927][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.09562854468822479, acc: 0.9694656729698181)
[2025-02-13 20:00:33,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33,276][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.12918628752231598, acc: 0.9558823704719543)
[2025-02-13 20:00:33,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33,654][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.12438786774873734, acc: 0.9695122241973877)
[2025-02-13 20:00:33,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34,025][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.12778645753860474, acc: 0.9655172228813171)
[2025-02-13 20:00:34,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34,409][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.1584784835577011, acc: 0.9604519605636597)
[2025-02-13 20:00:34,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34,792][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.12900394201278687, acc: 0.97826087474823)
[2025-02-13 20:00:34,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35,167][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.15665888786315918, acc: 0.9754902124404907)
[2025-02-13 20:00:35,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35,501][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.10916373878717422, acc: 0.9776119589805603)
[2025-02-13 20:00:35,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35,881][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.1732879877090454, acc: 0.938144326210022)
[2025-02-13 20:00:36,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36,257][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.2198476642370224, acc: 0.9552238583564758)
[2025-02-13 20:00:36,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36,631][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.14696578681468964, acc: 0.9560439586639404)
[2025-02-13 20:00:36,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37,006][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.18651871383190155, acc: 0.9415204524993896)
[2025-02-13 20:00:37,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37,368][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.160088911652565, acc: 0.9534883499145508)
[2025-02-13 20:00:37,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37,745][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.36550918221473694, acc: 0.9085366129875183)
[2025-02-13 20:00:37,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38,122][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.27663499116897583, acc: 0.9304812550544739)
[2025-02-13 20:00:38,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38,489][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.11346092820167542, acc: 0.9631901979446411)
[2025-02-13 20:00:38,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38,897][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.06787242740392685, acc: 0.9840425252914429)
[2025-02-13 20:00:39,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39,345][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.18112412095069885, acc: 0.969924807548523)
[2025-02-13 20:00:39,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39,743][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.1270754039287567, acc: 0.954285740852356)
[2025-02-13 20:00:39,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40,163][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.25626635551452637, acc: 0.9485294222831726)
[2025-02-13 20:00:40,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40,567][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.19384030997753143, acc: 0.9573459625244141)
[2025-02-13 20:00:40,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40,953][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.2654358744621277, acc: 0.9179487228393555)
[2025-02-13 20:00:41,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:41,324][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.18755005300045013, acc: 0.9450549483299255)
[2025-02-13 20:00:41,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:41,708][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.17574156820774078, acc: 0.9603960514068604)
[2025-02-13 20:00:41,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42,065][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.23081167042255402, acc: 0.9627659320831299)
[2025-02-13 20:00:42,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42,439][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.09545871615409851, acc: 0.9735099077224731)
[2025-02-13 20:00:42,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42,817][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.2835291922092438, acc: 0.9382022619247437)
[2025-02-13 20:00:42,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43,185][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.08464789390563965, acc: 0.9793814420700073)
[2025-02-13 20:00:43,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43,563][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.10881087183952332, acc: 0.9783783555030823)
[2025-02-13 20:00:43,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43,969][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.13481327891349792, acc: 0.9671361446380615)
[2025-02-13 20:00:44,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44,339][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.25539588928222656, acc: 0.9685534834861755)
[2025-02-13 20:00:44,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44,717][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.28554919362068176, acc: 0.9485714435577393)
[2025-02-13 20:00:44,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45,067][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.13087086379528046, acc: 0.969924807548523)
[2025-02-13 20:00:45,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45,466][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.0462275967001915, acc: 1.0)
[2025-02-13 20:00:45,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45,854][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.0977591797709465, acc: 0.9722222089767456)
[2025-02-13 20:00:45,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46,246][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.05028602480888367, acc: 0.9888268113136292)
[2025-02-13 20:00:46,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46,627][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.05161590874195099, acc: 0.9932885766029358)
[2025-02-13 20:00:46,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47,008][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.3164246082305908, acc: 0.9265536665916443)
[2025-02-13 20:00:47,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47,405][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.21717068552970886, acc: 0.9386503100395203)
[2025-02-13 20:00:47,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47,810][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.20964349806308746, acc: 0.9395604133605957)
[2025-02-13 20:00:47,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48,200][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.37607240676879883, acc: 0.9182389974594116)
[2025-02-13 20:00:48,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48,605][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.11051010340452194, acc: 0.9826589822769165)
[2025-02-13 20:00:48,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48,987][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.127028688788414, acc: 0.971222996711731)
[2025-02-13 20:00:49,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49,384][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.15802983939647675, acc: 0.9660193920135498)
[2025-02-13 20:00:49,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49,764][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.12809260189533234, acc: 0.9591836929321289)
[2025-02-13 20:00:49,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50,126][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.16391073167324066, acc: 0.9476743936538696)
[2025-02-13 20:00:50,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50,512][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.08616580814123154, acc: 0.9647058844566345)
[2025-02-13 20:00:50,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50,888][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.15014034509658813, acc: 0.9640718698501587)
[2025-02-13 20:00:51,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51,301][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.11102636903524399, acc: 0.9846153855323792)
[2025-02-13 20:00:51,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51,677][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.08744170516729355, acc: 0.9852941036224365)
[2025-02-13 20:00:51,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52,066][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.0619642473757267, acc: 0.9818181991577148)
[2025-02-13 20:00:52,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52,455][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.06939934194087982, acc: 0.9874213933944702)
[2025-02-13 20:00:52,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52,856][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.05305900424718857, acc: 0.9930555820465088)
[2025-02-13 20:00:52,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53,222][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.193182572722435, acc: 0.9292035102844238)
[2025-02-13 20:00:53,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53,599][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.14092904329299927, acc: 0.9645389914512634)
[2025-02-13 20:00:53,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53,971][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.11795918643474579, acc: 0.9681528806686401)
[2025-02-13 20:00:54,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54,314][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.3929567337036133, acc: 0.9154929518699646)
[2025-02-13 20:00:54,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54,694][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.11282409727573395, acc: 0.9691358208656311)
[2025-02-13 20:00:54,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55,062][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.18458466231822968, acc: 0.961240291595459)
[2025-02-13 20:00:55,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55,419][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.11911024153232574, acc: 0.9743589758872986)
[2025-02-13 20:00:55,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55,774][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.080172598361969, acc: 0.9811320900917053)
[2025-02-13 20:00:55,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56,154][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.17790727317333221, acc: 0.9624999761581421)
[2025-02-13 20:00:56,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56,556][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.23937523365020752, acc: 0.9397590160369873)
[2025-02-13 20:00:56,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56,970][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.1470567286014557, acc: 0.9503546357154846)
[2025-02-13 20:00:57,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57,442][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.09754451364278793, acc: 0.9735099077224731)
[2025-02-13 20:00:57,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57,890][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.06556900590658188, acc: 0.9831932783126831)
[2025-02-13 20:00:58,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58,296][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.05953620374202728, acc: 0.9885714054107666)
[2025-02-13 20:00:58,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58,682][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.16816338896751404, acc: 0.9503105878829956)
[2025-02-13 20:00:58,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59,063][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.05085262283682823, acc: 0.9935064911842346)
[2025-02-13 20:00:59,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59,503][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.02883678488433361, acc: 1.0)
[2025-02-13 20:00:59,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59,977][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.09139279276132584, acc: 0.9738562107086182)
[2025-02-13 20:01:00,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00,385][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.03382661193609238, acc: 0.9884393215179443)
[2025-02-13 20:01:00,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00,828][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.07603728771209717, acc: 0.9888268113136292)
[2025-02-13 20:01:00,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01,244][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.14076454937458038, acc: 0.9701492786407471)
[2025-02-13 20:01:01,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01,642][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.09901716560125351, acc: 0.9716312289237976)
[2025-02-13 20:01:01,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02,018][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.2887929081916809, acc: 0.9469026327133179)
[2025-02-13 20:01:02,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02,403][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.07059326767921448, acc: 0.970588207244873)
[2025-02-13 20:01:02,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02,818][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.025879772379994392, acc: 1.0)
[2025-02-13 20:01:02,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03,213][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.04155958443880081, acc: 0.9876543283462524)
[2025-02-13 20:01:03,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03,614][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.026121389120817184, acc: 0.9885057210922241)
[2025-02-13 20:01:03,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04,018][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.12347745895385742, acc: 0.9627329111099243)
[2025-02-13 20:01:04,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04,392][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.119589664041996, acc: 0.9627659320831299)
[2025-02-13 20:01:04,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04,805][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.036146316677331924, acc: 1.0)
[2025-02-13 20:01:04,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05,197][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.2266089767217636, acc: 0.9350649118423462)
[2025-02-13 20:01:05,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05,579][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.335927277803421, acc: 0.8918918967247009)
[2025-02-13 20:01:05,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05,941][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.503669023513794, acc: 0.9067796468734741)
[2025-02-13 20:01:06,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06,406][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.09652832895517349, acc: 0.9734042286872864)
[2025-02-13 20:01:06,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06,827][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.11798117309808731, acc: 0.984000027179718)
[2025-02-13 20:01:06,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07,217][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.1251702755689621, acc: 0.9724770784378052)
[2025-02-13 20:01:07,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07,640][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.04905858635902405, acc: 0.9855072498321533)
[2025-02-13 20:01:07,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08,067][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.03778393566608429, acc: 1.0)
[2025-02-13 20:01:08,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08,508][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.11999277025461197, acc: 0.9664804339408875)
[2025-02-13 20:01:08,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08,961][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.10574852675199509, acc: 0.9900990128517151)
[2025-02-13 20:01:09,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09,450][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.05844084173440933, acc: 0.9813664555549622)
[2025-02-13 20:01:09,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09,855][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.2785063683986664, acc: 0.9378882050514221)
[2025-02-13 20:01:09,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10,263][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.11441703140735626, acc: 0.969924807548523)
[2025-02-13 20:01:10,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10,687][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.3302786648273468, acc: 0.9085714221000671)
[2025-02-13 20:01:10,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11,158][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.17647238075733185, acc: 0.9681528806686401)
[2025-02-13 20:01:11,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11,547][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.09668492525815964, acc: 0.9736841917037964)
[2025-02-13 20:01:11,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12,003][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.2495863437652588, acc: 0.9166666865348816)
[2025-02-13 20:01:12,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12,446][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.0880928784608841, acc: 0.977142870426178)
[2025-02-13 20:01:12,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12,867][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.09016198664903641, acc: 0.9759036302566528)
[2025-02-13 20:01:13,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13,248][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.09078522771596909, acc: 0.976047933101654)
[2025-02-13 20:01:13,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13,636][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.09275292605161667, acc: 0.9791666865348816)
[2025-02-13 20:01:13,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14,010][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.07390806823968887, acc: 0.9923664331436157)
[2025-02-13 20:01:14,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14,419][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.11218084394931793, acc: 0.9767441749572754)
[2025-02-13 20:01:14,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14,800][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.08981160074472427, acc: 0.9740932583808899)
[2025-02-13 20:01:14,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15,203][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.10100088268518448, acc: 0.9789473414421082)
[2025-02-13 20:01:15,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15,594][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.09501403570175171, acc: 0.9763033390045166)
[2025-02-13 20:01:15,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15,973][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.12488172203302383, acc: 0.9615384340286255)
[2025-02-13 20:01:16,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16,348][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.10972139239311218, acc: 0.9742268323898315)
[2025-02-13 20:01:16,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16,705][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.12160000205039978, acc: 0.9638554453849792)
[2025-02-13 20:01:16,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17,107][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.04402930662035942, acc: 0.9941176176071167)
[2025-02-13 20:01:17,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17,505][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.16672298312187195, acc: 0.9513513445854187)
[2025-02-13 20:01:17,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17,948][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.1576748639345169, acc: 0.950276255607605)
[2025-02-13 20:01:18,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18,351][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.19323743879795074, acc: 0.9541984796524048)
[2025-02-13 20:01:18,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18,777][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.0721914991736412, acc: 0.9934640526771545)
[2025-02-13 20:01:18,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19,195][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.09891712665557861, acc: 0.9765625)
[2025-02-13 20:01:19,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19,604][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.11599474400281906, acc: 0.9722222089767456)
[2025-02-13 20:01:19,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20,054][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.2556718587875366, acc: 0.9383561611175537)
[2025-02-13 20:01:20,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20,466][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.0573049858212471, acc: 0.981249988079071)
[2025-02-13 20:01:20,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20,862][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.03865085169672966, acc: 0.9878048896789551)
[2025-02-13 20:01:21,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21,284][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.05736871063709259, acc: 0.9813664555549622)
[2025-02-13 20:01:21,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21,656][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.11043114215135574, acc: 0.9716312289237976)
[2025-02-13 20:01:21,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22,073][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.31055524945259094, acc: 0.9270073175430298)
[2025-02-13 20:01:22,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22,482][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.2518095374107361, acc: 0.936170220375061)
[2025-02-13 20:01:22,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22,858][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.11965712904930115, acc: 0.9557521939277649)
[2025-02-13 20:01:23,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23,261][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.3901403844356537, acc: 0.9182389974594116)
[2025-02-13 20:01:23,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23,668][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.177742138504982, acc: 0.9493087530136108)
[2025-02-13 20:01:23,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24,068][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.18495865166187286, acc: 0.9689922332763672)
[2025-02-13 20:01:24,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24,492][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.15868090093135834, acc: 0.9638554453849792)
[2025-02-13 20:01:24,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24,896][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.04981055110692978, acc: 0.9775784611701965)
[2025-02-13 20:01:25,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25,349][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.11981664597988129, acc: 0.9746192693710327)
[2025-02-13 20:01:25,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25,752][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.05822398141026497, acc: 0.9840425252914429)
[2025-02-13 20:01:25,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26,138][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.06266874819993973, acc: 0.9811320900917053)
[2025-02-13 20:01:26,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26,555][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.16198861598968506, acc: 0.9354838728904724)
[2025-02-13 20:01:26,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26,957][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.09094854444265366, acc: 0.9784946441650391)
[2025-02-13 20:01:27,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27,350][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.09571672230958939, acc: 0.9829545617103577)
[2025-02-13 20:01:27,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27,784][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.17470017075538635, acc: 0.9631901979446411)
[2025-02-13 20:01:27,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28,251][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.17323021590709686, acc: 0.9523809552192688)
[2025-02-13 20:01:28,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28,680][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.12232107669115067, acc: 0.9627329111099243)
[2025-02-13 20:01:28,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29,071][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.0864076241850853, acc: 0.9880239367485046)
[2025-02-13 20:01:29,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29,458][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.09006261825561523, acc: 0.9795918464660645)
[2025-02-13 20:01:29,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29,830][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.32786861062049866, acc: 0.9398496150970459)
[2025-02-13 20:01:29,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30,236][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.12399955093860626, acc: 0.9788359999656677)
[2025-02-13 20:01:30,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30,627][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.2237999141216278, acc: 0.9301075339317322)
[2025-02-13 20:01:30,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31,021][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.2053062617778778, acc: 0.9562841653823853)
[2025-02-13 20:01:31,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31,439][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.05529990792274475, acc: 0.9833333492279053)
[2025-02-13 20:01:31,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31,831][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.15022951364517212, acc: 0.9583333134651184)
[2025-02-13 20:01:31,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32,242][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.10787638276815414, acc: 0.9649122953414917)
[2025-02-13 20:01:32,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32,633][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.2274176925420761, acc: 0.9487179517745972)
[2025-02-13 20:01:32,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33,009][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.3401614725589752, acc: 0.9097222089767456)
[2025-02-13 20:01:33,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33,397][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.40277358889579773, acc: 0.9047619104385376)
[2025-02-13 20:01:33,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33,775][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.09361770004034042, acc: 0.9733333587646484)
[2025-02-13 20:01:33,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34,152][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.12209697812795639, acc: 0.9506173133850098)
[2025-02-13 20:01:34,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34,531][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.05992277339100838, acc: 0.9735099077224731)
[2025-02-13 20:01:34,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34,929][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.038414422422647476, acc: 0.9944444298744202)
[2025-02-13 20:01:35,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35,351][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.06549457460641861, acc: 0.9822485446929932)
[2025-02-13 20:01:35,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35,720][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.11975476145744324, acc: 0.9653179049491882)
[2025-02-13 20:01:35,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36,131][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.13609427213668823, acc: 0.9731183052062988)
[2025-02-13 20:01:36,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36,515][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.08410190790891647, acc: 0.987500011920929)
[2025-02-13 20:01:36,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36,923][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.058165524154901505, acc: 0.9860140085220337)
[2025-02-13 20:01:37,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37,339][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.09768199920654297, acc: 0.9748427867889404)
[2025-02-13 20:01:37,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37,772][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.18046389520168304, acc: 0.9576719403266907)
[2025-02-13 20:01:37,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38,165][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.06099674478173256, acc: 0.9806451797485352)
[2025-02-13 20:01:38,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38,546][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.09429727494716644, acc: 0.9512194991111755)
[2025-02-13 20:01:38,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38,924][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.055207062512636185, acc: 0.9868420958518982)
[2025-02-13 20:01:39,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39,301][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.08109214901924133, acc: 0.9694656729698181)
[2025-02-13 20:01:39,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39,679][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.1073240116238594, acc: 0.9793103337287903)
[2025-02-13 20:01:39,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40,071][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.1458243727684021, acc: 0.9770992398262024)
[2025-02-13 20:01:40,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40,438][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.10777958482503891, acc: 0.9741935729980469)
[2025-02-13 20:01:40,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40,841][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.02322627790272236, acc: 1.0)
[2025-02-13 20:01:40,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41,221][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.047834958881139755, acc: 0.9862068891525269)
[2025-02-13 20:01:41,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41,616][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.09027368575334549, acc: 0.9662162065505981)
[2025-02-13 20:01:41,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42,006][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.04770027473568916, acc: 0.988095223903656)
[2025-02-13 20:01:42,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42,413][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.09598591923713684, acc: 0.9775280952453613)
[2025-02-13 20:01:42,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42,834][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.047027770429849625, acc: 0.9852941036224365)
[2025-02-13 20:01:42,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43,222][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.04246047884225845, acc: 1.0)
[2025-02-13 20:01:43,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43,637][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.13300476968288422, acc: 0.9750000238418579)
[2025-02-13 20:01:43,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44,057][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.025987984612584114, acc: 0.9928057789802551)
[2025-02-13 20:01:44,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44,461][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.10356053709983826, acc: 0.9898989796638489)
[2025-02-13 20:01:44,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44,903][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.3066653609275818, acc: 0.9328858852386475)
[2025-02-13 20:01:45,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45,289][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.15987543761730194, acc: 0.9710982441902161)
[2025-02-13 20:01:45,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45,695][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.14097872376441956, acc: 0.9725274443626404)
[2025-02-13 20:01:45,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46,081][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.13687558472156525, acc: 0.9664429426193237)
[2025-02-13 20:01:46,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46,504][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.21938343346118927, acc: 0.9387755393981934)
[2025-02-13 20:01:46,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46,882][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.13714858889579773, acc: 0.9716981053352356)
[2025-02-13 20:01:47,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47,267][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.11544906347990036, acc: 0.9649999737739563)
[2025-02-13 20:01:47,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47,646][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.18034400045871735, acc: 0.9560439586639404)
[2025-02-13 20:01:47,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48,033][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.05635282024741173, acc: 0.9894737005233765)
[2025-02-13 20:01:48,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48,429][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.04077036306262016, acc: 0.9909090995788574)
[2025-02-13 20:01:48,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48,887][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.13818268477916718, acc: 0.9666666388511658)
[2025-02-13 20:01:49,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49,256][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.21160122752189636, acc: 0.9397590160369873)
[2025-02-13 20:01:49,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49,631][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.1984187513589859, acc: 0.976190447807312)
[2025-02-13 20:01:49,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50,050][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.094504714012146, acc: 0.9888268113136292)
[2025-02-13 20:01:50,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50,464][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.026494961231946945, acc: 1.0)
[2025-02-13 20:01:50,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50,853][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.03346700593829155, acc: 0.9945945739746094)
[2025-02-13 20:01:50,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51,245][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.06589124351739883, acc: 0.9784172773361206)
[2025-02-13 20:01:51,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51,638][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.12977364659309387, acc: 0.9652777910232544)
[2025-02-13 20:01:51,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52,036][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.07243682444095612, acc: 0.9803921580314636)
[2025-02-13 20:01:52,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52,420][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.0642767995595932, acc: 0.9915966391563416)
[2025-02-13 20:01:52,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52,797][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.035956237465143204, acc: 1.0)
[2025-02-13 20:01:52,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53,204][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.1574058085680008, acc: 0.9803921580314636)
[2025-02-13 20:01:53,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53,598][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.09734155237674713, acc: 0.984375)
[2025-02-13 20:01:53,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54,003][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.20026849210262299, acc: 0.9316239356994629)
[2025-02-13 20:01:54,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54,403][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.044487226754426956, acc: 0.9924242496490479)
[2025-02-13 20:01:54,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54,792][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.27514559030532837, acc: 0.9379310607910156)
[2025-02-13 20:01:54,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55,223][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.22736327350139618, acc: 0.9652174115180969)
[2025-02-13 20:01:55,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55,644][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.12304306030273438, acc: 0.970059871673584)
[2025-02-13 20:01:55,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56,048][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.057729728519916534, acc: 0.9876543283462524)
[2025-02-13 20:01:56,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56,501][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.0916425883769989, acc: 0.9710144996643066)
[2025-02-13 20:01:56,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56,911][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.1795533001422882, acc: 0.949367105960846)
[2025-02-13 20:01:57,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57,284][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.06378590315580368, acc: 0.9862068891525269)
[2025-02-13 20:01:57,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57,657][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.12064225971698761, acc: 0.9668508172035217)
[2025-02-13 20:01:57,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58,058][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.026403693482279778, acc: 0.9948717951774597)
[2025-02-13 20:01:58,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58,433][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.04291777312755585, acc: 0.9946808218955994)
[2025-02-13 20:01:58,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58,807][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.021395180374383926, acc: 1.0)
[2025-02-13 20:01:58,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59,199][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.07384482026100159, acc: 0.9841269850730896)
[2025-02-13 20:01:59,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59,582][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.1355975866317749, acc: 0.9680851101875305)
[2025-02-13 20:01:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00,008][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.21625182032585144, acc: 0.9491525292396545)
[2025-02-13 20:02:00,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00,384][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.041321758180856705, acc: 0.9933333396911621)
[2025-02-13 20:02:00,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00,778][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.12495101243257523, acc: 0.976190447807312)
[2025-02-13 20:02:00,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01,161][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.08890824019908905, acc: 0.9788359999656677)
[2025-02-13 20:02:01,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01,570][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.055004265159368515, acc: 0.9754601120948792)
[2025-02-13 20:02:01,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02,043][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.125360369682312, acc: 0.9764705896377563)
[2025-02-13 20:02:02,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02,442][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.040610477328300476, acc: 0.9861111044883728)
[2025-02-13 20:02:02,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02,835][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.021799685433506966, acc: 1.0)
[2025-02-13 20:02:02,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03,205][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.06388489156961441, acc: 0.9807692170143127)
[2025-02-13 20:02:03,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03,581][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.035335056483745575, acc: 0.9919354915618896)
[2025-02-13 20:02:03,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03,978][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.058146875351667404, acc: 0.9864864945411682)
[2025-02-13 20:02:04,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04,370][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.04399602860212326, acc: 0.9890109896659851)
[2025-02-13 20:02:04,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04,804][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.033476006239652634, acc: 0.9931972622871399)
[2025-02-13 20:02:04,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05,199][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.024042312055826187, acc: 0.9942196607589722)
[2025-02-13 20:02:05,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05,583][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.0958874300122261, acc: 0.9710982441902161)
[2025-02-13 20:02:05,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05,947][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.02879934012889862, acc: 0.9939393997192383)
[2025-02-13 20:02:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06,401][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.04973525181412697, acc: 0.9945651888847351)
[2025-02-13 20:02:06,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06,788][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.07451702654361725, acc: 0.9821428656578064)
[2025-02-13 20:02:06,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07,159][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.0418996587395668, acc: 0.9892473220825195)
[2025-02-13 20:02:07,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07,534][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.10129036009311676, acc: 0.976331353187561)
[2025-02-13 20:02:07,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07,907][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.03299010172486305, acc: 0.9777777791023254)
[2025-02-13 20:02:08,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08,350][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.310045063495636, acc: 0.9455782175064087)
[2025-02-13 20:02:08,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08,781][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.023008650168776512, acc: 1.0)
[2025-02-13 20:02:08,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09,193][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.08907783776521683, acc: 0.9921259880065918)
[2025-02-13 20:02:09,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09,606][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.08438634872436523, acc: 0.9731543660163879)
[2025-02-13 20:02:09,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09,988][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.05897348374128342, acc: 0.9923076629638672)
[2025-02-13 20:02:10,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10,368][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.1891280561685562, acc: 0.9645389914512634)
[2025-02-13 20:02:10,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10,773][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.09789223968982697, acc: 0.9863013625144958)
[2025-02-13 20:02:10,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11,182][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.13058337569236755, acc: 0.9750000238418579)
[2025-02-13 20:02:11,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11,613][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.09971562027931213, acc: 0.9805194735527039)
[2025-02-13 20:02:11,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12,018][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.038603775203228, acc: 1.0)
[2025-02-13 20:02:12,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12,431][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.12410148233175278, acc: 0.9685534834861755)
[2025-02-13 20:02:12,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12,860][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.11675439029932022, acc: 0.97826087474823)
[2025-02-13 20:02:12,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13,267][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.06426340341567993, acc: 0.9800000190734863)
[2025-02-13 20:02:13,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13,695][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.1715151071548462, acc: 0.970588207244873)
[2025-02-13 20:02:13,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14,191][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.23183679580688477, acc: 0.9482758641242981)
[2025-02-13 20:02:14,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14,679][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.1346929967403412, acc: 0.979899525642395)
[2025-02-13 20:02:14,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15,129][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.09361698478460312, acc: 0.9838709831237793)
[2025-02-13 20:02:15,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15,558][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.1990601271390915, acc: 0.9541984796524048)
[2025-02-13 20:02:15,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15,970][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.22028125822544098, acc: 0.9262295365333557)
[2025-02-13 20:02:16,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16,351][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.20054911077022552, acc: 0.9399999976158142)
[2025-02-13 20:02:16,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16,747][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.5427007079124451, acc: 0.8920863270759583)
[2025-02-13 20:02:16,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17,113][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.2657715380191803, acc: 0.9416058659553528)
[2025-02-13 20:02:17,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17,525][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.2564171254634857, acc: 0.9349112510681152)
[2025-02-13 20:02:17,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17,912][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.26725003123283386, acc: 0.9133333563804626)
[2025-02-13 20:02:18,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18,314][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.24941852688789368, acc: 0.929411768913269)
[2025-02-13 20:02:18,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18,699][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.1293451488018036, acc: 0.9668874144554138)
[2025-02-13 20:02:18,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19,088][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.1639784574508667, acc: 0.9671052694320679)
[2025-02-13 20:02:19,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19,547][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.14202377200126648, acc: 0.9624999761581421)
[2025-02-13 20:02:19,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19,988][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.2496340125799179, acc: 0.9466666579246521)
[2025-02-13 20:02:20,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20,355][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.2170623540878296, acc: 0.9536082744598389)
[2025-02-13 20:02:20,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20,749][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.1688769906759262, acc: 0.9547511339187622)
[2025-02-13 20:02:20,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21,152][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.3318374752998352, acc: 0.9320388436317444)
[2025-02-13 20:02:21,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21,590][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.30202099680900574, acc: 0.9375)
[2025-02-13 20:02:21,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21,953][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.17673762142658234, acc: 0.9528301954269409)
[2025-02-13 20:02:22,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22,377][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.2582578957080841, acc: 0.9406392574310303)
[2025-02-13 20:02:22,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22,787][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.379841685295105, acc: 0.9126983880996704)
[2025-02-13 20:02:22,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23,171][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.3706696927547455, acc: 0.9278350472450256)
[2025-02-13 20:02:23,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23,545][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.40797650814056396, acc: 0.9360465407371521)
[2025-02-13 20:02:23,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23,914][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.12284716963768005, acc: 0.9801980257034302)
[2025-02-13 20:02:24,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24,299][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.29085665941238403, acc: 0.9245283007621765)
[2025-02-13 20:02:24,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24,756][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.24064713716506958, acc: 0.9353448152542114)
[2025-02-13 20:02:24,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25,182][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.25645968317985535, acc: 0.9372197389602661)
[2025-02-13 20:02:25,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25,583][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.30463141202926636, acc: 0.9095022678375244)
[2025-02-13 20:02:25,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26,027][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.1076861247420311, acc: 0.9615384340286255)
[2025-02-13 20:02:26,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26,429][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.17584317922592163, acc: 0.9693877696990967)
[2025-02-13 20:02:26,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26,864][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.26480570435523987, acc: 0.929411768913269)
[2025-02-13 20:02:27,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27,280][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.19651617109775543, acc: 0.9602649211883545)
[2025-02-13 20:02:27,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27,668][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.7214102149009705, acc: 0.8186046481132507)
[2025-02-13 20:02:27,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28,056][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.2825517952442169, acc: 0.9329268336296082)
[2025-02-13 20:02:28,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28,435][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.30386918783187866, acc: 0.9127907156944275)
[2025-02-13 20:02:28,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28,810][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.060243818908929825, acc: 0.9836065769195557)
[2025-02-13 20:02:28,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29,209][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.2728447914123535, acc: 0.9319371581077576)
[2025-02-13 20:02:29,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29,598][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.2616702914237976, acc: 0.9342105388641357)
[2025-02-13 20:02:29,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29,975][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.20197629928588867, acc: 0.9577465057373047)
[2025-02-13 20:02:30,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30,392][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.2917768061161041, acc: 0.939393937587738)
[2025-02-13 20:02:30,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30,813][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.33895784616470337, acc: 0.9105263352394104)
[2025-02-13 20:02:30,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31,235][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.148133784532547, acc: 0.9759036302566528)
[2025-02-13 20:02:31,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31,676][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.1337069571018219, acc: 0.9599999785423279)
[2025-02-13 20:02:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32,067][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.08633190393447876, acc: 0.9913793206214905)
[2025-02-13 20:02:32,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32,474][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.06736177206039429, acc: 0.9866666793823242)
[2025-02-13 20:02:32,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32,883][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.09028665721416473, acc: 0.9939393997192383)
[2025-02-13 20:02:33,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33,283][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.08960617333650589, acc: 0.9763779640197754)
[2025-02-13 20:02:33,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33,748][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.20104563236236572, acc: 0.9658119678497314)
[2025-02-13 20:02:33,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34,126][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.19764374196529388, acc: 0.95333331823349)
[2025-02-13 20:02:34,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34,564][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.12262442708015442, acc: 0.9862068891525269)
[2025-02-13 20:02:34,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34,962][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.08713649213314056, acc: 0.9847328066825867)
[2025-02-13 20:02:35,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35,344][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.07944772392511368, acc: 0.9694656729698181)
[2025-02-13 20:02:35,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35,721][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.1628565788269043, acc: 0.9702380895614624)
[2025-02-13 20:02:35,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36,081][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.24697190523147583, acc: 0.9519230723381042)
[2025-02-13 20:02:36,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36,458][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.09118856489658356, acc: 0.9865771532058716)
[2025-02-13 20:02:36,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36,864][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.11776657402515411, acc: 0.9864864945411682)
[2025-02-13 20:02:36,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37,266][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.016395503655076027, acc: 1.0)
[2025-02-13 20:02:37,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37,641][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.06079654023051262, acc: 0.9870967864990234)
[2025-02-13 20:02:37,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38,026][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.05416186898946762, acc: 0.9851852059364319)
[2025-02-13 20:02:38,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38,480][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.087801992893219, acc: 0.9828571677207947)
[2025-02-13 20:02:38,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38,865][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.11645122617483139, acc: 0.9541984796524048)
[2025-02-13 20:02:39,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39,238][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.06727667897939682, acc: 0.9870129823684692)
[2025-02-13 20:02:39,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39,647][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.17019778490066528, acc: 0.9407407641410828)
[2025-02-13 20:02:39,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40,033][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.46979716420173645, acc: 0.939393937587738)
[2025-02-13 20:02:40,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40,448][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.09783453494310379, acc: 0.9794520735740662)
[2025-02-13 20:02:40,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40,854][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.19910751283168793, acc: 0.9583333134651184)
[2025-02-13 20:02:40,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41,251][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.033217187970876694, acc: 0.9937106966972351)
[2025-02-13 20:02:41,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41,666][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.1269141286611557, acc: 0.970370352268219)
[2025-02-13 20:02:41,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42,034][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.07562173157930374, acc: 0.9834710955619812)
[2025-02-13 20:02:42,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42,406][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.06569524854421616, acc: 0.991304337978363)
[2025-02-13 20:02:42,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42,768][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.07590791583061218, acc: 0.9772727489471436)
[2025-02-13 20:02:42,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43,172][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.035515930503606796, acc: 1.0)
[2025-02-13 20:02:43,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43,560][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.20317447185516357, acc: 0.949999988079071)
[2025-02-13 20:02:43,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43,940][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.13283422589302063, acc: 0.9510869383811951)
[2025-02-13 20:02:44,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44,315][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.09179815649986267, acc: 0.9732620120048523)
[2025-02-13 20:02:44,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44,716][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.1638837456703186, acc: 0.9621211886405945)
[2025-02-13 20:02:44,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45,113][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.42008692026138306, acc: 0.9125683307647705)
[2025-02-13 20:02:45,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45,521][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.08116179704666138, acc: 0.9767441749572754)
[2025-02-13 20:02:45,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45,884][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.213616281747818, acc: 0.9520958065986633)
[2025-02-13 20:02:46,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46,296][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.18232960999011993, acc: 0.9750000238418579)
[2025-02-13 20:02:46,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46,690][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.14652180671691895, acc: 0.9817073345184326)
[2025-02-13 20:02:46,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47,101][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.1696176677942276, acc: 0.9481481313705444)
[2025-02-13 20:02:47,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47,464][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.08122314512729645, acc: 0.984455943107605)
[2025-02-13 20:02:47,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47,831][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.22403690218925476, acc: 0.951724112033844)
[2025-02-13 20:02:47,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48,203][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.06598358601331711, acc: 0.9880239367485046)
[2025-02-13 20:02:48,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48,613][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.1639498919248581, acc: 0.9647058844566345)
[2025-02-13 20:02:48,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49,014][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.10105423629283905, acc: 0.9670329689979553)
[2025-02-13 20:02:49,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49,412][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.11056205630302429, acc: 0.9743589758872986)
[2025-02-13 20:02:49,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49,805][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.16299690306186676, acc: 0.9508196711540222)
[2025-02-13 20:02:49,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50,280][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.07330544292926788, acc: 0.9850746393203735)
[2025-02-13 20:02:50,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50,818][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.06711962074041367, acc: 0.9807692170143127)
[2025-02-13 20:02:50,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51,233][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.18065492808818817, acc: 0.9553571343421936)
[2025-02-13 20:02:51,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51,635][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.0796871930360794, acc: 0.9834710955619812)
[2025-02-13 20:02:51,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51,968][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.08339818567037582, acc: 0.9759036302566528)
[2025-02-13 20:02:52,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52,405][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.09478242695331573, acc: 0.9851484894752502)
[2025-02-13 20:02:52,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52,793][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.06916505843400955, acc: 0.9868420958518982)
[2025-02-13 20:02:52,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53,177][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.20217767357826233, acc: 0.9405940771102905)
[2025-02-13 20:02:53,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53,592][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.07352510094642639, acc: 0.9900000095367432)
[2025-02-13 20:02:53,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54,026][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.168214350938797, acc: 0.9604519605636597)
[2025-02-13 20:02:54,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54,436][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.15872634947299957, acc: 0.969072163105011)
[2025-02-13 20:02:54,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54,864][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.16263918578624725, acc: 0.9473684430122375)
[2025-02-13 20:02:54,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55,246][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.045534420758485794, acc: 0.9856114983558655)
[2025-02-13 20:02:55,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55,616][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.10079783201217651, acc: 0.9741935729980469)
[2025-02-13 20:02:55,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56,015][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.12949034571647644, acc: 0.9516907930374146)
[2025-02-13 20:02:56,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56,438][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.12490297108888626, acc: 0.9558011293411255)
[2025-02-13 20:02:56,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56,823][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.08620517700910568, acc: 0.9711538553237915)
[2025-02-13 20:02:56,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57,237][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.17319060862064362, acc: 0.9689440727233887)
[2025-02-13 20:02:57,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57,640][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.2658425569534302, acc: 0.9444444179534912)
[2025-02-13 20:02:57,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57,998][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.06921009719371796, acc: 0.9855072498321533)
[2025-02-13 20:02:58,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58,368][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.2016247659921646, acc: 0.9513513445854187)
[2025-02-13 20:02:58,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58,749][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.2387840300798416, acc: 0.9593495726585388)
[2025-02-13 20:02:58,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59,140][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.1224028468132019, acc: 0.9552238583564758)
[2025-02-13 20:02:59,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59,507][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.10267594456672668, acc: 0.9670329689979553)
[2025-02-13 20:02:59,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59,863][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.1432199329137802, acc: 0.9818181991577148)
[2025-02-13 20:02:59,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00,221][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.22384706139564514, acc: 0.9470198750495911)
[2025-02-13 20:03:00,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00,602][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.08969028294086456, acc: 0.9792746305465698)
[2025-02-13 20:03:00,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00,979][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.13635937869548798, acc: 0.9659863710403442)
[2025-02-13 20:03:01,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01,427][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.2203587144613266, acc: 0.9718309640884399)
[2025-02-13 20:03:01,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01,811][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.15076711773872375, acc: 0.9607843160629272)
[2025-02-13 20:03:01,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02,226][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.11614301055669785, acc: 0.9611650705337524)
[2025-02-13 20:03:02,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02,668][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.27210280299186707, acc: 0.9497717022895813)
[2025-02-13 20:03:02,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03,041][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.13363279402256012, acc: 0.9644970297813416)
[2025-02-13 20:03:03,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03,414][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.17266635596752167, acc: 0.9603960514068604)
[2025-02-13 20:03:03,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03,797][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.1808241456747055, acc: 0.9599999785423279)
[2025-02-13 20:03:03,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04,151][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.06863141059875488, acc: 0.9856114983558655)
[2025-02-13 20:03:04,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04,531][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.05154154822230339, acc: 0.98591548204422)
[2025-02-13 20:03:04,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04,875][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.08301935344934464, acc: 0.9649122953414917)
[2025-02-13 20:03:05,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05,264][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.15641498565673828, acc: 0.9605262875556946)
[2025-02-13 20:03:05,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05,672][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.4274061322212219, acc: 0.895061731338501)
[2025-02-13 20:03:05,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06,110][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.29049891233444214, acc: 0.9424083828926086)
[2025-02-13 20:03:06,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06,517][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.15394355356693268, acc: 0.9679144620895386)
[2025-02-13 20:03:06,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06,905][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.17268630862236023, acc: 0.9741935729980469)
[2025-02-13 20:03:07,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07,335][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.17067022621631622, acc: 0.96875)
[2025-02-13 20:03:07,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07,720][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.1814829409122467, acc: 0.9591836929321289)
[2025-02-13 20:03:07,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08,111][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.2758699953556061, acc: 0.9675324559211731)
[2025-02-13 20:03:08,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08,510][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.05168124660849571, acc: 0.9780219793319702)
[2025-02-13 20:03:08,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08,882][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.09287883341312408, acc: 0.9735099077224731)
[2025-02-13 20:03:09,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09,287][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.18378333747386932, acc: 0.9485294222831726)
[2025-02-13 20:03:09,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09,655][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.0966666042804718, acc: 0.9816513657569885)
[2025-02-13 20:03:09,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09,985][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.1206250935792923, acc: 0.9784946441650391)
[2025-02-13 20:03:10,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10,391][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.11216948181390762, acc: 0.9836065769195557)
[2025-02-13 20:03:10,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10,792][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.11329041421413422, acc: 0.9722222089767456)
[2025-02-13 20:03:10,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11,227][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.048556987196207047, acc: 0.9944751262664795)
[2025-02-13 20:03:11,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11,617][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.025357889011502266, acc: 1.0)
[2025-02-13 20:03:11,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12,000][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.040315233170986176, acc: 0.9880239367485046)
[2025-02-13 20:03:12,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12,431][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.024556338787078857, acc: 1.0)
[2025-02-13 20:03:12,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12,807][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.06411249190568924, acc: 0.9878048896789551)
[2025-02-13 20:03:12,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13,221][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.06444321572780609, acc: 0.9916666746139526)
[2025-02-13 20:03:13,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13,652][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.017441319301724434, acc: 1.0)
[2025-02-13 20:03:13,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14,075][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.020138273015618324, acc: 0.9935483932495117)
[2025-02-13 20:03:14,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14,528][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.06049351394176483, acc: 0.9825581312179565)
[2025-02-13 20:03:14,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14,948][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.029332417994737625, acc: 0.9947090148925781)
[2025-02-13 20:03:15,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15,389][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.06255842000246048, acc: 0.9886363744735718)
[2025-02-13 20:03:15,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15,833][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.021268509328365326, acc: 1.0)
[2025-02-13 20:03:15,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16,297][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.0144578218460083, acc: 1.0)
[2025-02-13 20:03:16,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16,769][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.05656451731920242, acc: 0.9881656765937805)
[2025-02-13 20:03:16,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17,235][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.05723116174340248, acc: 0.9836065769195557)
[2025-02-13 20:03:17,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17,668][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.05438537523150444, acc: 0.982758641242981)
[2025-02-13 20:03:17,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18,083][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.06616811454296112, acc: 0.9823529124259949)
[2025-02-13 20:03:18,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18,524][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.013573974370956421, acc: 1.0)
[2025-02-13 20:03:18,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18,939][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.02197348326444626, acc: 0.9918699264526367)
[2025-02-13 20:03:19,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19,365][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.01845088228583336, acc: 1.0)
[2025-02-13 20:03:19,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19,827][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.04818752035498619, acc: 0.982758641242981)
[2025-02-13 20:03:19,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20,279][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.050959181040525436, acc: 0.9927007555961609)
[2025-02-13 20:03:20,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20,761][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.08371560275554657, acc: 0.9795918464660645)
[2025-02-13 20:03:20,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21,186][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.08584907650947571, acc: 0.9814814925193787)
[2025-02-13 20:03:21,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21,652][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.12466839700937271, acc: 0.9545454382896423)
[2025-02-13 20:03:21,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22,045][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.0837840735912323, acc: 0.9844961166381836)
[2025-02-13 20:03:22,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22,470][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.1561911702156067, acc: 0.9583333134651184)
[2025-02-13 20:03:22,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22,872][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.16499042510986328, acc: 0.9509803652763367)
[2025-02-13 20:03:23,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23,302][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.18989421427249908, acc: 0.9375)
[2025-02-13 20:03:23,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23,728][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.27140218019485474, acc: 0.9313725233078003)
[2025-02-13 20:03:23,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24,093][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.19503217935562134, acc: 0.9615384340286255)
[2025-02-13 20:03:24,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24,454][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.1529511958360672, acc: 0.9603960514068604)
[2025-02-13 20:03:24,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24,853][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.14539887011051178, acc: 0.9814814925193787)
[2025-02-13 20:03:25,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25,245][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.07404765486717224, acc: 0.9637681245803833)
[2025-02-13 20:03:25,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25,646][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.1861162781715393, acc: 0.9538461565971375)
[2025-02-13 20:03:25,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26,041][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.18142655491828918, acc: 0.970588207244873)
[2025-02-13 20:03:26,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26,421][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.24683518707752228, acc: 0.936170220375061)
[2025-02-13 20:03:26,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26,818][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.19579890370368958, acc: 0.9440000057220459)
[2025-02-13 20:03:26,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27,201][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.08724502474069595, acc: 0.9788732528686523)
[2025-02-13 20:03:27,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27,558][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.08918905258178711, acc: 0.9790209531784058)
[2025-02-13 20:03:27,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27,928][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.15822118520736694, acc: 0.9520000219345093)
[2025-02-13 20:03:28,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28,289][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.23187583684921265, acc: 0.9477611780166626)
[2025-02-13 20:03:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28,662][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.1366027146577835, acc: 0.970802903175354)
[2025-02-13 20:03:28,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29,022][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.22031846642494202, acc: 0.9338235259056091)
[2025-02-13 20:03:29,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29,437][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.09276050329208374, acc: 0.9837398529052734)
[2025-02-13 20:03:29,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29,840][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.2103736251592636, acc: 0.9583333134651184)
[2025-02-13 20:03:29,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30,248][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.10500586777925491, acc: 0.9905660152435303)
[2025-02-13 20:03:30,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30,638][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.18761591613292694, acc: 0.96875)
[2025-02-13 20:03:30,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31,069][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.09016162902116776, acc: 0.9760000109672546)
[2025-02-13 20:03:31,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31,460][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.36463308334350586, acc: 0.9054054021835327)
[2025-02-13 20:03:31,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31,835][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.059042029082775116, acc: 0.9900990128517151)
[2025-02-13 20:03:31,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32,262][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.16395454108715057, acc: 0.9568345546722412)
[2025-02-13 20:03:32,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32,644][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.08275704830884933, acc: 0.9927536249160767)
[2025-02-13 20:03:32,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33,007][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.07803121209144592, acc: 0.9851852059364319)
[2025-02-13 20:03:33,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33,397][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.08933427184820175, acc: 0.9850746393203735)
[2025-02-13 20:03:33,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33,770][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.08640174567699432, acc: 0.9822485446929932)
[2025-02-13 20:03:33,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34,152][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.0667228102684021, acc: 0.9821428656578064)
[2025-02-13 20:03:34,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34,484][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.073737733066082, acc: 0.988095223903656)
[2025-02-13 20:03:34,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34,901][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.028185056522488594, acc: 0.9937888383865356)
[2025-02-13 20:03:35,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35,382][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.047421395778656006, acc: 0.9941860437393188)
[2025-02-13 20:03:35,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35,787][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.026539187878370285, acc: 0.9931034445762634)
[2025-02-13 20:03:35,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36,188][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.05542406812310219, acc: 0.9874213933944702)
[2025-02-13 20:03:36,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36,574][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.08477538079023361, acc: 0.9779005646705627)
[2025-02-13 20:03:36,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36,958][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.03533458709716797, acc: 0.9942857027053833)
[2025-02-13 20:03:37,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37,335][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.07316526770591736, acc: 0.9842105507850647)
[2025-02-13 20:03:37,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37,715][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.06908128410577774, acc: 0.9820359349250793)
[2025-02-13 20:03:37,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38,117][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.02981533855199814, acc: 1.0)
[2025-02-13 20:03:38,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38,511][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.13197432458400726, acc: 0.9635036587715149)
[2025-02-13 20:03:38,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38,884][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.050981681793928146, acc: 0.994350254535675)
[2025-02-13 20:03:39,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39,270][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.025755293667316437, acc: 1.0)
[2025-02-13 20:03:39,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39,647][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.01655024290084839, acc: 1.0)
[2025-02-13 20:03:39,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40,082][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.11062738299369812, acc: 0.9772727489471436)
[2025-02-13 20:03:40,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40,485][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.0530649833381176, acc: 0.9894737005233765)
[2025-02-13 20:03:40,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40,866][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.027262579649686813, acc: 0.9932885766029358)
[2025-02-13 20:03:40,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41,235][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.13528354465961456, acc: 0.9638554453849792)
[2025-02-13 20:03:41,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41,612][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.28009775280952454, acc: 0.9236640930175781)
[2025-02-13 20:03:41,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41,999][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.1654670089483261, acc: 0.9430894255638123)
[2025-02-13 20:03:42,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42,373][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.15783430635929108, acc: 0.9537572264671326)
[2025-02-13 20:03:42,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42,765][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.10827852785587311, acc: 0.9742268323898315)
[2025-02-13 20:03:42,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43,154][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.17345088720321655, acc: 0.9265536665916443)
[2025-02-13 20:03:43,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43,519][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.8609142303466797, acc: 0.7983871102333069)
[2025-02-13 20:03:43,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43,894][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.1371137946844101, acc: 0.9814814925193787)
[2025-02-13 20:03:44,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44,282][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.10047268122434616, acc: 0.9856114983558655)
[2025-02-13 20:03:44,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44,656][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.07674197107553482, acc: 0.9784946441650391)
[2025-02-13 20:03:44,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45,090][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.0655953511595726, acc: 0.979899525642395)
[2025-02-13 20:03:45,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45,497][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.07523957639932632, acc: 0.9803921580314636)
[2025-02-13 20:03:45,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45,899][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.0706419125199318, acc: 0.9850746393203735)
[2025-02-13 20:03:46,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46,333][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.06378518790006638, acc: 0.9946236610412598)
[2025-02-13 20:03:46,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46,769][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.082882821559906, acc: 0.9768518805503845)
[2025-02-13 20:03:46,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47,163][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.08930546790361404, acc: 0.9731183052062988)
[2025-02-13 20:03:47,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47,550][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.0603700689971447, acc: 0.9878787994384766)
[2025-02-13 20:03:47,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47,957][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.057988978922367096, acc: 0.9851852059364319)
[2025-02-13 20:03:48,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48,383][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.10010496526956558, acc: 0.9689440727233887)
[2025-02-13 20:03:48,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48,758][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.04098128154873848, acc: 0.9942528605461121)
[2025-02-13 20:03:48,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49,108][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.05907250568270683, acc: 0.9821428656578064)
[2025-02-13 20:03:49,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49,485][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.22773624956607819, acc: 0.9506173133850098)
[2025-02-13 20:03:49,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49,860][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.08416138589382172, acc: 0.9791666865348816)
[2025-02-13 20:03:50,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50,273][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.13316549360752106, acc: 0.9745222926139832)
[2025-02-13 20:03:50,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50,649][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.07788471132516861, acc: 0.9848484992980957)
[2025-02-13 20:03:50,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51,059][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.05958321690559387, acc: 0.9892473220825195)
[2025-02-13 20:03:51,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51,502][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.3209020495414734, acc: 0.9477611780166626)
[2025-02-13 20:03:51,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51,890][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.25825729966163635, acc: 0.9289617538452148)
[2025-02-13 20:03:52,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52,249][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.1628275364637375, acc: 0.9518716335296631)
[2025-02-13 20:03:52,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52,618][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.10855323076248169, acc: 0.963350772857666)
[2025-02-13 20:03:52,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53,005][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.1420145034790039, acc: 0.9722222089767456)
[2025-02-13 20:03:53,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53,386][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.38400915265083313, acc: 0.9011628031730652)
[2025-02-13 20:03:53,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53,798][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.14344586431980133, acc: 0.9427083134651184)
[2025-02-13 20:03:53,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54,173][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.3129537105560303, acc: 0.931034505367279)
[2025-02-13 20:03:54,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54,537][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.17095908522605896, acc: 0.9504132270812988)
[2025-02-13 20:03:54,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54,908][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.054647836834192276, acc: 0.9875776171684265)
[2025-02-13 20:03:55,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55,277][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.04569002240896225, acc: 0.9940828680992126)
[2025-02-13 20:03:55,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55,650][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.035544320940971375, acc: 1.0)
[2025-02-13 20:03:55,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56,028][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.18925414979457855, acc: 0.9644970297813416)
[2025-02-13 20:03:56,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56,394][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.16932059824466705, acc: 0.9536423683166504)
[2025-02-13 20:03:56,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56,836][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.08550912141799927, acc: 0.9701492786407471)
[2025-02-13 20:03:56,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57,251][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.20710140466690063, acc: 0.9411764740943909)
[2025-02-13 20:03:57,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57,648][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.19535009562969208, acc: 0.9638554453849792)
[2025-02-13 20:03:57,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58,058][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.11893432587385178, acc: 0.9679487347602844)
[2025-02-13 20:03:58,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58,455][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.018773779273033142, acc: 0.994413435459137)
[2025-02-13 20:03:58,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58,874][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.05000218003988266, acc: 0.9939393997192383)
[2025-02-13 20:03:59,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59,289][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.06971142441034317, acc: 0.983146071434021)
[2025-02-13 20:03:59,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59,697][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.039736613631248474, acc: 0.9886363744735718)
[2025-02-13 20:03:59,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00,057][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.0979209914803505, acc: 0.9808917045593262)
[2025-02-13 20:04:00,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00,456][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.1699407696723938, acc: 0.9461538195610046)
[2025-02-13 20:04:00,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00,883][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.05215585231781006, acc: 0.9824561476707458)
[2025-02-13 20:04:01,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01,297][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.09729291498661041, acc: 0.9696969985961914)
[2025-02-13 20:04:01,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01,681][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.10750257223844528, acc: 0.9793103337287903)
[2025-02-13 20:04:01,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02,061][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.22876593470573425, acc: 0.9515151381492615)
[2025-02-13 20:04:02,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02,482][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.12493326514959335, acc: 0.9733333587646484)
[2025-02-13 20:04:02,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02,862][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.09145812690258026, acc: 0.9793103337287903)
[2025-02-13 20:04:03,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03,289][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.16094492375850677, acc: 0.9802631735801697)
[2025-02-13 20:04:03,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03,700][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.04937925562262535, acc: 0.982758641242981)
[2025-02-13 20:04:03,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04,131][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.07908786833286285, acc: 0.9834254384040833)
[2025-02-13 20:04:04,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04,529][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.11083948612213135, acc: 0.9833333492279053)
[2025-02-13 20:04:04,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05,000][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.1517677754163742, acc: 0.9585798978805542)
[2025-02-13 20:04:05,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05,509][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.024346686899662018, acc: 1.0)
[2025-02-13 20:04:05,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05,989][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.23899799585342407, acc: 0.9638554453849792)
[2025-02-13 20:04:06,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06,409][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.07416652888059616, acc: 0.9824561476707458)
[2025-02-13 20:04:06,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06,803][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.0392807275056839, acc: 1.0)
[2025-02-13 20:04:06,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07,209][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.0724877268075943, acc: 0.9837837815284729)
[2025-02-13 20:04:07,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07,613][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.07203910499811172, acc: 0.9720930457115173)
[2025-02-13 20:04:07,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08,005][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.12161813676357269, acc: 0.9744898080825806)
[2025-02-13 20:04:08,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08,479][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.0982251763343811, acc: 0.982300877571106)
[2025-02-13 20:04:08,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08,887][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.05260641500353813, acc: 0.9952380657196045)
[2025-02-13 20:04:09,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09,288][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.10699480772018433, acc: 0.9763033390045166)
[2025-02-13 20:04:09,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09,671][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.07976192235946655, acc: 0.9813084006309509)
[2025-02-13 20:04:09,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10,130][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.062749944627285, acc: 0.984455943107605)
[2025-02-13 20:04:10,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10,556][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.046202365309000015, acc: 0.995121955871582)
[2025-02-13 20:04:10,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10,938][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.06649287790060043, acc: 0.981249988079071)
[2025-02-13 20:04:11,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11,416][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.04883432388305664, acc: 0.9907407164573669)
[2025-02-13 20:04:11,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11,849][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.06312020123004913, acc: 0.9837837815284729)
[2025-02-13 20:04:12,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12,270][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.11860346049070358, acc: 0.9627906680107117)
[2025-02-13 20:04:12,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12,714][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.14852577447891235, acc: 0.9589040875434875)
[2025-02-13 20:04:12,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13,122][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.15868693590164185, acc: 0.976331353187561)
[2025-02-13 20:04:13,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13,519][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.12529174983501434, acc: 0.9681817889213562)
[2025-02-13 20:04:13,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14,005][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.15424621105194092, acc: 0.9714285731315613)
[2025-02-13 20:04:14,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14,415][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.08765865862369537, acc: 0.9735449552536011)
[2025-02-13 20:04:14,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14,809][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.07315164804458618, acc: 0.9810126423835754)
[2025-02-13 20:04:14,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15,253][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.13783393800258636, acc: 0.9685534834861755)
[2025-02-13 20:04:15,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15,668][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.11371292918920517, acc: 0.96875)
[2025-02-13 20:04:15,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16,063][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.06989598274230957, acc: 0.9813084006309509)
[2025-02-13 20:04:16,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16,442][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.08368300646543503, acc: 0.9777777791023254)
[2025-02-13 20:04:16,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16,849][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.16572369635105133, acc: 0.9677419066429138)
[2025-02-13 20:04:16,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17,263][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.14519095420837402, acc: 0.9553072452545166)
[2025-02-13 20:04:17,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17,650][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.021784231066703796, acc: 1.0)
[2025-02-13 20:04:17,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18,024][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.06696661561727524, acc: 0.9723756909370422)
[2025-02-13 20:04:18,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18,459][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.08139189332723618, acc: 0.9740932583808899)
[2025-02-13 20:04:18,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18,854][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.11479058861732483, acc: 0.9729729890823364)
[2025-02-13 20:04:18,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19,225][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.18335436284542084, acc: 0.949999988079071)
[2025-02-13 20:04:19,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19,674][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.1982247680425644, acc: 0.9553571343421936)
[2025-02-13 20:04:19,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20,090][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.08095389604568481, acc: 0.969072163105011)
[2025-02-13 20:04:20,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20,508][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.11701711267232895, acc: 0.9708737730979919)
[2025-02-13 20:04:20,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20,914][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.04386276379227638, acc: 0.9927007555961609)
[2025-02-13 20:04:21,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21,340][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.5623116493225098, acc: 0.875)
[2025-02-13 20:04:21,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21,733][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.23408348858356476, acc: 0.9365079402923584)
[2025-02-13 20:04:21,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22,122][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.3925052881240845, acc: 0.875)
[2025-02-13 20:04:22,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22,506][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.11859942227602005, acc: 0.9726027250289917)
[2025-02-13 20:04:22,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22,964][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.21470533311367035, acc: 0.949999988079071)
[2025-02-13 20:04:23,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23,444][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.262492835521698, acc: 0.9047619104385376)
[2025-02-13 20:04:23,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23,865][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.4594460129737854, acc: 0.9026548862457275)
[2025-02-13 20:04:24,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24,270][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.16088129580020905, acc: 0.9710144996643066)
[2025-02-13 20:04:24,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24,644][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.10041853040456772, acc: 0.978723406791687)
[2025-02-13 20:04:24,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25,057][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.20391322672367096, acc: 0.9464285969734192)
[2025-02-13 20:04:25,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25,419][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.5802621841430664, acc: 0.8712121248245239)
[2025-02-13 20:04:25,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25,845][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.08712355047464371, acc: 0.9794871807098389)
[2025-02-13 20:04:25,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26,246][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.1997927725315094, acc: 0.9518072009086609)
[2025-02-13 20:04:26,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26,653][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.1322699338197708, acc: 0.9784172773361206)
[2025-02-13 20:04:26,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27,068][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.1378834843635559, acc: 0.9485714435577393)
[2025-02-13 20:04:27,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27,439][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.06695564836263657, acc: 0.9876543283462524)
[2025-02-13 20:04:27,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27,785][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.20661194622516632, acc: 0.9415204524993896)
[2025-02-13 20:04:27,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28,161][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.3317951560020447, acc: 0.9464285969734192)
[2025-02-13 20:04:28,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28,540][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.22351236641407013, acc: 0.9575757384300232)
[2025-02-13 20:04:28,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28,947][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.1403050720691681, acc: 0.9620253443717957)
[2025-02-13 20:04:29,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29,329][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.1464826911687851, acc: 0.9631578922271729)
[2025-02-13 20:04:29,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29,735][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.10141084343194962, acc: 0.966292142868042)
[2025-02-13 20:04:29,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30,120][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.06579552590847015, acc: 0.9904761910438538)
[2025-02-13 20:04:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30,574][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.090629443526268, acc: 0.9800000190734863)
[2025-02-13 20:04:30,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30,947][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.11518938094377518, acc: 0.9642857313156128)
[2025-02-13 20:04:31,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31,323][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.40313589572906494, acc: 0.9176470637321472)
[2025-02-13 20:04:31,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31,703][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.2170562744140625, acc: 0.9444444179534912)
[2025-02-13 20:04:31,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32,087][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.09758437424898148, acc: 0.9842105507850647)
[2025-02-13 20:04:32,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32,492][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.038341596722602844, acc: 0.9942857027053833)
[2025-02-13 20:04:32,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32,903][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.0460476353764534, acc: 0.9882352948188782)
[2025-02-13 20:04:33,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33,311][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.04870320484042168, acc: 0.9933775067329407)
[2025-02-13 20:04:33,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33,698][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.05278736725449562, acc: 0.9846938848495483)
[2025-02-13 20:04:33,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34,069][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.07172923535108566, acc: 0.9805194735527039)
[2025-02-13 20:04:34,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34,476][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.09189769625663757, acc: 0.9710982441902161)
[2025-02-13 20:04:34,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34,903][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.04200511798262596, acc: 0.987500011920929)
[2025-02-13 20:04:35,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35,283][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.11183347553014755, acc: 0.9717513918876648)
[2025-02-13 20:04:35,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35,662][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.08349427580833435, acc: 0.9722222089767456)
[2025-02-13 20:04:35,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36,036][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.1646198183298111, acc: 0.9589040875434875)
[2025-02-13 20:04:36,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36,444][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.054011836647987366, acc: 1.0)
[2025-02-13 20:04:36,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36,813][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.07955805957317352, acc: 0.9746192693710327)
[2025-02-13 20:04:36,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37,195][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.24111276865005493, acc: 0.9568965435028076)
[2025-02-13 20:04:37,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37,559][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.1051567792892456, acc: 0.9704142212867737)
[2025-02-13 20:04:37,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37,917][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.14367379248142242, acc: 0.9509803652763367)
[2025-02-13 20:04:38,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38,323][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.21383103728294373, acc: 0.9306930899620056)
[2025-02-13 20:04:38,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38,724][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.11319989711046219, acc: 0.9685534834861755)
[2025-02-13 20:04:38,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39,093][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.13677293062210083, acc: 0.9723756909370422)
[2025-02-13 20:04:39,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39,485][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.29201459884643555, acc: 0.9317073225975037)
[2025-02-13 20:04:39,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39,857][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.2500476837158203, acc: 0.9375)
[2025-02-13 20:04:39,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40,258][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.13798773288726807, acc: 0.9454545378684998)
[2025-02-13 20:04:40,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40,654][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.4318309426307678, acc: 0.9154929518699646)
[2025-02-13 20:04:40,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41,036][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.20038053393363953, acc: 0.9631336331367493)
[2025-02-13 20:04:41,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41,429][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.3044338822364807, acc: 0.9530516266822815)
[2025-02-13 20:04:41,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41,840][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.0939640924334526, acc: 0.970059871673584)
[2025-02-13 20:04:41,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42,219][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.204537495970726, acc: 0.9461883306503296)
[2025-02-13 20:04:42,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42,626][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.13793031871318817, acc: 0.9681817889213562)
[2025-02-13 20:04:42,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43,028][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.13086169958114624, acc: 0.9718309640884399)
[2025-02-13 20:04:43,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43,424][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.12092495709657669, acc: 0.966183602809906)
[2025-02-13 20:04:43,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43,838][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.16966718435287476, acc: 0.9545454382896423)
[2025-02-13 20:04:43,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44,270][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.08091955631971359, acc: 0.9759615659713745)
[2025-02-13 20:04:44,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44,678][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.24350661039352417, acc: 0.9275362491607666)
[2025-02-13 20:04:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45,113][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.2692979872226715, acc: 0.9285714030265808)
[2025-02-13 20:04:45,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45,532][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.19242869317531586, acc: 0.95333331823349)
[2025-02-13 20:04:45,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45,966][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.12946228682994843, acc: 0.9828571677207947)
[2025-02-13 20:04:46,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46,363][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.1500043272972107, acc: 0.954954981803894)
[2025-02-13 20:04:46,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46,783][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.09733859449625015, acc: 0.9757281541824341)
[2025-02-13 20:04:46,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47,164][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.16692061722278595, acc: 0.9516907930374146)
[2025-02-13 20:04:47,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47,517][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.13523653149604797, acc: 0.9617834687232971)
[2025-02-13 20:04:47,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47,922][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.07349775731563568, acc: 0.987261176109314)
[2025-02-13 20:04:48,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48,313][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.08108377456665039, acc: 0.9939024448394775)
[2025-02-13 20:04:48,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48,718][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.08455689996480942, acc: 0.9714285731315613)
[2025-02-13 20:04:48,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49,182][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.07799290865659714, acc: 0.9767441749572754)
[2025-02-13 20:04:49,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49,642][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.07517192512750626, acc: 0.9805194735527039)
[2025-02-13 20:04:49,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50,091][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.07229016721248627, acc: 0.9893048405647278)
[2025-02-13 20:04:50,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50,485][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.04497362673282623, acc: 0.9921875)
[2025-02-13 20:04:50,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50,889][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.051065851002931595, acc: 0.9915966391563416)
[2025-02-13 20:04:51,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51,299][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.22660928964614868, acc: 0.9652777910232544)
[2025-02-13 20:04:51,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51,698][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.033889010548591614, acc: 0.9933775067329407)
[2025-02-13 20:04:51,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52,096][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.15003718435764313, acc: 0.9666666388511658)
[2025-02-13 20:04:52,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52,540][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.1967279613018036, acc: 0.9379310607910156)
[2025-02-13 20:04:52,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52,950][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.18896183371543884, acc: 0.9642857313156128)
[2025-02-13 20:04:53,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53,352][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.07548575848340988, acc: 0.9818181991577148)
[2025-02-13 20:04:53,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53,750][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.12347768992185593, acc: 0.9743589758872986)
[2025-02-13 20:04:53,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54,123][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.08562152832746506, acc: 0.9759036302566528)
[2025-02-13 20:04:54,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54,558][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.07669702917337418, acc: 0.9818181991577148)
[2025-02-13 20:04:54,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54,941][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.09761368483304977, acc: 0.9764705896377563)
[2025-02-13 20:04:55,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55,305][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.06378118693828583, acc: 0.9931972622871399)
[2025-02-13 20:04:55,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55,678][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.020710278302431107, acc: 1.0)
[2025-02-13 20:04:55,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56,059][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.030332323163747787, acc: 0.9921259880065918)
[2025-02-13 20:04:56,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56,468][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.17791566252708435, acc: 0.9513888955116272)
[2025-02-13 20:04:56,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56,854][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.09101727604866028, acc: 0.9770992398262024)
[2025-02-13 20:04:56,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57,241][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.13185296952724457, acc: 0.9691358208656311)
[2025-02-13 20:04:57,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57,664][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.15160158276557922, acc: 0.9578313231468201)
[2025-02-13 20:04:57,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58,116][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.12267528474330902, acc: 0.9627329111099243)
[2025-02-13 20:04:58,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58,502][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.12537463009357452, acc: 0.9605262875556946)
[2025-02-13 20:04:58,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58,926][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.09880981594324112, acc: 0.9638554453849792)
[2025-02-13 20:04:59,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59,311][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.09345915913581848, acc: 0.9870967864990234)
[2025-02-13 20:04:59,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59,734][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.11605613678693771, acc: 0.9856114983558655)
[2025-02-13 20:04:59,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00,129][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.26029765605926514, acc: 0.9465649127960205)
[2025-02-13 20:05:00,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00,557][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.1631961613893509, acc: 0.957317054271698)
[2025-02-13 20:05:00,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00,949][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.16776950657367706, acc: 0.948051929473877)
[2025-02-13 20:05:01,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01,370][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.21930105984210968, acc: 0.9477124214172363)
[2025-02-13 20:05:01,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01,769][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.12445412576198578, acc: 0.9542483687400818)
[2025-02-13 20:05:01,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02,164][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.08691858500242233, acc: 0.9743589758872986)
[2025-02-13 20:05:02,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02,560][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.15309752523899078, acc: 0.9545454382896423)
[2025-02-13 20:05:02,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03,102][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.08383379131555557, acc: 0.9750000238418579)
[2025-02-13 20:05:03,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03,560][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.14179734885692596, acc: 0.9587628841400146)
[2025-02-13 20:05:03,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04,001][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.1431887149810791, acc: 0.9512194991111755)
[2025-02-13 20:05:04,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04,418][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.030876589938998222, acc: 0.9814814925193787)
[2025-02-13 20:05:04,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04,817][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.1508433222770691, acc: 0.9848484992980957)
[2025-02-13 20:05:04,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05,292][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.2516157329082489, acc: 0.957446813583374)
[2025-02-13 20:05:05,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05,732][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.18061524629592896, acc: 0.935251772403717)
[2025-02-13 20:05:05,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06,115][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.06431913375854492, acc: 0.9850746393203735)
[2025-02-13 20:05:06,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06,538][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.07915793359279633, acc: 0.9857142567634583)
[2025-02-13 20:05:06,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06,945][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.13960178196430206, acc: 0.9622641801834106)
[2025-02-13 20:05:07,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07,306][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.10262563824653625, acc: 0.9629629850387573)
[2025-02-13 20:05:07,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07,730][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.22789938747882843, acc: 0.9357798099517822)
[2025-02-13 20:05:07,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08,135][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.05558943375945091, acc: 0.9875776171684265)
[2025-02-13 20:05:08,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08,540][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.2625845968723297, acc: 0.9291338324546814)
[2025-02-13 20:05:08,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08,919][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.4103054404258728, acc: 0.8913043737411499)
[2025-02-13 20:05:09,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09,301][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.18209783732891083, acc: 0.9420289993286133)
[2025-02-13 20:05:09,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09,712][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.09893844276666641, acc: 0.9736841917037964)
[2025-02-13 20:05:09,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10,068][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.11293471604585648, acc: 0.9748427867889404)
[2025-02-13 20:05:10,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10,460][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.34156548976898193, acc: 0.918181836605072)
[2025-02-13 20:05:10,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10,859][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.14193575084209442, acc: 0.9622641801834106)
[2025-02-13 20:05:11,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11,330][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.21506629884243011, acc: 0.9655172228813171)
[2025-02-13 20:05:11,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11,723][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.1541513204574585, acc: 0.9548386931419373)
[2025-02-13 20:05:11,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12,130][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.12532243132591248, acc: 0.9671361446380615)
[2025-02-13 20:05:12,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12,526][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.17206451296806335, acc: 0.96517413854599)
[2025-02-13 20:05:12,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12,908][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.05601786449551582, acc: 0.991150438785553)
[2025-02-13 20:05:13,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13,319][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.05512983724474907, acc: 0.9949748516082764)
[2025-02-13 20:05:13,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13,776][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.14272548258304596, acc: 0.9818181991577148)
[2025-02-13 20:05:13,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14,236][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.2086630016565323, acc: 0.9444444179534912)
[2025-02-13 20:05:14,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14,648][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.12019368261098862, acc: 0.9617486596107483)
[2025-02-13 20:05:14,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15,078][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.11293592303991318, acc: 0.9742489457130432)
[2025-02-13 20:05:15,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15,521][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.06868388503789902, acc: 0.9897959232330322)
[2025-02-13 20:05:15,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15,940][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.1441676765680313, acc: 0.9686098694801331)
[2025-02-13 20:05:16,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16,414][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.1007622703909874, acc: 0.961240291595459)
[2025-02-13 20:05:16,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16,899][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.09425405412912369, acc: 0.9751037359237671)
[2025-02-13 20:05:17,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17,305][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.1016976609826088, acc: 0.9679999947547913)
[2025-02-13 20:05:17,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17,707][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.11430113017559052, acc: 0.9759615659713745)
[2025-02-13 20:05:17,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18,110][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.08678914606571198, acc: 0.9870129823684692)
[2025-02-13 20:05:18,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18,519][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.0667794942855835, acc: 0.9857819676399231)
[2025-02-13 20:05:18,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18,967][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.07386131584644318, acc: 0.9829787015914917)
[2025-02-13 20:05:19,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19,365][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.07165567576885223, acc: 0.9800000190734863)
[2025-02-13 20:05:19,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19,781][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.08358809351921082, acc: 0.9754601120948792)
[2025-02-13 20:05:19,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20,235][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.03768322989344597, acc: 0.9957982897758484)
[2025-02-13 20:05:20,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20,642][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.083397276699543, acc: 0.9747474789619446)
[2025-02-13 20:05:20,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21,077][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.052162691950798035, acc: 0.9865471124649048)
[2025-02-13 20:05:21,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21,506][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.10708210617303848, acc: 0.9809160232543945)
[2025-02-13 20:05:21,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21,926][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.17449140548706055, acc: 0.9657794833183289)
[2025-02-13 20:05:22,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22,335][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.11469906568527222, acc: 0.9744898080825806)
[2025-02-13 20:05:22,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22,749][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.16876360774040222, acc: 0.9766082167625427)
[2025-02-13 20:05:22,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23,213][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.1712893694639206, acc: 0.9627329111099243)
[2025-02-13 20:05:23,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23,622][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.12091246247291565, acc: 0.9671052694320679)
[2025-02-13 20:05:23,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24,050][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.18477153778076172, acc: 0.9589040875434875)
[2025-02-13 20:05:24,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24,447][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.12817256152629852, acc: 0.9743589758872986)
[2025-02-13 20:05:24,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24,848][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.2708348333835602, acc: 0.9432623982429504)
[2025-02-13 20:05:24,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25,246][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.1928519308567047, acc: 0.9640718698501587)
[2025-02-13 20:05:25,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25,650][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.10360708832740784, acc: 0.9817073345184326)
[2025-02-13 20:05:25,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26,063][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.10064750164747238, acc: 0.9823529124259949)
[2025-02-13 20:05:26,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26,477][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.11689221113920212, acc: 0.9811320900917053)
[2025-02-13 20:05:26,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26,897][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.1102219671010971, acc: 0.9726027250289917)
[2025-02-13 20:05:27,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27,311][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.1179950162768364, acc: 0.9793103337287903)
[2025-02-13 20:05:27,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27,700][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.06568891555070877, acc: 0.9858155846595764)
[2025-02-13 20:05:27,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28,058][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.10503623634576797, acc: 0.9878787994384766)
[2025-02-13 20:05:28,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28,443][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.048217423260211945, acc: 0.9924812316894531)
[2025-02-13 20:05:28,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28,836][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.08239664137363434, acc: 0.982300877571106)
[2025-02-13 20:05:28,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29,247][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.05827454477548599, acc: 0.9794520735740662)
[2025-02-13 20:05:29,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29,628][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.09289298206567764, acc: 0.9806451797485352)
[2025-02-13 20:05:29,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30,025][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.05021242797374725, acc: 1.0)
[2025-02-13 20:05:30,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30,423][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.057934705168008804, acc: 0.9858155846595764)
[2025-02-13 20:05:30,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30,807][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.07449781894683838, acc: 0.9837398529052734)
[2025-02-13 20:05:30,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31,205][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.07567606121301651, acc: 0.9795918464660645)
[2025-02-13 20:05:31,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31,636][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.10553862154483795, acc: 0.9793103337287903)
[2025-02-13 20:05:31,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32,053][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.10001515597105026, acc: 0.9747899174690247)
[2025-02-13 20:05:32,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32,450][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.16922174394130707, acc: 0.9624060392379761)
[2025-02-13 20:05:32,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32,817][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.06954453140497208, acc: 0.9860140085220337)
[2025-02-13 20:05:32,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33,214][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.05064476281404495, acc: 0.9790209531784058)
[2025-02-13 20:05:33,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33,608][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.05643971264362335, acc: 0.9850746393203735)
[2025-02-13 20:05:33,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33,999][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.09502808004617691, acc: 0.9626865386962891)
[2025-02-13 20:05:34,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34,371][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.14003878831863403, acc: 0.9666666388511658)
[2025-02-13 20:05:34,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34,778][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.10278905928134918, acc: 0.976047933101654)
[2025-02-13 20:05:34,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35,194][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.3078668713569641, acc: 0.9271523356437683)
[2025-02-13 20:05:35,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35,609][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.28059884905815125, acc: 0.9329268336296082)
[2025-02-13 20:05:35,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36,015][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.21238313615322113, acc: 0.9679999947547913)
[2025-02-13 20:05:36,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36,408][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.04061207175254822, acc: 1.0)
[2025-02-13 20:05:36,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36,828][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.27247482538223267, acc: 0.9482758641242981)
[2025-02-13 20:05:36,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37,211][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.08096713572740555, acc: 0.9925925731658936)
[2025-02-13 20:05:37,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37,575][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.12504678964614868, acc: 0.9599999785423279)
[2025-02-13 20:05:37,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37,971][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.14557509124279022, acc: 0.9662162065505981)
[2025-02-13 20:05:38,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38,331][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.08089303970336914, acc: 0.9919354915618896)
[2025-02-13 20:05:38,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38,698][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.067649707198143, acc: 0.9841269850730896)
[2025-02-13 20:05:38,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39,119][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.09605356305837631, acc: 0.9809523820877075)
[2025-02-13 20:05:39,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39,506][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.11960190534591675, acc: 0.9716312289237976)
[2025-02-13 20:05:39,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39,867][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.04719354957342148, acc: 0.9905660152435303)
[2025-02-13 20:05:40,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40,230][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.26301082968711853, acc: 0.9605262875556946)
[2025-02-13 20:05:40,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40,615][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.1138678714632988, acc: 0.9534883499145508)
[2025-02-13 20:05:40,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40,985][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.1611114740371704, acc: 0.9658119678497314)
[2025-02-13 20:05:41,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41,373][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.17880259454250336, acc: 0.976331353187561)
[2025-02-13 20:05:41,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41,773][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.20580261945724487, acc: 0.9307692050933838)
[2025-02-13 20:05:41,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42,176][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.09194938838481903, acc: 0.984375)
[2025-02-13 20:05:42,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42,616][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.06782833486795425, acc: 0.9898989796638489)
[2025-02-13 20:05:42,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42,999][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.21169166266918182, acc: 0.9567901492118835)
[2025-02-13 20:05:43,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43,428][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.1337512582540512, acc: 0.9765625)
[2025-02-13 20:05:43,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43,759][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.0610029436647892, acc: 1.0)
[2025-02-13 20:05:43,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44,119][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.09548444300889969, acc: 0.9677419066429138)
[2025-02-13 20:05:44,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44,543][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.06085333228111267, acc: 0.982300877571106)
[2025-02-13 20:05:44,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44,939][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.05466509982943535, acc: 0.9846153855323792)
[2025-02-13 20:05:45,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45,286][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.1441732496023178, acc: 0.954954981803894)
[2025-02-13 20:05:45,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45,682][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.08190374821424484, acc: 0.9813084006309509)
[2025-02-13 20:05:45,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46,065][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.300487756729126, acc: 0.9156626462936401)
[2025-02-13 20:05:46,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46,431][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.05233201012015343, acc: 0.9907407164573669)
[2025-02-13 20:05:46,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46,891][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.14506252110004425, acc: 0.9541284441947937)
[2025-02-13 20:05:47,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47,278][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.1959444284439087, acc: 0.9457831382751465)
[2025-02-13 20:05:47,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47,670][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.0693618655204773, acc: 0.9867549538612366)
[2025-02-13 20:05:47,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48,056][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.16506870090961456, acc: 0.9620253443717957)
[2025-02-13 20:05:48,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48,436][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.1224154531955719, acc: 0.9800000190734863)
[2025-02-13 20:05:48,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48,835][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.06629559397697449, acc: 0.9664804339408875)
[2025-02-13 20:05:48,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49,220][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.069781593978405, acc: 0.9849624037742615)
[2025-02-13 20:05:49,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49,596][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.07975789904594421, acc: 0.9797297120094299)
[2025-02-13 20:05:49,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49,985][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.0766056701540947, acc: 0.9813664555549622)
[2025-02-13 20:05:50,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50,344][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.18605932593345642, acc: 0.9602649211883545)
[2025-02-13 20:05:50,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50,717][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.1614142507314682, acc: 0.9795918464660645)
[2025-02-13 20:05:50,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51,085][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.09288235008716583, acc: 0.969924807548523)
[2025-02-13 20:05:51,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51,444][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.06764596700668335, acc: 0.9823529124259949)
[2025-02-13 20:05:51,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51,826][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.0588008388876915, acc: 0.9929078221321106)
[2025-02-13 20:05:51,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52,216][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.12644048035144806, acc: 0.957317054271698)
[2025-02-13 20:05:52,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52,607][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.07256818562746048, acc: 0.9823529124259949)
[2025-02-13 20:05:52,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52,990][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.1712699830532074, acc: 0.9685863852500916)
[2025-02-13 20:05:53,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53,371][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.17728763818740845, acc: 0.9822485446929932)
[2025-02-13 20:05:53,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53,744][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.18011592328548431, acc: 0.9583333134651184)
[2025-02-13 20:05:53,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54,120][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.229789599776268, acc: 0.9636363387107849)
[2025-02-13 20:05:54,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54,481][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.1637212634086609, acc: 0.9602649211883545)
[2025-02-13 20:05:54,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54,879][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.12038224935531616, acc: 0.9673202633857727)
[2025-02-13 20:05:55,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55,245][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.04843531548976898, acc: 1.0)
[2025-02-13 20:05:55,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55,615][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.1494203507900238, acc: 0.9800000190734863)
[2025-02-13 20:05:55,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56,020][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.05025699734687805, acc: 0.9917355179786682)
[2025-02-13 20:05:56,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56,422][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.09250905364751816, acc: 0.9823529124259949)
[2025-02-13 20:05:56,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56,821][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.06713367253541946, acc: 0.9868420958518982)
[2025-02-13 20:05:56,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57,204][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.04635041952133179, acc: 0.9918032884597778)
[2025-02-13 20:05:57,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57,576][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.05127518251538277, acc: 1.0)
[2025-02-13 20:05:57,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57,964][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.056960444897413254, acc: 0.9902912378311157)
[2025-02-13 20:05:58,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58,358][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.0761423110961914, acc: 0.9863013625144958)
[2025-02-13 20:05:58,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58,740][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.16669337451457977, acc: 0.975806474685669)
[2025-02-13 20:05:58,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59,104][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.0975336879491806, acc: 0.9542483687400818)
[2025-02-13 20:05:59,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59,465][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.13476301729679108, acc: 0.9740259647369385)
[2025-02-13 20:05:59,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59,841][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.2027333527803421, acc: 0.9661017060279846)
[2025-02-13 20:05:59,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00,250][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.06057167425751686, acc: 0.9821428656578064)
[2025-02-13 20:06:00,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00,640][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.10791971534490585, acc: 0.9599999785423279)
[2025-02-13 20:06:00,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01,015][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.10700167715549469, acc: 0.9794520735740662)
[2025-02-13 20:06:01,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01,395][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.12662047147750854, acc: 0.9675324559211731)
[2025-02-13 20:06:01,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01,789][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.13467150926589966, acc: 0.9571428298950195)
[2025-02-13 20:06:01,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02,174][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.08833406120538712, acc: 0.9858155846595764)
[2025-02-13 20:06:02,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02,575][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.041704315692186356, acc: 1.0)
[2025-02-13 20:06:02,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02,982][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.05516006425023079, acc: 0.9931507110595703)
[2025-02-13 20:06:03,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03,337][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.07936438918113708, acc: 0.9655172228813171)
[2025-02-13 20:06:03,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03,698][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.03533457964658737, acc: 0.9808917045593262)
[2025-02-13 20:06:03,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04,105][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.20108459889888763, acc: 0.9496855139732361)
[2025-02-13 20:06:04,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04,488][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.1276087909936905, acc: 0.982300877571106)
[2025-02-13 20:06:04,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04,894][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.04823782294988632, acc: 1.0)
[2025-02-13 20:06:06,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:50,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10,083][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2625, device='cuda:0') eval_epoch_loss=tensor(0.2331, device='cuda:0') eval_epoch_acc=tensor(0.9465, device='cuda:0')
[2025-02-13 20:10:10,086][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:10:10,087][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:10:10,386][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_1781_loss_0.23309457302093506/model.pt
[2025-02-13 20:10:10,392][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:10:10,393][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.23309457302093506
[2025-02-13 20:10:10,393][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9465076923370361
[2025-02-13 20:10:10,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10,842][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.06064863130450249, acc: 0.9798657894134521)
[2025-02-13 20:10:10,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11,207][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.03277727589011192, acc: 0.9935897588729858)
[2025-02-13 20:10:11,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11,589][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.0994749665260315, acc: 0.9710982441902161)
[2025-02-13 20:10:11,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11,971][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.07133372128009796, acc: 0.9935897588729858)
[2025-02-13 20:10:12,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12,353][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.12764333188533783, acc: 0.9578313231468201)
[2025-02-13 20:10:12,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12,733][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.048015251755714417, acc: 1.0)
[2025-02-13 20:10:12,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13,121][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.15134143829345703, acc: 0.9626865386962891)
[2025-02-13 20:10:13,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13,477][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.12653368711471558, acc: 0.9793103337287903)
[2025-02-13 20:10:13,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13,853][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.10286734253168106, acc: 0.9802631735801697)
[2025-02-13 20:10:14,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14,235][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.33681485056877136, acc: 0.9290322661399841)
[2025-02-13 20:10:14,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14,622][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.3712834417819977, acc: 0.9386503100395203)
[2025-02-13 20:10:14,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14,973][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.6406745910644531, acc: 0.8938547372817993)
[2025-02-13 20:10:15,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15,351][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.36636626720428467, acc: 0.9349112510681152)
[2025-02-13 20:10:15,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15,738][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.3376375138759613, acc: 0.9276315569877625)
[2025-02-13 20:10:15,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16,110][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.3650044798851013, acc: 0.8999999761581421)
[2025-02-13 20:10:16,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16,506][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.3532712161540985, acc: 0.9152542352676392)
[2025-02-13 20:10:16,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16,890][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.278685063123703, acc: 0.9142857193946838)
[2025-02-13 20:10:17,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17,328][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.3143383264541626, acc: 0.9224806427955627)
[2025-02-13 20:10:17,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17,734][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.3168017268180847, acc: 0.9191918969154358)
[2025-02-13 20:10:17,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18,113][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.22500601410865784, acc: 0.938095211982727)
[2025-02-13 20:10:18,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18,490][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.2771458923816681, acc: 0.9459459185600281)
[2025-02-13 20:10:18,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18,856][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.201494038105011, acc: 0.9521276354789734)
[2025-02-13 20:10:18,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19,231][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.3066960275173187, acc: 0.9207317233085632)
[2025-02-13 20:10:19,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19,657][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.07845373451709747, acc: 0.9735449552536011)
[2025-02-13 20:10:19,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20,025][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.13575580716133118, acc: 0.9766082167625427)
[2025-02-13 20:10:20,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20,393][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.13623638451099396, acc: 0.9508196711540222)
[2025-02-13 20:10:20,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20,766][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.204396054148674, acc: 0.9578313231468201)
[2025-02-13 20:10:20,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21,135][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.06916973739862442, acc: 0.9830508232116699)
[2025-02-13 20:10:21,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21,518][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.06748567521572113, acc: 0.9884393215179443)
[2025-02-13 20:10:21,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21,945][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.08592686057090759, acc: 0.9835164546966553)
[2025-02-13 20:10:22,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22,326][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.21155765652656555, acc: 0.957446813583374)
[2025-02-13 20:10:22,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22,706][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.13413777947425842, acc: 0.9717513918876648)
[2025-02-13 20:10:22,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23,092][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.1648210883140564, acc: 0.9613259434700012)
[2025-02-13 20:10:23,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23,468][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.09458935260772705, acc: 0.988950252532959)
[2025-02-13 20:10:23,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23,803][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.12694625556468964, acc: 0.9745222926139832)
[2025-02-13 20:10:23,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24,167][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.18524973094463348, acc: 0.9513888955116272)
[2025-02-13 20:10:24,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24,551][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.12207362055778503, acc: 0.9583333134651184)
[2025-02-13 20:10:24,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24,942][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.13635393977165222, acc: 0.9685863852500916)
[2025-02-13 20:10:25,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25,305][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.08109889179468155, acc: 0.9767441749572754)
[2025-02-13 20:10:25,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25,713][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.08306589722633362, acc: 0.9858490824699402)
[2025-02-13 20:10:25,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26,079][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.07564859837293625, acc: 0.9766355156898499)
[2025-02-13 20:10:26,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26,436][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.0527469739317894, acc: 0.9934210777282715)
[2025-02-13 20:10:26,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26,806][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.12404987215995789, acc: 0.9767441749572754)
[2025-02-13 20:10:26,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27,174][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.048175398260354996, acc: 0.9890710115432739)
[2025-02-13 20:10:27,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27,558][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.058065664023160934, acc: 0.9906542301177979)
[2025-02-13 20:10:27,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27,968][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.05066012218594551, acc: 0.9824561476707458)
[2025-02-13 20:10:28,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28,368][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.10898688435554504, acc: 0.9763033390045166)
[2025-02-13 20:10:28,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28,767][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.05309337377548218, acc: 0.9777777791023254)
[2025-02-13 20:10:28,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29,164][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.07556670159101486, acc: 0.9838709831237793)
[2025-02-13 20:10:29,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29,543][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.06726823002099991, acc: 0.9792746305465698)
[2025-02-13 20:10:29,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29,879][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.062244921922683716, acc: 0.9772727489471436)
[2025-02-13 20:10:30,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30,248][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.1651952564716339, acc: 0.9534883499145508)
[2025-02-13 20:10:30,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30,657][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.515930712223053, acc: 0.9024389982223511)
[2025-02-13 20:10:30,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31,037][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.054623957723379135, acc: 0.9871794581413269)
[2025-02-13 20:10:31,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31,397][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.20378246903419495, acc: 0.9415584206581116)
[2025-02-13 20:10:31,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31,771][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.14855505526065826, acc: 0.9679144620895386)
[2025-02-13 20:10:31,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32,161][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.10632870346307755, acc: 0.9670329689979553)
[2025-02-13 20:10:32,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32,500][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.11823153495788574, acc: 0.9816513657569885)
[2025-02-13 20:10:32,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32,883][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.09276199340820312, acc: 0.9670329689979553)
[2025-02-13 20:10:33,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33,247][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.1430639624595642, acc: 0.9620253443717957)
[2025-02-13 20:10:33,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33,634][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.14617878198623657, acc: 0.9636363387107849)
[2025-02-13 20:10:33,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34,003][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.14500141143798828, acc: 0.9634146094322205)
[2025-02-13 20:10:34,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34,368][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.15817944705486298, acc: 0.9702380895614624)
[2025-02-13 20:10:34,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34,761][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.1033441498875618, acc: 0.9716312289237976)
[2025-02-13 20:10:34,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35,143][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.0747494250535965, acc: 0.989130437374115)
[2025-02-13 20:10:35,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35,585][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.0999390259385109, acc: 0.9743589758872986)
[2025-02-13 20:10:35,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35,979][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.12015867233276367, acc: 0.9772727489471436)
[2025-02-13 20:10:36,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36,390][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.1472042053937912, acc: 0.9680851101875305)
[2025-02-13 20:10:36,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36,790][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.09016454219818115, acc: 0.9647887349128723)
[2025-02-13 20:10:36,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37,192][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.2232436090707779, acc: 0.938144326210022)
[2025-02-13 20:10:37,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37,595][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.13902337849140167, acc: 0.9487179517745972)
[2025-02-13 20:10:37,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38,039][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.26750656962394714, acc: 0.9437500238418579)
[2025-02-13 20:10:38,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38,421][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.17417973279953003, acc: 0.9497487545013428)
[2025-02-13 20:10:38,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38,792][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.1818254441022873, acc: 0.959276020526886)
[2025-02-13 20:10:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39,172][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.2379373162984848, acc: 0.9431279897689819)
[2025-02-13 20:10:39,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39,529][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.12124449759721756, acc: 0.9591836929321289)
[2025-02-13 20:10:39,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39,922][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.23836511373519897, acc: 0.9264705777168274)
[2025-02-13 20:10:40,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40,281][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.106581911444664, acc: 0.9693877696990967)
[2025-02-13 20:10:40,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40,643][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.0933147743344307, acc: 0.9856459498405457)
[2025-02-13 20:10:40,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41,010][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.1161259338259697, acc: 0.9672130942344666)
[2025-02-13 20:10:41,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41,381][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.11750246584415436, acc: 0.9738219976425171)
[2025-02-13 20:10:41,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41,751][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.0520206093788147, acc: 0.9894179701805115)
[2025-02-13 20:10:41,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42,126][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.11944743245840073, acc: 0.9802955389022827)
[2025-02-13 20:10:42,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42,571][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.4438976049423218, acc: 0.9187816977500916)
[2025-02-13 20:10:42,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42,965][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.1854398399591446, acc: 0.9731543660163879)
[2025-02-13 20:10:43,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43,343][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.0351732075214386, acc: 0.9941176176071167)
[2025-02-13 20:10:43,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43,756][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.08353201299905777, acc: 0.9829545617103577)
[2025-02-13 20:10:43,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44,152][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.23014633357524872, acc: 0.9386503100395203)
[2025-02-13 20:10:44,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44,552][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.11954602599143982, acc: 0.9794520735740662)
[2025-02-13 20:10:44,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44,905][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.05574136599898338, acc: 0.9891892075538635)
[2025-02-13 20:10:45,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45,298][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.26056352257728577, acc: 0.9548386931419373)
[2025-02-13 20:10:45,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45,668][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 1.488696813583374, acc: 0.695652186870575)
[2025-02-13 20:10:45,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46,069][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.5423002243041992, acc: 0.8925619721412659)
[2025-02-13 20:10:46,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46,463][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.07038075476884842, acc: 0.9942528605461121)
[2025-02-13 20:10:46,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46,844][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.10513774305582047, acc: 0.9553571343421936)
[2025-02-13 20:10:46,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47,263][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.4465840458869934, acc: 0.8571428656578064)
[2025-02-13 20:10:47,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47,693][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.08252520114183426, acc: 0.9774011373519897)
[2025-02-13 20:10:47,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48,068][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.08113755285739899, acc: 0.9803921580314636)
[2025-02-13 20:10:48,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48,427][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.6242859959602356, acc: 0.8765432238578796)
[2025-02-13 20:10:48,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48,804][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.17814260721206665, acc: 0.9593495726585388)
[2025-02-13 20:10:48,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49,198][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.11860968172550201, acc: 0.9577465057373047)
[2025-02-13 20:10:49,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49,615][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.2149326652288437, acc: 0.95652174949646)
[2025-02-13 20:10:49,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50,033][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.22333352267742157, acc: 0.9457364082336426)
[2025-02-13 20:10:50,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50,424][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.16834552586078644, acc: 0.9594594836235046)
[2025-02-13 20:10:50,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50,797][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.2997439503669739, acc: 0.9507042169570923)
[2025-02-13 20:10:50,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51,178][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.27098408341407776, acc: 0.9285714030265808)
[2025-02-13 20:10:51,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51,554][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.21916650235652924, acc: 0.9536423683166504)
[2025-02-13 20:10:51,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51,939][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.1290900856256485, acc: 0.9743589758872986)
[2025-02-13 20:10:52,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52,316][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.12898030877113342, acc: 0.9824561476707458)
[2025-02-13 20:10:52,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52,692][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.1054326519370079, acc: 0.9602649211883545)
[2025-02-13 20:10:52,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53,063][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.2396310269832611, acc: 0.9186046719551086)
[2025-02-13 20:10:53,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53,453][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.15544261038303375, acc: 0.9640718698501587)
[2025-02-13 20:10:53,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53,867][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.18931160867214203, acc: 0.9382715821266174)
[2025-02-13 20:10:54,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54,303][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.2064848393201828, acc: 0.9594594836235046)
[2025-02-13 20:10:54,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54,717][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.5957300662994385, acc: 0.8651685118675232)
[2025-02-13 20:10:54,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55,180][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.16451208293437958, acc: 0.9454545378684998)
[2025-02-13 20:10:55,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55,562][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.13247962296009064, acc: 0.9647887349128723)
[2025-02-13 20:10:55,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55,932][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.12333979457616806, acc: 0.9793103337287903)
[2025-02-13 20:10:56,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56,311][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.0608457513153553, acc: 1.0)
[2025-02-13 20:10:56,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56,733][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.0667654499411583, acc: 0.9852941036224365)
[2025-02-13 20:10:56,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57,116][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.13919682800769806, acc: 0.9856114983558655)
[2025-02-13 20:10:57,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57,500][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.10876483470201492, acc: 0.971222996711731)
[2025-02-13 20:10:57,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57,878][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.09065958112478256, acc: 0.9738562107086182)
[2025-02-13 20:10:58,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58,245][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.20339713990688324, acc: 0.9428571462631226)
[2025-02-13 20:10:58,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58,575][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.12125683575868607, acc: 0.978723406791687)
[2025-02-13 20:10:58,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58,988][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.09834194928407669, acc: 0.985401451587677)
[2025-02-13 20:10:59,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59,399][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.1358187347650528, acc: 0.9795918464660645)
[2025-02-13 20:10:59,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59,778][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.17154328525066376, acc: 0.9766355156898499)
[2025-02-13 20:10:59,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00,151][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.17701372504234314, acc: 0.9591836929321289)
[2025-02-13 20:11:00,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00,571][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.10295175760984421, acc: 0.9767441749572754)
[2025-02-13 20:11:00,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01,026][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.08554226160049438, acc: 0.9796954393386841)
[2025-02-13 20:11:01,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01,437][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.046884581446647644, acc: 0.9757575988769531)
[2025-02-13 20:11:01,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01,845][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.16665925085544586, acc: 0.9685039520263672)
[2025-02-13 20:11:01,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02,274][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.1703333705663681, acc: 0.9617486596107483)
[2025-02-13 20:11:02,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02,682][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.07906442135572433, acc: 0.9860140085220337)
[2025-02-13 20:11:02,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03,085][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.09659436345100403, acc: 0.9725274443626404)
[2025-02-13 20:11:03,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03,490][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.06547514349222183, acc: 0.9878787994384766)
[2025-02-13 20:11:03,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03,842][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.15512828528881073, acc: 0.9668874144554138)
[2025-02-13 20:11:03,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04,228][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.0868355929851532, acc: 0.9817073345184326)
[2025-02-13 20:11:04,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04,629][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.05155090242624283, acc: 0.9931972622871399)
[2025-02-13 20:11:04,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04,996][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.09064775705337524, acc: 0.9766082167625427)
[2025-02-13 20:11:05,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05,397][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.050449758768081665, acc: 0.9836065769195557)
[2025-02-13 20:11:05,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05,779][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.04550368711352348, acc: 0.9930070042610168)
[2025-02-13 20:11:05,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06,146][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.14659816026687622, acc: 0.9453125)
[2025-02-13 20:11:06,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06,521][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.1291639357805252, acc: 0.9748954176902771)
[2025-02-13 20:11:06,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06,884][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.05693592503666878, acc: 0.9910714030265808)
[2025-02-13 20:11:07,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07,256][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.1401837170124054, acc: 0.9716981053352356)
[2025-02-13 20:11:07,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07,634][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.08188154548406601, acc: 0.988095223903656)
[2025-02-13 20:11:07,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07,977][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.05603209510445595, acc: 0.9888268113136292)
[2025-02-13 20:11:08,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08,347][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.245293989777565, acc: 0.9227052927017212)
[2025-02-13 20:11:08,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08,720][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.3107753098011017, acc: 0.9351851940155029)
[2025-02-13 20:11:08,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09,078][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.17041638493537903, acc: 0.9664804339408875)
[2025-02-13 20:11:09,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09,474][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.28316715359687805, acc: 0.9396985173225403)
[2025-02-13 20:11:09,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09,833][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.06620492041110992, acc: 0.9828571677207947)
[2025-02-13 20:11:09,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10,217][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.36868026852607727, acc: 0.9390243887901306)
[2025-02-13 20:11:10,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10,603][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.06432514637708664, acc: 0.9738562107086182)
[2025-02-13 20:11:10,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11,048][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.1865222007036209, acc: 0.9512194991111755)
[2025-02-13 20:11:11,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11,473][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.3414457440376282, acc: 0.9239130616188049)
[2025-02-13 20:11:11,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11,904][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.11066900938749313, acc: 0.9568965435028076)
[2025-02-13 20:11:12,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12,270][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.18897542357444763, acc: 0.940119743347168)
[2025-02-13 20:11:12,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12,645][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.09374789148569107, acc: 0.9704142212867737)
[2025-02-13 20:11:12,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13,048][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.08244498819112778, acc: 0.9722222089767456)
[2025-02-13 20:11:13,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13,430][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.08046791702508926, acc: 0.9789473414421082)
[2025-02-13 20:11:13,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13,804][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.061187442392110825, acc: 0.9836065769195557)
[2025-02-13 20:11:13,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14,170][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.04576112702488899, acc: 1.0)
[2025-02-13 20:11:14,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14,568][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.11643014848232269, acc: 0.9714285731315613)
[2025-02-13 20:11:14,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15,026][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.11367665231227875, acc: 0.9768785834312439)
[2025-02-13 20:11:15,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15,497][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.10394084453582764, acc: 0.9615384340286255)
[2025-02-13 20:11:15,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15,903][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.1830926239490509, acc: 0.9870129823684692)
[2025-02-13 20:11:16,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16,312][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.07058852165937424, acc: 0.9855072498321533)
[2025-02-13 20:11:16,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16,757][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.08326876908540726, acc: 0.9849624037742615)
[2025-02-13 20:11:16,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17,161][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.06574684381484985, acc: 0.9819276928901672)
[2025-02-13 20:11:17,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17,567][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.057476453483104706, acc: 0.9873417615890503)
[2025-02-13 20:11:17,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17,962][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.03512471541762352, acc: 1.0)
[2025-02-13 20:11:18,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18,362][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.05428381636738777, acc: 1.0)
[2025-02-13 20:11:18,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18,716][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.02768438309431076, acc: 1.0)
[2025-02-13 20:11:18,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19,107][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.10850245505571365, acc: 0.9624060392379761)
[2025-02-13 20:11:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19,449][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.13129045069217682, acc: 0.9555555582046509)
[2025-02-13 20:11:19,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19,831][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.10729098320007324, acc: 0.9732142686843872)
[2025-02-13 20:11:19,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20,226][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.051344260573387146, acc: 0.9833333492279053)
[2025-02-13 20:11:20,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20,628][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.045993365347385406, acc: 0.9873417615890503)
[2025-02-13 20:11:20,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21,029][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.03680003434419632, acc: 1.0)
[2025-02-13 20:11:21,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21,457][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.15549318492412567, acc: 0.9602649211883545)
[2025-02-13 20:11:21,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21,883][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.1251119077205658, acc: 0.9797979593276978)
[2025-02-13 20:11:22,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22,296][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.12046699225902557, acc: 0.966292142868042)
[2025-02-13 20:11:22,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22,740][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.1996966004371643, acc: 0.9617834687232971)
[2025-02-13 20:11:22,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23,210][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.13310639560222626, acc: 0.9684210419654846)
[2025-02-13 20:11:23,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23,670][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.08321298658847809, acc: 0.9680851101875305)
[2025-02-13 20:11:23,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24,068][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.12010335177183151, acc: 0.9661017060279846)
[2025-02-13 20:11:24,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24,454][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.10705890506505966, acc: 0.9822485446929932)
[2025-02-13 20:11:24,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24,833][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.3678518235683441, acc: 0.9371727705001831)
[2025-02-13 20:11:25,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25,270][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.38009920716285706, acc: 0.9210526347160339)
[2025-02-13 20:11:25,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25,687][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.634811520576477, acc: 0.8680555820465088)
[2025-02-13 20:11:25,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26,139][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.10539129376411438, acc: 0.9742268323898315)
[2025-02-13 20:11:26,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26,541][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.03471320495009422, acc: 0.9937499761581421)
[2025-02-13 20:11:26,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26,924][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.23460902273654938, acc: 0.9471153616905212)
[2025-02-13 20:11:27,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27,300][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.08522709459066391, acc: 0.9880239367485046)
[2025-02-13 20:11:27,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27,677][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.054117489606142044, acc: 0.9941860437393188)
[2025-02-13 20:11:27,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28,077][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.0635540634393692, acc: 0.9890109896659851)
[2025-02-13 20:11:28,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28,474][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.13552115857601166, acc: 0.9790576100349426)
[2025-02-13 20:11:28,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28,868][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.12049245089292526, acc: 0.9780219793319702)
[2025-02-13 20:11:29,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29,256][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.07739244401454926, acc: 0.9743589758872986)
[2025-02-13 20:11:29,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29,640][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.08000915497541428, acc: 0.9892473220825195)
[2025-02-13 20:11:29,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30,022][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.06817393749952316, acc: 0.9825581312179565)
[2025-02-13 20:11:30,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30,397][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.1033758819103241, acc: 0.9689440727233887)
[2025-02-13 20:11:30,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30,778][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.029089083895087242, acc: 0.9888268113136292)
[2025-02-13 20:11:30,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31,226][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.12854400277137756, acc: 0.966292142868042)
[2025-02-13 20:11:31,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31,621][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.11913959681987762, acc: 0.9757575988769531)
[2025-02-13 20:11:31,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32,011][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.14408017694950104, acc: 0.9726027250289917)
[2025-02-13 20:11:32,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32,384][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.19661279022693634, acc: 0.9723756909370422)
[2025-02-13 20:11:32,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32,775][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.06330209225416183, acc: 0.9838709831237793)
[2025-02-13 20:11:32,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33,160][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.22580119967460632, acc: 0.9347826242446899)
[2025-02-13 20:11:33,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33,601][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.08885079622268677, acc: 0.9826086759567261)
[2025-02-13 20:11:33,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34,047][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.05254039913415909, acc: 0.9922480583190918)
[2025-02-13 20:11:34,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34,483][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.060857221484184265, acc: 0.9862068891525269)
[2025-02-13 20:11:34,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34,942][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.09739359468221664, acc: 0.9635036587715149)
[2025-02-13 20:11:35,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35,355][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.10041515529155731, acc: 0.9714285731315613)
[2025-02-13 20:11:35,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35,752][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.12316384166479111, acc: 0.9647887349128723)
[2025-02-13 20:11:35,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36,143][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.1202273890376091, acc: 0.9580419659614563)
[2025-02-13 20:11:36,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36,544][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.08659341931343079, acc: 0.9861111044883728)
[2025-02-13 20:11:36,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36,905][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.05784681439399719, acc: 0.9862068891525269)
[2025-02-13 20:11:37,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37,301][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.10419730842113495, acc: 0.9849624037742615)
[2025-02-13 20:11:37,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37,663][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.030966557562351227, acc: 0.9912280440330505)
[2025-02-13 20:11:37,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38,111][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.48904484510421753, acc: 0.8934426307678223)
[2025-02-13 20:11:38,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38,575][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.21458427608013153, acc: 0.918367326259613)
[2025-02-13 20:11:38,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38,933][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.026513492688536644, acc: 0.9883720874786377)
[2025-02-13 20:11:39,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39,302][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.06523669511079788, acc: 0.9747899174690247)
[2025-02-13 20:11:39,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39,675][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.10139212012290955, acc: 0.9726027250289917)
[2025-02-13 20:11:39,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40,063][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.15224161744117737, acc: 0.9629629850387573)
[2025-02-13 20:11:40,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40,492][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.06902599334716797, acc: 0.9781022071838379)
[2025-02-13 20:11:40,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40,843][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.05636036768555641, acc: 0.9841269850730896)
[2025-02-13 20:11:40,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41,240][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.0826496034860611, acc: 0.9745762944221497)
[2025-02-13 20:11:41,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41,672][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.0424795001745224, acc: 0.9918032884597778)
[2025-02-13 20:11:41,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42,088][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.05336439982056618, acc: 0.9876543283462524)
[2025-02-13 20:11:42,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42,463][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.03266448155045509, acc: 0.9937499761581421)
[2025-02-13 20:11:42,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42,834][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.16580414772033691, acc: 0.9523809552192688)
[2025-02-13 20:11:42,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43,200][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.20176851749420166, acc: 0.9536423683166504)
[2025-02-13 20:11:43,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43,590][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.03445126861333847, acc: 1.0)
[2025-02-13 20:11:43,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43,980][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.06054719164967537, acc: 0.9867549538612366)
[2025-02-13 20:11:44,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44,413][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.11760411411523819, acc: 0.9586777091026306)
[2025-02-13 20:11:44,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44,870][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.1326865702867508, acc: 0.9621621370315552)
[2025-02-13 20:11:44,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45,221][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.17707288265228271, acc: 0.9552238583564758)
[2025-02-13 20:11:45,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45,605][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.10721731185913086, acc: 0.9693251252174377)
[2025-02-13 20:11:45,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46,003][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.08415775001049042, acc: 0.9767441749572754)
[2025-02-13 20:11:46,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46,414][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.19049592316150665, acc: 0.9766082167625427)
[2025-02-13 20:11:46,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46,837][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.07473216205835342, acc: 0.9824561476707458)
[2025-02-13 20:11:46,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47,261][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.07562894374132156, acc: 0.9849624037742615)
[2025-02-13 20:11:47,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47,707][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.10822068154811859, acc: 0.9693251252174377)
[2025-02-13 20:11:47,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48,104][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.139298215508461, acc: 0.9588235020637512)
[2025-02-13 20:11:48,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48,485][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.14264893531799316, acc: 0.9518072009086609)
[2025-02-13 20:11:48,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48,913][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.07008535414934158, acc: 0.9834254384040833)
[2025-02-13 20:11:49,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49,362][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.24050383269786835, acc: 0.9415204524993896)
[2025-02-13 20:11:49,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49,815][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.0352429561316967, acc: 0.9923664331436157)
[2025-02-13 20:11:49,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50,262][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.24472109973430634, acc: 0.9328858852386475)
[2025-02-13 20:11:50,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50,687][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.05591903626918793, acc: 0.9946236610412598)
[2025-02-13 20:11:50,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51,069][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.05364974960684776, acc: 0.9923664331436157)
[2025-02-13 20:11:51,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51,444][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.07704632729291916, acc: 0.9815950989723206)
[2025-02-13 20:11:51,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51,826][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.050714101642370224, acc: 0.9780219793319702)
[2025-02-13 20:11:51,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52,254][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.04513685032725334, acc: 0.9927007555961609)
[2025-02-13 20:11:52,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52,781][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.18915243446826935, acc: 0.9741379022598267)
[2025-02-13 20:11:52,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53,226][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.07083368301391602, acc: 0.9878787994384766)
[2025-02-13 20:11:53,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53,712][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.06194177642464638, acc: 0.9830508232116699)
[2025-02-13 20:11:53,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54,114][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.050392262637615204, acc: 0.9932885766029358)
[2025-02-13 20:11:54,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54,555][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.06380156427621841, acc: 0.9919999837875366)
[2025-02-13 20:11:54,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54,971][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.10154637694358826, acc: 0.9750000238418579)
[2025-02-13 20:11:55,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55,383][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.20473521947860718, acc: 0.9579831957817078)
[2025-02-13 20:11:55,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55,797][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.2171102911233902, acc: 0.931034505367279)
[2025-02-13 20:11:55,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56,186][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.2659551203250885, acc: 0.9464285969734192)
[2025-02-13 20:11:56,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56,593][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.3224160075187683, acc: 0.9363636374473572)
[2025-02-13 20:11:56,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56,954][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.1883547008037567, acc: 0.9520547986030579)
[2025-02-13 20:11:57,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57,326][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.13029712438583374, acc: 0.9557521939277649)
[2025-02-13 20:11:57,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57,772][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.18754927814006805, acc: 0.9513888955116272)
[2025-02-13 20:11:57,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58,167][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.3219933807849884, acc: 0.9465649127960205)
[2025-02-13 20:11:58,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58,548][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.29217737913131714, acc: 0.9294871687889099)
[2025-02-13 20:11:58,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58,920][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.1607159823179245, acc: 0.965753436088562)
[2025-02-13 20:11:59,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59,315][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.2558368146419525, acc: 0.9322034120559692)
[2025-02-13 20:11:59,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59,786][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.15755009651184082, acc: 0.9606741666793823)
[2025-02-13 20:11:59,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00,228][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.3047740161418915, acc: 0.9421965479850769)
[2025-02-13 20:12:00,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00,599][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.31652045249938965, acc: 0.9219858050346375)
[2025-02-13 20:12:00,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00,974][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.15598490834236145, acc: 0.9490445852279663)
[2025-02-13 20:12:01,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01,359][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.24102015793323517, acc: 0.9523809552192688)
[2025-02-13 20:12:01,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01,744][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.2470598667860031, acc: 0.9453125)
[2025-02-13 20:12:01,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02,156][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.107747882604599, acc: 0.9645389914512634)
[2025-02-13 20:12:02,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02,544][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.127996563911438, acc: 0.9779411554336548)
[2025-02-13 20:12:02,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02,918][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.1530732810497284, acc: 0.9785714149475098)
[2025-02-13 20:12:03,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03,320][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.09899099171161652, acc: 0.9837398529052734)
[2025-02-13 20:12:03,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03,707][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.1572355329990387, acc: 0.9583333134651184)
[2025-02-13 20:12:03,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04,137][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.10016115009784698, acc: 0.9509803652763367)
[2025-02-13 20:12:04,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04,594][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.08838336169719696, acc: 0.9865771532058716)
[2025-02-13 20:12:04,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04,997][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.036110054701566696, acc: 1.0)
[2025-02-13 20:12:05,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05,443][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.020853359252214432, acc: 1.0)
[2025-02-13 20:12:05,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05,858][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.06460992991924286, acc: 0.970588207244873)
[2025-02-13 20:12:06,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06,310][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.08287880569696426, acc: 0.9752066135406494)
[2025-02-13 20:12:06,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06,718][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.14103730022907257, acc: 0.9738219976425171)
[2025-02-13 20:12:06,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07,131][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.0926176905632019, acc: 0.9685534834861755)
[2025-02-13 20:12:07,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07,528][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.11130159348249435, acc: 0.9795918464660645)
[2025-02-13 20:12:07,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07,925][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.12863750755786896, acc: 0.9661017060279846)
[2025-02-13 20:12:08,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08,339][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.09492088109254837, acc: 0.9796954393386841)
[2025-02-13 20:12:08,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08,752][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.11750941723585129, acc: 0.9578313231468201)
[2025-02-13 20:12:08,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09,137][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.09908131510019302, acc: 0.9738562107086182)
[2025-02-13 20:12:09,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09,548][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.08432050794363022, acc: 0.9745222926139832)
[2025-02-13 20:12:09,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10,003][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.11248227208852768, acc: 0.9661017060279846)
[2025-02-13 20:12:10,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10,400][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.10590793192386627, acc: 0.9767441749572754)
[2025-02-13 20:12:10,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10,821][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.10339450091123581, acc: 0.9666666388511658)
[2025-02-13 20:12:10,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11,188][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.01652584783732891, acc: 1.0)
[2025-02-13 20:12:11,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11,554][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.03029564395546913, acc: 1.0)
[2025-02-13 20:12:11,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11,971][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.019759073853492737, acc: 1.0)
[2025-02-13 20:12:12,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12,353][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.18662875890731812, acc: 0.9644970297813416)
[2025-02-13 20:12:12,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12,788][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.11475978791713715, acc: 0.9702380895614624)
[2025-02-13 20:12:12,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13,172][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.035867173224687576, acc: 1.0)
[2025-02-13 20:12:13,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13,556][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.02914181724190712, acc: 0.9931034445762634)
[2025-02-13 20:12:13,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14,020][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.050272829830646515, acc: 0.9867549538612366)
[2025-02-13 20:12:14,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14,476][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.056241411715745926, acc: 0.9894737005233765)
[2025-02-13 20:12:14,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14,880][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.12408187985420227, acc: 0.9552238583564758)
[2025-02-13 20:12:15,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15,289][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.08770398050546646, acc: 0.9800994992256165)
[2025-02-13 20:12:15,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15,657][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.06421486288309097, acc: 0.9860140085220337)
[2025-02-13 20:12:15,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16,016][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.04368515685200691, acc: 0.9939758777618408)
[2025-02-13 20:12:16,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16,440][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.07023786008358002, acc: 0.9671052694320679)
[2025-02-13 20:12:16,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16,884][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.050663478672504425, acc: 0.984375)
[2025-02-13 20:12:17,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17,297][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.1092468798160553, acc: 0.9657142758369446)
[2025-02-13 20:12:17,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17,686][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.06355901807546616, acc: 0.9821428656578064)
[2025-02-13 20:12:17,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18,046][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.05306970328092575, acc: 0.9837837815284729)
[2025-02-13 20:12:18,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18,452][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.0793914794921875, acc: 0.981249988079071)
[2025-02-13 20:12:18,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18,925][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.029780300334095955, acc: 1.0)
[2025-02-13 20:12:19,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19,321][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.06205637753009796, acc: 0.9929078221321106)
[2025-02-13 20:12:19,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19,716][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.08825832605361938, acc: 0.9671052694320679)
[2025-02-13 20:12:19,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20,091][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.06679193675518036, acc: 0.9779005646705627)
[2025-02-13 20:12:20,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20,528][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.05505915358662605, acc: 0.9937888383865356)
[2025-02-13 20:12:20,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20,919][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.1296825408935547, acc: 0.9710144996643066)
[2025-02-13 20:12:21,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21,362][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.12334941327571869, acc: 0.9580838084220886)
[2025-02-13 20:12:21,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21,785][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.23784588277339935, acc: 0.9481865167617798)
[2025-02-13 20:12:21,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22,148][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.09775074571371078, acc: 0.9666666388511658)
[2025-02-13 20:12:22,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22,552][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.060523536056280136, acc: 0.9866666793823242)
[2025-02-13 20:12:22,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22,969][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.16213169693946838, acc: 0.9704142212867737)
[2025-02-13 20:12:23,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23,418][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.12347118556499481, acc: 0.9655172228813171)
[2025-02-13 20:12:23,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23,851][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.07239094376564026, acc: 0.9863945841789246)
[2025-02-13 20:12:23,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24,336][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.09977686405181885, acc: 0.9717513918876648)
[2025-02-13 20:12:24,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24,768][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.08036106824874878, acc: 0.976331353187561)
[2025-02-13 20:12:24,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25,173][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.07970607280731201, acc: 0.9768785834312439)
[2025-02-13 20:12:25,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25,570][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.10716951638460159, acc: 0.9814814925193787)
[2025-02-13 20:12:25,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25,972][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.19069331884384155, acc: 0.9530201554298401)
[2025-02-13 20:12:26,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26,369][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.11412261426448822, acc: 0.9661017060279846)
[2025-02-13 20:12:26,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26,757][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.3109257221221924, acc: 0.9342105388641357)
[2025-02-13 20:12:26,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27,228][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.09472446143627167, acc: 0.9662162065505981)
[2025-02-13 20:12:27,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27,698][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.04557347297668457, acc: 0.9880239367485046)
[2025-02-13 20:12:27,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28,113][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.10652598738670349, acc: 0.9645389914512634)
[2025-02-13 20:12:28,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28,513][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.04376935958862305, acc: 1.0)
[2025-02-13 20:12:28,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28,940][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.08554999530315399, acc: 0.9777777791023254)
[2025-02-13 20:12:29,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29,328][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.15671104192733765, acc: 0.9644970297813416)
[2025-02-13 20:12:29,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29,751][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.11316652595996857, acc: 0.9662162065505981)
[2025-02-13 20:12:29,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30,142][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.05311409384012222, acc: 0.987730085849762)
[2025-02-13 20:12:30,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30,525][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.036751966923475266, acc: 0.9878048896789551)
[2025-02-13 20:12:30,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30,928][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.1065671369433403, acc: 0.9707602262496948)
[2025-02-13 20:12:31,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31,356][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.06248675286769867, acc: 0.982758641242981)
[2025-02-13 20:12:31,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31,762][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.04388388618826866, acc: 0.9933775067329407)
[2025-02-13 20:12:31,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32,148][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.07890147715806961, acc: 0.9835164546966553)
[2025-02-13 20:12:32,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32,527][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.09192457795143127, acc: 0.978723406791687)
[2025-02-13 20:12:32,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32,915][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.03660622984170914, acc: 0.9940119981765747)
[2025-02-13 20:12:33,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33,324][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.03612156957387924, acc: 0.9825581312179565)
[2025-02-13 20:12:33,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33,730][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.1971389204263687, acc: 0.9491525292396545)
[2025-02-13 20:12:33,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34,079][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.0841711163520813, acc: 0.9893617033958435)
[2025-02-13 20:12:34,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34,545][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.12248925119638443, acc: 0.9774011373519897)
[2025-02-13 20:12:34,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34,959][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.037384994328022, acc: 0.9931034445762634)
[2025-02-13 20:12:35,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35,377][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.0620197057723999, acc: 0.9886363744735718)
[2025-02-13 20:12:35,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35,763][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.08170469850301743, acc: 0.9883720874786377)
[2025-02-13 20:12:35,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36,152][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.052281513810157776, acc: 0.9864864945411682)
[2025-02-13 20:12:36,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36,541][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.13507325947284698, acc: 0.9595959782600403)
[2025-02-13 20:12:36,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36,947][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.06482920795679092, acc: 0.9829545617103577)
[2025-02-13 20:12:37,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37,336][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.03559769690036774, acc: 1.0)
[2025-02-13 20:12:37,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37,792][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.04744532331824303, acc: 0.9887005686759949)
[2025-02-13 20:12:37,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38,191][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.12954619526863098, acc: 0.9638554453849792)
[2025-02-13 20:12:38,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38,595][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.12617424130439758, acc: 0.9741935729980469)
[2025-02-13 20:12:38,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39,056][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.11070963740348816, acc: 0.9803921580314636)
[2025-02-13 20:12:39,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39,499][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.13609647750854492, acc: 0.9885057210922241)
[2025-02-13 20:12:39,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39,958][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.13678431510925293, acc: 0.9743589758872986)
[2025-02-13 20:12:40,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40,317][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.06019586697220802, acc: 0.9779411554336548)
[2025-02-13 20:12:40,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40,778][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.09086710214614868, acc: 0.9747474789619446)
[2025-02-13 20:12:40,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41,166][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.11917325854301453, acc: 0.969072163105011)
[2025-02-13 20:12:41,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41,591][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.06600241363048553, acc: 0.976190447807312)
[2025-02-13 20:12:41,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41,967][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.06516345590353012, acc: 0.9756097793579102)
[2025-02-13 20:12:42,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42,394][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.08925364911556244, acc: 0.9888888597488403)
[2025-02-13 20:12:42,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42,807][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.09122762829065323, acc: 0.9741935729980469)
[2025-02-13 20:12:42,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43,207][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.057507991790771484, acc: 0.9800000190734863)
[2025-02-13 20:12:43,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43,627][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.03712475672364235, acc: 1.0)
[2025-02-13 20:12:43,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44,023][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.0835997685790062, acc: 0.9817073345184326)
[2025-02-13 20:12:44,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44,430][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.0343683660030365, acc: 0.994350254535675)
[2025-02-13 20:12:44,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44,855][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.14495499432086945, acc: 0.9708737730979919)
[2025-02-13 20:12:44,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45,206][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.029517119750380516, acc: 0.9928057789802551)
[2025-02-13 20:12:45,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45,602][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.12082452327013016, acc: 0.979899525642395)
[2025-02-13 20:12:45,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45,974][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.04969429969787598, acc: 0.9869281053543091)
[2025-02-13 20:12:46,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46,349][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.21801836788654327, acc: 0.9567567706108093)
[2025-02-13 20:12:46,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46,769][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.062175873667001724, acc: 0.9952606558799744)
[2025-02-13 20:12:46,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47,168][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.10737629234790802, acc: 0.9863013625144958)
[2025-02-13 20:12:47,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47,562][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.05880327150225639, acc: 0.9940119981765747)
[2025-02-13 20:12:47,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47,940][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.06558072566986084, acc: 0.9757575988769531)
[2025-02-13 20:12:48,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48,364][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.08161959052085876, acc: 0.97826087474823)
[2025-02-13 20:12:48,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48,791][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.09894423186779022, acc: 0.9781420826911926)
[2025-02-13 20:12:48,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49,205][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.029575960710644722, acc: 0.9947916865348816)
[2025-02-13 20:12:49,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49,649][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.07682574540376663, acc: 0.982758641242981)
[2025-02-13 20:12:49,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50,033][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.12719479203224182, acc: 0.9780219793319702)
[2025-02-13 20:12:50,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50,427][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.06780318915843964, acc: 0.9813664555549622)
[2025-02-13 20:12:50,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50,790][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.17485520243644714, acc: 0.9538461565971375)
[2025-02-13 20:12:50,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51,193][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.09649685770273209, acc: 0.9944751262664795)
[2025-02-13 20:12:51,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51,606][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.06807807832956314, acc: 0.9718309640884399)
[2025-02-13 20:12:51,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52,009][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.1114584356546402, acc: 0.9748427867889404)
[2025-02-13 20:12:52,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52,392][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.2155359387397766, acc: 0.9466666579246521)
[2025-02-13 20:12:52,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52,799][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.3012460470199585, acc: 0.935251772403717)
[2025-02-13 20:12:52,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53,198][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.04942281171679497, acc: 0.9939393997192383)
[2025-02-13 20:12:53,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53,595][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.10516036301851273, acc: 0.9740259647369385)
[2025-02-13 20:12:53,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54,010][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.12831760942935944, acc: 0.9720670580863953)
[2025-02-13 20:12:54,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54,414][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.06921490281820297, acc: 0.9789473414421082)
[2025-02-13 20:12:54,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54,811][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.029820315539836884, acc: 0.9925925731658936)
[2025-02-13 20:12:54,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55,222][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.10630636662244797, acc: 0.9672130942344666)
[2025-02-13 20:12:55,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55,620][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.12370938807725906, acc: 0.9671052694320679)
[2025-02-13 20:12:55,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56,031][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.04037480801343918, acc: 0.9876543283462524)
[2025-02-13 20:12:56,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56,430][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.041019417345523834, acc: 0.9835164546966553)
[2025-02-13 20:12:56,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56,807][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.11439996212720871, acc: 0.9763779640197754)
[2025-02-13 20:12:56,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57,203][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.060419946908950806, acc: 0.9885057210922241)
[2025-02-13 20:12:57,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57,584][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.020895862951874733, acc: 0.9936708807945251)
[2025-02-13 20:12:57,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57,971][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.05304879695177078, acc: 0.9842519760131836)
[2025-02-13 20:12:58,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58,354][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.06789515167474747, acc: 0.9932432174682617)
[2025-02-13 20:12:58,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58,731][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.05882519483566284, acc: 0.985401451587677)
[2025-02-13 20:12:58,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59,149][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.04015817865729332, acc: 0.9940119981765747)
[2025-02-13 20:12:59,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59,531][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.03413950279355049, acc: 0.9885057210922241)
[2025-02-13 20:12:59,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59,903][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.017545277252793312, acc: 1.0)
[2025-02-13 20:13:00,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00,285][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.06811564415693283, acc: 0.9932885766029358)
[2025-02-13 20:13:00,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00,714][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.03694264963269234, acc: 0.9863945841789246)
[2025-02-13 20:13:00,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01,137][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.07317406684160233, acc: 0.9870967864990234)
[2025-02-13 20:13:01,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01,547][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.043227262794971466, acc: 0.9878048896789551)
[2025-02-13 20:13:01,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01,949][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.07288173586130142, acc: 0.9817073345184326)
[2025-02-13 20:13:02,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02,319][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.025684500113129616, acc: 1.0)
[2025-02-13 20:13:02,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02,697][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.03387720137834549, acc: 0.977142870426178)
[2025-02-13 20:13:02,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03,072][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.05280512571334839, acc: 0.9823529124259949)
[2025-02-13 20:13:03,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03,452][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.09177651256322861, acc: 0.9797297120094299)
[2025-02-13 20:13:03,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03,820][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.032269399613142014, acc: 1.0)
[2025-02-13 20:13:03,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04,194][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.0393030121922493, acc: 0.988304078578949)
[2025-02-13 20:13:04,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04,576][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.03161736577749252, acc: 0.9880239367485046)
[2025-02-13 20:13:04,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04,959][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.08397123217582703, acc: 0.9691358208656311)
[2025-02-13 20:13:05,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05,318][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.05395446717739105, acc: 0.9857142567634583)
[2025-02-13 20:13:05,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05,685][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.05063696205615997, acc: 0.9925925731658936)
[2025-02-13 20:13:05,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06,059][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.07768826186656952, acc: 0.9875776171684265)
[2025-02-13 20:13:06,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06,417][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.12270428240299225, acc: 0.965753436088562)
[2025-02-13 20:13:06,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06,774][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.07924071699380875, acc: 0.9798657894134521)
[2025-02-13 20:13:06,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07,125][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.14956338703632355, acc: 0.9556962251663208)
[2025-02-13 20:13:07,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07,512][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.052696239203214645, acc: 0.9863945841789246)
[2025-02-13 20:13:07,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07,956][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.04943050816655159, acc: 0.9865771532058716)
[2025-02-13 20:13:08,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08,370][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.0493825301527977, acc: 0.9805194735527039)
[2025-02-13 20:13:08,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08,760][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.07333339005708694, acc: 0.9767441749572754)
[2025-02-13 20:13:08,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09,132][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.07741004973649979, acc: 0.9902912378311157)
[2025-02-13 20:13:09,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09,490][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.018423832952976227, acc: 1.0)
[2025-02-13 20:13:09,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09,857][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.10763203352689743, acc: 0.9663865566253662)
[2025-02-13 20:13:10,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10,245][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.07708672434091568, acc: 0.9655172228813171)
[2025-02-13 20:13:10,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10,648][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.09024263918399811, acc: 0.9807692170143127)
[2025-02-13 20:13:10,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11,018][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.03065735101699829, acc: 0.9935064911842346)
[2025-02-13 20:13:11,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11,433][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.1065768450498581, acc: 0.9790209531784058)
[2025-02-13 20:13:11,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11,828][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.0669461116194725, acc: 0.9933333396911621)
[2025-02-13 20:13:11,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12,257][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.07311663776636124, acc: 0.9733333587646484)
[2025-02-13 20:13:12,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12,649][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.1181558221578598, acc: 0.9691358208656311)
[2025-02-13 20:13:12,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13,076][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.3448159992694855, acc: 0.9197860956192017)
[2025-02-13 20:13:13,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13,492][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.13238562643527985, acc: 0.9659090638160706)
[2025-02-13 20:13:13,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13,953][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.16481590270996094, acc: 0.96875)
[2025-02-13 20:13:14,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14,337][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.16485480964183807, acc: 0.9629629850387573)
[2025-02-13 20:13:14,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14,793][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.3814271092414856, acc: 0.8903225660324097)
[2025-02-13 20:13:14,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15,204][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.14912369847297668, acc: 0.9589040875434875)
[2025-02-13 20:13:15,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15,630][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.2569446265697479, acc: 0.9346405267715454)
[2025-02-13 20:13:15,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16,085][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.49997401237487793, acc: 0.874316930770874)
[2025-02-13 20:13:16,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16,568][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.2818682789802551, acc: 0.9306358098983765)
[2025-02-13 20:13:16,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16,975][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.33586570620536804, acc: 0.913294792175293)
[2025-02-13 20:13:17,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17,399][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.16460929811000824, acc: 0.9613259434700012)
[2025-02-13 20:13:17,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17,825][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.17689715325832367, acc: 0.978723406791687)
[2025-02-13 20:13:17,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18,204][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.055064596235752106, acc: 0.9836065769195557)
[2025-02-13 20:13:18,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18,579][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.052341144531965256, acc: 0.989130437374115)
[2025-02-13 20:13:18,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18,936][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.2028951346874237, acc: 0.9681528806686401)
[2025-02-13 20:13:19,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19,298][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.23013335466384888, acc: 0.9596773982048035)
[2025-02-13 20:13:19,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19,687][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.07906392961740494, acc: 0.9874213933944702)
[2025-02-13 20:13:19,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20,049][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.1478535681962967, acc: 0.9512194991111755)
[2025-02-13 20:13:20,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20,431][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.08666638284921646, acc: 0.9814814925193787)
[2025-02-13 20:13:20,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20,819][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.10940656810998917, acc: 0.9844961166381836)
[2025-02-13 20:13:20,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21,200][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.13629741966724396, acc: 0.9743589758872986)
[2025-02-13 20:13:21,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21,610][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.2881148159503937, acc: 0.9350649118423462)
[2025-02-13 20:13:21,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22,048][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.19392423331737518, acc: 0.9320987462997437)
[2025-02-13 20:13:22,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22,432][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.08710336685180664, acc: 0.969924807548523)
[2025-02-13 20:13:22,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22,800][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.28084060549736023, acc: 0.932330846786499)
[2025-02-13 20:13:22,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23,160][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.32328686118125916, acc: 0.929729700088501)
[2025-02-13 20:13:23,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23,552][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.25954577326774597, acc: 0.948051929473877)
[2025-02-13 20:13:23,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23,931][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.16939084231853485, acc: 0.950276255607605)
[2025-02-13 20:13:24,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24,301][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.10281068831682205, acc: 0.9837398529052734)
[2025-02-13 20:13:24,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24,692][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.10092684626579285, acc: 0.9589040875434875)
[2025-02-13 20:13:24,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25,075][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.06674408912658691, acc: 0.9832402467727661)
[2025-02-13 20:13:25,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25,465][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.20866207778453827, acc: 0.9453125)
[2025-02-13 20:13:25,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25,859][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.18298523128032684, acc: 0.9459459185600281)
[2025-02-13 20:13:26,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26,269][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.1307055950164795, acc: 0.9516128897666931)
[2025-02-13 20:13:26,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26,665][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.07422158867120743, acc: 0.9833333492279053)
[2025-02-13 20:13:26,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27,078][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.452680379152298, acc: 0.9333333373069763)
[2025-02-13 20:13:27,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27,483][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.10907972604036331, acc: 0.9852941036224365)
[2025-02-13 20:13:27,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27,876][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.15448053181171417, acc: 0.9894179701805115)
[2025-02-13 20:13:28,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28,311][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.12740716338157654, acc: 0.9743589758872986)
[2025-02-13 20:13:28,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28,697][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.2838052213191986, acc: 0.9225806593894958)
[2025-02-13 20:13:28,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29,074][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.08885521441698074, acc: 0.9847715497016907)
[2025-02-13 20:13:29,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29,498][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.173065647482872, acc: 0.9495412707328796)
[2025-02-13 20:13:29,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29,910][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.37520378828048706, acc: 0.9202127456665039)
[2025-02-13 20:13:30,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30,336][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.03146081045269966, acc: 0.9947368502616882)
[2025-02-13 20:13:30,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30,752][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.17314177751541138, acc: 0.9711538553237915)
[2025-02-13 20:13:30,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31,152][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.06567885726690292, acc: 0.9935483932495117)
[2025-02-13 20:13:31,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31,624][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.07684281468391418, acc: 0.9850000143051147)
[2025-02-13 20:13:31,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32,061][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.1771758794784546, acc: 0.9482758641242981)
[2025-02-13 20:13:32,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32,537][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.08488277345895767, acc: 0.9815950989723206)
[2025-02-13 20:13:32,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32,958][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.05241293087601662, acc: 0.9945054650306702)
[2025-02-13 20:13:33,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33,391][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.11024939268827438, acc: 0.9679144620895386)
[2025-02-13 20:13:33,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33,770][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.08021940290927887, acc: 0.9838709831237793)
[2025-02-13 20:13:33,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34,167][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.07463763654232025, acc: 0.9834254384040833)
[2025-02-13 20:13:34,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34,619][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.18402163684368134, acc: 0.9595959782600403)
[2025-02-13 20:13:34,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35,070][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.10471883416175842, acc: 0.9771689772605896)
[2025-02-13 20:13:35,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35,450][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.09188855439424515, acc: 0.9856459498405457)
[2025-02-13 20:13:35,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35,854][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.15213502943515778, acc: 0.95703125)
[2025-02-13 20:13:35,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36,262][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.04954824596643448, acc: 0.9903846383094788)
[2025-02-13 20:13:36,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36,661][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.05081874132156372, acc: 0.9858490824699402)
[2025-02-13 20:13:36,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37,064][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.08725905418395996, acc: 0.976190447807312)
[2025-02-13 20:13:37,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37,501][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.1539769172668457, acc: 0.9784946441650391)
[2025-02-13 20:13:37,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37,897][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.08688843995332718, acc: 0.9747474789619446)
[2025-02-13 20:13:38,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38,265][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.12879987061023712, acc: 0.9728260636329651)
[2025-02-13 20:13:38,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38,660][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.13632342219352722, acc: 0.9692307710647583)
[2025-02-13 20:13:38,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39,073][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.11790184676647186, acc: 0.9585253596305847)
[2025-02-13 20:13:39,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39,511][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.21959109604358673, acc: 0.9515418410301208)
[2025-02-13 20:13:39,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39,979][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.07705849409103394, acc: 0.9858490824699402)
[2025-02-13 20:13:40,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40,412][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.042036399245262146, acc: 0.9953488111495972)
[2025-02-13 20:13:40,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40,808][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.06771773099899292, acc: 0.9872881174087524)
[2025-02-13 20:13:40,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41,222][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.08532004058361053, acc: 0.9823788404464722)
[2025-02-13 20:13:41,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41,620][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.058070555329322815, acc: 0.9894179701805115)
[2025-02-13 20:13:41,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42,022][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.08209938555955887, acc: 0.9904761910438538)
[2025-02-13 20:13:42,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42,414][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.04451759159564972, acc: 0.9901477694511414)
[2025-02-13 20:13:42,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42,819][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.050651513040065765, acc: 0.9953051805496216)
[2025-02-13 20:13:42,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43,189][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.1280457228422165, acc: 0.9663461446762085)
[2025-02-13 20:13:43,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43,564][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.08650673180818558, acc: 0.9724770784378052)
[2025-02-13 20:13:43,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43,957][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.13431207835674286, acc: 0.9627906680107117)
[2025-02-13 20:13:44,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44,322][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.06277740001678467, acc: 0.9841269850730896)
[2025-02-13 20:13:44,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44,731][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.12818025052547455, acc: 0.9663865566253662)
[2025-02-13 20:13:44,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45,118][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.017658574506640434, acc: 1.0)
[2025-02-13 20:13:45,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45,497][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.04062572866678238, acc: 0.9864864945411682)
[2025-02-13 20:13:45,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45,917][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.04521556571125984, acc: 0.9897435903549194)
[2025-02-13 20:13:46,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46,294][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.047928713262081146, acc: 0.9882352948188782)
[2025-02-13 20:13:46,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46,694][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.0765499547123909, acc: 0.9754601120948792)
[2025-02-13 20:13:46,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47,074][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.1347518414258957, acc: 0.9490445852279663)
[2025-02-13 20:13:47,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47,482][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.23622262477874756, acc: 0.9698795080184937)
[2025-02-13 20:13:47,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47,915][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.09863672405481339, acc: 0.9857142567634583)
[2025-02-13 20:13:48,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48,274][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.04730364680290222, acc: 0.9846153855323792)
[2025-02-13 20:13:48,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48,641][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.052795689553022385, acc: 0.9865771532058716)
[2025-02-13 20:13:48,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49,017][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.1390896886587143, acc: 0.9597315192222595)
[2025-02-13 20:13:49,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49,401][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.10568318516016006, acc: 0.9673202633857727)
[2025-02-13 20:13:49,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49,787][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.014608926139771938, acc: 1.0)
[2025-02-13 20:13:49,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50,158][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.028921378776431084, acc: 0.9935483932495117)
[2025-02-13 20:13:50,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50,537][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.03275567665696144, acc: 0.9939393997192383)
[2025-02-13 20:13:50,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50,911][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.08524680882692337, acc: 0.9878048896789551)
[2025-02-13 20:13:51,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51,255][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.17130513489246368, acc: 0.9681528806686401)
[2025-02-13 20:13:51,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51,634][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.032694440335035324, acc: 0.9873417615890503)
[2025-02-13 20:13:51,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51,967][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.038256578147411346, acc: 0.9865771532058716)
[2025-02-13 20:13:52,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52,331][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.03682338073849678, acc: 0.9930555820465088)
[2025-02-13 20:13:52,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52,705][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.03136666491627693, acc: 1.0)
[2025-02-13 20:13:52,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53,132][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.10136377811431885, acc: 0.9743589758872986)
[2025-02-13 20:13:53,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53,508][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.0261156614869833, acc: 1.0)
[2025-02-13 20:13:53,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53,875][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.042141616344451904, acc: 0.9917355179786682)
[2025-02-13 20:13:54,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54,235][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.035189930349588394, acc: 0.9940476417541504)
[2025-02-13 20:13:54,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54,597][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.027928059920668602, acc: 0.9937499761581421)
[2025-02-13 20:13:54,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54,971][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.07330217957496643, acc: 0.9691358208656311)
[2025-02-13 20:13:55,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55,334][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.07123131304979324, acc: 0.982758641242981)
[2025-02-13 20:13:55,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55,711][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.03788413107395172, acc: 0.988950252532959)
[2025-02-13 20:13:55,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56,075][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.06399572640657425, acc: 0.98591548204422)
[2025-02-13 20:13:56,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56,502][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.2817363142967224, acc: 0.9226804375648499)
[2025-02-13 20:13:56,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56,942][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.19992920756340027, acc: 0.9312499761581421)
[2025-02-13 20:13:57,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57,354][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.19756820797920227, acc: 0.9620253443717957)
[2025-02-13 20:13:57,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57,735][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.07750797271728516, acc: 0.9781420826911926)
[2025-02-13 20:13:57,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58,107][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.16954104602336884, acc: 0.9548386931419373)
[2025-02-13 20:13:58,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58,485][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.1753920465707779, acc: 0.9612902998924255)
[2025-02-13 20:13:58,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58,851][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.07782404869794846, acc: 0.9836065769195557)
[2025-02-13 20:13:58,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59,225][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.1449361890554428, acc: 0.9723756909370422)
[2025-02-13 20:13:59,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59,592][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.17426106333732605, acc: 0.9637681245803833)
[2025-02-13 20:13:59,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59,937][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.1184118390083313, acc: 0.9682539701461792)
[2025-02-13 20:14:00,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00,307][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.2579159140586853, acc: 0.9336734414100647)
[2025-02-13 20:14:00,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00,670][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.11161305010318756, acc: 0.96875)
[2025-02-13 20:14:00,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01,038][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.09536966681480408, acc: 0.9746835231781006)
[2025-02-13 20:14:01,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01,412][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.10115688294172287, acc: 0.9642857313156128)
[2025-02-13 20:14:01,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01,788][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.13756759464740753, acc: 0.9639639854431152)
[2025-02-13 20:14:01,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02,183][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.10535762459039688, acc: 0.9790576100349426)
[2025-02-13 20:14:02,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02,573][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.11306726187467575, acc: 0.9803921580314636)
[2025-02-13 20:14:02,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02,973][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.08680307120084763, acc: 0.9709302186965942)
[2025-02-13 20:14:03,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03,373][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.08209406584501266, acc: 0.9794871807098389)
[2025-02-13 20:14:03,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03,797][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.07846375554800034, acc: 0.9797979593276978)
[2025-02-13 20:14:03,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04,251][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.16152289509773254, acc: 0.9597989916801453)
[2025-02-13 20:14:04,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04,647][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.12277910113334656, acc: 0.9764150977134705)
[2025-02-13 20:14:04,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05,011][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.03510219603776932, acc: 1.0)
[2025-02-13 20:14:05,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05,418][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.08524172753095627, acc: 0.9751552939414978)
[2025-02-13 20:14:05,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05,815][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.17769131064414978, acc: 0.9750000238418579)
[2025-02-13 20:14:05,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06,204][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.07539694011211395, acc: 0.9792746305465698)
[2025-02-13 20:14:06,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06,607][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.0818314403295517, acc: 0.9779005646705627)
[2025-02-13 20:14:06,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07,006][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.11243986338376999, acc: 0.9515151381492615)
[2025-02-13 20:14:07,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07,406][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.23478703200817108, acc: 0.9411764740943909)
[2025-02-13 20:14:07,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07,816][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.1462816298007965, acc: 0.9672897458076477)
[2025-02-13 20:14:07,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08,222][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.16928890347480774, acc: 0.9653465151786804)
[2025-02-13 20:14:08,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08,632][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.27490657567977905, acc: 0.918367326259613)
[2025-02-13 20:14:08,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09,052][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.1532433182001114, acc: 0.95652174949646)
[2025-02-13 20:14:09,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09,429][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.41477563977241516, acc: 0.8819875717163086)
[2025-02-13 20:14:09,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09,812][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.12466749548912048, acc: 0.9691358208656311)
[2025-02-13 20:14:09,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10,195][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.12866367399692535, acc: 0.9672130942344666)
[2025-02-13 20:14:10,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10,597][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.2169039249420166, acc: 0.954023003578186)
[2025-02-13 20:14:10,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10,996][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.0713249146938324, acc: 0.9780219793319702)
[2025-02-13 20:14:11,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11,391][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.15882527828216553, acc: 0.9470899701118469)
[2025-02-13 20:14:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11,790][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.15691491961479187, acc: 0.976190447807312)
[2025-02-13 20:14:11,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12,174][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.2721119821071625, acc: 0.9230769276618958)
[2025-02-13 20:14:12,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12,605][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.21964210271835327, acc: 0.9526066184043884)
[2025-02-13 20:14:12,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12,990][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.3028382658958435, acc: 0.928909957408905)
[2025-02-13 20:14:13,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13,367][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.18315647542476654, acc: 0.9567567706108093)
[2025-02-13 20:14:13,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13,780][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.21688513457775116, acc: 0.9617486596107483)
[2025-02-13 20:14:13,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14,167][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.16271451115608215, acc: 0.9533678889274597)
[2025-02-13 20:14:14,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14,575][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.11582104116678238, acc: 0.9642857313156128)
[2025-02-13 20:14:14,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15,016][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.38666588068008423, acc: 0.8914285898208618)
[2025-02-13 20:14:15,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15,426][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.3940875828266144, acc: 0.8787878751754761)
[2025-02-13 20:14:15,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15,861][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.28289246559143066, acc: 0.9329268336296082)
[2025-02-13 20:14:15,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16,250][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.07776416838169098, acc: 0.9891892075538635)
[2025-02-13 20:14:16,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16,633][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.33678776025772095, acc: 0.9015544056892395)
[2025-02-13 20:14:16,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17,025][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.3097974956035614, acc: 0.909547746181488)
[2025-02-13 20:14:17,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17,414][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.7502071857452393, acc: 0.8269230723381042)
[2025-02-13 20:14:17,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17,820][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.28892627358436584, acc: 0.9130434989929199)
[2025-02-13 20:14:17,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18,201][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.12188474088907242, acc: 0.970059871673584)
[2025-02-13 20:14:18,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18,579][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.09502147883176804, acc: 0.9802631735801697)
[2025-02-13 20:14:18,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18,964][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.2665277123451233, acc: 0.9554139971733093)
[2025-02-13 20:14:19,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19,329][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.05693340301513672, acc: 0.9940828680992126)
[2025-02-13 20:14:19,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19,765][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.08704762160778046, acc: 0.9673202633857727)
[2025-02-13 20:14:19,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20,152][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.214776873588562, acc: 0.950276255607605)
[2025-02-13 20:14:20,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20,510][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.14830251038074493, acc: 0.956250011920929)
[2025-02-13 20:14:20,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20,857][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.05337059870362282, acc: 0.9929577708244324)
[2025-02-13 20:14:20,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21,229][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.07637438923120499, acc: 0.9723756909370422)
[2025-02-13 20:14:21,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21,609][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.048673126846551895, acc: 0.9772727489471436)
[2025-02-13 20:14:21,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21,978][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.045348525047302246, acc: 0.9935064911842346)
[2025-02-13 20:14:22,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22,326][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.0645766332745552, acc: 0.9836065769195557)
[2025-02-13 20:14:22,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22,701][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.29158535599708557, acc: 0.9634146094322205)
[2025-02-13 20:14:22,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23,060][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.05640285834670067, acc: 0.9937106966972351)
[2025-02-13 20:14:23,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23,460][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.15807956457138062, acc: 0.9756097793579102)
[2025-02-13 20:14:23,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23,827][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.1081811785697937, acc: 0.9720279574394226)
[2025-02-13 20:14:23,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24,275][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.0771503821015358, acc: 0.987730085849762)
[2025-02-13 20:14:24,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24,710][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.09499894827604294, acc: 0.9722222089767456)
[2025-02-13 20:14:24,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25,124][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.06295133382081985, acc: 0.9837837815284729)
[2025-02-13 20:14:25,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25,568][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.11781412363052368, acc: 0.977142870426178)
[2025-02-13 20:14:25,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25,953][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.07244640588760376, acc: 0.9940476417541504)
[2025-02-13 20:14:26,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26,367][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.10932376235723495, acc: 0.9715909361839294)
[2025-02-13 20:14:26,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26,743][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.12451422959566116, acc: 0.9719101190567017)
[2025-02-13 20:14:26,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27,143][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.08287424594163895, acc: 0.982758641242981)
[2025-02-13 20:14:27,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27,515][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.07273686677217484, acc: 0.9739583134651184)
[2025-02-13 20:14:27,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27,910][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.08323456346988678, acc: 0.9736841917037964)
[2025-02-13 20:14:28,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28,275][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.13400733470916748, acc: 0.9556962251663208)
[2025-02-13 20:14:28,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28,651][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.06159891188144684, acc: 0.9800000190734863)
[2025-02-13 20:14:28,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29,042][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.0436219684779644, acc: 0.9930070042610168)
[2025-02-13 20:14:29,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29,444][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.18812154233455658, acc: 0.9504132270812988)
[2025-02-13 20:14:29,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29,851][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.11742934584617615, acc: 0.9784172773361206)
[2025-02-13 20:14:29,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30,179][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.21767300367355347, acc: 0.9530201554298401)
[2025-02-13 20:14:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30,523][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.18299423158168793, acc: 0.9470198750495911)
[2025-02-13 20:14:30,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30,885][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.07518476247787476, acc: 0.9746192693710327)
[2025-02-13 20:14:31,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31,245][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.1167706623673439, acc: 0.9788732528686523)
[2025-02-13 20:14:31,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31,640][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.15867017209529877, acc: 0.9691358208656311)
[2025-02-13 20:14:31,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32,010][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.14367136359214783, acc: 0.9679144620895386)
[2025-02-13 20:14:32,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32,374][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.05275193229317665, acc: 1.0)
[2025-02-13 20:14:32,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32,752][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.33017727732658386, acc: 0.9359999895095825)
[2025-02-13 20:14:32,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33,121][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.20614184439182281, acc: 0.9704142212867737)
[2025-02-13 20:14:33,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33,455][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.1383231282234192, acc: 0.9754601120948792)
[2025-02-13 20:14:33,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33,813][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.18664589524269104, acc: 0.961240291595459)
[2025-02-13 20:14:33,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34,188][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.14546708762645721, acc: 0.9745222926139832)
[2025-02-13 20:14:34,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34,621][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.17203772068023682, acc: 0.9415584206581116)
[2025-02-13 20:14:34,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34,974][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.074367955327034, acc: 0.983146071434021)
[2025-02-13 20:14:35,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35,374][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.11203599721193314, acc: 0.9731543660163879)
[2025-02-13 20:14:35,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35,772][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.2525691092014313, acc: 0.9418604373931885)
[2025-02-13 20:14:35,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36,152][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.0884602889418602, acc: 0.9823529124259949)
[2025-02-13 20:14:36,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36,497][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.03974887356162071, acc: 0.9893048405647278)
[2025-02-13 20:14:36,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36,925][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.06352166086435318, acc: 0.9838709831237793)
[2025-02-13 20:14:37,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37,328][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.12387088686227798, acc: 0.9740932583808899)
[2025-02-13 20:14:37,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37,685][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.08191407471895218, acc: 0.978723406791687)
[2025-02-13 20:14:37,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38,049][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.049840278923511505, acc: 0.9938650131225586)
[2025-02-13 20:14:38,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38,398][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.13559888303279877, acc: 0.9585492014884949)
[2025-02-13 20:14:38,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38,791][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.13135656714439392, acc: 0.9738562107086182)
[2025-02-13 20:14:38,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39,153][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.2110375612974167, acc: 0.9624999761581421)
[2025-02-13 20:14:39,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39,556][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.053176265209913254, acc: 0.988304078578949)
[2025-02-13 20:14:39,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39,962][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.12417241185903549, acc: 0.9670329689979553)
[2025-02-13 20:14:40,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40,347][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.03990840166807175, acc: 0.9874213933944702)
[2025-02-13 20:14:40,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40,731][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.18025268614292145, acc: 0.9575757384300232)
[2025-02-13 20:14:40,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41,117][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.020769957453012466, acc: 1.0)
[2025-02-13 20:14:41,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41,493][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.11665905267000198, acc: 0.9666666388511658)
[2025-02-13 20:14:41,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41,884][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.08733987808227539, acc: 0.9832402467727661)
[2025-02-13 20:14:42,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42,265][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.035769231617450714, acc: 0.9880239367485046)
[2025-02-13 20:14:42,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42,647][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.13928373157978058, acc: 0.9606741666793823)
[2025-02-13 20:14:42,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43,021][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.07595235109329224, acc: 0.9767441749572754)
[2025-02-13 20:14:43,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43,402][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.02640908770263195, acc: 0.9941860437393188)
[2025-02-13 20:14:43,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43,764][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.13786838948726654, acc: 0.957446813583374)
[2025-02-13 20:14:43,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44,135][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.05632161721587181, acc: 0.978723406791687)
[2025-02-13 20:14:44,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44,558][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.017483755946159363, acc: 1.0)
[2025-02-13 20:14:44,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44,957][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.08412537723779678, acc: 0.9906542301177979)
[2025-02-13 20:14:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45,380][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.021836766973137856, acc: 1.0)
[2025-02-13 20:14:45,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45,757][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.062452591955661774, acc: 0.9918032884597778)
[2025-02-13 20:14:45,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46,150][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.042385682463645935, acc: 0.988304078578949)
[2025-02-13 20:14:46,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46,577][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.02774648927152157, acc: 1.0)
[2025-02-13 20:14:46,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46,992][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.0427401103079319, acc: 0.9865771532058716)
[2025-02-13 20:14:47,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47,420][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.19687630236148834, acc: 0.9387755393981934)
[2025-02-13 20:14:47,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47,817][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.05949680507183075, acc: 0.9807692170143127)
[2025-02-13 20:14:47,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48,182][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.03698809817433357, acc: 1.0)
[2025-02-13 20:14:48,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48,607][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.06559474021196365, acc: 0.9870129823684692)
[2025-02-13 20:14:48,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49,075][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.08214258402585983, acc: 0.9820359349250793)
[2025-02-13 20:14:49,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49,496][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.0981515422463417, acc: 0.9638554453849792)
[2025-02-13 20:14:49,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49,903][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.05921434611082077, acc: 0.9851852059364319)
[2025-02-13 20:14:50,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50,285][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.058940134942531586, acc: 0.9931507110595703)
[2025-02-13 20:14:50,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50,685][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.15287767350673676, acc: 0.9652777910232544)
[2025-02-13 20:14:50,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51,133][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.08316687494516373, acc: 0.9878787994384766)
[2025-02-13 20:14:51,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51,606][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.07039064168930054, acc: 0.9822485446929932)
[2025-02-13 20:14:51,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52,090][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.08150263875722885, acc: 0.9935064911842346)
[2025-02-13 20:14:52,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52,489][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.19216769933700562, acc: 0.9701492786407471)
[2025-02-13 20:14:52,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52,902][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.07660380750894547, acc: 0.9811320900917053)
[2025-02-13 20:14:53,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53,323][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.07909232378005981, acc: 0.976190447807312)
[2025-02-13 20:14:53,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53,722][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.08295129239559174, acc: 0.9753086566925049)
[2025-02-13 20:14:53,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54,168][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.116395965218544, acc: 0.9764705896377563)
[2025-02-13 20:14:54,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54,619][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.09443879127502441, acc: 0.9746835231781006)
[2025-02-13 20:14:54,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55,028][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.11208447813987732, acc: 0.977011501789093)
[2025-02-13 20:14:55,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55,456][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.07689296454191208, acc: 0.9828571677207947)
[2025-02-13 20:14:55,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55,878][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.05600130558013916, acc: 0.9939758777618408)
[2025-02-13 20:14:56,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56,270][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.10399486869573593, acc: 0.9864864945411682)
[2025-02-13 20:14:56,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56,616][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.13740457594394684, acc: 0.966292142868042)
[2025-02-13 20:14:56,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57,016][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.02840820699930191, acc: 1.0)
[2025-02-13 20:14:57,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57,390][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.06528768688440323, acc: 0.97826087474823)
[2025-02-13 20:14:57,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57,785][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.1292056143283844, acc: 0.9620253443717957)
[2025-02-13 20:14:57,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58,212][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.08995798230171204, acc: 0.9767441749572754)
[2025-02-13 20:14:58,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58,620][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.03981463238596916, acc: 1.0)
[2025-02-13 20:14:58,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59,067][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.03551141545176506, acc: 0.9888888597488403)
[2025-02-13 20:14:59,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59,545][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.09765549749135971, acc: 0.9830508232116699)
[2025-02-13 20:14:59,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59,977][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.03691880404949188, acc: 0.9882352948188782)
[2025-02-13 20:15:00,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00,358][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.03766319528222084, acc: 0.9857142567634583)
[2025-02-13 20:15:00,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00,801][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.14119669795036316, acc: 0.9726775884628296)
[2025-02-13 20:15:00,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01,170][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.13557922840118408, acc: 0.9529411792755127)
[2025-02-13 20:15:01,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01,550][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.05124349892139435, acc: 0.9892473220825195)
[2025-02-13 20:15:01,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01,993][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.058394238352775574, acc: 0.987261176109314)
[2025-02-13 20:15:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02,387][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.04413362592458725, acc: 0.9870129823684692)
[2025-02-13 20:15:02,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02,796][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.11631932854652405, acc: 0.9888268113136292)
[2025-02-13 20:15:02,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03,220][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.0967579111456871, acc: 0.9833333492279053)
[2025-02-13 20:15:03,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03,649][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.08470212668180466, acc: 0.9607843160629272)
[2025-02-13 20:15:03,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04,096][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.020345309749245644, acc: 1.0)
[2025-02-13 20:15:04,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04,513][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.046979039907455444, acc: 0.9934210777282715)
[2025-02-13 20:15:04,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04,906][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.16301563382148743, acc: 0.9670329689979553)
[2025-02-13 20:15:05,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05,298][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.04332038760185242, acc: 1.0)
[2025-02-13 20:15:05,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05,696][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.09045811742544174, acc: 0.9754601120948792)
[2025-02-13 20:15:05,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06,094][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.03250932693481445, acc: 0.9878048896789551)
[2025-02-13 20:15:06,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06,477][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.07656031847000122, acc: 0.9794520735740662)
[2025-02-13 20:15:06,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06,854][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.008873559534549713, acc: 1.0)
[2025-02-13 20:15:06,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07,248][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.04560308903455734, acc: 0.994535505771637)
[2025-02-13 20:15:07,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07,650][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.03646063059568405, acc: 0.9939758777618408)
[2025-02-13 20:15:07,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08,035][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.06156530976295471, acc: 0.988095223903656)
[2025-02-13 20:15:08,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08,418][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.05922674387693405, acc: 0.9887640476226807)
[2025-02-13 20:15:08,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08,859][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.03371570631861687, acc: 0.9939758777618408)
[2025-02-13 20:15:08,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09,273][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.029133211821317673, acc: 1.0)
[2025-02-13 20:15:09,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09,680][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.06440338492393494, acc: 0.9807692170143127)
[2025-02-13 20:15:09,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10,056][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.07327108830213547, acc: 0.9832402467727661)
[2025-02-13 20:15:10,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10,510][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.028815923258662224, acc: 1.0)
[2025-02-13 20:15:10,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10,971][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.05959346890449524, acc: 0.984455943107605)
[2025-02-13 20:15:11,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11,361][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.08203936368227005, acc: 0.9794520735740662)
[2025-02-13 20:15:11,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11,734][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.1488949954509735, acc: 0.977011501789093)
[2025-02-13 20:15:11,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12,092][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.09995871037244797, acc: 0.9849624037742615)
[2025-02-13 20:15:12,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12,464][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.07560135424137115, acc: 0.9801324605941772)
[2025-02-13 20:15:12,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12,833][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.0311652272939682, acc: 1.0)
[2025-02-13 20:15:12,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13,212][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.09862534701824188, acc: 0.9750000238418579)
[2025-02-13 20:15:13,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13,637][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.10516524314880371, acc: 0.9748427867889404)
[2025-02-13 20:15:13,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14,052][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.07351423054933548, acc: 0.9702380895614624)
[2025-02-13 20:15:14,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14,432][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.03479590266942978, acc: 0.9925373196601868)
[2025-02-13 20:15:14,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14,822][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.20754612982273102, acc: 0.9336734414100647)
[2025-02-13 20:15:14,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15,202][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.13138385117053986, acc: 0.9568345546722412)
[2025-02-13 20:15:15,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15,569][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.02349849045276642, acc: 1.0)
[2025-02-13 20:15:15,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15,979][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.05138253793120384, acc: 0.9918699264526367)
[2025-02-13 20:15:16,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16,405][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.02695300243794918, acc: 0.9863013625144958)
[2025-02-13 20:15:16,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16,798][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.13284863531589508, acc: 0.9710144996643066)
[2025-02-13 20:15:16,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17,192][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.013369562104344368, acc: 1.0)
[2025-02-13 20:15:17,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17,574][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.08610368520021439, acc: 0.9868420958518982)
[2025-02-13 20:15:17,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17,966][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.1492813676595688, acc: 0.9763779640197754)
[2025-02-13 20:15:18,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18,382][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.04995567724108696, acc: 0.9909909963607788)
[2025-02-13 20:15:18,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18,784][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.06652677804231644, acc: 0.9900000095367432)
[2025-02-13 20:15:18,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19,160][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.08055025339126587, acc: 0.9795918464660645)
[2025-02-13 20:15:19,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19,548][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.027642566710710526, acc: 1.0)
[2025-02-13 20:15:19,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19,931][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.06491781026124954, acc: 0.981249988079071)
[2025-02-13 20:15:20,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20,348][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.0622144378721714, acc: 0.9797297120094299)
[2025-02-13 20:15:20,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20,749][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.03417183831334114, acc: 1.0)
[2025-02-13 20:15:20,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21,129][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.023642608895897865, acc: 0.9919999837875366)
[2025-02-13 20:15:21,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21,511][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.0693436861038208, acc: 0.9940828680992126)
[2025-02-13 20:15:21,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21,892][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.18795345723628998, acc: 0.9568345546722412)
[2025-02-13 20:15:22,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22,261][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.04998531565070152, acc: 0.9928057789802551)
[2025-02-13 20:15:22,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22,633][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.0401621051132679, acc: 1.0)
[2025-02-13 20:15:22,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22,965][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.04758762568235397, acc: 0.9924812316894531)
[2025-02-13 20:15:23,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23,357][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.11194006353616714, acc: 0.9871794581413269)
[2025-02-13 20:15:23,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23,814][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.03192134201526642, acc: 0.98591548204422)
[2025-02-13 20:15:23,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24,277][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.020200615748763084, acc: 1.0)
[2025-02-13 20:15:24,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24,717][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.08388132601976395, acc: 0.9836065769195557)
[2025-02-13 20:15:24,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25,153][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.08709434419870377, acc: 0.9795918464660645)
[2025-02-13 20:15:25,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25,542][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.05868399888277054, acc: 0.9928057789802551)
[2025-02-13 20:15:25,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25,935][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.02166368067264557, acc: 1.0)
[2025-02-13 20:15:26,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26,321][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.11802016943693161, acc: 0.9918032884597778)
[2025-02-13 20:15:26,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26,703][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.04077991843223572, acc: 0.9918699264526367)
[2025-02-13 20:15:26,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27,078][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.1882970929145813, acc: 0.9763779640197754)
[2025-02-13 20:15:27,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27,488][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.21902532875537872, acc: 0.9411764740943909)
[2025-02-13 20:15:27,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27,869][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.1804378181695938, acc: 0.9487179517745972)
[2025-02-13 20:15:28,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28,276][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.09549623727798462, acc: 0.9768785834312439)
[2025-02-13 20:15:28,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28,633][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.19236890971660614, acc: 0.9661017060279846)
[2025-02-13 20:15:28,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29,019][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.22120067477226257, acc: 0.9604519605636597)
[2025-02-13 20:15:29,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29,408][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.15959767997264862, acc: 0.9482758641242981)
[2025-02-13 20:15:29,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29,767][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.21310864388942719, acc: 0.957446813583374)
[2025-02-13 20:15:29,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30,135][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.2001098245382309, acc: 0.9602649211883545)
[2025-02-13 20:15:30,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30,530][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.1387774497270584, acc: 0.9538461565971375)
[2025-02-13 20:15:30,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30,931][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.0956353172659874, acc: 0.9750000238418579)
[2025-02-13 20:15:31,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31,319][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.05326681584119797, acc: 0.9863945841789246)
[2025-02-13 20:15:31,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31,686][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.20614418387413025, acc: 0.9444444179534912)
[2025-02-13 20:15:31,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32,085][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.09653220325708389, acc: 0.970370352268219)
[2025-02-13 20:15:32,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32,514][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.24008043110370636, acc: 0.9356725215911865)
[2025-02-13 20:15:32,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32,907][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.18871764838695526, acc: 0.9553072452545166)
[2025-02-13 20:15:33,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33,296][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.11992346495389938, acc: 0.9817073345184326)
[2025-02-13 20:15:33,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33,676][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.12700733542442322, acc: 0.9673202633857727)
[2025-02-13 20:15:33,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34,070][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.06576038897037506, acc: 0.9939024448394775)
[2025-02-13 20:15:34,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34,486][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.10208704322576523, acc: 0.9764705896377563)
[2025-02-13 20:15:34,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34,872][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.1449572890996933, acc: 0.9717513918876648)
[2025-02-13 20:15:35,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35,247][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.15562336146831512, acc: 0.9536423683166504)
[2025-02-13 20:15:35,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35,611][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.055618707090616226, acc: 0.9937106966972351)
[2025-02-13 20:15:35,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35,992][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.1584990918636322, acc: 0.9719101190567017)
[2025-02-13 20:15:36,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36,400][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.13894878327846527, acc: 0.9741935729980469)
[2025-02-13 20:15:36,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36,778][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.08987520635128021, acc: 0.9702380895614624)
[2025-02-13 20:15:36,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37,172][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.0751853734254837, acc: 0.9934640526771545)
[2025-02-13 20:15:37,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37,534][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.0891270637512207, acc: 0.9788359999656677)
[2025-02-13 20:15:37,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37,972][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.0730779841542244, acc: 0.991150438785553)
[2025-02-13 20:15:38,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38,362][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.10107090324163437, acc: 0.9733333587646484)
[2025-02-13 20:15:38,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38,806][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.07786086201667786, acc: 0.9887640476226807)
[2025-02-13 20:15:38,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39,231][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.10555040091276169, acc: 0.9841269850730896)
[2025-02-13 20:15:39,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39,610][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.16669818758964539, acc: 0.9738562107086182)
[2025-02-13 20:15:39,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39,996][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.10644785314798355, acc: 0.9784172773361206)
[2025-02-13 20:15:40,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40,442][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.22401873767375946, acc: 0.9553072452545166)
[2025-02-13 20:15:40,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40,914][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.12656816840171814, acc: 0.9599999785423279)
[2025-02-13 20:15:41,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41,340][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.13689777255058289, acc: 0.9642857313156128)
[2025-02-13 20:15:41,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41,739][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.17489853501319885, acc: 0.9627659320831299)
[2025-02-13 20:15:41,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42,140][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.14680135250091553, acc: 0.983146071434021)
[2025-02-13 20:15:42,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42,490][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.16682575643062592, acc: 0.9831932783126831)
[2025-02-13 20:15:42,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42,878][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.3203754723072052, acc: 0.9289340376853943)
[2025-02-13 20:15:43,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43,269][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.06856931746006012, acc: 0.9743589758872986)
[2025-02-13 20:15:43,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43,662][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.09440044313669205, acc: 0.9637681245803833)
[2025-02-13 20:15:43,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44,049][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.1120191216468811, acc: 0.9735449552536011)
[2025-02-13 20:15:44,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44,448][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.2254505306482315, acc: 0.9411764740943909)
[2025-02-13 20:15:44,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44,979][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.1378176510334015, acc: 0.9520547986030579)
[2025-02-13 20:15:45,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45,391][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.1896725594997406, acc: 0.9449541568756104)
[2025-02-13 20:15:45,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45,774][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.21759003400802612, acc: 0.9428571462631226)
[2025-02-13 20:15:45,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46,224][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.19670291244983673, acc: 0.9464285969734192)
[2025-02-13 20:15:46,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46,609][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.07154495269060135, acc: 0.9885057210922241)
[2025-02-13 20:15:46,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47,051][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.11454935371875763, acc: 0.9815950989723206)
[2025-02-13 20:15:47,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47,428][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.21041607856750488, acc: 0.9666666388511658)
[2025-02-13 20:15:47,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47,878][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.07439795136451721, acc: 0.9880239367485046)
[2025-02-13 20:15:48,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48,248][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.10471111536026001, acc: 0.9710144996643066)
[2025-02-13 20:15:48,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48,650][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.13925185799598694, acc: 0.9746192693710327)
[2025-02-13 20:15:48,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49,058][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.040260326117277145, acc: 0.9826086759567261)
[2025-02-13 20:15:49,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49,448][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.18483687937259674, acc: 0.9505494236946106)
[2025-02-13 20:15:49,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49,842][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.04675609990954399, acc: 0.9940828680992126)
[2025-02-13 20:15:49,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50,223][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.2791078984737396, acc: 0.9388889074325562)
[2025-02-13 20:15:50,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50,640][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.17540283501148224, acc: 0.9497206807136536)
[2025-02-13 20:15:50,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51,074][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.30284541845321655, acc: 0.9202454090118408)
[2025-02-13 20:15:51,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51,490][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.25627049803733826, acc: 0.9402984976768494)
[2025-02-13 20:15:51,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51,899][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.4164181649684906, acc: 0.9097744226455688)
[2025-02-13 20:15:52,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52,298][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.3815464675426483, acc: 0.9353233575820923)
[2025-02-13 20:15:52,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52,722][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.16421599686145782, acc: 0.977142870426178)
[2025-02-13 20:15:52,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53,119][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.49766331911087036, acc: 0.9107142686843872)
[2025-02-13 20:15:53,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53,517][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.3254142701625824, acc: 0.9476743936538696)
[2025-02-13 20:15:53,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53,951][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.18888765573501587, acc: 0.9629629850387573)
[2025-02-13 20:15:54,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54,330][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.16472128033638, acc: 0.954285740852356)
[2025-02-13 20:15:54,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54,716][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.32391706109046936, acc: 0.9130434989929199)
[2025-02-13 20:15:54,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55,140][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.23984093964099884, acc: 0.945652186870575)
[2025-02-13 20:15:55,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55,565][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.3055664002895355, acc: 0.9306930899620056)
[2025-02-13 20:15:55,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56,049][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.34315934777259827, acc: 0.8972973227500916)
[2025-02-13 20:15:56,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56,432][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.35655367374420166, acc: 0.9235293865203857)
[2025-02-13 20:15:56,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56,814][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.14533886313438416, acc: 0.9658536314964294)
[2025-02-13 20:15:56,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:57,210][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.10985460877418518, acc: 0.9719101190567017)
[2025-02-13 20:15:57,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:57,622][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.0799858570098877, acc: 0.9869281053543091)
[2025-02-13 20:15:57,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58,028][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.07836978882551193, acc: 0.9714285731315613)
[2025-02-13 20:15:58,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58,409][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.08905930817127228, acc: 0.9689440727233887)
[2025-02-13 20:15:58,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58,802][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.23659829795360565, acc: 0.9593023061752319)
[2025-02-13 20:15:58,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59,196][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.05817210674285889, acc: 0.9873417615890503)
[2025-02-13 20:15:59,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59,573][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.2536751627922058, acc: 0.9292929172515869)
[2025-02-13 20:15:59,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59,947][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.10954093188047409, acc: 0.9850746393203735)
[2025-02-13 20:16:00,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00,359][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.22991590201854706, acc: 0.94017094373703)
[2025-02-13 20:16:00,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00,719][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.05874449387192726, acc: 0.9822485446929932)
[2025-02-13 20:16:00,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01,106][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.06642808020114899, acc: 0.9876543283462524)
[2025-02-13 20:16:01,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01,461][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.10582824051380157, acc: 0.9689119458198547)
[2025-02-13 20:16:01,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01,861][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.08386018872261047, acc: 0.9825581312179565)
[2025-02-13 20:16:01,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02,307][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.05848704278469086, acc: 0.9819276928901672)
[2025-02-13 20:16:02,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02,780][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.025936542078852654, acc: 1.0)
[2025-02-13 20:16:02,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03,233][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.02500961720943451, acc: 1.0)
[2025-02-13 20:16:03,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03,627][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.0740574300289154, acc: 0.9885714054107666)
[2025-02-13 20:16:03,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04,074][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.01418122835457325, acc: 1.0)
[2025-02-13 20:16:04,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04,494][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.04824914038181305, acc: 0.9828571677207947)
[2025-02-13 20:16:04,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04,878][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.07324138283729553, acc: 0.977142870426178)
[2025-02-13 20:16:05,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05,273][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.021935252472758293, acc: 0.9939758777618408)
[2025-02-13 20:16:05,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05,664][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.028692912310361862, acc: 0.9944751262664795)
[2025-02-13 20:16:05,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06,030][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.07801475375890732, acc: 0.9814814925193787)
[2025-02-13 20:16:06,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06,424][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.01987147517502308, acc: 1.0)
[2025-02-13 20:16:06,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06,788][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.04306003078818321, acc: 0.9943181872367859)
[2025-02-13 20:16:06,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07,168][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.025234360247850418, acc: 1.0)
[2025-02-13 20:16:07,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07,555][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.02250353991985321, acc: 0.994350254535675)
[2025-02-13 20:16:07,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07,932][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.0197866540402174, acc: 0.9949495196342468)
[2025-02-13 20:16:08,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08,318][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.04720922186970711, acc: 0.9835164546966553)
[2025-02-13 20:16:08,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08,749][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.039646588265895844, acc: 0.9873417615890503)
[2025-02-13 20:16:08,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09,215][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.06092566251754761, acc: 0.9731543660163879)
[2025-02-13 20:16:09,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09,608][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.02758411131799221, acc: 0.9933775067329407)
[2025-02-13 20:16:09,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10,034][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.1255902796983719, acc: 0.96875)
[2025-02-13 20:16:10,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10,420][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.03049049712717533, acc: 1.0)
[2025-02-13 20:16:10,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10,810][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.08641644567251205, acc: 0.9750000238418579)
[2025-02-13 20:16:10,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11,183][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.12973685562610626, acc: 0.9831932783126831)
[2025-02-13 20:16:11,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11,605][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.09443818777799606, acc: 0.9774436354637146)
[2025-02-13 20:16:11,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12,033][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.100617915391922, acc: 0.9756097793579102)
[2025-02-13 20:16:12,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12,412][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.09120691567659378, acc: 1.0)
[2025-02-13 20:16:12,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12,846][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.19054840505123138, acc: 0.9380530714988708)
[2025-02-13 20:16:12,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13,223][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.2064630687236786, acc: 0.942148745059967)
[2025-02-13 20:16:13,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13,584][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.21974067389965057, acc: 0.9646017551422119)
[2025-02-13 20:16:13,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13,943][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.2568033039569855, acc: 0.9271523356437683)
[2025-02-13 20:16:14,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14,334][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.08188523352146149, acc: 0.9777777791023254)
[2025-02-13 20:16:14,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14,782][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.14656586945056915, acc: 0.9571428298950195)
[2025-02-13 20:16:14,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15,195][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.21323107182979584, acc: 0.9415584206581116)
[2025-02-13 20:16:15,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15,603][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.23348036408424377, acc: 0.9455782175064087)
[2025-02-13 20:16:15,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16,011][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.05529481917619705, acc: 0.9913793206214905)
[2025-02-13 20:16:16,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16,386][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.05758460611104965, acc: 0.9857142567634583)
[2025-02-13 20:16:16,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16,802][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.11391769349575043, acc: 0.9774436354637146)
[2025-02-13 20:16:16,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17,165][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.27928996086120605, acc: 0.9444444179534912)
[2025-02-13 20:16:17,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17,610][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.04157590866088867, acc: 0.9866666793823242)
[2025-02-13 20:16:17,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18,054][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.1749885380268097, acc: 0.9642857313156128)
[2025-02-13 20:16:18,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18,499][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.23527516424655914, acc: 0.9436619877815247)
[2025-02-13 20:16:18,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18,939][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.0900757759809494, acc: 0.9931507110595703)
[2025-02-13 20:16:19,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19,386][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.14142508804798126, acc: 0.9666666388511658)
[2025-02-13 20:16:19,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19,801][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.15933528542518616, acc: 0.9599999785423279)
[2025-02-13 20:16:19,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20,245][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.08393656462430954, acc: 0.9647887349128723)
[2025-02-13 20:16:20,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20,691][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.09459300339221954, acc: 0.9800000190734863)
[2025-02-13 20:16:20,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21,140][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.07273881137371063, acc: 0.9793814420700073)
[2025-02-13 20:16:21,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21,545][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.10039685666561127, acc: 0.978723406791687)
[2025-02-13 20:16:21,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21,930][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.10438716411590576, acc: 0.9750000238418579)
[2025-02-13 20:16:22,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22,320][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.04792894050478935, acc: 0.9779411554336548)
[2025-02-13 20:16:22,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22,697][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.07244788110256195, acc: 0.9793103337287903)
[2025-02-13 20:16:22,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23,090][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.053207363933324814, acc: 0.9883720874786377)
[2025-02-13 20:16:23,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23,526][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.04807518795132637, acc: 1.0)
[2025-02-13 20:16:23,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23,928][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.22154200077056885, acc: 0.9305555820465088)
[2025-02-13 20:16:24,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24,331][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.14911574125289917, acc: 0.9856114983558655)
[2025-02-13 20:16:24,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24,721][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.20223267376422882, acc: 0.9489051103591919)
[2025-02-13 20:16:24,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25,128][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.13173089921474457, acc: 0.9801324605941772)
[2025-02-13 20:16:25,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25,534][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.1356774866580963, acc: 0.978723406791687)
[2025-02-13 20:16:25,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25,909][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.30763188004493713, acc: 0.9520958065986633)
[2025-02-13 20:16:26,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26,281][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.060182876884937286, acc: 0.9887005686759949)
[2025-02-13 20:16:26,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26,658][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.0726601853966713, acc: 0.9701492786407471)
[2025-02-13 20:16:26,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27,018][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.11420820653438568, acc: 0.9593495726585388)
[2025-02-13 20:16:27,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27,407][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.037976909428834915, acc: 0.9912280440330505)
[2025-02-13 20:16:27,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27,781][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.5983225703239441, acc: 0.8296296000480652)
[2025-02-13 20:16:27,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28,129][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.44774407148361206, acc: 0.9071428775787354)
[2025-02-13 20:16:28,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28,509][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.1654035449028015, acc: 0.9646017551422119)
[2025-02-13 20:16:28,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28,872][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.19826586544513702, acc: 0.9611650705337524)
[2025-02-13 20:16:29,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29,269][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.29684606194496155, acc: 0.9264705777168274)
[2025-02-13 20:16:29,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29,662][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.21553966403007507, acc: 0.949999988079071)
[2025-02-13 20:16:29,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30,104][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.22271887958049774, acc: 0.9669421315193176)
[2025-02-13 20:16:30,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30,551][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.1757756620645523, acc: 0.9634146094322205)
[2025-02-13 20:16:30,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30,947][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.2506645917892456, acc: 0.9433962106704712)
[2025-02-13 20:16:31,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31,380][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.2180173248052597, acc: 0.9571428298950195)
[2025-02-13 20:16:31,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31,796][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.2197897732257843, acc: 0.940119743347168)
[2025-02-13 20:16:31,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32,204][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.18699981272220612, acc: 0.95652174949646)
[2025-02-13 20:16:32,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32,638][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.1677880436182022, acc: 0.9591836929321289)
[2025-02-13 20:16:32,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33,001][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.08101895451545715, acc: 0.9913793206214905)
[2025-02-13 20:16:33,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33,374][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.0837320014834404, acc: 0.9663865566253662)
[2025-02-13 20:16:33,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33,762][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.09338745474815369, acc: 0.978723406791687)
[2025-02-13 20:16:33,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34,153][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.04154285043478012, acc: 1.0)
[2025-02-13 20:16:34,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34,559][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.4016368091106415, acc: 0.886904776096344)
[2025-02-13 20:16:34,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34,950][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.19559413194656372, acc: 0.9545454382896423)
[2025-02-13 20:16:35,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35,338][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.19980968534946442, acc: 0.9629629850387573)
[2025-02-13 20:16:35,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35,735][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.04524974524974823, acc: 0.9870129823684692)
[2025-02-13 20:16:35,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36,105][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.17079918086528778, acc: 0.9587628841400146)
[2025-02-13 20:16:36,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36,488][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.16275162994861603, acc: 0.9638554453849792)
[2025-02-13 20:16:36,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36,826][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.1344965547323227, acc: 0.954954981803894)
[2025-02-13 20:16:36,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37,182][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.21294273436069489, acc: 0.9343065619468689)
[2025-02-13 20:16:37,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37,544][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.13406464457511902, acc: 0.9663865566253662)
[2025-02-13 20:16:37,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37,915][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.14624513685703278, acc: 0.9750000238418579)
[2025-02-13 20:16:38,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38,319][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.194483682513237, acc: 0.9609375)
[2025-02-13 20:16:38,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38,778][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.1798071265220642, acc: 0.9594594836235046)
[2025-02-13 20:16:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39,151][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.21053408086299896, acc: 0.9347826242446899)
[2025-02-13 20:16:39,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39,537][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.2359165996313095, acc: 0.9590163826942444)
[2025-02-13 20:16:39,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39,939][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.07634786516427994, acc: 0.9571428298950195)
[2025-02-13 20:16:40,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40,334][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.19645626842975616, acc: 0.9666666388511658)
[2025-02-13 20:16:40,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40,768][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.2070440798997879, acc: 0.9578947424888611)
[2025-02-13 20:16:40,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41,165][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.20667453110218048, acc: 0.9408866763114929)
[2025-02-13 20:16:41,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41,547][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.09604649990797043, acc: 0.9781420826911926)
[2025-02-13 20:16:41,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41,961][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.15605734288692474, acc: 0.9583333134651184)
[2025-02-13 20:16:42,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42,364][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.17596547305583954, acc: 0.970588207244873)
[2025-02-13 20:16:42,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42,819][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.07937025278806686, acc: 0.987261176109314)
[2025-02-13 20:16:42,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43,235][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.1381252557039261, acc: 0.9850000143051147)
[2025-02-13 20:16:43,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43,649][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.08073348551988602, acc: 0.9759036302566528)
[2025-02-13 20:16:43,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44,026][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.0948464572429657, acc: 0.9684210419654846)
[2025-02-13 20:16:44,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44,435][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.1036657840013504, acc: 0.9753086566925049)
[2025-02-13 20:16:44,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44,867][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.25263670086860657, acc: 0.9383886456489563)
[2025-02-13 20:16:45,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45,297][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.1095719113945961, acc: 0.9750000238418579)
[2025-02-13 20:16:45,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45,669][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.13198940455913544, acc: 0.9748743772506714)
[2025-02-13 20:16:45,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46,096][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.2109414041042328, acc: 0.9512194991111755)
[2025-02-13 20:16:46,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46,441][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.16897258162498474, acc: 0.9453551769256592)
[2025-02-13 20:16:46,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46,854][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.11783468723297119, acc: 0.9870129823684692)
[2025-02-13 20:16:46,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47,233][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.03626800701022148, acc: 0.9942857027053833)
[2025-02-13 20:16:47,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47,650][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.13791097700595856, acc: 0.9646464586257935)
[2025-02-13 20:16:47,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48,050][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.1521604210138321, acc: 0.9634703397750854)
[2025-02-13 20:16:48,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48,437][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.16131557524204254, acc: 0.9469026327133179)
[2025-02-13 20:16:48,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48,845][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.11625038087368011, acc: 0.967391312122345)
[2025-02-13 20:16:48,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49,252][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.07445868104696274, acc: 0.9910714030265808)
[2025-02-13 20:16:49,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49,635][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.04440454766154289, acc: 1.0)
[2025-02-13 20:16:49,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50,059][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.07943660020828247, acc: 0.9710144996643066)
[2025-02-13 20:16:50,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50,473][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.0919804498553276, acc: 0.9757281541824341)
[2025-02-13 20:16:50,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50,886][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.17528598010540009, acc: 0.9511111378669739)
[2025-02-13 20:16:51,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51,265][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.0453236848115921, acc: 0.9884393215179443)
[2025-02-13 20:16:51,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51,651][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.16589288413524628, acc: 0.9681528806686401)
[2025-02-13 20:16:51,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52,037][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 1.0400902032852173, acc: 0.7714285850524902)
[2025-02-13 20:16:52,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52,433][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.8081162571907043, acc: 0.8083333373069763)
[2025-02-13 20:16:52,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52,813][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.2174636870622635, acc: 0.9536423683166504)
[2025-02-13 20:16:52,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53,200][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.16111387312412262, acc: 0.9651162624359131)
[2025-02-13 20:16:53,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53,596][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.08035773783922195, acc: 0.9880239367485046)
[2025-02-13 20:16:53,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53,964][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.03292153775691986, acc: 1.0)
[2025-02-13 20:16:54,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54,338][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.30395957827568054, acc: 0.9259259104728699)
[2025-02-13 20:16:54,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54,668][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.16525019705295563, acc: 0.9593495726585388)
[2025-02-13 20:16:54,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55,028][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.16380177438259125, acc: 0.9268292784690857)
[2025-02-13 20:16:55,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55,395][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.21607984602451324, acc: 0.9333333373069763)
[2025-02-13 20:16:55,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55,766][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.12290284037590027, acc: 0.9921875)
[2025-02-13 20:16:55,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56,108][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.1827269047498703, acc: 0.9635036587715149)
[2025-02-13 20:16:56,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56,493][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.2293405383825302, acc: 0.9453551769256592)
[2025-02-13 20:16:56,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56,848][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.17834457755088806, acc: 0.9648241400718689)
[2025-02-13 20:16:56,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57,265][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.09631650894880295, acc: 0.9814814925193787)
[2025-02-13 20:16:57,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57,687][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.12228851020336151, acc: 0.9710982441902161)
[2025-02-13 20:16:57,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58,092][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.06339295208454132, acc: 0.9883720874786377)
[2025-02-13 20:16:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58,485][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.22840182483196259, acc: 0.9558823704719543)
[2025-02-13 20:16:58,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58,878][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.09909176826477051, acc: 0.9805825352668762)
[2025-02-13 20:16:59,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59,281][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.17744871973991394, acc: 0.9648241400718689)
[2025-02-13 20:16:59,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59,709][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.11045707017183304, acc: 0.9685039520263672)
[2025-02-13 20:16:59,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00,118][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.12322616577148438, acc: 0.9726775884628296)
[2025-02-13 20:17:00,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00,512][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.0379817895591259, acc: 0.9944751262664795)
[2025-02-13 20:17:00,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00,914][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.048561595380306244, acc: 0.9933775067329407)
[2025-02-13 20:17:01,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01,308][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.059832047671079636, acc: 0.9885057210922241)
[2025-02-13 20:17:01,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01,666][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.1557922512292862, acc: 0.9767441749572754)
[2025-02-13 20:17:01,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02,051][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.08227347582578659, acc: 0.9725274443626404)
[2025-02-13 20:17:02,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02,451][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.10035143792629242, acc: 0.9738562107086182)
[2025-02-13 20:17:02,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02,862][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.10612320154905319, acc: 0.9695431590080261)
[2025-02-13 20:17:03,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03,236][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.07644254714250565, acc: 0.9893617033958435)
[2025-02-13 20:17:03,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03,600][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.05222838744521141, acc: 0.9863013625144958)
[2025-02-13 20:17:03,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03,972][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.22445252537727356, acc: 0.9325153231620789)
[2025-02-13 20:17:04,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04,368][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.14550013840198517, acc: 0.9533678889274597)
[2025-02-13 20:17:04,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04,765][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.27688416838645935, acc: 0.9405405521392822)
[2025-02-13 20:17:04,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05,154][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.11666062474250793, acc: 0.9751552939414978)
[2025-02-13 20:17:05,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05,563][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.3003472685813904, acc: 0.9182389974594116)
[2025-02-13 20:17:05,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05,938][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.10352667421102524, acc: 0.9833333492279053)
[2025-02-13 20:17:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06,330][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.15317785739898682, acc: 0.977011501789093)
[2025-02-13 20:17:06,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06,719][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.10968600958585739, acc: 0.9856459498405457)
[2025-02-13 20:17:06,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07,102][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.09811662882566452, acc: 0.9810126423835754)
[2025-02-13 20:17:07,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07,510][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.15831582248210907, acc: 0.9642857313156128)
[2025-02-13 20:17:07,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07,901][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.02532022073864937, acc: 1.0)
[2025-02-13 20:17:08,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08,350][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.06023465096950531, acc: 0.9864864945411682)
[2025-02-13 20:17:08,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08,762][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.12347251921892166, acc: 0.9784946441650391)
[2025-02-13 20:17:08,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09,172][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.17185628414154053, acc: 0.9788359999656677)
[2025-02-13 20:17:09,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09,578][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.16025178134441376, acc: 0.9555555582046509)
[2025-02-13 20:17:09,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09,962][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.048605870455503464, acc: 0.9802631735801697)
[2025-02-13 20:17:10,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10,387][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.1009933203458786, acc: 0.9800000190734863)
[2025-02-13 20:17:10,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10,792][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.06301217526197433, acc: 0.9824561476707458)
[2025-02-13 20:17:10,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11,176][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.1679357886314392, acc: 0.9655172228813171)
[2025-02-13 20:17:11,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11,548][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.17116668820381165, acc: 0.9552238583564758)
[2025-02-13 20:17:11,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11,926][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.07450597733259201, acc: 0.9823529124259949)
[2025-02-13 20:17:12,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12,286][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.10377558320760727, acc: 0.9622641801834106)
[2025-02-13 20:17:12,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12,704][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.20045173168182373, acc: 0.9427083134651184)
[2025-02-13 20:17:12,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13,132][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.09381342679262161, acc: 0.9748427867889404)
[2025-02-13 20:17:13,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13,565][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.20472700893878937, acc: 0.9510869383811951)
[2025-02-13 20:17:13,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13,965][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.11725009232759476, acc: 0.9594594836235046)
[2025-02-13 20:17:14,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14,358][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.14524975419044495, acc: 0.9542483687400818)
[2025-02-13 20:17:14,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14,750][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.09613314270973206, acc: 0.987261176109314)
[2025-02-13 20:17:14,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15,177][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.06243492290377617, acc: 0.9908257126808167)
[2025-02-13 20:17:15,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15,561][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.23727211356163025, acc: 0.9428571462631226)
[2025-02-13 20:17:15,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15,936][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.18982715904712677, acc: 0.9572649598121643)
[2025-02-13 20:17:16,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16,328][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.10072910785675049, acc: 0.970370352268219)
[2025-02-13 20:17:16,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16,705][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.14897164702415466, acc: 0.9593495726585388)
[2025-02-13 20:17:16,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17,056][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.08406376093626022, acc: 0.9897959232330322)
[2025-02-13 20:17:17,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17,450][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.1676478385925293, acc: 0.9523809552192688)
[2025-02-13 20:17:17,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17,846][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.1525982916355133, acc: 0.9527027010917664)
[2025-02-13 20:17:18,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18,235][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.07638516277074814, acc: 0.9937888383865356)
[2025-02-13 20:17:18,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18,641][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.17628981173038483, acc: 0.976047933101654)
[2025-02-13 20:17:18,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19,073][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.05117933079600334, acc: 0.9848484992980957)
[2025-02-13 20:17:19,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19,496][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.34912437200546265, acc: 0.9285714030265808)
[2025-02-13 20:17:19,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19,898][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.12034245580434799, acc: 0.9609375)
[2025-02-13 20:17:20,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20,264][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.047178126871585846, acc: 0.9910714030265808)
[2025-02-13 20:17:20,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20,627][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.07273266464471817, acc: 0.9924812316894531)
[2025-02-13 20:17:20,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21,033][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.04763861000537872, acc: 0.9944751262664795)
[2025-02-13 20:17:21,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21,437][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.06510314345359802, acc: 0.9894737005233765)
[2025-02-13 20:17:21,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21,801][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.0877203568816185, acc: 0.9826086759567261)
[2025-02-13 20:17:21,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22,191][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.09966188669204712, acc: 0.9677419066429138)
[2025-02-13 20:17:22,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22,628][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.051511023193597794, acc: 0.9876543283462524)
[2025-02-13 20:17:22,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23,030][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.06971003860235214, acc: 0.9764705896377563)
[2025-02-13 20:17:23,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23,406][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.12131284177303314, acc: 0.9793103337287903)
[2025-02-13 20:17:23,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23,784][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.3063660264015198, acc: 0.9545454382896423)
[2025-02-13 20:17:23,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24,167][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.14815369248390198, acc: 0.9724137783050537)
[2025-02-13 20:17:24,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24,553][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.03532686457037926, acc: 0.991150438785553)
[2025-02-13 20:17:24,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24,930][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.08672577142715454, acc: 0.9767441749572754)
[2025-02-13 20:17:25,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25,343][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.04455395042896271, acc: 0.9861111044883728)
[2025-02-13 20:17:25,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25,742][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.16695483028888702, acc: 0.9583333134651184)
[2025-02-13 20:17:25,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26,106][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.286392480134964, acc: 0.9150943160057068)
[2025-02-13 20:17:26,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26,468][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.044655703008174896, acc: 0.9914529919624329)
[2025-02-13 20:17:26,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26,834][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.10753417760133743, acc: 0.9912280440330505)
[2025-02-13 20:17:26,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27,267][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.040358856320381165, acc: 0.9863945841789246)
[2025-02-13 20:17:27,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27,646][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.03447907418012619, acc: 0.9863013625144958)
[2025-02-13 20:17:27,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28,039][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.13252106308937073, acc: 0.9682539701461792)
[2025-02-13 20:17:28,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28,438][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.1615789830684662, acc: 0.9537572264671326)
[2025-02-13 20:17:28,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28,860][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.22920866310596466, acc: 0.9580838084220886)
[2025-02-13 20:17:29,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29,257][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.2078114151954651, acc: 0.9515151381492615)
[2025-02-13 20:17:29,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29,628][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.2739061117172241, acc: 0.9151515364646912)
[2025-02-13 20:17:29,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30,060][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.2555042803287506, acc: 0.9588235020637512)
[2025-02-13 20:17:30,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30,496][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.18728500604629517, acc: 0.9475982785224915)
[2025-02-13 20:17:30,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30,924][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.1746329665184021, acc: 0.9473684430122375)
[2025-02-13 20:17:31,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31,339][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.11356448382139206, acc: 0.9832402467727661)
[2025-02-13 20:17:31,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31,752][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.09157387912273407, acc: 0.9743589758872986)
[2025-02-13 20:17:31,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32,102][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.05787068232893944, acc: 0.9777777791023254)
[2025-02-13 20:17:32,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32,476][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.10365156829357147, acc: 0.9704142212867737)
[2025-02-13 20:17:32,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32,851][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.15957802534103394, acc: 0.9508196711540222)
[2025-02-13 20:17:32,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33,205][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.2531810998916626, acc: 0.9407407641410828)
[2025-02-13 20:17:33,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33,550][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.054547205567359924, acc: 0.991525411605835)
[2025-02-13 20:17:33,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33,917][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.15448488295078278, acc: 0.970059871673584)
[2025-02-13 20:17:34,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34,279][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.2252356857061386, acc: 0.9473684430122375)
[2025-02-13 20:17:34,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34,650][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.08394785225391388, acc: 0.9728260636329651)
[2025-02-13 20:17:34,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35,022][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.10981222987174988, acc: 0.9640718698501587)
[2025-02-13 20:17:35,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35,396][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.12599536776542664, acc: 0.978723406791687)
[2025-02-13 20:17:35,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35,786][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.07444699108600616, acc: 0.9864864945411682)
[2025-02-13 20:17:35,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36,146][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.08472493290901184, acc: 0.9743589758872986)
[2025-02-13 20:17:36,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36,541][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.04393262416124344, acc: 1.0)
[2025-02-13 20:17:36,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36,928][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.045027412474155426, acc: 0.9891892075538635)
[2025-02-13 20:17:37,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37,339][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.10578552633523941, acc: 0.9786096215248108)
[2025-02-13 20:17:37,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37,758][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.09799212217330933, acc: 0.9842932224273682)
[2025-02-13 20:17:37,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38,152][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.07498694956302643, acc: 0.9880239367485046)
[2025-02-13 20:17:38,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38,537][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.12413206696510315, acc: 0.9712643623352051)
[2025-02-13 20:17:38,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38,926][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.05710217356681824, acc: 0.9836065769195557)
[2025-02-13 20:17:39,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39,296][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.10411583632230759, acc: 0.9839572310447693)
[2025-02-13 20:17:39,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39,693][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.07585727423429489, acc: 0.9790209531784058)
[2025-02-13 20:17:39,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40,129][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.06732122600078583, acc: 0.9863013625144958)
[2025-02-13 20:17:40,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40,534][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.1656171977519989, acc: 0.9640287756919861)
[2025-02-13 20:17:40,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40,922][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.06947946548461914, acc: 0.9878787994384766)
[2025-02-13 20:17:41,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41,275][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.2590893507003784, acc: 0.9379310607910156)
[2025-02-13 20:17:41,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41,640][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.14240668714046478, acc: 0.9649122953414917)
[2025-02-13 20:17:41,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42,004][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.09696000814437866, acc: 0.9649122953414917)
[2025-02-13 20:17:42,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42,365][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.08608498424291611, acc: 0.9707602262496948)
[2025-02-13 20:17:42,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42,725][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.10180236399173737, acc: 0.9698795080184937)
[2025-02-13 20:17:42,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43,152][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.13883566856384277, acc: 0.9622641801834106)
[2025-02-13 20:17:43,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43,558][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.07383015006780624, acc: 0.9857142567634583)
[2025-02-13 20:17:43,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43,971][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.2602720558643341, acc: 0.9386503100395203)
[2025-02-13 20:17:44,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44,352][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.4781261384487152, acc: 0.922535240650177)
[2025-02-13 20:17:44,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44,741][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.16212888062000275, acc: 0.9695122241973877)
[2025-02-13 20:17:44,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45,118][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.167231485247612, acc: 0.9823529124259949)
[2025-02-13 20:17:45,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45,531][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.4080607295036316, acc: 0.9497487545013428)
[2025-02-13 20:17:45,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45,913][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.09071847051382065, acc: 0.9826589822769165)
[2025-02-13 20:17:46,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46,301][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.14526742696762085, acc: 0.9652777910232544)
[2025-02-13 20:17:46,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46,680][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.059847183525562286, acc: 1.0)
[2025-02-13 20:17:46,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47,052][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.042614664882421494, acc: 0.9863945841789246)
[2025-02-13 20:17:47,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47,413][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.37422409653663635, acc: 0.9102563858032227)
[2025-02-13 20:17:47,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47,783][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.08132342249155045, acc: 0.9839572310447693)
[2025-02-13 20:17:47,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48,168][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.08837719261646271, acc: 0.9781022071838379)
[2025-02-13 20:17:48,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48,583][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.03678765892982483, acc: 1.0)
[2025-02-13 20:17:48,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48,958][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.04903404414653778, acc: 0.9947090148925781)
[2025-02-13 20:17:49,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49,373][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.1942535638809204, acc: 0.9690265655517578)
[2025-02-13 20:17:49,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49,784][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.13344600796699524, acc: 0.970588207244873)
[2025-02-13 20:17:49,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50,166][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.09784158319234848, acc: 0.9781420826911926)
[2025-02-13 20:17:50,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50,540][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.08803706616163254, acc: 0.9801324605941772)
[2025-02-13 20:17:50,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50,941][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.10190040618181229, acc: 0.9781420826911926)
[2025-02-13 20:17:51,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51,331][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.1698412150144577, acc: 0.9550561904907227)
[2025-02-13 20:17:51,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51,769][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.06496509164571762, acc: 0.9878787994384766)
[2025-02-13 20:17:51,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52,203][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.08605305850505829, acc: 0.9797297120094299)
[2025-02-13 20:17:52,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52,620][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.08294151723384857, acc: 0.9698795080184937)
[2025-02-13 20:17:52,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53,050][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.1217951700091362, acc: 0.9798657894134521)
[2025-02-13 20:17:53,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53,474][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.15341250598430634, acc: 0.9444444179534912)
[2025-02-13 20:17:53,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53,906][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.16871699690818787, acc: 0.956250011920929)
[2025-02-13 20:17:54,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54,347][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.08301354199647903, acc: 0.9930555820465088)
[2025-02-13 20:17:54,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54,764][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.06125981733202934, acc: 0.9934640526771545)
[2025-02-13 20:17:54,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55,201][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.113481305539608, acc: 0.9608938694000244)
[2025-02-13 20:17:55,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55,589][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.1211514100432396, acc: 0.9813664555549622)
[2025-02-13 20:17:55,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55,981][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.044443279504776, acc: 1.0)
[2025-02-13 20:17:56,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56,334][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.10491981357336044, acc: 0.9795918464660645)
[2025-02-13 20:17:56,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56,713][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.14633044600486755, acc: 0.9587628841400146)
[2025-02-13 20:17:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57,063][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.08875219523906708, acc: 0.9696969985961914)
[2025-02-13 20:17:57,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57,441][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.06924089044332504, acc: 0.9815950989723206)
[2025-02-13 20:17:57,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57,831][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.04093090444803238, acc: 0.9885057210922241)
[2025-02-13 20:17:57,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58,200][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.05122818052768707, acc: 0.9861111044883728)
[2025-02-13 20:17:58,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58,590][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.026725947856903076, acc: 1.0)
[2025-02-13 20:17:58,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58,967][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.053711529821157455, acc: 0.9945054650306702)
[2025-02-13 20:17:59,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59,338][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.055026065558195114, acc: 0.9940119981765747)
[2025-02-13 20:17:59,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59,714][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.045768897980451584, acc: 0.9940476417541504)
[2025-02-13 20:17:59,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00,091][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.044379521161317825, acc: 0.9888268113136292)
[2025-02-13 20:18:00,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00,449][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.024198591709136963, acc: 1.0)
[2025-02-13 20:18:00,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00,812][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.1281803697347641, acc: 0.9829545617103577)
[2025-02-13 20:18:00,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:01,184][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.016521278768777847, acc: 1.0)
[2025-02-13 20:18:01,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:01,564][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.21571549773216248, acc: 0.9344262480735779)
[2025-02-13 20:18:01,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02,019][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.41287097334861755, acc: 0.9125000238418579)
[2025-02-13 20:18:02,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02,424][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.9045803546905518, acc: 0.8203883767127991)
[2025-02-13 20:18:02,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02,833][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.6440486907958984, acc: 0.8656716346740723)
[2025-02-13 20:18:02,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03,209][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.18847860395908356, acc: 0.9613259434700012)
[2025-02-13 20:18:03,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03,646][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.2323138415813446, acc: 0.9399999976158142)
[2025-02-13 20:18:03,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04,026][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.12430651485919952, acc: 0.9757575988769531)
[2025-02-13 20:18:04,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04,398][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.15821507573127747, acc: 0.9707602262496948)
[2025-02-13 20:18:04,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04,785][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.11856262385845184, acc: 0.9651162624359131)
[2025-02-13 20:18:04,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05,163][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.13434071838855743, acc: 0.9716981053352356)
[2025-02-13 20:18:05,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05,523][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.126680389046669, acc: 0.9586206674575806)
[2025-02-13 20:18:05,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05,967][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.12412696331739426, acc: 0.9759036302566528)
[2025-02-13 20:18:06,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06,335][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.09254763275384903, acc: 0.9793103337287903)
[2025-02-13 20:18:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06,727][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.2471141517162323, acc: 0.9371428489685059)
[2025-02-13 20:18:06,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07,096][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.22419971227645874, acc: 0.959770143032074)
[2025-02-13 20:18:07,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07,559][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.2445133924484253, acc: 0.9395604133605957)
[2025-02-13 20:18:07,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07,949][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.17215676605701447, acc: 0.9631901979446411)
[2025-02-13 20:18:08,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08,354][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.18952369689941406, acc: 0.9615384340286255)
[2025-02-13 20:18:08,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08,771][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.10405515134334564, acc: 0.9804878234863281)
[2025-02-13 20:18:08,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09,168][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.14970983564853668, acc: 0.9587628841400146)
[2025-02-13 20:18:09,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09,547][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.1314399093389511, acc: 0.9836956262588501)
[2025-02-13 20:18:09,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09,934][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.11197992414236069, acc: 0.9783783555030823)
[2025-02-13 20:18:10,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10,365][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.13132697343826294, acc: 0.970059871673584)
[2025-02-13 20:18:10,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10,751][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.28801462054252625, acc: 0.9189189076423645)
[2025-02-13 20:18:10,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11,129][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.49018990993499756, acc: 0.8541666865348816)
[2025-02-13 20:18:11,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11,498][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.28577709197998047, acc: 0.9433962106704712)
[2025-02-13 20:18:11,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11,852][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.1309105008840561, acc: 0.9597989916801453)
[2025-02-13 20:18:11,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12,243][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.027296166867017746, acc: 0.9920634627342224)
[2025-02-13 20:18:12,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12,667][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.08079730719327927, acc: 0.9902912378311157)
[2025-02-13 20:18:12,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13,099][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.13308340311050415, acc: 0.9587628841400146)
[2025-02-13 20:18:13,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13,496][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.05477633699774742, acc: 0.9858490824699402)
[2025-02-13 20:18:13,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13,862][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.09773657470941544, acc: 0.988095223903656)
[2025-02-13 20:18:14,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14,282][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.08802953362464905, acc: 0.981249988079071)
[2025-02-13 20:18:14,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14,674][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.21523459255695343, acc: 0.9444444179534912)
[2025-02-13 20:18:14,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15,070][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.10823672264814377, acc: 0.9593908786773682)
[2025-02-13 20:18:15,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15,446][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.14562073349952698, acc: 0.9631336331367493)
[2025-02-13 20:18:15,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15,818][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.0963645651936531, acc: 0.9949238300323486)
[2025-02-13 20:18:15,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16,182][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.038135770708322525, acc: 0.9851484894752502)
[2025-02-13 20:18:16,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16,563][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.027484722435474396, acc: 1.0)
[2025-02-13 20:18:16,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16,934][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.03613622859120369, acc: 0.9942857027053833)
[2025-02-13 20:18:17,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17,329][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.07202040404081345, acc: 0.9767441749572754)
[2025-02-13 20:18:17,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17,696][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.17184330523014069, acc: 0.9727891087532043)
[2025-02-13 20:18:17,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18,083][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.18845322728157043, acc: 0.976190447807312)
[2025-02-13 20:18:18,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18,453][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.09924066811800003, acc: 0.9714285731315613)
[2025-02-13 20:18:18,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18,819][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.31692442297935486, acc: 0.9212121367454529)
[2025-02-13 20:18:18,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19,171][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.2343684434890747, acc: 0.9640287756919861)
[2025-02-13 20:18:19,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19,568][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.02883128821849823, acc: 1.0)
[2025-02-13 20:18:19,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19,931][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.09181657433509827, acc: 0.9873417615890503)
[2025-02-13 20:18:20,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20,275][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.06840422004461288, acc: 0.9788732528686523)
[2025-02-13 20:18:20,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20,717][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.08660668134689331, acc: 0.9794520735740662)
[2025-02-13 20:18:20,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21,152][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.038351383060216904, acc: 0.9834710955619812)
[2025-02-13 20:18:21,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21,612][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.09140356630086899, acc: 0.9694656729698181)
[2025-02-13 20:18:21,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22,057][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.20072048902511597, acc: 0.970588207244873)
[2025-02-13 20:18:22,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22,510][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.05046219751238823, acc: 0.9928057789802551)
[2025-02-13 20:18:22,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22,899][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.07596035301685333, acc: 0.9848484992980957)
[2025-02-13 20:18:23,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23,347][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.089767687022686, acc: 0.969924807548523)
[2025-02-13 20:18:23,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23,733][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.08997385203838348, acc: 0.9692307710647583)
[2025-02-13 20:18:23,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24,105][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.034176964312791824, acc: 0.9833333492279053)
[2025-02-13 20:18:24,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24,471][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.1337871104478836, acc: 0.9923076629638672)
[2025-02-13 20:18:24,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24,838][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.1277085244655609, acc: 0.9754098653793335)
[2025-02-13 20:18:24,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25,211][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.13212339580059052, acc: 0.9583333134651184)
[2025-02-13 20:18:25,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25,568][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.07498166710138321, acc: 0.9793814420700073)
[2025-02-13 20:18:25,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25,927][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.08204780519008636, acc: 0.9931507110595703)
[2025-02-13 20:18:26,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26,329][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.051826804876327515, acc: 0.9864864945411682)
[2025-02-13 20:18:26,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26,684][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.039073646068573, acc: 1.0)
[2025-02-13 20:18:26,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27,074][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.10027776658535004, acc: 0.975806474685669)
[2025-02-13 20:18:27,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27,444][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.06660100817680359, acc: 0.984375)
[2025-02-13 20:18:27,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27,840][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.09993001818656921, acc: 0.9819819927215576)
[2025-02-13 20:18:27,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28,223][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.04535292461514473, acc: 0.9795918464660645)
[2025-02-13 20:18:28,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28,593][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.15102475881576538, acc: 0.9520000219345093)
[2025-02-13 20:18:28,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28,997][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.09503272920846939, acc: 0.9597315192222595)
[2025-02-13 20:18:29,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29,390][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.07761454582214355, acc: 0.9925373196601868)
[2025-02-13 20:18:29,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29,764][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.12231079488992691, acc: 0.9797297120094299)
[2025-02-13 20:18:29,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30,127][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.07867033779621124, acc: 0.9844961166381836)
[2025-02-13 20:18:30,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30,487][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.17340654134750366, acc: 0.9642857313156128)
[2025-02-13 20:18:30,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30,848][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.08165967464447021, acc: 0.9807692170143127)
[2025-02-13 20:18:30,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31,203][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.06857704371213913, acc: 0.9821428656578064)
[2025-02-13 20:18:31,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31,558][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.05781504884362221, acc: 0.9849624037742615)
[2025-02-13 20:18:31,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31,923][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.03426554799079895, acc: 0.9922480583190918)
[2025-02-13 20:18:32,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32,318][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.06589862704277039, acc: 0.9822485446929932)
[2025-02-13 20:18:32,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32,726][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.10311923176050186, acc: 0.9830508232116699)
[2025-02-13 20:18:32,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33,165][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.1306115835905075, acc: 0.9631578922271729)
[2025-02-13 20:18:33,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33,552][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.08328063786029816, acc: 0.981249988079071)
[2025-02-13 20:18:33,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33,948][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.08545536547899246, acc: 0.9796954393386841)
[2025-02-13 20:18:34,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34,347][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.09132636338472366, acc: 0.9702380895614624)
[2025-02-13 20:18:34,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34,743][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.07505034655332565, acc: 0.9891892075538635)
[2025-02-13 20:18:34,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35,133][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.07219281047582626, acc: 0.9701492786407471)
[2025-02-13 20:18:35,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35,518][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.05605364218354225, acc: 0.978723406791687)
[2025-02-13 20:18:35,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35,899][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.09802335500717163, acc: 0.9729729890823364)
[2025-02-13 20:18:36,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36,284][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.06705500930547714, acc: 0.9729729890823364)
[2025-02-13 20:18:36,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36,693][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.06681980192661285, acc: 0.9815950989723206)
[2025-02-13 20:18:36,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37,072][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.11115308851003647, acc: 0.9736841917037964)
[2025-02-13 20:18:37,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37,511][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.028050841763615608, acc: 0.9942857027053833)
[2025-02-13 20:18:37,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37,925][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.11469832062721252, acc: 0.963350772857666)
[2025-02-13 20:18:38,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38,323][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.07035321742296219, acc: 0.981249988079071)
[2025-02-13 20:18:38,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38,757][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.03524799644947052, acc: 0.9937106966972351)
[2025-02-13 20:18:38,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39,188][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.09042684733867645, acc: 0.9829545617103577)
[2025-02-13 20:18:39,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39,628][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.041458286345005035, acc: 0.9940828680992126)
[2025-02-13 20:18:39,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40,059][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.034938983619213104, acc: 0.9887640476226807)
[2025-02-13 20:18:40,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40,495][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.039410799741744995, acc: 0.9933775067329407)
[2025-02-13 20:18:40,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40,938][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.026620611548423767, acc: 1.0)
[2025-02-13 20:18:41,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41,358][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.08300960063934326, acc: 0.9776536226272583)
[2025-02-13 20:18:41,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41,807][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.01462570857256651, acc: 1.0)
[2025-02-13 20:18:41,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42,231][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.022516626864671707, acc: 1.0)
[2025-02-13 20:18:42,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42,651][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.06101769208908081, acc: 0.9944444298744202)
[2025-02-13 20:18:42,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43,052][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.060780592262744904, acc: 0.9906103014945984)
[2025-02-13 20:18:43,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43,431][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.041940946131944656, acc: 0.9830508232116699)
[2025-02-13 20:18:43,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43,833][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.0216426532715559, acc: 0.9933775067329407)
[2025-02-13 20:18:43,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44,268][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.045858483761548996, acc: 0.9876543283462524)
[2025-02-13 20:18:44,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44,660][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.034166719764471054, acc: 1.0)
[2025-02-13 20:18:44,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45,058][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.1240217462182045, acc: 0.9790576100349426)
[2025-02-13 20:18:45,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45,460][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.07422249019145966, acc: 0.9791666865348816)
[2025-02-13 20:18:45,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45,851][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.13166135549545288, acc: 0.9735449552536011)
[2025-02-13 20:18:45,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46,227][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.1855945885181427, acc: 0.9583333134651184)
[2025-02-13 20:18:46,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46,592][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.13732494413852692, acc: 0.9603960514068604)
[2025-02-13 20:18:46,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46,950][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.23306308686733246, acc: 0.9552238583564758)
[2025-02-13 20:18:47,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47,320][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.0669398307800293, acc: 0.9891892075538635)
[2025-02-13 20:18:47,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47,691][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.12193375080823898, acc: 0.9581151604652405)
[2025-02-13 20:18:47,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48,085][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.1573760062456131, acc: 0.9677419066429138)
[2025-02-13 20:18:48,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48,500][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.12934668362140656, acc: 0.9817351698875427)
[2025-02-13 20:18:48,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48,911][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.0936521515250206, acc: 0.9822485446929932)
[2025-02-13 20:18:49,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49,341][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.13421638309955597, acc: 0.9672897458076477)
[2025-02-13 20:18:49,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49,788][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.060721445828676224, acc: 0.9860464930534363)
[2025-02-13 20:18:49,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50,176][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.29499921202659607, acc: 0.9548022747039795)
[2025-02-13 20:18:50,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50,593][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.3523156940937042, acc: 0.9184549450874329)
[2025-02-13 20:18:50,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50,968][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.22427713871002197, acc: 0.9236640930175781)
[2025-02-13 20:18:51,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51,361][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.13856148719787598, acc: 0.9718309640884399)
[2025-02-13 20:18:51,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51,782][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.07983892410993576, acc: 0.9884393215179443)
[2025-02-13 20:18:51,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52,180][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.19447112083435059, acc: 0.9666666388511658)
[2025-02-13 20:18:52,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52,561][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.12079406529664993, acc: 0.963350772857666)
[2025-02-13 20:18:52,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52,984][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.10301879793405533, acc: 0.9756097793579102)
[2025-02-13 20:18:53,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53,379][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.1962575912475586, acc: 0.9673202633857727)
[2025-02-13 20:18:53,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53,813][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.12454479932785034, acc: 0.956204354763031)
[2025-02-13 20:18:53,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54,201][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.10896018892526627, acc: 0.9865471124649048)
[2025-02-13 20:18:54,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54,581][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.09699130803346634, acc: 0.9746192693710327)
[2025-02-13 20:18:54,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54,980][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.06655842065811157, acc: 0.9938650131225586)
[2025-02-13 20:18:55,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55,387][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.09634663909673691, acc: 0.9784482717514038)
[2025-02-13 20:18:55,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55,799][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.05434265732765198, acc: 0.9844961166381836)
[2025-02-13 20:18:55,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56,203][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.13311444222927094, acc: 0.97826087474823)
[2025-02-13 20:18:56,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56,648][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.09519555419683456, acc: 0.9597315192222595)
[2025-02-13 20:18:56,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57,106][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.06082627549767494, acc: 0.9767441749572754)
[2025-02-13 20:18:57,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57,509][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.09470450133085251, acc: 0.9727891087532043)
[2025-02-13 20:18:57,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57,925][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.11418084800243378, acc: 0.9769230484962463)
[2025-02-13 20:18:58,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58,280][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.06408035755157471, acc: 0.9708737730979919)
[2025-02-13 20:18:58,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58,694][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.2643987238407135, acc: 0.9202898740768433)
[2025-02-13 20:18:58,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59,059][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.25812071561813354, acc: 0.9489051103591919)
[2025-02-13 20:18:59,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59,468][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.08124460279941559, acc: 0.9817073345184326)
[2025-02-13 20:18:59,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59,843][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.06245013698935509, acc: 0.9849624037742615)
[2025-02-13 20:18:59,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00,257][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.10689021646976471, acc: 0.9802631735801697)
[2025-02-13 20:19:00,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00,650][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.1339440941810608, acc: 0.9774011373519897)
[2025-02-13 20:19:00,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01,056][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.1784716546535492, acc: 0.966292142868042)
[2025-02-13 20:19:01,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01,442][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.3018394708633423, acc: 0.9558011293411255)
[2025-02-13 20:19:01,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01,822][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.2392028570175171, acc: 0.9576719403266907)
[2025-02-13 20:19:01,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02,207][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.23947666585445404, acc: 0.9567901492118835)
[2025-02-13 20:19:02,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02,601][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.11367134749889374, acc: 0.9715909361839294)
[2025-02-13 20:19:02,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03,043][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.15257783234119415, acc: 0.9631578922271729)
[2025-02-13 20:19:03,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03,411][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.12641845643520355, acc: 0.9567567706108093)
[2025-02-13 20:19:03,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03,809][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.1883115917444229, acc: 0.9555555582046509)
[2025-02-13 20:19:03,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04,210][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.05965656414628029, acc: 0.9863945841789246)
[2025-02-13 20:19:04,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04,656][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.17178933322429657, acc: 0.9664804339408875)
[2025-02-13 20:19:04,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05,079][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.10582657158374786, acc: 0.9812206625938416)
[2025-02-13 20:19:05,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05,496][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.21218430995941162, acc: 0.9572192430496216)
[2025-02-13 20:19:05,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05,944][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.12915605306625366, acc: 0.9772727489471436)
[2025-02-13 20:19:06,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06,409][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.04891001060605049, acc: 0.9847715497016907)
[2025-02-13 20:19:06,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06,821][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.13576364517211914, acc: 0.9661017060279846)
[2025-02-13 20:19:06,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07,202][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.028112463653087616, acc: 1.0)
[2025-02-13 20:19:07,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07,611][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.2545769214630127, acc: 0.9659090638160706)
[2025-02-13 20:19:07,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08,007][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.09008399397134781, acc: 0.9765625)
[2025-02-13 20:19:08,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08,387][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.12155238538980484, acc: 0.9587628841400146)
[2025-02-13 20:19:08,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08,792][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.07271755486726761, acc: 0.9770992398262024)
[2025-02-13 20:19:08,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09,181][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.07383554428815842, acc: 0.9858155846595764)
[2025-02-13 20:19:09,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09,561][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.19630999863147736, acc: 0.9777777791023254)
[2025-02-13 20:19:09,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09,989][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.16315717995166779, acc: 0.9640287756919861)
[2025-02-13 20:19:10,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10,483][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.11331571638584137, acc: 0.9794520735740662)
[2025-02-13 20:19:10,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10,865][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.17025746405124664, acc: 0.9729729890823364)
[2025-02-13 20:19:11,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11,275][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.08018699288368225, acc: 0.976047933101654)
[2025-02-13 20:19:11,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11,714][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.15280967950820923, acc: 0.9677419066429138)
[2025-02-13 20:19:11,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12,137][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.10417509078979492, acc: 0.9807692170143127)
[2025-02-13 20:19:12,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12,528][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.13568100333213806, acc: 0.9729729890823364)
[2025-02-13 20:19:12,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12,907][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.1590574085712433, acc: 0.9545454382896423)
[2025-02-13 20:19:13,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13,295][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.10375300794839859, acc: 0.9863013625144958)
[2025-02-13 20:19:13,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13,673][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.05788882449269295, acc: 0.9888268113136292)
[2025-02-13 20:19:13,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14,098][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.05147701874375343, acc: 0.9935483932495117)
[2025-02-13 20:19:14,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14,516][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.7463499903678894, acc: 0.8500000238418579)
[2025-02-13 20:19:14,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14,896][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.18755249679088593, acc: 0.9618320465087891)
[2025-02-13 20:19:15,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15,269][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.09304818511009216, acc: 0.9788732528686523)
[2025-02-13 20:19:15,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15,685][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.13063615560531616, acc: 0.9719101190567017)
[2025-02-13 20:19:15,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16,068][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.18985295295715332, acc: 0.9624060392379761)
[2025-02-13 20:19:16,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16,437][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.3136417269706726, acc: 0.9398496150970459)
[2025-02-13 20:19:16,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16,823][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.34773871302604675, acc: 0.9244186282157898)
[2025-02-13 20:19:16,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17,209][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.14570534229278564, acc: 0.947826087474823)
[2025-02-13 20:19:17,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17,634][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.0986727848649025, acc: 0.978723406791687)
[2025-02-13 20:19:17,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18,050][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.19139645993709564, acc: 0.9587628841400146)
[2025-02-13 20:19:18,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18,513][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.07835087925195694, acc: 0.9807692170143127)
[2025-02-13 20:19:18,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18,937][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.15197929739952087, acc: 0.9617834687232971)
[2025-02-13 20:19:19,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19,315][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.056208278983831406, acc: 0.9863013625144958)
[2025-02-13 20:19:19,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19,688][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.0333615206182003, acc: 1.0)
[2025-02-13 20:19:19,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20,073][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.0898727998137474, acc: 0.9855072498321533)
[2025-02-13 20:19:20,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20,446][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.09422291815280914, acc: 0.9860140085220337)
[2025-02-13 20:19:20,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20,829][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.2097092568874359, acc: 0.9556962251663208)
[2025-02-13 20:19:20,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21,230][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.1310475766658783, acc: 0.9814814925193787)
[2025-02-13 20:19:21,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21,629][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.12750446796417236, acc: 0.9673202633857727)
[2025-02-13 20:19:21,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21,985][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.1692122370004654, acc: 0.9508196711540222)
[2025-02-13 20:19:22,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22,345][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.07720430940389633, acc: 0.9779411554336548)
[2025-02-13 20:19:22,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22,743][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.09530513733625412, acc: 0.9725274443626404)
[2025-02-13 20:19:22,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23,116][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.09446821361780167, acc: 0.9603174328804016)
[2025-02-13 20:19:23,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23,448][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.08207530528306961, acc: 0.9797979593276978)
[2025-02-13 20:19:23,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23,823][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.17225636541843414, acc: 0.9518072009086609)
[2025-02-13 20:19:23,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24,227][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.13794362545013428, acc: 0.9709302186965942)
[2025-02-13 20:19:24,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24,634][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.2464742511510849, acc: 0.9360465407371521)
[2025-02-13 20:19:24,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25,035][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.17121849954128265, acc: 0.9753086566925049)
[2025-02-13 20:19:25,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25,405][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.06895595788955688, acc: 0.9800000190734863)
[2025-02-13 20:19:25,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25,767][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.1700371503829956, acc: 0.9464285969734192)
[2025-02-13 20:19:25,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26,133][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.12354554235935211, acc: 0.9691358208656311)
[2025-02-13 20:19:26,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26,512][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.11909398436546326, acc: 0.9668508172035217)
[2025-02-13 20:19:26,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26,897][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.07584378123283386, acc: 0.9817073345184326)
[2025-02-13 20:19:27,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27,271][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.09045014530420303, acc: 0.9781022071838379)
[2025-02-13 20:19:27,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27,658][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.08606290817260742, acc: 0.987500011920929)
[2025-02-13 20:19:27,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28,011][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.05399397388100624, acc: 0.9874213933944702)
[2025-02-13 20:19:28,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28,401][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.10245835781097412, acc: 0.9807692170143127)
[2025-02-13 20:19:28,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28,765][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.14014282822608948, acc: 0.9642857313156128)
[2025-02-13 20:19:28,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29,174][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.06592655181884766, acc: 0.9832402467727661)
[2025-02-13 20:19:29,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29,546][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.16571097075939178, acc: 0.9753086566925049)
[2025-02-13 20:19:29,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29,926][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.13887083530426025, acc: 0.956250011920929)
[2025-02-13 20:19:30,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:30,283][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.07071288675069809, acc: 0.9937888383865356)
[2025-02-13 20:19:30,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:30,627][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.3536500632762909, acc: 0.956204354763031)
[2025-02-13 20:19:30,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31,036][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.18622364103794098, acc: 0.9459459185600281)
[2025-02-13 20:19:31,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31,430][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.10157245397567749, acc: 0.9757575988769531)
[2025-02-13 20:19:31,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31,847][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.178836852312088, acc: 0.9503105878829956)
[2025-02-13 20:19:31,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32,215][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.04668784141540527, acc: 0.9855072498321533)
[2025-02-13 20:19:32,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32,572][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.10830885171890259, acc: 0.9880239367485046)
[2025-02-13 20:19:32,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32,968][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.10003330558538437, acc: 0.9750000238418579)
[2025-02-13 20:19:33,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33,385][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.06201205030083656, acc: 0.9883720874786377)
[2025-02-13 20:19:33,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33,754][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.18997922539710999, acc: 0.957446813583374)
[2025-02-13 20:19:33,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34,136][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.12965045869350433, acc: 0.9763779640197754)
[2025-02-13 20:19:34,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34,557][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.10896208137273788, acc: 0.966292142868042)
[2025-02-13 20:19:34,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34,965][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.12362001091241837, acc: 0.96875)
[2025-02-13 20:19:35,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35,336][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.1269095540046692, acc: 0.9589040875434875)
[2025-02-13 20:19:35,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35,704][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.06747418642044067, acc: 0.9858155846595764)
[2025-02-13 20:19:35,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36,088][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.0843428373336792, acc: 0.9818181991577148)
[2025-02-13 20:19:36,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36,458][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.14996220171451569, acc: 0.9631901979446411)
[2025-02-13 20:19:36,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36,836][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.11230059713125229, acc: 0.9640287756919861)
[2025-02-13 20:19:36,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37,206][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.20624741911888123, acc: 0.96875)
[2025-02-13 20:19:37,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37,591][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.042194779962301254, acc: 0.9870967864990234)
[2025-02-13 20:19:37,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37,988][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.09646609425544739, acc: 0.9915966391563416)
[2025-02-13 20:19:38,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38,343][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.05712033808231354, acc: 0.988304078578949)
[2025-02-13 20:19:38,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38,773][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.16324609518051147, acc: 0.9642857313156128)
[2025-02-13 20:19:38,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39,199][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.09083490073680878, acc: 0.9766082167625427)
[2025-02-13 20:19:39,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39,576][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.07489430159330368, acc: 0.9704142212867737)
[2025-02-13 20:19:39,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39,936][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.0637446939945221, acc: 0.9753086566925049)
[2025-02-13 20:19:40,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40,319][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.05586624518036842, acc: 0.983146071434021)
[2025-02-13 20:19:40,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40,681][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.08542027324438095, acc: 0.9870967864990234)
[2025-02-13 20:19:40,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41,051][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.161844402551651, acc: 0.9583333134651184)
[2025-02-13 20:19:41,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41,433][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.09291303902864456, acc: 0.9604519605636597)
[2025-02-13 20:19:41,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41,845][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.052141014486551285, acc: 0.9838709831237793)
[2025-02-13 20:19:41,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42,235][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.18638069927692413, acc: 0.9452736377716064)
[2025-02-13 20:19:42,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42,663][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.2425815612077713, acc: 0.9570552110671997)
[2025-02-13 20:19:42,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43,083][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.15304400026798248, acc: 0.9653179049491882)
[2025-02-13 20:19:43,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43,512][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.07911577075719833, acc: 0.9750000238418579)
[2025-02-13 20:19:43,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43,882][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.14867256581783295, acc: 0.9668874144554138)
[2025-02-13 20:19:44,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44,283][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.20261411368846893, acc: 0.9506173133850098)
[2025-02-13 20:19:44,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44,727][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.09755407273769379, acc: 0.9747474789619446)
[2025-02-13 20:19:44,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45,132][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.1582939475774765, acc: 0.9646464586257935)
[2025-02-13 20:19:45,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45,549][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.1035834327340126, acc: 0.96875)
[2025-02-13 20:19:45,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45,953][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.14837005734443665, acc: 0.9615384340286255)
[2025-02-13 20:19:46,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46,338][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.04930698499083519, acc: 0.9907407164573669)
[2025-02-13 20:19:46,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46,732][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.1262378841638565, acc: 0.9685039520263672)
[2025-02-13 20:19:46,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47,116][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.04572861269116402, acc: 0.9937106966972351)
[2025-02-13 20:19:47,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47,484][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.05817882716655731, acc: 0.9858155846595764)
[2025-02-13 20:19:47,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47,845][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.18743491172790527, acc: 0.9661017060279846)
[2025-02-13 20:19:47,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48,211][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.08066235482692719, acc: 0.9805194735527039)
[2025-02-13 20:19:48,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48,581][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.1258114129304886, acc: 0.9722222089767456)
[2025-02-13 20:19:48,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48,950][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.10805357992649078, acc: 0.9817073345184326)
[2025-02-13 20:19:49,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49,316][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.056712426245212555, acc: 0.9776536226272583)
[2025-02-13 20:19:49,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49,683][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.06068747863173485, acc: 0.9882352948188782)
[2025-02-13 20:19:49,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:50,038][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.029333526268601418, acc: 0.9870129823684692)
[2025-02-13 20:19:50,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:50,420][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.06449885666370392, acc: 0.9786096215248108)
[2025-02-13 20:19:50,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:50,807][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.13141830265522003, acc: 0.9768785834312439)
[2025-02-13 20:19:50,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51,189][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.09165332466363907, acc: 0.9794871807098389)
[2025-02-13 20:19:51,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51,580][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.11737751960754395, acc: 0.9794871807098389)
[2025-02-13 20:19:51,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51,965][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.06544314324855804, acc: 0.9807692170143127)
[2025-02-13 20:19:52,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52,377][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.05477874353528023, acc: 0.9927007555961609)
[2025-02-13 20:19:52,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52,803][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.05110328271985054, acc: 0.976331353187561)
[2025-02-13 20:19:52,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53,226][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.11992552876472473, acc: 0.976047933101654)
[2025-02-13 20:19:53,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53,630][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.06101510301232338, acc: 0.9871794581413269)
[2025-02-13 20:19:53,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54,020][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.029549168422818184, acc: 0.9939758777618408)
[2025-02-13 20:19:54,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54,417][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.0753030776977539, acc: 0.9736841917037964)
[2025-02-13 20:19:54,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54,815][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.05454060807824135, acc: 0.9805194735527039)
[2025-02-13 20:19:54,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55,197][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.06783239543437958, acc: 0.9886363744735718)
[2025-02-13 20:19:55,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55,567][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.08843951672315598, acc: 0.9813664555549622)
[2025-02-13 20:19:55,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55,944][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.15724387764930725, acc: 0.9757575988769531)
[2025-02-13 20:19:56,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56,334][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.22634072601795197, acc: 0.9512194991111755)
[2025-02-13 20:19:56,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56,713][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.13992154598236084, acc: 0.95652174949646)
[2025-02-13 20:19:56,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57,074][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.22438983619213104, acc: 0.9655172228813171)
[2025-02-13 20:19:57,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57,463][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.09277694672346115, acc: 0.9781022071838379)
[2025-02-13 20:19:57,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57,868][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.13118226826190948, acc: 0.9691358208656311)
[2025-02-13 20:19:58,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58,262][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.1053033098578453, acc: 0.976331353187561)
[2025-02-13 20:19:58,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58,659][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.06586333364248276, acc: 0.9878048896789551)
[2025-02-13 20:19:58,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59,061][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.15631358325481415, acc: 0.9554139971733093)
[2025-02-13 20:19:59,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59,464][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.06408178061246872, acc: 0.9862068891525269)
[2025-02-13 20:19:59,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59,849][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.14312715828418732, acc: 0.9757575988769531)
[2025-02-13 20:19:59,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00,212][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.08257946372032166, acc: 0.9863013625144958)
[2025-02-13 20:20:00,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00,579][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.11044543981552124, acc: 0.978723406791687)
[2025-02-13 20:20:00,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00,977][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.08196023106575012, acc: 0.9675675630569458)
[2025-02-13 20:20:01,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01,357][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.0518898107111454, acc: 0.98591548204422)
[2025-02-13 20:20:01,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01,770][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.09610358625650406, acc: 0.9866666793823242)
[2025-02-13 20:20:01,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02,219][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.055379703640937805, acc: 0.9828571677207947)
[2025-02-13 20:20:02,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02,634][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.05083513632416725, acc: 0.9875776171684265)
[2025-02-13 20:20:02,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03,025][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.4825077950954437, acc: 0.916167676448822)
[2025-02-13 20:20:03,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03,423][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.1259583979845047, acc: 0.9673202633857727)
[2025-02-13 20:20:03,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03,833][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.13060559332370758, acc: 0.9640287756919861)
[2025-02-13 20:20:03,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04,235][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.09805923700332642, acc: 0.975806474685669)
[2025-02-13 20:20:04,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04,639][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.14920227229595184, acc: 0.9588235020637512)
[2025-02-13 20:20:04,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05,039][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.2583916485309601, acc: 0.932692289352417)
[2025-02-13 20:20:05,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05,446][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.1883895993232727, acc: 0.9727891087532043)
[2025-02-13 20:20:05,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05,832][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.05528223142027855, acc: 0.9847328066825867)
[2025-02-13 20:20:05,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06,229][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.07400525361299515, acc: 0.9901960492134094)
[2025-02-13 20:20:06,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06,612][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.07400441914796829, acc: 0.9793103337287903)
[2025-02-13 20:20:06,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07,026][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.10137534141540527, acc: 0.9779411554336548)
[2025-02-13 20:20:07,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07,404][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.12052633613348007, acc: 0.9769230484962463)
[2025-02-13 20:20:07,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07,776][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.11045750230550766, acc: 0.970370352268219)
[2025-02-13 20:20:07,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08,146][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.1120559498667717, acc: 0.9784172773361206)
[2025-02-13 20:20:08,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08,493][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.1237478107213974, acc: 0.970588207244873)
[2025-02-13 20:20:08,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08,857][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.11137323826551437, acc: 0.9579831957817078)
[2025-02-13 20:20:08,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09,256][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.11734205484390259, acc: 0.9702970385551453)
[2025-02-13 20:20:09,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09,653][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.14946424961090088, acc: 0.9831932783126831)
[2025-02-13 20:20:09,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10,039][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.08382254093885422, acc: 0.9924242496490479)
[2025-02-13 20:20:10,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10,417][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.13630811870098114, acc: 0.9696969985961914)
[2025-02-13 20:20:10,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10,770][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.027489716187119484, acc: 0.9919354915618896)
[2025-02-13 20:20:10,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11,173][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.1372119039297104, acc: 0.9461538195610046)
[2025-02-13 20:20:11,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11,571][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.14332441985607147, acc: 0.9622641801834106)
[2025-02-13 20:20:11,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11,971][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.19302725791931152, acc: 0.9624060392379761)
[2025-02-13 20:20:12,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12,401][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.09924816340208054, acc: 0.9842519760131836)
[2025-02-13 20:20:12,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12,839][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.12523436546325684, acc: 0.9624060392379761)
[2025-02-13 20:20:12,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13,195][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.08518647402524948, acc: 0.9708737730979919)
[2025-02-13 20:20:13,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13,563][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.18662668764591217, acc: 0.9580419659614563)
[2025-02-13 20:20:13,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13,955][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.0859031081199646, acc: 0.9702970385551453)
[2025-02-13 20:20:14,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14,343][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.08992604911327362, acc: 0.9724770784378052)
[2025-02-13 20:20:14,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14,822][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.11306607723236084, acc: 0.9811320900917053)
[2025-02-13 20:20:14,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15,216][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.09664696455001831, acc: 0.959770143032074)
[2025-02-13 20:20:15,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15,570][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.053583066910505295, acc: 0.9851852059364319)
[2025-02-13 20:20:15,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15,966][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.1409444510936737, acc: 0.970588207244873)
[2025-02-13 20:20:16,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16,349][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.129917174577713, acc: 0.9743589758872986)
[2025-02-13 20:20:16,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16,756][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.0353718027472496, acc: 1.0)
[2025-02-13 20:20:16,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17,151][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.10601138323545456, acc: 0.9702970385551453)
[2025-02-13 20:20:17,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17,560][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.09712787717580795, acc: 0.9774436354637146)
[2025-02-13 20:20:17,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17,945][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.12573277950286865, acc: 0.9795918464660645)
[2025-02-13 20:20:18,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18,356][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.08896604925394058, acc: 0.9724770784378052)
[2025-02-13 20:20:18,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18,756][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.12663261592388153, acc: 0.9704142212867737)
[2025-02-13 20:20:18,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19,167][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.09440888464450836, acc: 0.9776119589805603)
[2025-02-13 20:20:19,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19,564][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.11675411462783813, acc: 0.9752066135406494)
[2025-02-13 20:20:19,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20,000][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.08168338239192963, acc: 0.9661017060279846)
[2025-02-13 20:20:20,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20,478][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.15695305168628693, acc: 0.9492753744125366)
[2025-02-13 20:20:20,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20,972][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.1649133861064911, acc: 0.9661017060279846)
[2025-02-13 20:20:21,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21,341][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.16098913550376892, acc: 0.9714285731315613)
[2025-02-13 20:20:21,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21,758][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.11017987877130508, acc: 0.9671052694320679)
[2025-02-13 20:20:21,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22,136][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.12429473549127579, acc: 0.9590643048286438)
[2025-02-13 20:20:22,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22,522][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.16585353016853333, acc: 0.9430052042007446)
[2025-02-13 20:20:22,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22,914][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.1763981133699417, acc: 0.9580838084220886)
[2025-02-13 20:20:23,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23,281][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.21240051090717316, acc: 0.9515151381492615)
[2025-02-13 20:20:23,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23,681][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.10162784159183502, acc: 0.9723756909370422)
[2025-02-13 20:20:23,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24,061][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.11618265509605408, acc: 0.9840425252914429)
[2025-02-13 20:20:24,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24,431][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.18624430894851685, acc: 0.9635416865348816)
[2025-02-13 20:20:24,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24,817][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.12972837686538696, acc: 0.9692307710647583)
[2025-02-13 20:20:24,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25,182][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.2993563711643219, acc: 0.9171597361564636)
[2025-02-13 20:20:25,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25,558][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.26130735874176025, acc: 0.95333331823349)
[2025-02-13 20:20:25,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25,951][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.15187260508537292, acc: 0.9652174115180969)
[2025-02-13 20:20:26,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26,332][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.11937518417835236, acc: 0.9836956262588501)
[2025-02-13 20:20:26,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26,714][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.04302673414349556, acc: 0.9866666793823242)
[2025-02-13 20:20:26,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27,090][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.02101455070078373, acc: 1.0)
[2025-02-13 20:20:27,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27,487][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.06656457483768463, acc: 0.9915966391563416)
[2025-02-13 20:20:27,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27,911][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.08988307416439056, acc: 0.984455943107605)
[2025-02-13 20:20:28,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28,317][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.18814435601234436, acc: 0.9613526463508606)
[2025-02-13 20:20:28,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28,735][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.1722833216190338, acc: 0.9345238208770752)
[2025-02-13 20:20:28,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29,127][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.09854469448328018, acc: 0.976047933101654)
[2025-02-13 20:20:29,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29,485][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.06314975768327713, acc: 0.9863013625144958)
[2025-02-13 20:20:29,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29,880][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.05130474269390106, acc: 0.9913793206214905)
[2025-02-13 20:20:30,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30,262][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.08687017112970352, acc: 0.9779411554336548)
[2025-02-13 20:20:30,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30,644][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.09691174328327179, acc: 0.9790209531784058)
[2025-02-13 20:20:30,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31,070][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.05936248600482941, acc: 0.9878048896789551)
[2025-02-13 20:20:31,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31,478][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.2230508029460907, acc: 0.9542483687400818)
[2025-02-13 20:20:31,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31,872][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.09597121924161911, acc: 0.9810126423835754)
[2025-02-13 20:20:32,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32,272][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.062886543571949, acc: 0.9922480583190918)
[2025-02-13 20:20:32,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32,656][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.07171604037284851, acc: 0.9752066135406494)
[2025-02-13 20:20:32,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33,066][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.1866319179534912, acc: 0.9468085169792175)
[2025-02-13 20:20:33,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33,441][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.1111781969666481, acc: 0.970370352268219)
[2025-02-13 20:20:33,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33,848][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.07514530420303345, acc: 0.9756097793579102)
[2025-02-13 20:20:33,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34,238][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.0406503789126873, acc: 0.9873417615890503)
[2025-02-13 20:20:34,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34,684][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.03515021875500679, acc: 0.9878048896789551)
[2025-02-13 20:20:34,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35,073][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.020995942875742912, acc: 1.0)
[2025-02-13 20:20:35,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35,456][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.04698384553194046, acc: 0.9880239367485046)
[2025-02-13 20:20:35,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35,869][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.22851118445396423, acc: 0.9558823704719543)
[2025-02-13 20:20:36,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36,280][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.34381020069122314, acc: 0.965753436088562)
[2025-02-13 20:20:36,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36,699][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.15289293229579926, acc: 0.95652174949646)
[2025-02-13 20:20:36,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37,104][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.16141057014465332, acc: 0.9821428656578064)
[2025-02-13 20:20:37,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37,540][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.12984928488731384, acc: 0.9727272987365723)
[2025-02-13 20:20:37,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37,985][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.07043072581291199, acc: 0.9906542301177979)
[2025-02-13 20:20:38,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38,372][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.27689123153686523, acc: 0.9411764740943909)
[2025-02-13 20:20:38,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38,772][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.04929426312446594, acc: 0.991150438785553)
[2025-02-13 20:20:38,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39,162][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.06183801591396332, acc: 0.9907407164573669)
[2025-02-13 20:20:39,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39,644][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.058285903185606, acc: 0.9839572310447693)
[2025-02-13 20:20:39,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40,076][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.08253458887338638, acc: 0.9689440727233887)
[2025-02-13 20:20:40,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40,557][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.08939315378665924, acc: 0.9891892075538635)
[2025-02-13 20:20:40,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40,959][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.044533152133226395, acc: 0.9815950989723206)
[2025-02-13 20:20:41,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41,303][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.037142541259527206, acc: 0.9930555820465088)
[2025-02-13 20:20:41,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41,749][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.08136694878339767, acc: 0.9863945841789246)
[2025-02-13 20:20:41,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42,142][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.03510267287492752, acc: 0.9934640526771545)
[2025-02-13 20:20:42,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42,569][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.03560981899499893, acc: 0.9942857027053833)
[2025-02-13 20:20:42,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43,003][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.08791599422693253, acc: 0.9820359349250793)
[2025-02-13 20:20:43,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43,470][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.03918501362204552, acc: 0.9947090148925781)
[2025-02-13 20:20:43,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43,906][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.09564049541950226, acc: 0.979899525642395)
[2025-02-13 20:20:44,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44,311][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.020022844895720482, acc: 1.0)
[2025-02-13 20:20:44,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44,744][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.10585025697946548, acc: 0.9594594836235046)
[2025-02-13 20:20:44,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45,152][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.10516331344842911, acc: 0.9710144996643066)
[2025-02-13 20:20:45,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45,566][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.06532429158687592, acc: 0.9756097793579102)
[2025-02-13 20:20:45,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45,971][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.21018005907535553, acc: 0.96875)
[2025-02-13 20:20:46,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46,369][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.0725654736161232, acc: 0.9860140085220337)
[2025-02-13 20:20:46,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46,724][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.10120384395122528, acc: 0.956204354763031)
[2025-02-13 20:20:46,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47,125][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.046151746064424515, acc: 0.9861111044883728)
[2025-02-13 20:20:47,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47,518][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.07142335176467896, acc: 0.976331353187561)
[2025-02-13 20:20:47,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47,919][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.08216072618961334, acc: 0.9825581312179565)
[2025-02-13 20:20:48,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48,293][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.2139899730682373, acc: 0.9572649598121643)
[2025-02-13 20:20:48,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48,662][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.08401918411254883, acc: 0.9781420826911926)
[2025-02-13 20:20:48,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49,030][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.07027725130319595, acc: 0.9715909361839294)
[2025-02-13 20:20:49,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49,380][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.049805715680122375, acc: 0.9939024448394775)
[2025-02-13 20:20:49,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49,763][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.05031576380133629, acc: 0.9923664331436157)
[2025-02-13 20:20:49,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50,153][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.1314704269170761, acc: 0.9440559148788452)
[2025-02-13 20:20:50,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50,544][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.087056964635849, acc: 0.9714285731315613)
[2025-02-13 20:20:50,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50,937][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.1322496384382248, acc: 0.9736841917037964)
[2025-02-13 20:20:51,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51,337][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.10556688904762268, acc: 0.9855769276618958)
[2025-02-13 20:20:51,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51,754][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.0472266785800457, acc: 0.9900497794151306)
[2025-02-13 20:20:51,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52,153][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.08493927121162415, acc: 0.9710144996643066)
[2025-02-13 20:20:52,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52,509][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.21717458963394165, acc: 0.9444444179534912)
[2025-02-13 20:20:52,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52,904][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.10060325264930725, acc: 0.9527559280395508)
[2025-02-13 20:20:53,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53,294][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.13377420604228973, acc: 0.9591836929321289)
[2025-02-13 20:20:53,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53,685][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.3148399889469147, acc: 0.9358288645744324)
[2025-02-13 20:20:53,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54,084][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.31166598200798035, acc: 0.939393937587738)
[2025-02-13 20:20:54,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54,447][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.05010943114757538, acc: 0.9937106966972351)
[2025-02-13 20:20:54,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54,816][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.2513546645641327, acc: 0.9485294222831726)
[2025-02-13 20:20:54,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55,198][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.1859225481748581, acc: 0.9350000023841858)
[2025-02-13 20:20:55,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55,561][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.06727641075849533, acc: 0.9815950989723206)
[2025-02-13 20:20:55,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55,927][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.048764418810606, acc: 0.9905660152435303)
[2025-02-13 20:20:56,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56,312][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.1005808487534523, acc: 0.977011501789093)
[2025-02-13 20:20:56,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56,703][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.2049015313386917, acc: 0.9695122241973877)
[2025-02-13 20:20:56,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57,117][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.14312510192394257, acc: 0.9670329689979553)
[2025-02-13 20:20:57,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57,513][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.12058315426111221, acc: 0.9666666388511658)
[2025-02-13 20:20:57,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57,921][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.12403450161218643, acc: 0.9829059839248657)
[2025-02-13 20:20:58,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58,305][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.07481372356414795, acc: 0.9791666865348816)
[2025-02-13 20:20:58,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58,690][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.04864658787846565, acc: 0.9928571581840515)
[2025-02-13 20:20:58,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59,070][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.1414499580860138, acc: 0.9726775884628296)
[2025-02-13 20:20:59,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59,443][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.05573417618870735, acc: 0.9880239367485046)
[2025-02-13 20:20:59,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59,818][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.11388979107141495, acc: 0.9754601120948792)
[2025-02-13 20:20:59,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00,205][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.08223853260278702, acc: 0.9819276928901672)
[2025-02-13 20:21:00,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00,574][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.05302288383245468, acc: 0.9933775067329407)
[2025-02-13 20:21:00,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00,958][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.07821130752563477, acc: 0.9807692170143127)
[2025-02-13 20:21:01,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01,315][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.05563545972108841, acc: 0.9852941036224365)
[2025-02-13 20:21:01,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01,743][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.02568916045129299, acc: 0.9934210777282715)
[2025-02-13 20:21:01,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02,113][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.1606704294681549, acc: 0.9599999785423279)
[2025-02-13 20:21:02,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02,492][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.1349489390850067, acc: 0.9684210419654846)
[2025-02-13 20:21:02,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02,883][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.15415875613689423, acc: 0.988095223903656)
[2025-02-13 20:21:03,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03,259][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.10159991681575775, acc: 0.9805194735527039)
[2025-02-13 20:21:03,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03,654][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.1491023153066635, acc: 0.9588235020637512)
[2025-02-13 20:21:03,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04,035][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.06737728416919708, acc: 0.9788732528686523)
[2025-02-13 20:21:04,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04,434][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.20768816769123077, acc: 0.9589040875434875)
[2025-02-13 20:21:04,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04,832][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.07494139671325684, acc: 0.97826087474823)
[2025-02-13 20:21:04,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05,230][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.08441261202096939, acc: 0.9754601120948792)
[2025-02-13 20:21:05,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05,620][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.1637687087059021, acc: 0.970588207244873)
[2025-02-13 20:21:05,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06,011][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.13177606463432312, acc: 0.9571428298950195)
[2025-02-13 20:21:06,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06,389][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.11470315605401993, acc: 0.9724137783050537)
[2025-02-13 20:21:06,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06,758][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.12223221361637115, acc: 0.970802903175354)
[2025-02-13 20:21:06,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07,142][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.09966790676116943, acc: 0.9752066135406494)
[2025-02-13 20:21:07,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07,509][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.07441935688257217, acc: 0.9912280440330505)
[2025-02-13 20:21:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07,893][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.1369006186723709, acc: 0.9603174328804016)
[2025-02-13 20:21:08,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08,295][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.17498132586479187, acc: 0.9568965435028076)
[2025-02-13 20:21:08,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08,680][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.18662407994270325, acc: 0.9534883499145508)
[2025-02-13 20:21:08,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09,076][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.06146230921149254, acc: 0.9858155846595764)
[2025-02-13 20:21:09,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09,440][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.17847788333892822, acc: 0.966292142868042)
[2025-02-13 20:21:09,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09,828][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.1416192203760147, acc: 0.9426751732826233)
[2025-02-13 20:21:09,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10,286][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.254517138004303, acc: 0.9548872113227844)
[2025-02-13 20:21:10,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10,667][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.16730628907680511, acc: 0.9666666388511658)
[2025-02-13 20:21:10,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11,088][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.22213809192180634, acc: 0.9567901492118835)
[2025-02-13 20:21:11,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11,447][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.1617160588502884, acc: 0.9653179049491882)
[2025-02-13 20:21:11,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11,836][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.08801037073135376, acc: 0.9702970385551453)
[2025-02-13 20:21:11,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12,218][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.17006507515907288, acc: 0.9607843160629272)
[2025-02-13 20:21:12,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12,603][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.16917581856250763, acc: 0.957446813583374)
[2025-02-13 20:21:12,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12,995][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.24544037878513336, acc: 0.9430894255638123)
[2025-02-13 20:21:13,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13,385][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.17068302631378174, acc: 0.9513888955116272)
[2025-02-13 20:21:13,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13,780][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.1941172182559967, acc: 0.9578313231468201)
[2025-02-13 20:21:13,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14,157][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.1120949238538742, acc: 0.9937888383865356)
[2025-02-13 20:21:14,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14,527][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.07364349067211151, acc: 0.9849246144294739)
[2025-02-13 20:21:14,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14,930][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.06406588107347488, acc: 0.9929577708244324)
[2025-02-13 20:21:15,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15,331][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.21117694675922394, acc: 0.9670329689979553)
[2025-02-13 20:21:15,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15,699][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.16233079135417938, acc: 0.9496855139732361)
[2025-02-13 20:21:15,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16,086][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.5186678767204285, acc: 0.8888888955116272)
[2025-02-13 20:21:16,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16,443][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.598808765411377, acc: 0.8615384697914124)
[2025-02-13 20:21:16,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16,835][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.22870612144470215, acc: 0.9473684430122375)
[2025-02-13 20:21:16,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17,225][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.05307378992438316, acc: 0.9938271641731262)
[2025-02-13 20:21:17,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17,609][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.08739739656448364, acc: 0.9719101190567017)
[2025-02-13 20:21:17,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18,010][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.14476646482944489, acc: 0.9555555582046509)
[2025-02-13 20:21:18,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18,406][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.06419599056243896, acc: 0.9814814925193787)
[2025-02-13 20:21:18,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18,785][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.051467861980199814, acc: 0.9830508232116699)
[2025-02-13 20:21:18,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19,177][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.03202540799975395, acc: 1.0)
[2025-02-13 20:21:19,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19,581][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.06511298567056656, acc: 0.9849624037742615)
[2025-02-13 20:21:19,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19,952][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.021284576505422592, acc: 1.0)
[2025-02-13 20:21:20,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20,349][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.07592537999153137, acc: 0.9777777791023254)
[2025-02-13 20:21:20,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20,729][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.04243869706988335, acc: 0.993630588054657)
[2025-02-13 20:21:20,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21,185][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.07466680556535721, acc: 0.9736841917037964)
[2025-02-13 20:21:21,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21,546][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.13760791718959808, acc: 0.9710144996643066)
[2025-02-13 20:21:21,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21,921][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.05196850374341011, acc: 0.9848484992980957)
[2025-02-13 20:21:22,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22,293][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.21548216044902802, acc: 0.9383561611175537)
[2025-02-13 20:21:22,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22,689][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.10712289810180664, acc: 0.9797297120094299)
[2025-02-13 20:21:22,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23,067][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.026479030027985573, acc: 0.9940119981765747)
[2025-02-13 20:21:23,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23,446][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.013777405954897404, acc: 1.0)
[2025-02-13 20:21:23,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23,829][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.06518792361021042, acc: 0.9835164546966553)
[2025-02-13 20:21:23,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24,221][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.061776239424943924, acc: 0.9849624037742615)
[2025-02-13 20:21:24,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24,589][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.07048285752534866, acc: 0.9921875)
[2025-02-13 20:21:24,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24,971][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.03642713651061058, acc: 0.9870967864990234)
[2025-02-13 20:21:25,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25,342][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.06743261218070984, acc: 0.9821428656578064)
[2025-02-13 20:21:25,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25,702][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.10689787566661835, acc: 0.9757575988769531)
[2025-02-13 20:21:25,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26,104][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.17513328790664673, acc: 0.9572649598121643)
[2025-02-13 20:21:26,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26,478][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.15919068455696106, acc: 0.9638554453849792)
[2025-02-13 20:21:26,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26,849][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.07225710898637772, acc: 0.9857142567634583)
[2025-02-13 20:21:26,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27,222][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.1619689017534256, acc: 0.9481481313705444)
[2025-02-13 20:21:27,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27,597][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.11769828200340271, acc: 0.9790209531784058)
[2025-02-13 20:21:27,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27,941][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.14791347086429596, acc: 0.9617834687232971)
[2025-02-13 20:21:28,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28,296][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.10804960131645203, acc: 0.963302731513977)
[2025-02-13 20:21:28,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28,684][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.05556391924619675, acc: 1.0)
[2025-02-13 20:21:28,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29,044][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.10221035778522491, acc: 0.9729729890823364)
[2025-02-13 20:21:29,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29,395][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.045940764248371124, acc: 0.9896907210350037)
[2025-02-13 20:21:29,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29,767][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.07542046904563904, acc: 0.9803921580314636)
[2025-02-13 20:21:29,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30,146][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.10578010976314545, acc: 0.9810126423835754)
[2025-02-13 20:21:30,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30,508][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.17760303616523743, acc: 0.9683544039726257)
[2025-02-13 20:21:30,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30,879][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.08642003685235977, acc: 0.9738562107086182)
[2025-02-13 20:21:31,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31,247][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.18681387603282928, acc: 0.9707602262496948)
[2025-02-13 20:21:31,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31,628][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.12385033071041107, acc: 0.9772727489471436)
[2025-02-13 20:21:31,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32,011][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.2476961612701416, acc: 0.9459459185600281)
[2025-02-13 20:21:32,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32,360][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.16963987052440643, acc: 0.9568345546722412)
[2025-02-13 20:21:32,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32,721][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.03347637876868248, acc: 0.9866666793823242)
[2025-02-13 20:21:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33,083][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.06016499176621437, acc: 0.9841269850730896)
[2025-02-13 20:21:33,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33,421][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.0546911247074604, acc: 0.9844961166381836)
[2025-02-13 20:21:33,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33,812][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.04517103359103203, acc: 0.9861111044883728)
[2025-02-13 20:21:33,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34,179][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.061708685010671616, acc: 0.9867549538612366)
[2025-02-13 20:21:34,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34,572][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.14574342966079712, acc: 0.9754601120948792)
[2025-02-13 20:21:34,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34,959][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.09169736504554749, acc: 0.9849624037742615)
[2025-02-13 20:21:35,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35,366][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.024087755009531975, acc: 1.0)
[2025-02-13 20:21:35,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35,750][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.1906784027814865, acc: 0.9615384340286255)
[2025-02-13 20:21:35,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36,126][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.028480490669608116, acc: 0.9927536249160767)
[2025-02-13 20:21:36,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36,494][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.02912876568734646, acc: 0.9904305934906006)
[2025-02-13 20:21:36,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36,828][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.009962978772819042, acc: 1.0)
[2025-02-13 20:21:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37,213][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.023465236648917198, acc: 1.0)
[2025-02-13 20:21:37,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37,591][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.052224621176719666, acc: 0.9797979593276978)
[2025-02-13 20:21:37,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37,974][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.10054535418748856, acc: 0.9629629850387573)
[2025-02-13 20:21:38,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38,357][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.04966136813163757, acc: 0.9911110997200012)
[2025-02-13 20:21:38,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38,737][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.1259700357913971, acc: 0.9696969985961914)
[2025-02-13 20:21:38,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39,135][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.029859479516744614, acc: 1.0)
[2025-02-13 20:21:39,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39,465][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.026144417002797127, acc: 1.0)
[2025-02-13 20:21:39,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39,825][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.014124141074717045, acc: 1.0)
[2025-02-13 20:21:39,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40,194][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.05146085470914841, acc: 0.9890109896659851)
[2025-02-13 20:21:40,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40,550][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.05338840186595917, acc: 0.9864864945411682)
[2025-02-13 20:21:40,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40,941][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.12478744983673096, acc: 0.9696969985961914)
[2025-02-13 20:21:41,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41,334][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.10305566340684891, acc: 0.978723406791687)
[2025-02-13 20:21:41,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41,709][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.012284093536436558, acc: 1.0)
[2025-02-13 20:21:41,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42,099][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.05206262320280075, acc: 0.9818181991577148)
[2025-02-13 20:21:42,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42,496][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.12551330029964447, acc: 0.9601770043373108)
[2025-02-13 20:21:42,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42,862][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.0493098683655262, acc: 0.987500011920929)
[2025-02-13 20:21:43,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43,258][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.04264385253190994, acc: 0.9932432174682617)
[2025-02-13 20:21:43,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43,629][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.055307112634181976, acc: 0.9866666793823242)
[2025-02-13 20:21:43,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43,994][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.037054643034935, acc: 1.0)
[2025-02-13 20:21:44,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44,356][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.10421202331781387, acc: 0.971222996711731)
[2025-02-13 20:21:44,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44,720][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.14265888929367065, acc: 0.9629629850387573)
[2025-02-13 20:21:44,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45,101][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.059807438403367996, acc: 0.9921875)
[2025-02-13 20:21:45,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45,481][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.048003893345594406, acc: 0.9747899174690247)
[2025-02-13 20:21:45,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45,858][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.08158904314041138, acc: 0.9655172228813171)
[2025-02-13 20:21:45,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46,209][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.06130514666438103, acc: 0.9846153855323792)
[2025-02-13 20:21:46,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46,555][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.05693314969539642, acc: 0.9866666793823242)
[2025-02-13 20:21:46,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46,899][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.06071510165929794, acc: 0.9863013625144958)
[2025-02-13 20:21:47,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47,251][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.13354460895061493, acc: 0.9718309640884399)
[2025-02-13 20:21:47,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47,644][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.12519127130508423, acc: 0.9735099077224731)
[2025-02-13 20:21:47,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48,026][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.038473229855298996, acc: 1.0)
[2025-02-13 20:21:48,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48,371][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.11273914575576782, acc: 0.971222996711731)
[2025-02-13 20:21:48,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48,755][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.1191081702709198, acc: 0.9774011373519897)
[2025-02-13 20:21:48,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49,137][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.052051763981580734, acc: 0.9908257126808167)
[2025-02-13 20:21:49,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49,560][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.07579293847084045, acc: 0.969924807548523)
[2025-02-13 20:21:49,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49,968][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.11043621599674225, acc: 0.985401451587677)
[2025-02-13 20:21:50,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50,344][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.24587826430797577, acc: 0.918367326259613)
[2025-02-13 20:21:50,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50,786][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.15349827706813812, acc: 0.9527559280395508)
[2025-02-13 20:21:50,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51,205][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.1513444036245346, acc: 0.970588207244873)
[2025-02-13 20:21:51,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51,581][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.19817622005939484, acc: 0.9570552110671997)
[2025-02-13 20:21:51,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52,013][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.25027257204055786, acc: 0.9548386931419373)
[2025-02-13 20:21:52,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52,374][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.05652645230293274, acc: 0.9924242496490479)
[2025-02-13 20:21:52,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52,743][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.10382581502199173, acc: 0.9645389914512634)
[2025-02-13 20:21:52,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53,121][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.22363871335983276, acc: 0.9440000057220459)
[2025-02-13 20:21:53,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53,490][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.09419789165258408, acc: 0.988095223903656)
[2025-02-13 20:21:53,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53,841][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.09072395414113998, acc: 0.9836065769195557)
[2025-02-13 20:21:53,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54,239][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.18121400475502014, acc: 0.9717513918876648)
[2025-02-13 20:21:54,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54,607][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.06369099020957947, acc: 0.9852941036224365)
[2025-02-13 20:21:54,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54,996][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.10196025669574738, acc: 0.9716312289237976)
[2025-02-13 20:21:56,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:32,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52,452][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2616, device='cuda:0') eval_epoch_loss=tensor(0.2324, device='cuda:0') eval_epoch_acc=tensor(0.9450, device='cuda:0')
[2025-02-13 20:25:52,458][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:25:52,458][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:25:52,724][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_3564_loss_0.23238003253936768/model.pt
[2025-02-13 20:25:52,737][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:25:52,739][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.23238003253936768
[2025-02-13 20:25:52,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53,280][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.14013123512268066, acc: 0.9603174328804016)
[2025-02-13 20:25:53,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53,735][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.25136277079582214, acc: 0.9191176295280457)
[2025-02-13 20:25:53,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54,123][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.1585976630449295, acc: 0.9663865566253662)
[2025-02-13 20:25:54,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54,479][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.1475212723016739, acc: 0.9719101190567017)
[2025-02-13 20:25:54,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54,882][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.19517026841640472, acc: 0.936170220375061)
[2025-02-13 20:25:55,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55,338][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.17003311216831207, acc: 0.9750000238418579)
[2025-02-13 20:25:55,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55,775][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.2847023606300354, acc: 0.9558823704719543)
[2025-02-13 20:25:55,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56,193][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.252185195684433, acc: 0.949438214302063)
[2025-02-13 20:25:56,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56,575][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.05484209582209587, acc: 0.9821428656578064)
[2025-02-13 20:25:56,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57,004][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.17467419803142548, acc: 0.9605262875556946)
[2025-02-13 20:25:57,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57,451][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.11723670363426208, acc: 0.9752066135406494)
[2025-02-13 20:25:57,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57,837][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.2999703884124756, acc: 0.9342105388641357)
[2025-02-13 20:25:57,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58,224][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.06579411029815674, acc: 0.9925373196601868)
[2025-02-13 20:25:58,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58,713][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.13374541699886322, acc: 0.9639175534248352)
[2025-02-13 20:25:58,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59,123][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.1404874622821808, acc: 0.9698795080184937)
[2025-02-13 20:25:59,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59,523][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.12646499276161194, acc: 0.9767441749572754)
[2025-02-13 20:25:59,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59,941][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.2526664137840271, acc: 0.9479768872261047)
[2025-02-13 20:26:00,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00,325][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.14940331876277924, acc: 0.970588207244873)
[2025-02-13 20:26:00,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00,742][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.18244731426239014, acc: 0.94017094373703)
[2025-02-13 20:26:00,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01,141][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.20355147123336792, acc: 0.9520958065986633)
[2025-02-13 20:26:01,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01,500][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.22568769752979279, acc: 0.936170220375061)
[2025-02-13 20:26:01,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01,876][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.25681960582733154, acc: 0.9509202241897583)
[2025-02-13 20:26:02,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02,262][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.22589164972305298, acc: 0.9490445852279663)
[2025-02-13 20:26:02,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02,656][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.20884661376476288, acc: 0.9650349617004395)
[2025-02-13 20:26:02,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03,074][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.1602345108985901, acc: 0.9710982441902161)
[2025-02-13 20:26:03,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03,485][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.22930757701396942, acc: 0.9672130942344666)
[2025-02-13 20:26:03,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03,933][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.07309696078300476, acc: 0.9777777791023254)
[2025-02-13 20:26:04,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04,367][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.10998010635375977, acc: 0.9707602262496948)
[2025-02-13 20:26:04,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04,829][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.05406564474105835, acc: 0.9928571581840515)
[2025-02-13 20:26:04,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05,256][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.31921181082725525, acc: 0.9507042169570923)
[2025-02-13 20:26:05,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05,675][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.11316007375717163, acc: 0.9602649211883545)
[2025-02-13 20:26:05,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06,032][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.13201504945755005, acc: 0.9607843160629272)
[2025-02-13 20:26:06,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06,430][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.15947306156158447, acc: 0.9647058844566345)
[2025-02-13 20:26:06,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06,819][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.09575982391834259, acc: 0.9671052694320679)
[2025-02-13 20:26:06,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07,227][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.19511200487613678, acc: 0.9383886456489563)
[2025-02-13 20:26:07,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07,624][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.1619209349155426, acc: 0.9675675630569458)
[2025-02-13 20:26:07,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08,035][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.22143197059631348, acc: 0.9481865167617798)
[2025-02-13 20:26:08,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08,443][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.4431660771369934, acc: 0.8932584524154663)
[2025-02-13 20:26:08,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08,858][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.2412511259317398, acc: 0.930232584476471)
[2025-02-13 20:26:09,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09,274][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.20810112357139587, acc: 0.9433962106704712)
[2025-02-13 20:26:09,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09,653][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.17296655476093292, acc: 0.960629940032959)
[2025-02-13 20:26:09,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10,070][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.15890735387802124, acc: 0.9730941653251648)
[2025-02-13 20:26:10,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10,478][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.37692806124687195, acc: 0.9178082346916199)
[2025-02-13 20:26:10,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10,862][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.15056344866752625, acc: 0.965753436088562)
[2025-02-13 20:26:11,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11,256][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.384354829788208, acc: 0.946601927280426)
[2025-02-13 20:26:11,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11,689][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.08909587562084198, acc: 0.9789473414421082)
[2025-02-13 20:26:11,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12,092][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.2093561440706253, acc: 0.9505494236946106)
[2025-02-13 20:26:12,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12,482][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.1458950787782669, acc: 0.9657142758369446)
[2025-02-13 20:26:12,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12,880][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.31530818343162537, acc: 0.9224137663841248)
[2025-02-13 20:26:13,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13,273][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.16154393553733826, acc: 0.9617486596107483)
[2025-02-13 20:26:13,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13,648][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.07947121560573578, acc: 0.9893617033958435)
[2025-02-13 20:26:13,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14,069][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.07819803804159164, acc: 0.9864864945411682)
[2025-02-13 20:26:14,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14,463][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.2021530568599701, acc: 0.95652174949646)
[2025-02-13 20:26:14,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14,844][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.10429441928863525, acc: 0.9675675630569458)
[2025-02-13 20:26:14,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15,231][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.2149239331483841, acc: 0.9447852969169617)
[2025-02-13 20:26:15,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15,627][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.30944010615348816, acc: 0.9257642030715942)
[2025-02-13 20:26:15,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16,025][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.07989183813333511, acc: 0.9940119981765747)
[2025-02-13 20:26:16,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16,386][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.12913496792316437, acc: 0.9808917045593262)
[2025-02-13 20:26:16,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16,796][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.08111812174320221, acc: 0.9870129823684692)
[2025-02-13 20:26:16,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17,153][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.04740676283836365, acc: 1.0)
[2025-02-13 20:26:17,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17,499][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.11551246047019958, acc: 0.9784172773361206)
[2025-02-13 20:26:17,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17,879][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.25062912702560425, acc: 0.9075630307197571)
[2025-02-13 20:26:18,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18,312][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.25720325112342834, acc: 0.966292142868042)
[2025-02-13 20:26:18,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18,734][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.2837541997432709, acc: 0.9433962106704712)
[2025-02-13 20:26:18,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19,109][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.18170107901096344, acc: 0.9428571462631226)
[2025-02-13 20:26:19,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19,489][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.26573172211647034, acc: 0.9354838728904724)
[2025-02-13 20:26:19,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19,877][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.08668696135282516, acc: 0.9677419066429138)
[2025-02-13 20:26:20,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20,312][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.10056030750274658, acc: 0.9714285731315613)
[2025-02-13 20:26:20,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20,682][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.10730248689651489, acc: 0.9836065769195557)
[2025-02-13 20:26:20,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21,040][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.15015731751918793, acc: 0.9554139971733093)
[2025-02-13 20:26:21,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21,433][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.08110123872756958, acc: 0.9906542301177979)
[2025-02-13 20:26:21,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21,802][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.09026362746953964, acc: 0.9741379022598267)
[2025-02-13 20:26:21,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22,247][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.09850448369979858, acc: 0.9716981053352356)
[2025-02-13 20:26:22,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22,648][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.1607714742422104, acc: 0.9545454382896423)
[2025-02-13 20:26:22,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23,009][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.08424481004476547, acc: 0.9780219793319702)
[2025-02-13 20:26:23,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23,373][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.1410614550113678, acc: 0.9680851101875305)
[2025-02-13 20:26:23,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23,801][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.12090642750263214, acc: 0.9834710955619812)
[2025-02-13 20:26:23,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24,173][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.07520915567874908, acc: 0.9838709831237793)
[2025-02-13 20:26:24,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24,596][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.04178999736905098, acc: 0.9932885766029358)
[2025-02-13 20:26:24,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25,010][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.15713030099868774, acc: 0.96875)
[2025-02-13 20:26:25,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25,390][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.16290733218193054, acc: 0.9527027010917664)
[2025-02-13 20:26:25,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25,761][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.25536417961120605, acc: 0.9453125)
[2025-02-13 20:26:25,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26,163][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.2745656967163086, acc: 0.9318181872367859)
[2025-02-13 20:26:26,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26,564][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.06612220406532288, acc: 0.9887640476226807)
[2025-02-13 20:26:26,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26,960][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.1713542938232422, acc: 0.9467455744743347)
[2025-02-13 20:26:27,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27,381][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.20038501918315887, acc: 0.9597315192222595)
[2025-02-13 20:26:27,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27,774][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.1250285655260086, acc: 0.9704142212867737)
[2025-02-13 20:26:27,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28,147][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.16351953148841858, acc: 0.9518072009086609)
[2025-02-13 20:26:28,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28,503][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.11501742899417877, acc: 0.9795918464660645)
[2025-02-13 20:26:28,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28,943][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.1615460216999054, acc: 0.9570552110671997)
[2025-02-13 20:26:29,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29,306][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.03222915902733803, acc: 0.9919999837875366)
[2025-02-13 20:26:29,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29,670][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.2673584520816803, acc: 0.9520958065986633)
[2025-02-13 20:26:29,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30,089][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.2475757598876953, acc: 0.9492753744125366)
[2025-02-13 20:26:30,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30,467][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.0751703605055809, acc: 0.9790209531784058)
[2025-02-13 20:26:30,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30,885][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.11087371408939362, acc: 0.9861111044883728)
[2025-02-13 20:26:31,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31,286][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.15808410942554474, acc: 0.9523809552192688)
[2025-02-13 20:26:31,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31,669][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.34339991211891174, acc: 0.9135802388191223)
[2025-02-13 20:26:31,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32,048][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.10531940311193466, acc: 0.9571428298950195)
[2025-02-13 20:26:32,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32,475][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.31361153721809387, acc: 0.9378882050514221)
[2025-02-13 20:26:32,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32,875][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.18813207745552063, acc: 0.953125)
[2025-02-13 20:26:33,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33,297][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.27781376242637634, acc: 0.9415584206581116)
[2025-02-13 20:26:33,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33,722][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.11357476562261581, acc: 0.9729729890823364)
[2025-02-13 20:26:33,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34,104][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.21261419355869293, acc: 0.9589040875434875)
[2025-02-13 20:26:34,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34,472][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.30639609694480896, acc: 0.9224806427955627)
[2025-02-13 20:26:34,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34,855][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.24975822865962982, acc: 0.9448275566101074)
[2025-02-13 20:26:34,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35,215][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.06844674050807953, acc: 0.9752066135406494)
[2025-02-13 20:26:35,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35,552][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.047425299882888794, acc: 0.9879518151283264)
[2025-02-13 20:26:35,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35,931][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.0691409483551979, acc: 0.9873417615890503)
[2025-02-13 20:26:36,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36,358][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.08196111768484116, acc: 0.9834254384040833)
[2025-02-13 20:26:36,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36,745][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.10208673030138016, acc: 0.9793103337287903)
[2025-02-13 20:26:36,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37,126][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.05947404354810715, acc: 0.9932432174682617)
[2025-02-13 20:26:37,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37,537][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.07327568531036377, acc: 0.9834254384040833)
[2025-02-13 20:26:37,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37,939][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.1630931794643402, acc: 0.9743589758872986)
[2025-02-13 20:26:38,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38,346][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.1527017503976822, acc: 0.9715909361839294)
[2025-02-13 20:26:38,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38,770][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.17664079368114471, acc: 0.9620253443717957)
[2025-02-13 20:26:38,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39,210][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.09862072020769119, acc: 0.9769230484962463)
[2025-02-13 20:26:39,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39,629][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.10330972075462341, acc: 0.9666666388511658)
[2025-02-13 20:26:39,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40,030][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.04692266136407852, acc: 1.0)
[2025-02-13 20:26:40,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40,476][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.06058255210518837, acc: 0.9821428656578064)
[2025-02-13 20:26:40,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40,914][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.02692592516541481, acc: 1.0)
[2025-02-13 20:26:41,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41,322][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.03648695349693298, acc: 1.0)
[2025-02-13 20:26:41,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41,688][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.04891812801361084, acc: 0.9918699264526367)
[2025-02-13 20:26:41,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42,085][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.033190369606018066, acc: 1.0)
[2025-02-13 20:26:42,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42,462][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.0523531436920166, acc: 0.9937888383865356)
[2025-02-13 20:26:42,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42,868][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.10284347832202911, acc: 0.9666666388511658)
[2025-02-13 20:26:43,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43,242][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.07410617172718048, acc: 0.978723406791687)
[2025-02-13 20:26:43,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43,637][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.039180874824523926, acc: 0.9936708807945251)
[2025-02-13 20:26:43,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44,013][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.0794248953461647, acc: 0.9900990128517151)
[2025-02-13 20:26:44,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44,433][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.06776179373264313, acc: 0.987730085849762)
[2025-02-13 20:26:44,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44,816][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.044497858732938766, acc: 0.9924242496490479)
[2025-02-13 20:26:44,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45,202][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.02687421441078186, acc: 1.0)
[2025-02-13 20:26:45,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45,610][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.05312329903244972, acc: 0.987261176109314)
[2025-02-13 20:26:45,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46,006][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.0687548816204071, acc: 0.982758641242981)
[2025-02-13 20:26:46,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46,403][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.03826504945755005, acc: 0.9939024448394775)
[2025-02-13 20:26:46,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46,811][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.06493975222110748, acc: 0.9793103337287903)
[2025-02-13 20:26:46,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47,182][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.0623566173017025, acc: 0.9921259880065918)
[2025-02-13 20:26:47,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47,580][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.02713126316666603, acc: 0.9937106966972351)
[2025-02-13 20:26:47,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47,960][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.03285898268222809, acc: 0.9927007555961609)
[2025-02-13 20:26:48,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48,381][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.0643482506275177, acc: 0.9939393997192383)
[2025-02-13 20:26:48,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48,780][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.07917989790439606, acc: 0.9943181872367859)
[2025-02-13 20:26:48,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49,167][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.027690542861819267, acc: 0.9900990128517151)
[2025-02-13 20:26:49,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49,543][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.07859986275434494, acc: 0.9938650131225586)
[2025-02-13 20:26:49,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49,900][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.23598112165927887, acc: 0.9207921028137207)
[2025-02-13 20:26:50,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50,264][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.03172454982995987, acc: 0.9933333396911621)
[2025-02-13 20:26:50,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50,676][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.16577330231666565, acc: 0.9538461565971375)
[2025-02-13 20:26:50,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51,066][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.04780973866581917, acc: 0.9907407164573669)
[2025-02-13 20:26:51,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51,448][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.11310206353664398, acc: 0.9814814925193787)
[2025-02-13 20:26:51,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51,800][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.1728452444076538, acc: 0.9670329689979553)
[2025-02-13 20:26:51,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52,226][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.1306302398443222, acc: 0.9619565010070801)
[2025-02-13 20:26:52,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52,608][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.0909339115023613, acc: 0.9640287756919861)
[2025-02-13 20:26:52,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53,007][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.12986493110656738, acc: 0.9585492014884949)
[2025-02-13 20:26:53,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53,371][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.11443866789340973, acc: 0.9734042286872864)
[2025-02-13 20:26:53,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53,754][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.22310927510261536, acc: 0.9644669890403748)
[2025-02-13 20:26:53,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54,127][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.12657852470874786, acc: 0.9634146094322205)
[2025-02-13 20:26:54,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54,509][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.14961004257202148, acc: 0.96875)
[2025-02-13 20:26:54,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54,887][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.13449062407016754, acc: 0.9649122953414917)
[2025-02-13 20:26:55,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55,240][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.02611510083079338, acc: 1.0)
[2025-02-13 20:26:55,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55,634][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.0908740684390068, acc: 0.9826589822769165)
[2025-02-13 20:26:55,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55,982][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.11143998801708221, acc: 0.984455943107605)
[2025-02-13 20:26:56,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56,369][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.12538975477218628, acc: 0.9677419066429138)
[2025-02-13 20:26:56,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56,761][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.08472005277872086, acc: 0.9698795080184937)
[2025-02-13 20:26:56,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57,151][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.2584342360496521, acc: 0.957317054271698)
[2025-02-13 20:26:57,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57,550][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.10646315664052963, acc: 0.9648241400718689)
[2025-02-13 20:26:57,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57,954][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.06657338887453079, acc: 0.9826589822769165)
[2025-02-13 20:26:58,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58,309][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.03878069296479225, acc: 0.9950739145278931)
[2025-02-13 20:26:58,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58,679][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.09408056735992432, acc: 0.9669811129570007)
[2025-02-13 20:26:58,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59,069][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.10055910795927048, acc: 0.9696969985961914)
[2025-02-13 20:26:59,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59,441][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.03948717936873436, acc: 0.9885057210922241)
[2025-02-13 20:26:59,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59,819][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.07187492400407791, acc: 0.9794871807098389)
[2025-02-13 20:26:59,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00,183][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.16315069794654846, acc: 0.9689440727233887)
[2025-02-13 20:27:00,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00,568][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.11342041194438934, acc: 0.9679487347602844)
[2025-02-13 20:27:00,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00,932][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.0797247439622879, acc: 0.9870129823684692)
[2025-02-13 20:27:01,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01,302][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.1616695523262024, acc: 0.970588207244873)
[2025-02-13 20:27:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01,699][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.04835610091686249, acc: 1.0)
[2025-02-13 20:27:01,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02,089][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.04032057523727417, acc: 0.9791666865348816)
[2025-02-13 20:27:02,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02,431][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.0710396021604538, acc: 0.978723406791687)
[2025-02-13 20:27:02,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02,816][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.06020774319767952, acc: 0.987261176109314)
[2025-02-13 20:27:02,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03,189][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.14696767926216125, acc: 0.959770143032074)
[2025-02-13 20:27:03,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03,541][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.11931652575731277, acc: 0.9727891087532043)
[2025-02-13 20:27:03,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03,917][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.09865459054708481, acc: 0.9823529124259949)
[2025-02-13 20:27:04,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04,299][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.1555374413728714, acc: 0.9476743936538696)
[2025-02-13 20:27:04,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04,647][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.1547761708498001, acc: 0.961904764175415)
[2025-02-13 20:27:04,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05,020][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.10455770790576935, acc: 0.9717513918876648)
[2025-02-13 20:27:05,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05,406][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.11850301921367645, acc: 0.9611111283302307)
[2025-02-13 20:27:05,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05,761][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.11116936057806015, acc: 0.9800000190734863)
[2025-02-13 20:27:05,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06,146][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.2519594132900238, acc: 0.9404761791229248)
[2025-02-13 20:27:06,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06,522][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.09013219177722931, acc: 0.9938650131225586)
[2025-02-13 20:27:06,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06,917][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.10711972415447235, acc: 0.9698795080184937)
[2025-02-13 20:27:07,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07,311][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.14907440543174744, acc: 0.955974817276001)
[2025-02-13 20:27:07,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07,668][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.3194921016693115, acc: 0.9018405079841614)
[2025-02-13 20:27:07,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08,064][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.0348176471889019, acc: 0.9935483932495117)
[2025-02-13 20:27:08,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08,450][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.1210583746433258, acc: 0.9504132270812988)
[2025-02-13 20:27:08,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08,831][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.12840402126312256, acc: 0.9768785834312439)
[2025-02-13 20:27:08,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09,206][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.21096356213092804, acc: 0.948051929473877)
[2025-02-13 20:27:09,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09,588][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.13497762382030487, acc: 0.9527559280395508)
[2025-02-13 20:27:09,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09,974][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.13590285181999207, acc: 0.9720279574394226)
[2025-02-13 20:27:10,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10,348][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.18704380095005035, acc: 0.9512194991111755)
[2025-02-13 20:27:10,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10,702][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.052474793046712875, acc: 0.9772727489471436)
[2025-02-13 20:27:10,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11,075][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.06643865257501602, acc: 0.9934210777282715)
[2025-02-13 20:27:11,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11,477][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.06127571314573288, acc: 0.9928571581840515)
[2025-02-13 20:27:11,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11,865][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.08772201836109161, acc: 0.976047933101654)
[2025-02-13 20:27:12,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12,251][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.14344385266304016, acc: 0.9772727489471436)
[2025-02-13 20:27:12,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12,601][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.12201184779405594, acc: 0.9664429426193237)
[2025-02-13 20:27:12,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12,979][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.09740140289068222, acc: 0.9720670580863953)
[2025-02-13 20:27:13,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13,368][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.09574764221906662, acc: 0.9711538553237915)
[2025-02-13 20:27:13,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13,779][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.15851274132728577, acc: 0.9529411792755127)
[2025-02-13 20:27:13,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14,160][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.08643189817667007, acc: 0.9731543660163879)
[2025-02-13 20:27:14,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14,571][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.034291163086891174, acc: 0.9934210777282715)
[2025-02-13 20:27:14,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14,984][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.06655093282461166, acc: 0.9928571581840515)
[2025-02-13 20:27:15,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15,391][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.06190244108438492, acc: 0.9840425252914429)
[2025-02-13 20:27:15,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15,821][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.14445427060127258, acc: 0.9670329689979553)
[2025-02-13 20:27:15,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16,210][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.07986043393611908, acc: 0.9813664555549622)
[2025-02-13 20:27:16,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16,589][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.14277346432209015, acc: 0.9679487347602844)
[2025-02-13 20:27:16,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16,972][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.04297555610537529, acc: 0.9941176176071167)
[2025-02-13 20:27:17,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17,347][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.12998461723327637, acc: 0.9651162624359131)
[2025-02-13 20:27:17,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17,717][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.10282449424266815, acc: 0.981249988079071)
[2025-02-13 20:27:17,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18,077][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.060638103634119034, acc: 0.9757575988769531)
[2025-02-13 20:27:18,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18,436][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.10130062699317932, acc: 0.9689440727233887)
[2025-02-13 20:27:18,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18,804][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.0580894909799099, acc: 0.9937106966972351)
[2025-02-13 20:27:18,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19,187][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.15940475463867188, acc: 0.9644970297813416)
[2025-02-13 20:27:19,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19,537][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.12787705659866333, acc: 0.9664429426193237)
[2025-02-13 20:27:19,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19,904][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.11324179172515869, acc: 0.978723406791687)
[2025-02-13 20:27:20,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20,304][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.03193124383687973, acc: 0.988095223903656)
[2025-02-13 20:27:20,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20,701][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.0474739708006382, acc: 0.9923076629638672)
[2025-02-13 20:27:20,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21,108][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.14225496351718903, acc: 0.9754601120948792)
[2025-02-13 20:27:21,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21,486][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.14613334834575653, acc: 0.970588207244873)
[2025-02-13 20:27:21,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21,839][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.17987921833992004, acc: 0.970588207244873)
[2025-02-13 20:27:21,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22,234][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.08864225447177887, acc: 0.9768785834312439)
[2025-02-13 20:27:22,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22,606][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.06509087979793549, acc: 0.9849624037742615)
[2025-02-13 20:27:22,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22,954][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.1288750022649765, acc: 0.959770143032074)
[2025-02-13 20:27:23,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23,358][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.17110683023929596, acc: 0.9611111283302307)
[2025-02-13 20:27:23,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23,730][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.10957889258861542, acc: 0.9777777791023254)
[2025-02-13 20:27:23,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24,153][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.10139641910791397, acc: 0.9766082167625427)
[2025-02-13 20:27:24,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24,553][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.07547574490308762, acc: 0.9757575988769531)
[2025-02-13 20:27:24,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24,944][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.2530888020992279, acc: 0.9308176040649414)
[2025-02-13 20:27:25,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25,332][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.23747947812080383, acc: 0.9491525292396545)
[2025-02-13 20:27:25,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25,691][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.267221599817276, acc: 0.9379310607910156)
[2025-02-13 20:27:25,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26,059][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.1540999710559845, acc: 0.9613259434700012)
[2025-02-13 20:27:26,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26,494][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.13868804275989532, acc: 0.9479166865348816)
[2025-02-13 20:27:26,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26,897][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.11652638763189316, acc: 0.9681528806686401)
[2025-02-13 20:27:27,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27,308][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.1672007143497467, acc: 0.9647887349128723)
[2025-02-13 20:27:27,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27,711][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.17298859357833862, acc: 0.9657142758369446)
[2025-02-13 20:27:27,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28,148][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.07394261658191681, acc: 0.9907407164573669)
[2025-02-13 20:27:28,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28,540][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.07103586941957474, acc: 0.9794520735740662)
[2025-02-13 20:27:28,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28,958][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.16139103472232819, acc: 0.95652174949646)
[2025-02-13 20:27:29,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29,402][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.14948077499866486, acc: 0.9534883499145508)
[2025-02-13 20:27:29,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29,796][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.10795653611421585, acc: 0.9935064911842346)
[2025-02-13 20:27:29,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30,238][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.18526999652385712, acc: 0.9520958065986633)
[2025-02-13 20:27:30,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30,661][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.2493894100189209, acc: 0.9251700639724731)
[2025-02-13 20:27:30,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31,028][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.3151795566082001, acc: 0.9405405521392822)
[2025-02-13 20:27:31,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31,482][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.10961111634969711, acc: 0.9714285731315613)
[2025-02-13 20:27:31,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31,863][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.08625870198011398, acc: 0.9736841917037964)
[2025-02-13 20:27:32,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32,252][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.07196467369794846, acc: 0.9701492786407471)
[2025-02-13 20:27:32,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32,608][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.07941063493490219, acc: 0.966292142868042)
[2025-02-13 20:27:32,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32,995][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.1916283667087555, acc: 0.9561403393745422)
[2025-02-13 20:27:33,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33,404][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.014784954488277435, acc: 1.0)
[2025-02-13 20:27:33,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33,783][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.10357419401407242, acc: 0.9722222089767456)
[2025-02-13 20:27:33,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34,221][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.1312926560640335, acc: 0.9726027250289917)
[2025-02-13 20:27:34,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34,582][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.04295547679066658, acc: 0.9905660152435303)
[2025-02-13 20:27:34,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34,995][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.04711519926786423, acc: 0.9865771532058716)
[2025-02-13 20:27:35,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35,415][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.119513601064682, acc: 0.96875)
[2025-02-13 20:27:35,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35,823][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.13667403161525726, acc: 0.9444444179534912)
[2025-02-13 20:27:35,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36,269][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.09656663239002228, acc: 0.9647887349128723)
[2025-02-13 20:27:36,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36,665][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.10557775944471359, acc: 0.9808917045593262)
[2025-02-13 20:27:36,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37,079][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.060637325048446655, acc: 1.0)
[2025-02-13 20:27:37,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37,473][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.10097865760326385, acc: 0.9765625)
[2025-02-13 20:27:37,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37,880][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.09707906097173691, acc: 0.9925925731658936)
[2025-02-13 20:27:38,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38,275][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.15979976952075958, acc: 0.9473684430122375)
[2025-02-13 20:27:38,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38,693][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.0856458768248558, acc: 0.9910714030265808)
[2025-02-13 20:27:38,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39,124][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.061431948095560074, acc: 0.9781420826911926)
[2025-02-13 20:27:39,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39,487][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.008401446044445038, acc: 1.0)
[2025-02-13 20:27:39,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39,907][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.052559029310941696, acc: 0.9895833134651184)
[2025-02-13 20:27:40,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40,317][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.1941773146390915, acc: 0.9753086566925049)
[2025-02-13 20:27:40,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40,694][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.12692520022392273, acc: 0.9808917045593262)
[2025-02-13 20:27:40,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41,094][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.04143590107560158, acc: 0.9887640476226807)
[2025-02-13 20:27:41,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41,491][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.029395800083875656, acc: 0.9937888383865356)
[2025-02-13 20:27:41,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41,880][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.04376903176307678, acc: 0.9939393997192383)
[2025-02-13 20:27:42,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42,301][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.07264848053455353, acc: 0.9849246144294739)
[2025-02-13 20:27:42,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42,721][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.026859289035201073, acc: 0.9896907210350037)
[2025-02-13 20:27:42,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43,126][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.12493766844272614, acc: 0.977477490901947)
[2025-02-13 20:27:43,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43,479][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.1912321150302887, acc: 0.963350772857666)
[2025-02-13 20:27:43,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43,868][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.10293535888195038, acc: 0.9785714149475098)
[2025-02-13 20:27:44,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44,268][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.0400630384683609, acc: 0.9927007555961609)
[2025-02-13 20:27:44,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44,686][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.04996368661522865, acc: 0.9858155846595764)
[2025-02-13 20:27:44,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45,077][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.12507115304470062, acc: 0.9659090638160706)
[2025-02-13 20:27:45,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45,448][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.10081744939088821, acc: 0.9629629850387573)
[2025-02-13 20:27:45,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45,810][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.1263471245765686, acc: 0.9617486596107483)
[2025-02-13 20:27:45,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46,190][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.17006799578666687, acc: 0.9583333134651184)
[2025-02-13 20:27:46,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46,574][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.030524373054504395, acc: 1.0)
[2025-02-13 20:27:46,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46,932][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.14191342890262604, acc: 0.9553072452545166)
[2025-02-13 20:27:47,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47,278][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.08611368387937546, acc: 0.987261176109314)
[2025-02-13 20:27:47,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47,648][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.11191153526306152, acc: 0.9640718698501587)
[2025-02-13 20:27:47,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48,030][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.06911275535821915, acc: 0.987261176109314)
[2025-02-13 20:27:48,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48,444][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.09861729294061661, acc: 0.9662162065505981)
[2025-02-13 20:27:48,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48,797][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.29339057207107544, acc: 0.9375)
[2025-02-13 20:27:48,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49,171][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.12279858440160751, acc: 0.9717513918876648)
[2025-02-13 20:27:49,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49,539][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.06559469550848007, acc: 0.9738219976425171)
[2025-02-13 20:27:49,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49,978][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.14452700316905975, acc: 0.9607843160629272)
[2025-02-13 20:27:50,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50,384][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.0710303783416748, acc: 0.9923076629638672)
[2025-02-13 20:27:50,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50,768][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.027029240503907204, acc: 0.9941860437393188)
[2025-02-13 20:27:50,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51,157][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.09458120912313461, acc: 0.9870129823684692)
[2025-02-13 20:27:51,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51,532][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.19362057745456696, acc: 0.9196428656578064)
[2025-02-13 20:27:51,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51,933][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.1251777559518814, acc: 0.9599999785423279)
[2025-02-13 20:27:52,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52,300][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.1110212579369545, acc: 0.9636363387107849)
[2025-02-13 20:27:52,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52,688][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.038945265114307404, acc: 0.9926470518112183)
[2025-02-13 20:27:52,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53,120][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.09270475059747696, acc: 0.9720279574394226)
[2025-02-13 20:27:53,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53,540][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.217716783285141, acc: 0.9411764740943909)
[2025-02-13 20:27:53,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53,944][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.13656245172023773, acc: 0.9655172228813171)
[2025-02-13 20:27:54,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54,324][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.10424956679344177, acc: 0.9797979593276978)
[2025-02-13 20:27:54,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54,742][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.07717230916023254, acc: 0.9757575988769531)
[2025-02-13 20:27:54,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55,149][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.08361927419900894, acc: 0.9766082167625427)
[2025-02-13 20:27:55,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55,561][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.12344401329755783, acc: 0.9683544039726257)
[2025-02-13 20:27:55,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55,957][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.058790065348148346, acc: 0.984000027179718)
[2025-02-13 20:27:56,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56,327][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.14626263082027435, acc: 0.9767441749572754)
[2025-02-13 20:27:56,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56,686][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.38955867290496826, acc: 0.9127907156944275)
[2025-02-13 20:27:56,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57,048][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.110052689909935, acc: 0.9552238583564758)
[2025-02-13 20:27:57,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57,417][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.16505154967308044, acc: 0.9615384340286255)
[2025-02-13 20:27:57,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57,786][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.2788655161857605, acc: 0.9411764740943909)
[2025-02-13 20:27:57,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58,164][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.18011751770973206, acc: 0.9484536051750183)
[2025-02-13 20:27:58,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58,558][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.06918354332447052, acc: 0.9934640526771545)
[2025-02-13 20:27:58,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58,955][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.08171555399894714, acc: 0.9885714054107666)
[2025-02-13 20:27:59,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59,321][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.03553381189703941, acc: 0.993630588054657)
[2025-02-13 20:27:59,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59,727][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.11010463535785675, acc: 0.9788359999656677)
[2025-02-13 20:27:59,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00,081][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.13605313003063202, acc: 0.9685534834861755)
[2025-02-13 20:28:00,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00,461][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.069233737885952, acc: 0.9767441749572754)
[2025-02-13 20:28:00,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00,851][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.04066009446978569, acc: 0.994350254535675)
[2025-02-13 20:28:00,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01,265][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.044922418892383575, acc: 0.9896907210350037)
[2025-02-13 20:28:01,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01,642][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.046535734087228775, acc: 0.9942196607589722)
[2025-02-13 20:28:01,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02,011][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.12366512417793274, acc: 0.9733333587646484)
[2025-02-13 20:28:02,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02,391][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.17042189836502075, acc: 0.9484536051750183)
[2025-02-13 20:28:02,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02,772][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.06434757262468338, acc: 0.9936708807945251)
[2025-02-13 20:28:02,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03,159][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.10733567178249359, acc: 0.9723756909370422)
[2025-02-13 20:28:03,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03,569][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.09751954674720764, acc: 0.9763033390045166)
[2025-02-13 20:28:03,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03,958][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.25818678736686707, acc: 0.9651162624359131)
[2025-02-13 20:28:04,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04,337][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.14140157401561737, acc: 0.9655172228813171)
[2025-02-13 20:28:04,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04,723][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.08599380403757095, acc: 0.9884393215179443)
[2025-02-13 20:28:04,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05,171][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.09812625497579575, acc: 0.9742268323898315)
[2025-02-13 20:28:05,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05,554][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.2982282042503357, acc: 0.9337349534034729)
[2025-02-13 20:28:05,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05,922][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.2700457274913788, acc: 0.9270833134651184)
[2025-02-13 20:28:06,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06,309][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.3091205060482025, acc: 0.9222797751426697)
[2025-02-13 20:28:06,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06,685][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.17689310014247894, acc: 0.9298245906829834)
[2025-02-13 20:28:06,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07,077][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.07221252471208572, acc: 0.9722222089767456)
[2025-02-13 20:28:07,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07,455][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.07782858610153198, acc: 0.9775280952453613)
[2025-02-13 20:28:07,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07,805][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.06786173582077026, acc: 0.9793814420700073)
[2025-02-13 20:28:07,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08,162][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.16006602346897125, acc: 0.9653179049491882)
[2025-02-13 20:28:08,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08,531][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.07205948233604431, acc: 0.9748427867889404)
[2025-02-13 20:28:08,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08,958][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.052787281572818756, acc: 0.9886363744735718)
[2025-02-13 20:28:09,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09,383][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.09685118496417999, acc: 0.9874213933944702)
[2025-02-13 20:28:09,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09,769][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.019436560571193695, acc: 1.0)
[2025-02-13 20:28:09,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10,187][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.03897302225232124, acc: 0.9898989796638489)
[2025-02-13 20:28:10,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10,558][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.0643782690167427, acc: 0.9811320900917053)
[2025-02-13 20:28:10,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10,942][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.041439492255449295, acc: 0.9942528605461121)
[2025-02-13 20:28:11,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11,316][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.06265337020158768, acc: 0.9795918464660645)
[2025-02-13 20:28:11,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11,696][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.050587281584739685, acc: 0.9953488111495972)
[2025-02-13 20:28:11,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12,087][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.06112557649612427, acc: 0.9809523820877075)
[2025-02-13 20:28:12,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12,458][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.1880328208208084, acc: 0.9644970297813416)
[2025-02-13 20:28:12,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12,812][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.242436021566391, acc: 0.946107804775238)
[2025-02-13 20:28:12,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13,205][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.15919773280620575, acc: 0.9577465057373047)
[2025-02-13 20:28:13,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13,606][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.1804320365190506, acc: 0.9545454382896423)
[2025-02-13 20:28:13,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13,992][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.27264732122421265, acc: 0.9285714030265808)
[2025-02-13 20:28:14,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14,391][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.09430991858243942, acc: 0.9793814420700073)
[2025-02-13 20:28:14,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14,786][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.1449708342552185, acc: 0.9668508172035217)
[2025-02-13 20:28:14,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15,214][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.12092997133731842, acc: 0.9683544039726257)
[2025-02-13 20:28:15,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15,581][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.1380433887243271, acc: 0.9818181991577148)
[2025-02-13 20:28:15,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15,962][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.23446740210056305, acc: 0.9640718698501587)
[2025-02-13 20:28:16,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16,332][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.04320710524916649, acc: 0.9805194735527039)
[2025-02-13 20:28:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16,708][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.1564348340034485, acc: 0.9576271176338196)
[2025-02-13 20:28:16,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17,114][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.11539951711893082, acc: 0.9683544039726257)
[2025-02-13 20:28:17,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17,472][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.05124300345778465, acc: 1.0)
[2025-02-13 20:28:17,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17,852][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.023257523775100708, acc: 1.0)
[2025-02-13 20:28:17,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18,223][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.07023098319768906, acc: 0.9814814925193787)
[2025-02-13 20:28:18,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18,602][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.03184794634580612, acc: 1.0)
[2025-02-13 20:28:18,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18,979][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.052491672337055206, acc: 0.9919354915618896)
[2025-02-13 20:28:19,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19,332][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.16346058249473572, acc: 0.9576271176338196)
[2025-02-13 20:28:19,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19,781][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.07709614187479019, acc: 0.9729729890823364)
[2025-02-13 20:28:19,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20,137][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.1453186720609665, acc: 0.9629629850387573)
[2025-02-13 20:28:20,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20,486][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.05774284526705742, acc: 0.989130437374115)
[2025-02-13 20:28:20,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20,865][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.07823477685451508, acc: 0.9634146094322205)
[2025-02-13 20:28:20,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21,223][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.09928658604621887, acc: 0.9679999947547913)
[2025-02-13 20:28:21,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21,609][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.06692173331975937, acc: 0.9718309640884399)
[2025-02-13 20:28:21,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22,048][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.13620395958423615, acc: 0.9701492786407471)
[2025-02-13 20:28:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22,475][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.02984728291630745, acc: 0.9934210777282715)
[2025-02-13 20:28:22,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22,843][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.05438051372766495, acc: 0.9732142686843872)
[2025-02-13 20:28:22,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23,209][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.027505384758114815, acc: 1.0)
[2025-02-13 20:28:23,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23,588][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.02143757790327072, acc: 0.9900000095367432)
[2025-02-13 20:28:23,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24,017][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.13015905022621155, acc: 0.96875)
[2025-02-13 20:28:24,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24,434][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.0708666443824768, acc: 0.9912280440330505)
[2025-02-13 20:28:24,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24,834][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.030085964128375053, acc: 1.0)
[2025-02-13 20:28:24,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25,237][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.05858158692717552, acc: 0.9897959232330322)
[2025-02-13 20:28:25,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25,618][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.0419359914958477, acc: 0.9855072498321533)
[2025-02-13 20:28:25,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26,008][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.02326214499771595, acc: 1.0)
[2025-02-13 20:28:26,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26,390][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.05775336176156998, acc: 0.9842519760131836)
[2025-02-13 20:28:26,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26,789][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.07802809029817581, acc: 0.9869281053543091)
[2025-02-13 20:28:26,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27,157][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.05246025323867798, acc: 0.9914529919624329)
[2025-02-13 20:28:27,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27,549][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.07199878245592117, acc: 0.988095223903656)
[2025-02-13 20:28:27,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27,916][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.050872039049863815, acc: 0.9913793206214905)
[2025-02-13 20:28:28,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28,279][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.05076438933610916, acc: 0.9888268113136292)
[2025-02-13 20:28:28,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28,642][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.029347414150834084, acc: 0.9938271641731262)
[2025-02-13 20:28:28,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29,000][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.05969689041376114, acc: 0.9798657894134521)
[2025-02-13 20:28:29,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29,374][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.1273508220911026, acc: 0.97826087474823)
[2025-02-13 20:28:29,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29,755][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.16682177782058716, acc: 0.9461538195610046)
[2025-02-13 20:28:29,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30,120][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.1001598909497261, acc: 0.9696969985961914)
[2025-02-13 20:28:30,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30,511][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.040223460644483566, acc: 0.984375)
[2025-02-13 20:28:30,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30,908][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.18967305123806, acc: 0.9492753744125366)
[2025-02-13 20:28:31,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31,292][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.02384389564394951, acc: 0.9888268113136292)
[2025-02-13 20:28:31,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31,663][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.11253637820482254, acc: 0.9622641801834106)
[2025-02-13 20:28:31,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32,071][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.190422922372818, acc: 0.9595375657081604)
[2025-02-13 20:28:32,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32,462][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.43619874119758606, acc: 0.928205132484436)
[2025-02-13 20:28:32,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32,856][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.5327000021934509, acc: 0.8653846383094788)
[2025-02-13 20:28:32,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33,235][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.1816076934337616, acc: 0.9507042169570923)
[2025-02-13 20:28:33,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33,605][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.21891094744205475, acc: 0.9487179517745972)
[2025-02-13 20:28:33,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34,007][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.1161469891667366, acc: 0.9828571677207947)
[2025-02-13 20:28:34,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34,398][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.3681541979312897, acc: 0.9337349534034729)
[2025-02-13 20:28:34,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34,777][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.0492217130959034, acc: 0.9927536249160767)
[2025-02-13 20:28:34,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35,136][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.08541558682918549, acc: 0.9701492786407471)
[2025-02-13 20:28:35,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35,502][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.09178338199853897, acc: 0.9781022071838379)
[2025-02-13 20:28:35,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35,881][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.07802344113588333, acc: 0.9741935729980469)
[2025-02-13 20:28:36,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36,247][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.15096184611320496, acc: 0.9638554453849792)
[2025-02-13 20:28:36,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36,644][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.3094169497489929, acc: 0.930232584476471)
[2025-02-13 20:28:36,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37,035][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.6494107842445374, acc: 0.8372092843055725)
[2025-02-13 20:28:37,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37,405][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.2903882563114166, acc: 0.9255319237709045)
[2025-02-13 20:28:37,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37,792][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.10010246932506561, acc: 0.9692307710647583)
[2025-02-13 20:28:37,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38,184][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.4267745018005371, acc: 0.9147727489471436)
[2025-02-13 20:28:38,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38,548][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.1636584848165512, acc: 0.920634925365448)
[2025-02-13 20:28:38,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38,991][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.25297579169273376, acc: 0.9425837397575378)
[2025-02-13 20:28:39,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39,387][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.2564677298069, acc: 0.9504132270812988)
[2025-02-13 20:28:39,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39,784][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.19026347994804382, acc: 0.9536423683166504)
[2025-02-13 20:28:39,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40,199][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.09948799014091492, acc: 0.9733333587646484)
[2025-02-13 20:28:40,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40,582][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.13938291370868683, acc: 0.9780219793319702)
[2025-02-13 20:28:40,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40,978][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.1632075309753418, acc: 0.9722222089767456)
[2025-02-13 20:28:41,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41,358][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.15496493875980377, acc: 0.9607843160629272)
[2025-02-13 20:28:41,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41,717][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.4670124650001526, acc: 0.8759689927101135)
[2025-02-13 20:28:41,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42,073][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.18849678337574005, acc: 0.9586777091026306)
[2025-02-13 20:28:42,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42,449][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.23019474744796753, acc: 0.9482758641242981)
[2025-02-13 20:28:42,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42,838][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.1405992954969406, acc: 0.9615384340286255)
[2025-02-13 20:28:43,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43,245][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.11791253834962845, acc: 0.975806474685669)
[2025-02-13 20:28:43,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43,689][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.11600802838802338, acc: 0.9674796462059021)
[2025-02-13 20:28:43,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44,094][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.27763229608535767, acc: 0.9274193644523621)
[2025-02-13 20:28:44,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44,477][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.37307873368263245, acc: 0.9345794320106506)
[2025-02-13 20:28:44,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44,873][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.30592983961105347, acc: 0.9172932505607605)
[2025-02-13 20:28:45,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45,259][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.14299127459526062, acc: 0.949999988079071)
[2025-02-13 20:28:45,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45,674][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.05415167286992073, acc: 0.9869281053543091)
[2025-02-13 20:28:45,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46,062][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.05888877809047699, acc: 0.9819819927215576)
[2025-02-13 20:28:46,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46,429][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.07821532338857651, acc: 0.988095223903656)
[2025-02-13 20:28:46,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46,822][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.036260392516851425, acc: 0.9955157041549683)
[2025-02-13 20:28:46,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47,216][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.027600912377238274, acc: 0.994413435459137)
[2025-02-13 20:28:47,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47,632][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.05920858681201935, acc: 0.9856459498405457)
[2025-02-13 20:28:47,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48,016][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.05831129848957062, acc: 0.9828571677207947)
[2025-02-13 20:28:48,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48,407][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.05356811732053757, acc: 0.9759036302566528)
[2025-02-13 20:28:48,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48,796][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.039083585143089294, acc: 0.9860140085220337)
[2025-02-13 20:28:48,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49,166][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.02988521009683609, acc: 0.9866666793823242)
[2025-02-13 20:28:49,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49,546][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.0620356909930706, acc: 0.9862068891525269)
[2025-02-13 20:28:49,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49,955][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.1654682457447052, acc: 0.9585492014884949)
[2025-02-13 20:28:50,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50,338][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.07959841191768646, acc: 0.9742268323898315)
[2025-02-13 20:28:50,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50,746][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.08503221720457077, acc: 0.9710144996643066)
[2025-02-13 20:28:50,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51,148][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.07507669925689697, acc: 0.9833333492279053)
[2025-02-13 20:28:51,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51,554][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.09586703777313232, acc: 0.9829545617103577)
[2025-02-13 20:28:51,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51,930][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.05502493679523468, acc: 0.9926470518112183)
[2025-02-13 20:28:52,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52,329][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.06875202804803848, acc: 0.9807692170143127)
[2025-02-13 20:28:52,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52,708][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.17783638834953308, acc: 0.9599999785423279)
[2025-02-13 20:28:52,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53,094][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.07400576025247574, acc: 0.9885057210922241)
[2025-02-13 20:28:53,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53,473][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.11041328310966492, acc: 0.9738219976425171)
[2025-02-13 20:28:53,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53,865][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.19981537759304047, acc: 0.9689440727233887)
[2025-02-13 20:28:53,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54,239][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.10216470807790756, acc: 0.9606741666793823)
[2025-02-13 20:28:54,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54,623][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.14401103556156158, acc: 0.9685039520263672)
[2025-02-13 20:28:54,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55,008][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.04639590159058571, acc: 1.0)
[2025-02-13 20:28:55,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55,361][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.041447099298238754, acc: 0.9807692170143127)
[2025-02-13 20:28:55,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55,712][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.023738501593470573, acc: 0.9937888383865356)
[2025-02-13 20:28:55,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56,101][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.1738627851009369, acc: 0.9767441749572754)
[2025-02-13 20:28:56,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56,510][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.03271295875310898, acc: 0.9946523904800415)
[2025-02-13 20:28:56,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56,898][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.03269394487142563, acc: 0.9948979616165161)
[2025-02-13 20:28:57,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57,323][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.21782340109348297, acc: 0.9512194991111755)
[2025-02-13 20:28:57,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57,696][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.09351713210344315, acc: 0.9692307710647583)
[2025-02-13 20:28:57,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58,103][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.09732917696237564, acc: 0.9791666865348816)
[2025-02-13 20:28:58,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58,459][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.11945390701293945, acc: 0.9602272510528564)
[2025-02-13 20:28:58,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58,793][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.15680743753910065, acc: 0.9644970297813416)
[2025-02-13 20:28:58,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59,149][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.05842256173491478, acc: 0.9836065769195557)
[2025-02-13 20:28:59,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59,518][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.08479973673820496, acc: 0.9884393215179443)
[2025-02-13 20:28:59,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59,894][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.07999631017446518, acc: 0.9864864945411682)
[2025-02-13 20:29:00,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00,312][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.13062874972820282, acc: 0.9430379867553711)
[2025-02-13 20:29:00,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00,732][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.09656272083520889, acc: 0.9835164546966553)
[2025-02-13 20:29:00,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01,112][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.10163839906454086, acc: 0.977142870426178)
[2025-02-13 20:29:01,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01,508][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.1308949738740921, acc: 0.9649122953414917)
[2025-02-13 20:29:01,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01,922][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.14613263309001923, acc: 0.9781420826911926)
[2025-02-13 20:29:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02,321][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.0730474665760994, acc: 0.9746192693710327)
[2025-02-13 20:29:02,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02,749][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.1467711627483368, acc: 0.9601989984512329)
[2025-02-13 20:29:02,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03,130][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.1030978187918663, acc: 0.9624999761581421)
[2025-02-13 20:29:03,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03,537][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.15404145419597626, acc: 0.9664804339408875)
[2025-02-13 20:29:03,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03,916][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.08508395403623581, acc: 0.9830508232116699)
[2025-02-13 20:29:04,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04,296][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.07967015355825424, acc: 0.9821428656578064)
[2025-02-13 20:29:04,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04,676][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.265797883272171, acc: 0.939130425453186)
[2025-02-13 20:29:04,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05,077][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.09474092721939087, acc: 0.9734042286872864)
[2025-02-13 20:29:05,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05,470][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.07431259006261826, acc: 0.9947368502616882)
[2025-02-13 20:29:05,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05,840][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.1225014328956604, acc: 0.970588207244873)
[2025-02-13 20:29:05,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06,194][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.09613986313343048, acc: 0.9696969985961914)
[2025-02-13 20:29:06,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06,534][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.02521650493144989, acc: 1.0)
[2025-02-13 20:29:06,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06,903][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.05566549301147461, acc: 0.9864864945411682)
[2025-02-13 20:29:07,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07,337][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.01512376219034195, acc: 1.0)
[2025-02-13 20:29:07,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07,704][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.0325605683028698, acc: 0.9934210777282715)
[2025-02-13 20:29:07,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08,067][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.025570735335350037, acc: 1.0)
[2025-02-13 20:29:08,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08,476][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.02182869426906109, acc: 1.0)
[2025-02-13 20:29:08,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08,865][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.12137171626091003, acc: 0.9806451797485352)
[2025-02-13 20:29:09,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09,269][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.017847422510385513, acc: 1.0)
[2025-02-13 20:29:09,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09,665][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.10030357539653778, acc: 0.965753436088562)
[2025-02-13 20:29:09,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10,039][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.1676773875951767, acc: 0.9586777091026306)
[2025-02-13 20:29:10,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10,453][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.05491851270198822, acc: 0.9929577708244324)
[2025-02-13 20:29:10,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10,841][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.05848956108093262, acc: 0.9933775067329407)
[2025-02-13 20:29:10,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11,250][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.04306478798389435, acc: 0.9931507110595703)
[2025-02-13 20:29:11,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11,637][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.06128536909818649, acc: 0.9817073345184326)
[2025-02-13 20:29:11,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12,020][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.033782847225666046, acc: 0.9939758777618408)
[2025-02-13 20:29:12,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12,416][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.047350842505693436, acc: 0.9867549538612366)
[2025-02-13 20:29:12,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12,818][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.03442259877920151, acc: 0.9866666793823242)
[2025-02-13 20:29:12,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13,236][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.03412703424692154, acc: 0.9878048896789551)
[2025-02-13 20:29:13,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13,653][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.04803835228085518, acc: 0.9863945841789246)
[2025-02-13 20:29:13,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14,020][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.04814032465219498, acc: 0.9811320900917053)
[2025-02-13 20:29:14,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14,354][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.049230799078941345, acc: 0.9770992398262024)
[2025-02-13 20:29:14,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14,694][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.024941004812717438, acc: 0.9862068891525269)
[2025-02-13 20:29:14,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15,062][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.023845886811614037, acc: 0.9930555820465088)
[2025-02-13 20:29:15,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15,454][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.2403765469789505, acc: 0.9473684430122375)
[2025-02-13 20:29:15,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15,838][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.2685967683792114, acc: 0.9298245906829834)
[2025-02-13 20:29:15,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16,225][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.15621314942836761, acc: 0.9597315192222595)
[2025-02-13 20:29:16,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16,608][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.08172387629747391, acc: 0.9789473414421082)
[2025-02-13 20:29:16,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16,952][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.132711723446846, acc: 0.9551281929016113)
[2025-02-13 20:29:17,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17,349][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.08187307417392731, acc: 0.9946236610412598)
[2025-02-13 20:29:17,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17,755][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.2483399510383606, acc: 0.9390863180160522)
[2025-02-13 20:29:17,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18,145][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.26570823788642883, acc: 0.939393937587738)
[2025-02-13 20:29:18,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18,547][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.3815852999687195, acc: 0.9138755798339844)
[2025-02-13 20:29:18,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18,941][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.4452463686466217, acc: 0.898876428604126)
[2025-02-13 20:29:19,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19,348][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.10207053273916245, acc: 0.9738219976425171)
[2025-02-13 20:29:19,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19,719][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.08276945352554321, acc: 0.984000027179718)
[2025-02-13 20:29:19,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20,136][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.1289260983467102, acc: 0.957317054271698)
[2025-02-13 20:29:20,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20,549][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.10127317905426025, acc: 0.9681528806686401)
[2025-02-13 20:29:20,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20,960][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.09173806011676788, acc: 0.9870129823684692)
[2025-02-13 20:29:21,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21,381][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.18712963163852692, acc: 0.9599999785423279)
[2025-02-13 20:29:21,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21,789][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.08031119406223297, acc: 0.9885057210922241)
[2025-02-13 20:29:21,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22,181][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.05713196098804474, acc: 0.9848484992980957)
[2025-02-13 20:29:22,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22,593][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.02231142483651638, acc: 1.0)
[2025-02-13 20:29:22,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22,963][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.06288044154644012, acc: 0.9760000109672546)
[2025-02-13 20:29:23,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23,344][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.08122541755437851, acc: 0.9756097793579102)
[2025-02-13 20:29:23,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23,719][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.032906774431467056, acc: 0.9869281053543091)
[2025-02-13 20:29:23,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24,104][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.06488795578479767, acc: 0.9855072498321533)
[2025-02-13 20:29:24,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24,471][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.06186139956116676, acc: 0.9866666793823242)
[2025-02-13 20:29:24,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24,857][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.10084745287895203, acc: 0.9863945841789246)
[2025-02-13 20:29:24,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25,254][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.0674150362610817, acc: 0.9779411554336548)
[2025-02-13 20:29:25,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25,639][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.0576796680688858, acc: 0.9931972622871399)
[2025-02-13 20:29:25,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26,022][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.0334322452545166, acc: 1.0)
[2025-02-13 20:29:26,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26,412][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.027418065816164017, acc: 0.9922480583190918)
[2025-02-13 20:29:26,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26,816][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.048482123762369156, acc: 0.9931034445762634)
[2025-02-13 20:29:26,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27,207][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.07676941901445389, acc: 0.9767441749572754)
[2025-02-13 20:29:27,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27,584][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.04526672139763832, acc: 0.9930070042610168)
[2025-02-13 20:29:27,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27,938][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.1757471263408661, acc: 0.9739130139350891)
[2025-02-13 20:29:28,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28,288][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.03350389748811722, acc: 1.0)
[2025-02-13 20:29:28,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28,673][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.018446534872055054, acc: 1.0)
[2025-02-13 20:29:28,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29,009][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.05233524739742279, acc: 1.0)
[2025-02-13 20:29:29,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29,366][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.08009111136198044, acc: 0.9895833134651184)
[2025-02-13 20:29:29,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29,742][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.12384720146656036, acc: 0.9729729890823364)
[2025-02-13 20:29:29,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30,113][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.11161588877439499, acc: 0.9849624037742615)
[2025-02-13 20:29:30,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30,494][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.035021670162677765, acc: 0.9939024448394775)
[2025-02-13 20:29:30,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30,883][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.039600010961294174, acc: 0.9831932783126831)
[2025-02-13 20:29:31,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31,266][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.025631768628954887, acc: 0.9935064911842346)
[2025-02-13 20:29:31,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31,638][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.06591123342514038, acc: 0.9750000238418579)
[2025-02-13 20:29:31,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32,007][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.04591745138168335, acc: 0.9874213933944702)
[2025-02-13 20:29:32,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32,417][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.07133053988218307, acc: 0.9868420958518982)
[2025-02-13 20:29:32,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32,779][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.04033777490258217, acc: 0.9932885766029358)
[2025-02-13 20:29:32,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33,163][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.021669596433639526, acc: 1.0)
[2025-02-13 20:29:33,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33,583][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.04688878729939461, acc: 0.9932885766029358)
[2025-02-13 20:29:33,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33,954][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.04383492469787598, acc: 0.9913793206214905)
[2025-02-13 20:29:34,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34,347][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.06019323319196701, acc: 0.9777777791023254)
[2025-02-13 20:29:34,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34,732][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.02413676679134369, acc: 1.0)
[2025-02-13 20:29:34,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35,117][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.06960363686084747, acc: 0.9720279574394226)
[2025-02-13 20:29:35,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35,508][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.16976293921470642, acc: 0.9545454382896423)
[2025-02-13 20:29:35,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35,906][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.03827623650431633, acc: 0.9928571581840515)
[2025-02-13 20:29:36,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36,318][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.05823545902967453, acc: 0.9849624037742615)
[2025-02-13 20:29:36,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36,707][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.033578988164663315, acc: 0.9926470518112183)
[2025-02-13 20:29:36,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37,084][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.14346103370189667, acc: 0.9542483687400818)
[2025-02-13 20:29:37,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37,445][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.08026058226823807, acc: 0.9809523820877075)
[2025-02-13 20:29:37,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37,823][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.12644939124584198, acc: 0.9604519605636597)
[2025-02-13 20:29:37,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38,215][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.20098163187503815, acc: 0.9487179517745972)
[2025-02-13 20:29:38,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38,596][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.1437411904335022, acc: 0.9649999737739563)
[2025-02-13 20:29:38,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38,995][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.13079360127449036, acc: 0.960698664188385)
[2025-02-13 20:29:39,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39,389][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.15520820021629333, acc: 0.9692307710647583)
[2025-02-13 20:29:39,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39,776][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.0974537804722786, acc: 0.9830508232116699)
[2025-02-13 20:29:39,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40,136][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.27113214135169983, acc: 0.9464285969734192)
[2025-02-13 20:29:40,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40,524][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.26490071415901184, acc: 0.9406779408454895)
[2025-02-13 20:29:40,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40,967][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.4501851201057434, acc: 0.8791946172714233)
[2025-02-13 20:29:41,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41,395][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.2546798884868622, acc: 0.9534883499145508)
[2025-02-13 20:29:41,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41,865][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.47173234820365906, acc: 0.9059829115867615)
[2025-02-13 20:29:42,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42,314][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.26513350009918213, acc: 0.9173553586006165)
[2025-02-13 20:29:42,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42,741][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.27322328090667725, acc: 0.9659863710403442)
[2025-02-13 20:29:42,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43,144][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.17594510316848755, acc: 0.9714285731315613)
[2025-02-13 20:29:43,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43,522][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.1471421718597412, acc: 0.9569377899169922)
[2025-02-13 20:29:43,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43,896][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.18209344148635864, acc: 0.9664429426193237)
[2025-02-13 20:29:44,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44,276][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.21518047153949738, acc: 0.9532163739204407)
[2025-02-13 20:29:44,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44,685][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.12084300071001053, acc: 0.9663865566253662)
[2025-02-13 20:29:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45,142][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.13354167342185974, acc: 0.9780219793319702)
[2025-02-13 20:29:45,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45,581][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.1019383892416954, acc: 0.9924812316894531)
[2025-02-13 20:29:45,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46,005][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.07029596716165543, acc: 0.9915966391563416)
[2025-02-13 20:29:46,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46,434][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.07080438733100891, acc: 0.982758641242981)
[2025-02-13 20:29:46,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46,840][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.07334382086992264, acc: 0.9880239367485046)
[2025-02-13 20:29:46,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47,451][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.12936821579933167, acc: 0.9865771532058716)
[2025-02-13 20:29:47,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47,864][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.093012735247612, acc: 0.9813664555549622)
[2025-02-13 20:29:48,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48,264][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.12289117276668549, acc: 0.9784946441650391)
[2025-02-13 20:29:48,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48,682][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.16136600077152252, acc: 0.9615384340286255)
[2025-02-13 20:29:48,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49,047][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.08205199241638184, acc: 0.9756097793579102)
[2025-02-13 20:29:49,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49,425][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.06557246297597885, acc: 0.9774436354637146)
[2025-02-13 20:29:49,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49,845][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.11726510524749756, acc: 0.9769585132598877)
[2025-02-13 20:29:49,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50,232][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.058188892900943756, acc: 0.9809523820877075)
[2025-02-13 20:29:50,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50,618][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.05425352603197098, acc: 0.9852216839790344)
[2025-02-13 20:29:50,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51,010][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.10199850052595139, acc: 0.9777777791023254)
[2025-02-13 20:29:51,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51,417][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.08971511572599411, acc: 0.9722222089767456)
[2025-02-13 20:29:51,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51,781][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.030934207141399384, acc: 0.9841269850730896)
[2025-02-13 20:29:51,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52,178][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.05910814180970192, acc: 0.98591548204422)
[2025-02-13 20:29:52,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52,554][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.062439221888780594, acc: 0.9870967864990234)
[2025-02-13 20:29:52,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52,940][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.03452099859714508, acc: 0.988304078578949)
[2025-02-13 20:29:53,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53,322][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.09961412101984024, acc: 0.9802631735801697)
[2025-02-13 20:29:53,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53,707][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.02355336584150791, acc: 1.0)
[2025-02-13 20:29:53,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54,099][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.04787738621234894, acc: 0.987500011920929)
[2025-02-13 20:29:54,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54,494][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.011675477027893066, acc: 1.0)
[2025-02-13 20:29:54,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54,878][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.025265755131840706, acc: 0.9935064911842346)
[2025-02-13 20:29:55,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55,304][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.08307687193155289, acc: 0.9693251252174377)
[2025-02-13 20:29:55,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55,686][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.022944537922739983, acc: 1.0)
[2025-02-13 20:29:55,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56,075][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.014375029131770134, acc: 1.0)
[2025-02-13 20:29:56,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56,482][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.14074446260929108, acc: 0.9742268323898315)
[2025-02-13 20:29:56,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56,826][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.023703236132860184, acc: 1.0)
[2025-02-13 20:29:56,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57,235][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.07171837985515594, acc: 0.9842105507850647)
[2025-02-13 20:29:57,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57,619][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.038269221782684326, acc: 0.9940119981765747)
[2025-02-13 20:29:57,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58,042][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.04714656248688698, acc: 0.9824561476707458)
[2025-02-13 20:29:58,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58,431][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.08856289088726044, acc: 0.9903846383094788)
[2025-02-13 20:29:58,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58,864][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.03387540951371193, acc: 0.9863013625144958)
[2025-02-13 20:29:59,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59,311][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.07385142892599106, acc: 0.9888268113136292)
[2025-02-13 20:29:59,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59,704][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.03703298792243004, acc: 0.9935897588729858)
[2025-02-13 20:29:59,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00,079][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.15421615540981293, acc: 0.9714285731315613)
[2025-02-13 20:30:00,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00,443][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.029300374910235405, acc: 0.9856114983558655)
[2025-02-13 20:30:00,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00,825][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.04481087997555733, acc: 0.9945651888847351)
[2025-02-13 20:30:00,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01,187][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.06999177485704422, acc: 0.9942857027053833)
[2025-02-13 20:30:01,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01,559][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.0908912718296051, acc: 0.9718309640884399)
[2025-02-13 20:30:01,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01,925][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.049116164445877075, acc: 0.9862068891525269)
[2025-02-13 20:30:02,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02,299][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.06389013677835464, acc: 0.9858155846595764)
[2025-02-13 20:30:02,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02,741][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.0835299864411354, acc: 0.9849624037742615)
[2025-02-13 20:30:02,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03,201][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.05149523913860321, acc: 0.9873417615890503)
[2025-02-13 20:30:03,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03,605][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.11902385205030441, acc: 0.9657142758369446)
[2025-02-13 20:30:03,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04,021][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.10845288634300232, acc: 0.9647058844566345)
[2025-02-13 20:30:04,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04,433][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.10937292873859406, acc: 0.9946236610412598)
[2025-02-13 20:30:04,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04,818][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.14750991761684418, acc: 0.9748427867889404)
[2025-02-13 20:30:04,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05,205][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.15659531950950623, acc: 0.9885057210922241)
[2025-02-13 20:30:05,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05,588][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.08289899677038193, acc: 0.9834254384040833)
[2025-02-13 20:30:05,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06,080][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.02562698721885681, acc: 1.0)
[2025-02-13 20:30:06,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06,530][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.020901478826999664, acc: 1.0)
[2025-02-13 20:30:06,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06,984][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.05270172283053398, acc: 0.9741935729980469)
[2025-02-13 20:30:07,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07,408][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.026543499901890755, acc: 0.9942528605461121)
[2025-02-13 20:30:07,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07,831][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.14750593900680542, acc: 0.9766082167625427)
[2025-02-13 20:30:07,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08,215][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.09539125114679337, acc: 0.9842519760131836)
[2025-02-13 20:30:08,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08,591][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.12311699241399765, acc: 0.9846153855323792)
[2025-02-13 20:30:08,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08,937][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.033938441425561905, acc: 0.9888888597488403)
[2025-02-13 20:30:09,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09,351][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.16994376480579376, acc: 0.9776119589805603)
[2025-02-13 20:30:09,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09,785][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.02042398229241371, acc: 1.0)
[2025-02-13 20:30:09,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10,237][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.21540561318397522, acc: 0.9715909361839294)
[2025-02-13 20:30:10,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10,612][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.10942139476537704, acc: 0.9594594836235046)
[2025-02-13 20:30:10,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10,954][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.2517853379249573, acc: 0.9561403393745422)
[2025-02-13 20:30:11,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11,363][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.274789422750473, acc: 0.929648220539093)
[2025-02-13 20:30:11,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11,751][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.19578908383846283, acc: 0.9516128897666931)
[2025-02-13 20:30:11,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12,169][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.18400034308433533, acc: 0.9551281929016113)
[2025-02-13 20:30:12,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12,587][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.19768285751342773, acc: 0.9390863180160522)
[2025-02-13 20:30:12,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12,976][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.3931468725204468, acc: 0.8962264060974121)
[2025-02-13 20:30:13,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13,358][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.2814795672893524, acc: 0.9294871687889099)
[2025-02-13 20:30:13,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13,761][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.23137350380420685, acc: 0.9440000057220459)
[2025-02-13 20:30:13,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14,153][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.2802620530128479, acc: 0.9186046719551086)
[2025-02-13 20:30:14,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14,551][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.4404800236225128, acc: 0.9328858852386475)
[2025-02-13 20:30:14,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14,954][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.20767860114574432, acc: 0.9277777671813965)
[2025-02-13 20:30:15,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15,374][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.18990489840507507, acc: 0.957446813583374)
[2025-02-13 20:30:15,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15,766][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.08396319299936295, acc: 0.978723406791687)
[2025-02-13 20:30:15,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16,212][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.08758150786161423, acc: 0.9710144996643066)
[2025-02-13 20:30:16,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16,623][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.03220776468515396, acc: 1.0)
[2025-02-13 20:30:16,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17,040][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.05974390730261803, acc: 0.9906542301177979)
[2025-02-13 20:30:17,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17,436][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.1278855800628662, acc: 0.9769230484962463)
[2025-02-13 20:30:17,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17,827][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.050363488495349884, acc: 1.0)
[2025-02-13 20:30:17,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18,172][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.16634054481983185, acc: 0.9577465057373047)
[2025-02-13 20:30:18,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18,548][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.02783435583114624, acc: 1.0)
[2025-02-13 20:30:18,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19,017][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.14280366897583008, acc: 0.9489796161651611)
[2025-02-13 20:30:19,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19,420][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.1176811009645462, acc: 0.9669421315193176)
[2025-02-13 20:30:19,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19,781][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.04057988524436951, acc: 1.0)
[2025-02-13 20:30:19,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20,186][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.050033602863550186, acc: 1.0)
[2025-02-13 20:30:20,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20,586][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.10510746389627457, acc: 0.9794520735740662)
[2025-02-13 20:30:20,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20,984][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.0497833751142025, acc: 0.9919999837875366)
[2025-02-13 20:30:21,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21,382][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.16560561954975128, acc: 0.9794520735740662)
[2025-02-13 20:30:21,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21,833][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.1712636798620224, acc: 0.9632353186607361)
[2025-02-13 20:30:21,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22,194][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.31152766942977905, acc: 0.9636363387107849)
[2025-02-13 20:30:22,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22,617][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.06896678358316422, acc: 0.9774436354637146)
[2025-02-13 20:30:22,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23,011][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.14104603230953217, acc: 0.9677419066429138)
[2025-02-13 20:30:23,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23,412][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.0763649120926857, acc: 0.9826086759567261)
[2025-02-13 20:30:23,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23,775][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.04227776825428009, acc: 0.9886363744735718)
[2025-02-13 20:30:23,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24,168][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.1150350570678711, acc: 0.9615384340286255)
[2025-02-13 20:30:24,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24,546][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.07179316133260727, acc: 0.9743589758872986)
[2025-02-13 20:30:24,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24,946][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.05161565542221069, acc: 0.9924242496490479)
[2025-02-13 20:30:25,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25,339][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.029352732002735138, acc: 1.0)
[2025-02-13 20:30:25,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25,737][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.19337841868400574, acc: 0.9624999761581421)
[2025-02-13 20:30:25,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26,139][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.055500950664281845, acc: 0.9908257126808167)
[2025-02-13 20:30:26,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26,557][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.06086565554141998, acc: 0.991304337978363)
[2025-02-13 20:30:26,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26,978][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.1008911058306694, acc: 0.9790209531784058)
[2025-02-13 20:30:27,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27,378][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.10028520226478577, acc: 0.9925925731658936)
[2025-02-13 20:30:27,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27,761][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.14670614898204803, acc: 0.966292142868042)
[2025-02-13 20:30:27,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28,147][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.12642888724803925, acc: 0.9651162624359131)
[2025-02-13 20:30:28,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28,535][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.08678711950778961, acc: 0.9842932224273682)
[2025-02-13 20:30:28,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28,942][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.05939820408821106, acc: 0.9848484992980957)
[2025-02-13 20:30:29,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29,326][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.10689589381217957, acc: 0.9837398529052734)
[2025-02-13 20:30:29,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29,727][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.199943408370018, acc: 0.9421965479850769)
[2025-02-13 20:30:29,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30,110][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.09288736432790756, acc: 0.984375)
[2025-02-13 20:30:30,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30,507][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.061192650347948074, acc: 0.9945945739746094)
[2025-02-13 20:30:30,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30,952][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.07043277472257614, acc: 0.9850746393203735)
[2025-02-13 20:30:31,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31,372][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.05134475976228714, acc: 0.9894179701805115)
[2025-02-13 20:30:31,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31,824][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.1281132847070694, acc: 0.9754902124404907)
[2025-02-13 20:30:31,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32,251][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.053734298795461655, acc: 0.9888268113136292)
[2025-02-13 20:30:32,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32,672][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.11829297989606857, acc: 0.9891892075538635)
[2025-02-13 20:30:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33,152][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.02791311778128147, acc: 1.0)
[2025-02-13 20:30:33,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33,635][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.05608892813324928, acc: 0.9846938848495483)
[2025-02-13 20:30:33,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34,039][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.061069246381521225, acc: 0.9813664555549622)
[2025-02-13 20:30:34,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34,444][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.014814876951277256, acc: 1.0)
[2025-02-13 20:30:34,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34,838][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.10176923125982285, acc: 0.9892473220825195)
[2025-02-13 20:30:34,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35,318][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.03567417338490486, acc: 0.9913793206214905)
[2025-02-13 20:30:35,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35,722][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.04822392016649246, acc: 0.9846938848495483)
[2025-02-13 20:30:35,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36,086][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.13193164765834808, acc: 0.9767441749572754)
[2025-02-13 20:30:36,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36,465][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.02268655225634575, acc: 1.0)
[2025-02-13 20:30:36,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36,783][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.045813627541065216, acc: 0.9915966391563416)
[2025-02-13 20:30:36,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37,161][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.07402696460485458, acc: 0.989130437374115)
[2025-02-13 20:30:37,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37,544][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.06590651720762253, acc: 0.9882352948188782)
[2025-02-13 20:30:37,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37,933][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.10880760103464127, acc: 0.9815950989723206)
[2025-02-13 20:30:38,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38,351][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.05768059194087982, acc: 0.9884393215179443)
[2025-02-13 20:30:38,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38,771][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.036597687751054764, acc: 0.9942196607589722)
[2025-02-13 20:30:38,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39,161][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.06848527491092682, acc: 0.9806451797485352)
[2025-02-13 20:30:39,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39,549][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.08797735720872879, acc: 0.9833333492279053)
[2025-02-13 20:30:39,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39,961][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.15028542280197144, acc: 0.9714285731315613)
[2025-02-13 20:30:40,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40,375][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.14862902462482452, acc: 0.9714285731315613)
[2025-02-13 20:30:40,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40,804][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.045852649956941605, acc: 0.9900990128517151)
[2025-02-13 20:30:40,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41,209][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.016588877886533737, acc: 1.0)
[2025-02-13 20:30:41,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41,588][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.07346148788928986, acc: 0.9772727489471436)
[2025-02-13 20:30:41,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41,963][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.13630801439285278, acc: 0.9516128897666931)
[2025-02-13 20:30:42,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42,374][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.18129093945026398, acc: 0.9663865566253662)
[2025-02-13 20:30:42,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42,760][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.031018750742077827, acc: 1.0)
[2025-02-13 20:30:42,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43,169][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.0582856759428978, acc: 0.9808917045593262)
[2025-02-13 20:30:43,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43,625][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.08540470898151398, acc: 0.9702970385551453)
[2025-02-13 20:30:43,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44,069][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.061403822153806686, acc: 0.9881656765937805)
[2025-02-13 20:30:44,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44,473][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.07612050324678421, acc: 0.9850000143051147)
[2025-02-13 20:30:44,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44,877][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.1562010645866394, acc: 0.9790576100349426)
[2025-02-13 20:30:45,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45,289][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.09337777644395828, acc: 0.9738562107086182)
[2025-02-13 20:30:45,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45,736][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.05990539863705635, acc: 0.9839572310447693)
[2025-02-13 20:30:45,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46,149][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.08943551033735275, acc: 0.9772727489471436)
[2025-02-13 20:30:46,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46,583][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.0696544423699379, acc: 0.9796954393386841)
[2025-02-13 20:30:46,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47,031][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.06243623048067093, acc: 0.9858155846595764)
[2025-02-13 20:30:47,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47,382][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.06307347118854523, acc: 0.987730085849762)
[2025-02-13 20:30:47,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47,753][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.03188110515475273, acc: 1.0)
[2025-02-13 20:30:47,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48,177][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.04726492241024971, acc: 0.9869281053543091)
[2025-02-13 20:30:48,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48,651][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.06132727116346359, acc: 0.976190447807312)
[2025-02-13 20:30:48,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49,026][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.056110747158527374, acc: 0.9940119981765747)
[2025-02-13 20:30:49,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49,482][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.07196731120347977, acc: 0.9767441749572754)
[2025-02-13 20:30:49,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49,900][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.11650348454713821, acc: 0.9744898080825806)
[2025-02-13 20:30:50,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50,360][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.060436684638261795, acc: 0.9753694534301758)
[2025-02-13 20:30:50,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50,808][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.04410500079393387, acc: 0.9935897588729858)
[2025-02-13 20:30:50,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51,169][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.16685868799686432, acc: 0.9801980257034302)
[2025-02-13 20:30:51,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51,589][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.1406463384628296, acc: 0.9685534834861755)
[2025-02-13 20:30:51,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52,025][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.04389064759016037, acc: 0.9924812316894531)
[2025-02-13 20:30:52,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52,400][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.03793829306960106, acc: 1.0)
[2025-02-13 20:30:52,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52,794][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.12121046334505081, acc: 0.9726027250289917)
[2025-02-13 20:30:52,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53,198][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.1452043056488037, acc: 0.9772727489471436)
[2025-02-13 20:30:53,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53,595][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.051176998764276505, acc: 0.9781022071838379)
[2025-02-13 20:30:53,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54,032][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.03510456159710884, acc: 0.9921875)
[2025-02-13 20:30:54,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54,464][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.07106209546327591, acc: 0.988304078578949)
[2025-02-13 20:30:54,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54,855][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.054298557341098785, acc: 0.9909909963607788)
[2025-02-13 20:30:55,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55,286][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.0456397645175457, acc: 0.9876543283462524)
[2025-02-13 20:30:55,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55,718][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.10013751685619354, acc: 0.9841269850730896)
[2025-02-13 20:30:55,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56,150][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.049678631126880646, acc: 0.981249988079071)
[2025-02-13 20:30:56,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56,544][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.09780959784984589, acc: 0.9815950989723206)
[2025-02-13 20:30:56,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56,938][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.10481481999158859, acc: 0.9752066135406494)
[2025-02-13 20:30:57,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57,320][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.03508817031979561, acc: 0.9928057789802551)
[2025-02-13 20:30:57,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57,718][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.04376121982932091, acc: 0.9790209531784058)
[2025-02-13 20:30:57,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58,106][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.044419050216674805, acc: 0.9939393997192383)
[2025-02-13 20:30:58,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58,522][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.04446878284215927, acc: 0.9882352948188782)
[2025-02-13 20:30:58,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58,929][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.05043475702404976, acc: 0.9931034445762634)
[2025-02-13 20:30:59,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59,313][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.08098132908344269, acc: 0.9795918464660645)
[2025-02-13 20:30:59,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59,716][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.09532807767391205, acc: 0.9801324605941772)
[2025-02-13 20:30:59,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00,089][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.07167116552591324, acc: 0.970370352268219)
[2025-02-13 20:31:00,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00,493][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.054395489394664764, acc: 0.991304337978363)
[2025-02-13 20:31:00,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00,849][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.08656741678714752, acc: 0.9729729890823364)
[2025-02-13 20:31:00,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01,200][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.0317743718624115, acc: 0.9819819927215576)
[2025-02-13 20:31:01,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01,579][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.10903121531009674, acc: 0.9704142212867737)
[2025-02-13 20:31:01,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01,943][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.03264891356229782, acc: 0.9931972622871399)
[2025-02-13 20:31:02,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02,426][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.13886041939258575, acc: 0.96875)
[2025-02-13 20:31:02,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02,822][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.1075747087597847, acc: 0.9775280952453613)
[2025-02-13 20:31:02,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03,233][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.09045722335577011, acc: 0.9679487347602844)
[2025-02-13 20:31:03,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03,620][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.09390091150999069, acc: 0.9751552939414978)
[2025-02-13 20:31:03,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04,000][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.06275825947523117, acc: 0.9940119981765747)
[2025-02-13 20:31:04,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04,376][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.09880810230970383, acc: 0.9754601120948792)
[2025-02-13 20:31:04,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04,755][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.08648015558719635, acc: 0.9751552939414978)
[2025-02-13 20:31:04,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05,139][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.04443264380097389, acc: 1.0)
[2025-02-13 20:31:05,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05,545][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.2558249533176422, acc: 0.9395973086357117)
[2025-02-13 20:31:05,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05,965][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.189630389213562, acc: 0.9512194991111755)
[2025-02-13 20:31:06,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06,351][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.07049808651208878, acc: 0.976190447807312)
[2025-02-13 20:31:06,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06,765][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.06343311816453934, acc: 0.993630588054657)
[2025-02-13 20:31:06,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07,184][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.11031114310026169, acc: 0.9602649211883545)
[2025-02-13 20:31:07,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07,587][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.08620865643024445, acc: 0.9836956262588501)
[2025-02-13 20:31:07,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08,040][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.06964538991451263, acc: 0.9924242496490479)
[2025-02-13 20:31:08,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08,389][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.05073053389787674, acc: 0.9922480583190918)
[2025-02-13 20:31:08,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08,785][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.07083352655172348, acc: 0.9856114983558655)
[2025-02-13 20:31:08,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09,195][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.09588396549224854, acc: 0.9651162624359131)
[2025-02-13 20:31:09,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09,561][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.09946053475141525, acc: 0.9731543660163879)
[2025-02-13 20:31:09,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09,974][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.06356250494718552, acc: 0.9880239367485046)
[2025-02-13 20:31:10,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10,361][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.04612090811133385, acc: 0.9918699264526367)
[2025-02-13 20:31:10,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10,839][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.15271873772144318, acc: 0.9709302186965942)
[2025-02-13 20:31:11,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11,268][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.10962460935115814, acc: 0.9695122241973877)
[2025-02-13 20:31:11,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11,669][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.05047483369708061, acc: 0.9918032884597778)
[2025-02-13 20:31:11,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12,083][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.09343846142292023, acc: 0.988950252532959)
[2025-02-13 20:31:12,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12,485][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.1065688356757164, acc: 0.9695122241973877)
[2025-02-13 20:31:12,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12,858][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.08978968113660812, acc: 0.976190447807312)
[2025-02-13 20:31:12,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13,307][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.04076322540640831, acc: 1.0)
[2025-02-13 20:31:13,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13,700][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.16967137157917023, acc: 0.9882352948188782)
[2025-02-13 20:31:13,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14,068][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.3101675808429718, acc: 0.9774436354637146)
[2025-02-13 20:31:14,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14,466][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.19426357746124268, acc: 0.9632353186607361)
[2025-02-13 20:31:14,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14,850][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.06420471519231796, acc: 0.9860140085220337)
[2025-02-13 20:31:14,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:15,243][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.07151570171117783, acc: 0.9880239367485046)
[2025-02-13 20:31:15,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:15,607][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.13008800148963928, acc: 0.9677419066429138)
[2025-02-13 20:31:15,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16,006][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.07815366983413696, acc: 0.9796954393386841)
[2025-02-13 20:31:16,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16,400][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.03194216266274452, acc: 1.0)
[2025-02-13 20:31:16,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16,780][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.1271601915359497, acc: 0.9533678889274597)
[2025-02-13 20:31:16,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17,157][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.11751426011323929, acc: 0.9611650705337524)
[2025-02-13 20:31:17,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17,520][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.1895795315504074, acc: 0.9529411792755127)
[2025-02-13 20:31:17,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17,912][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.20092874765396118, acc: 0.9415584206581116)
[2025-02-13 20:31:18,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18,297][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.19167126715183258, acc: 0.9492753744125366)
[2025-02-13 20:31:18,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18,674][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.21089918911457062, acc: 0.9631901979446411)
[2025-02-13 20:31:18,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19,058][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.20422902703285217, acc: 0.931506872177124)
[2025-02-13 20:31:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19,446][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.06566238403320312, acc: 0.9947090148925781)
[2025-02-13 20:31:19,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19,851][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.11334746330976486, acc: 0.9666666388511658)
[2025-02-13 20:31:19,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20,243][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.14476501941680908, acc: 0.9463087320327759)
[2025-02-13 20:31:20,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20,625][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.13324780762195587, acc: 0.9751552939414978)
[2025-02-13 20:31:20,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21,020][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.35882294178009033, acc: 0.9100528955459595)
[2025-02-13 20:31:21,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21,384][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.3343043029308319, acc: 0.9099099040031433)
[2025-02-13 20:31:21,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21,756][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.16579794883728027, acc: 0.9666666388511658)
[2025-02-13 20:31:21,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22,125][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.17575567960739136, acc: 0.9652777910232544)
[2025-02-13 20:31:22,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22,542][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.09875020384788513, acc: 0.9842105507850647)
[2025-02-13 20:31:22,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22,969][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.11102462559938431, acc: 0.9677419066429138)
[2025-02-13 20:31:23,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23,394][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.34184056520462036, acc: 0.9135802388191223)
[2025-02-13 20:31:23,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23,791][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.3921106159687042, acc: 0.9318181872367859)
[2025-02-13 20:31:23,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24,189][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.3263870179653168, acc: 0.9226804375648499)
[2025-02-13 20:31:24,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24,599][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.20351609587669373, acc: 0.9603960514068604)
[2025-02-13 20:31:24,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25,026][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.08700060099363327, acc: 0.9756097793579102)
[2025-02-13 20:31:25,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25,440][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.20147767663002014, acc: 0.9591836929321289)
[2025-02-13 20:31:25,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25,827][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.22927260398864746, acc: 0.932692289352417)
[2025-02-13 20:31:25,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26,255][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.08289697766304016, acc: 0.984375)
[2025-02-13 20:31:26,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26,761][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.22520321607589722, acc: 0.9510489702224731)
[2025-02-13 20:31:26,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27,157][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.2854919135570526, acc: 0.9408602118492126)
[2025-02-13 20:31:27,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27,522][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.5872587561607361, acc: 0.8693467378616333)
[2025-02-13 20:31:27,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27,895][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.32043811678886414, acc: 0.9430052042007446)
[2025-02-13 20:31:28,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28,259][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.10364391654729843, acc: 0.9731183052062988)
[2025-02-13 20:31:28,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28,655][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.11456820368766785, acc: 0.9719101190567017)
[2025-02-13 20:31:28,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29,065][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.09986213594675064, acc: 0.9689119458198547)
[2025-02-13 20:31:29,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29,452][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.27727198600769043, acc: 0.939393937587738)
[2025-02-13 20:31:29,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29,830][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.1686478853225708, acc: 0.967391312122345)
[2025-02-13 20:31:29,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30,188][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.23285315930843353, acc: 0.9414634108543396)
[2025-02-13 20:31:30,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30,576][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.26718559861183167, acc: 0.9274611473083496)
[2025-02-13 20:31:30,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30,997][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.17720143496990204, acc: 0.9403669834136963)
[2025-02-13 20:31:31,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31,359][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.24262645840644836, acc: 0.9341317415237427)
[2025-02-13 20:31:31,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31,770][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.1638062745332718, acc: 0.96875)
[2025-02-13 20:31:31,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32,212][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.09770309180021286, acc: 0.9829545617103577)
[2025-02-13 20:31:32,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32,601][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.0625874474644661, acc: 0.9851484894752502)
[2025-02-13 20:31:32,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33,067][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.1994343400001526, acc: 0.9559999704360962)
[2025-02-13 20:31:33,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33,454][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.17726902663707733, acc: 0.9568965435028076)
[2025-02-13 20:31:33,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33,898][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.09484578669071198, acc: 0.9826589822769165)
[2025-02-13 20:31:34,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34,274][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.06773553043603897, acc: 0.9809523820877075)
[2025-02-13 20:31:34,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34,703][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.12194905430078506, acc: 0.9719101190567017)
[2025-02-13 20:31:34,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35,118][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.08597763627767563, acc: 0.9883720874786377)
[2025-02-13 20:31:35,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35,524][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.07902451604604721, acc: 0.9808917045593262)
[2025-02-13 20:31:35,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35,925][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.1002153754234314, acc: 0.9714285731315613)
[2025-02-13 20:31:36,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36,303][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.16832534968852997, acc: 0.954285740852356)
[2025-02-13 20:31:36,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36,721][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.08312293887138367, acc: 0.9791666865348816)
[2025-02-13 20:31:36,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37,121][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.09418376535177231, acc: 0.976331353187561)
[2025-02-13 20:31:37,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37,524][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.02833160199224949, acc: 0.9890710115432739)
[2025-02-13 20:31:37,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37,935][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.12912894785404205, acc: 0.9925373196601868)
[2025-02-13 20:31:38,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38,384][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.02284052036702633, acc: 0.9929078221321106)
[2025-02-13 20:31:38,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38,815][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.03451523929834366, acc: 1.0)
[2025-02-13 20:31:38,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39,229][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.07131507992744446, acc: 0.9748427867889404)
[2025-02-13 20:31:39,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39,631][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.16695892810821533, acc: 0.9775280952453613)
[2025-02-13 20:31:39,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40,014][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.11685232073068619, acc: 0.970588207244873)
[2025-02-13 20:31:40,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40,402][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.07639429718255997, acc: 0.9781022071838379)
[2025-02-13 20:31:40,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40,802][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.0968652218580246, acc: 0.9822485446929932)
[2025-02-13 20:31:40,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41,251][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.06508754193782806, acc: 0.9922480583190918)
[2025-02-13 20:31:41,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41,667][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.04895855486392975, acc: 0.982758641242981)
[2025-02-13 20:31:41,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42,096][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.14838944375514984, acc: 0.9689922332763672)
[2025-02-13 20:31:42,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42,490][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.1636764109134674, acc: 0.9661017060279846)
[2025-02-13 20:31:42,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42,904][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.1162068098783493, acc: 0.9734042286872864)
[2025-02-13 20:31:43,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43,314][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.2941736876964569, acc: 0.9490445852279663)
[2025-02-13 20:31:43,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43,727][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.12450063973665237, acc: 0.970370352268219)
[2025-02-13 20:31:43,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44,168][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.054087501019239426, acc: 0.9934640526771545)
[2025-02-13 20:31:44,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44,567][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.23052789270877838, acc: 0.95652174949646)
[2025-02-13 20:31:44,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44,984][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.25304654240608215, acc: 0.9814814925193787)
[2025-02-13 20:31:45,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45,410][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.03425615653395653, acc: 1.0)
[2025-02-13 20:31:45,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45,783][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.015241031534969807, acc: 1.0)
[2025-02-13 20:31:45,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46,152][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.02345297299325466, acc: 0.9930070042610168)
[2025-02-13 20:31:46,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46,538][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.031430210918188095, acc: 0.9941860437393188)
[2025-02-13 20:31:46,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46,965][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.04587502405047417, acc: 0.9950739145278931)
[2025-02-13 20:31:47,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47,400][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.0682232528924942, acc: 0.9791666865348816)
[2025-02-13 20:31:47,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47,821][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.03755098208785057, acc: 0.9885714054107666)
[2025-02-13 20:31:47,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48,235][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.029106631875038147, acc: 0.9947090148925781)
[2025-02-13 20:31:48,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48,651][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.11058834940195084, acc: 0.9794520735740662)
[2025-02-13 20:31:48,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49,089][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.10497549921274185, acc: 0.9848484992980957)
[2025-02-13 20:31:49,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49,489][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.03514938801527023, acc: 1.0)
[2025-02-13 20:31:49,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49,859][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.04483676701784134, acc: 0.9839572310447693)
[2025-02-13 20:31:49,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50,279][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.05242050811648369, acc: 0.9946523904800415)
[2025-02-13 20:31:50,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50,721][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.04919879883527756, acc: 0.9900990128517151)
[2025-02-13 20:31:50,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51,178][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.03511815518140793, acc: 0.995192289352417)
[2025-02-13 20:31:51,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51,592][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.01758364774286747, acc: 0.9949495196342468)
[2025-02-13 20:31:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51,979][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.03764745220541954, acc: 0.9888888597488403)
[2025-02-13 20:31:52,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52,387][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.12311308085918427, acc: 0.9644970297813416)
[2025-02-13 20:31:52,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52,830][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.14449810981750488, acc: 0.9813664555549622)
[2025-02-13 20:31:52,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53,263][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.21390819549560547, acc: 0.9414893388748169)
[2025-02-13 20:31:53,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53,765][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.19790932536125183, acc: 0.9450549483299255)
[2025-02-13 20:31:53,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54,192][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.06081118434667587, acc: 0.9795918464660645)
[2025-02-13 20:31:54,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54,607][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.07585585117340088, acc: 0.9842932224273682)
[2025-02-13 20:31:54,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55,054][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.05721345916390419, acc: 0.9833333492279053)
[2025-02-13 20:31:55,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55,465][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.10442417114973068, acc: 0.9752475023269653)
[2025-02-13 20:31:55,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55,948][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.08784140646457672, acc: 0.9800000190734863)
[2025-02-13 20:31:56,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56,355][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.03781786188483238, acc: 0.9851484894752502)
[2025-02-13 20:31:56,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56,778][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.254186749458313, acc: 0.9378882050514221)
[2025-02-13 20:31:56,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57,173][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.031186459586024284, acc: 1.0)
[2025-02-13 20:31:57,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57,538][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.02661597542464733, acc: 0.9934640526771545)
[2025-02-13 20:31:57,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57,897][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.03238551318645477, acc: 0.9875776171684265)
[2025-02-13 20:31:58,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58,314][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.05768727883696556, acc: 0.9815950989723206)
[2025-02-13 20:31:58,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58,690][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.06858015060424805, acc: 0.9767441749572754)
[2025-02-13 20:31:58,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59,100][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.07637277245521545, acc: 0.9931034445762634)
[2025-02-13 20:31:59,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59,502][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.07040233910083771, acc: 0.9760000109672546)
[2025-02-13 20:31:59,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59,922][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.06087210029363632, acc: 0.9847328066825867)
[2025-02-13 20:32:00,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00,308][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.05836903676390648, acc: 0.9924812316894531)
[2025-02-13 20:32:00,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00,763][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.04372091218829155, acc: 0.9941860437393188)
[2025-02-13 20:32:00,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01,213][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.04796149581670761, acc: 0.983146071434021)
[2025-02-13 20:32:01,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01,625][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.02900209091603756, acc: 0.9948453903198242)
[2025-02-13 20:32:01,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02,031][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.05679783225059509, acc: 0.9941520690917969)
[2025-02-13 20:32:02,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02,504][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.043247975409030914, acc: 0.9894179701805115)
[2025-02-13 20:32:02,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02,996][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.019871091470122337, acc: 1.0)
[2025-02-13 20:32:03,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03,413][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.04328744113445282, acc: 0.9858155846595764)
[2025-02-13 20:32:03,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03,836][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.039353810250759125, acc: 0.9893048405647278)
[2025-02-13 20:32:03,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04,286][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.05132388696074486, acc: 0.9834254384040833)
[2025-02-13 20:32:04,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04,721][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.029980797320604324, acc: 0.9941176176071167)
[2025-02-13 20:32:04,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05,100][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.03843565285205841, acc: 0.9937106966972351)
[2025-02-13 20:32:05,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05,526][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.11114998161792755, acc: 0.9875776171684265)
[2025-02-13 20:32:05,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05,992][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.10158415138721466, acc: 0.9783783555030823)
[2025-02-13 20:32:06,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06,437][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.15037721395492554, acc: 0.9631901979446411)
[2025-02-13 20:32:06,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06,869][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.03757641464471817, acc: 0.9935064911842346)
[2025-02-13 20:32:07,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07,285][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.13234704732894897, acc: 0.95652174949646)
[2025-02-13 20:32:07,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07,705][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.06953883171081543, acc: 0.9818181991577148)
[2025-02-13 20:32:07,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08,135][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.13332371413707733, acc: 0.9702380895614624)
[2025-02-13 20:32:08,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08,589][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.09181832522153854, acc: 0.9659090638160706)
[2025-02-13 20:32:08,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09,000][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.09923628717660904, acc: 0.9851852059364319)
[2025-02-13 20:32:09,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09,381][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.09051486849784851, acc: 0.9766082167625427)
[2025-02-13 20:32:09,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09,819][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.22468677163124084, acc: 0.9710982441902161)
[2025-02-13 20:32:09,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10,260][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.06965253502130508, acc: 0.9802631735801697)
[2025-02-13 20:32:10,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10,677][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.058557260781526566, acc: 0.9893048405647278)
[2025-02-13 20:32:10,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11,140][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.10685890913009644, acc: 0.9649999737739563)
[2025-02-13 20:32:11,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11,568][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.1015087217092514, acc: 0.9742268323898315)
[2025-02-13 20:32:11,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11,988][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.05799645557999611, acc: 0.9900497794151306)
[2025-02-13 20:32:12,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12,413][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.05362999811768532, acc: 0.9839572310447693)
[2025-02-13 20:32:12,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12,887][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.13878747820854187, acc: 0.9750000238418579)
[2025-02-13 20:32:13,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13,297][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.10328394919633865, acc: 0.9763033390045166)
[2025-02-13 20:32:13,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13,725][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.07326862215995789, acc: 0.9765258431434631)
[2025-02-13 20:32:13,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14,158][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.07127287238836288, acc: 0.9842932224273682)
[2025-02-13 20:32:14,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14,603][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.1152716651558876, acc: 0.9655172228813171)
[2025-02-13 20:32:14,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15,009][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.09748230129480362, acc: 0.9725274443626404)
[2025-02-13 20:32:15,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15,435][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.0903068259358406, acc: 0.9820627570152283)
[2025-02-13 20:32:15,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15,826][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.14034363627433777, acc: 0.9507389068603516)
[2025-02-13 20:32:15,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16,227][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.14264518022537231, acc: 0.9528796076774597)
[2025-02-13 20:32:16,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16,598][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.0441778339445591, acc: 0.9756097793579102)
[2025-02-13 20:32:16,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17,027][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.032352279871702194, acc: 0.9921875)
[2025-02-13 20:32:17,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17,426][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.12968239188194275, acc: 0.9662446975708008)
[2025-02-13 20:32:17,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17,833][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.042215730994939804, acc: 0.9907407164573669)
[2025-02-13 20:32:17,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18,219][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.06043454632163048, acc: 0.989847719669342)
[2025-02-13 20:32:18,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18,565][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.054799120873212814, acc: 0.9903846383094788)
[2025-02-13 20:32:18,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19,005][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.0386551171541214, acc: 0.9839357137680054)
[2025-02-13 20:32:19,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19,393][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.028361547738313675, acc: 0.9954751133918762)
[2025-02-13 20:32:19,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19,756][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.09201206266880035, acc: 0.990338146686554)
[2025-02-13 20:32:19,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20,132][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.08435314148664474, acc: 0.9718309640884399)
[2025-02-13 20:32:20,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20,558][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.13275516033172607, acc: 0.976190447807312)
[2025-02-13 20:32:20,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20,913][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.20411579310894012, acc: 0.9672130942344666)
[2025-02-13 20:32:21,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21,281][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.13542772829532623, acc: 0.9385964870452881)
[2025-02-13 20:32:21,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21,640][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.23457658290863037, acc: 0.9285714030265808)
[2025-02-13 20:32:21,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22,087][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.04987255856394768, acc: 0.9932432174682617)
[2025-02-13 20:32:22,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22,463][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.0699618011713028, acc: 0.9722222089767456)
[2025-02-13 20:32:22,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22,845][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.12724457681179047, acc: 0.9685039520263672)
[2025-02-13 20:32:22,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23,184][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.06180574372410774, acc: 0.991525411605835)
[2025-02-13 20:32:23,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23,615][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.08863435685634613, acc: 0.9685039520263672)
[2025-02-13 20:32:23,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23,996][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.10831357538700104, acc: 0.9736841917037964)
[2025-02-13 20:32:24,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24,436][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.12807278335094452, acc: 0.9849624037742615)
[2025-02-13 20:32:24,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24,826][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.19745823740959167, acc: 0.9662162065505981)
[2025-02-13 20:32:24,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25,218][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.026012137532234192, acc: 1.0)
[2025-02-13 20:32:25,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25,588][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.03936775401234627, acc: 1.0)
[2025-02-13 20:32:25,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25,986][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.1099272295832634, acc: 0.970802903175354)
[2025-02-13 20:32:26,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26,384][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.11975771933794022, acc: 0.9791666865348816)
[2025-02-13 20:32:26,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26,804][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.24128644168376923, acc: 0.9419354796409607)
[2025-02-13 20:32:26,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27,188][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.035902101546525955, acc: 0.9850746393203735)
[2025-02-13 20:32:27,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27,581][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.12545786798000336, acc: 0.9767441749572754)
[2025-02-13 20:32:27,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27,981][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.10216443240642548, acc: 0.9848484992980957)
[2025-02-13 20:32:28,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28,325][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.09344062954187393, acc: 0.975806474685669)
[2025-02-13 20:32:28,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28,730][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.11081042885780334, acc: 0.9672130942344666)
[2025-02-13 20:32:28,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29,083][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.13289563357830048, acc: 0.9593495726585388)
[2025-02-13 20:32:29,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29,456][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.06585109233856201, acc: 0.9929078221321106)
[2025-02-13 20:32:29,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29,840][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.15379203855991364, acc: 0.9545454382896423)
[2025-02-13 20:32:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30,238][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.09280306100845337, acc: 0.982300877571106)
[2025-02-13 20:32:30,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30,617][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.13799704611301422, acc: 0.9624060392379761)
[2025-02-13 20:32:30,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31,017][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.019466543570160866, acc: 1.0)
[2025-02-13 20:32:31,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31,426][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.12711554765701294, acc: 0.9684210419654846)
[2025-02-13 20:32:31,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31,850][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.17066748440265656, acc: 0.9734513163566589)
[2025-02-13 20:32:31,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32,271][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.12340330332517624, acc: 0.948051929473877)
[2025-02-13 20:32:32,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32,673][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.11559298634529114, acc: 0.9712643623352051)
[2025-02-13 20:32:32,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33,023][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.07037250697612762, acc: 0.9736841917037964)
[2025-02-13 20:32:33,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33,447][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.0616891123354435, acc: 0.9807692170143127)
[2025-02-13 20:32:33,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33,878][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.07121963798999786, acc: 0.9848484992980957)
[2025-02-13 20:32:34,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34,273][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.06735823303461075, acc: 0.988950252532959)
[2025-02-13 20:32:34,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34,655][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.09906790405511856, acc: 0.9828571677207947)
[2025-02-13 20:32:34,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35,073][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.11670701205730438, acc: 0.9767441749572754)
[2025-02-13 20:32:35,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35,461][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.06311998516321182, acc: 0.9878787994384766)
[2025-02-13 20:32:35,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35,852][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.18006755411624908, acc: 0.9683544039726257)
[2025-02-13 20:32:35,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36,289][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.05908718705177307, acc: 0.9936708807945251)
[2025-02-13 20:32:36,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36,692][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.0141775943338871, acc: 1.0)
[2025-02-13 20:32:36,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37,091][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.059143856167793274, acc: 0.9791666865348816)
[2025-02-13 20:32:37,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37,480][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.18818633258342743, acc: 0.9666666388511658)
[2025-02-13 20:32:37,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37,871][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.055053163319826126, acc: 0.9862068891525269)
[2025-02-13 20:32:38,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38,279][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.023135529831051826, acc: 0.9935897588729858)
[2025-02-13 20:32:38,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38,697][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.11466218531131744, acc: 0.9704142212867737)
[2025-02-13 20:32:38,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39,074][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.2190851867198944, acc: 0.9767441749572754)
[2025-02-13 20:32:39,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39,500][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.11188487708568573, acc: 0.9624999761581421)
[2025-02-13 20:32:39,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39,879][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.12774251401424408, acc: 0.9655172228813171)
[2025-02-13 20:32:40,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40,252][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.1060594916343689, acc: 0.9702380895614624)
[2025-02-13 20:32:40,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40,700][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.09945926070213318, acc: 0.9829545617103577)
[2025-02-13 20:32:40,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41,135][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.11687977612018585, acc: 0.9756097793579102)
[2025-02-13 20:32:41,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41,529][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.14423342049121857, acc: 0.9473684430122375)
[2025-02-13 20:32:41,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41,949][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.17330577969551086, acc: 0.9365079402923584)
[2025-02-13 20:32:42,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42,370][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.10248320549726486, acc: 0.9620253443717957)
[2025-02-13 20:32:42,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42,812][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.08356897532939911, acc: 0.981249988079071)
[2025-02-13 20:32:42,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43,236][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.09809724241495132, acc: 0.9692307710647583)
[2025-02-13 20:32:43,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43,687][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.18891428411006927, acc: 0.9513513445854187)
[2025-02-13 20:32:43,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44,076][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.048090267926454544, acc: 0.9927536249160767)
[2025-02-13 20:32:44,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44,502][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.07975300401449203, acc: 0.9811320900917053)
[2025-02-13 20:32:44,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44,915][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.07372012734413147, acc: 0.9887005686759949)
[2025-02-13 20:32:45,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45,360][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.08318775147199631, acc: 0.9833333492279053)
[2025-02-13 20:32:45,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45,756][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.10261817276477814, acc: 0.9709302186965942)
[2025-02-13 20:32:45,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46,181][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.06427577137947083, acc: 0.9930070042610168)
[2025-02-13 20:32:46,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46,621][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.11601301282644272, acc: 0.9545454382896423)
[2025-02-13 20:32:46,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47,046][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.05068805813789368, acc: 0.9811320900917053)
[2025-02-13 20:32:47,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47,490][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.02603510394692421, acc: 1.0)
[2025-02-13 20:32:47,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47,991][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.06219939887523651, acc: 0.988304078578949)
[2025-02-13 20:32:48,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48,417][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.02075807936489582, acc: 1.0)
[2025-02-13 20:32:48,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48,786][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.04380504786968231, acc: 0.9776536226272583)
[2025-02-13 20:32:48,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49,231][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.025507852435112, acc: 1.0)
[2025-02-13 20:32:49,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49,680][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.07626495510339737, acc: 0.9851852059364319)
[2025-02-13 20:32:49,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50,188][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.08664480596780777, acc: 0.9814814925193787)
[2025-02-13 20:32:50,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50,601][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.09108096361160278, acc: 0.9615384340286255)
[2025-02-13 20:32:50,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51,038][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.17618852853775024, acc: 0.9580838084220886)
[2025-02-13 20:32:51,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51,447][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.17441195249557495, acc: 0.9534883499145508)
[2025-02-13 20:32:51,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51,754][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.15291152894496918, acc: 0.970059871673584)
[2025-02-13 20:32:51,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52,088][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.10051894187927246, acc: 0.9767441749572754)
[2025-02-13 20:32:52,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52,559][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.15772555768489838, acc: 0.981249988079071)
[2025-02-13 20:32:52,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52,990][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.033073071390390396, acc: 1.0)
[2025-02-13 20:32:53,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53,382][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.07133902609348297, acc: 0.9878048896789551)
[2025-02-13 20:32:53,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53,766][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.060358110815286636, acc: 0.9777777791023254)
[2025-02-13 20:32:53,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54,163][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.0727536678314209, acc: 0.9941176176071167)
[2025-02-13 20:32:54,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54,656][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.3035795986652374, acc: 0.9529411792755127)
[2025-02-13 20:32:54,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55,053][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.252765029668808, acc: 0.9202454090118408)
[2025-02-13 20:32:55,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55,456][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.23556704819202423, acc: 0.9239130616188049)
[2025-02-13 20:32:55,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55,860][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.16035647690296173, acc: 0.9677419066429138)
[2025-02-13 20:32:56,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56,307][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.10861154645681381, acc: 0.9695122241973877)
[2025-02-13 20:32:56,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56,742][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.12952828407287598, acc: 0.9740259647369385)
[2025-02-13 20:32:56,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57,191][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.13496671617031097, acc: 0.9833333492279053)
[2025-02-13 20:32:57,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57,616][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.09356143325567245, acc: 0.9860464930534363)
[2025-02-13 20:32:57,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58,089][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.14704199135303497, acc: 0.9638554453849792)
[2025-02-13 20:32:58,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58,510][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.19421815872192383, acc: 0.9634703397750854)
[2025-02-13 20:32:58,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59,000][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.08612339198589325, acc: 0.9629629850387573)
[2025-02-13 20:32:59,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59,403][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.23339204490184784, acc: 0.9756097793579102)
[2025-02-13 20:32:59,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59,850][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.023953523486852646, acc: 1.0)
[2025-02-13 20:33:00,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00,284][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.12433281540870667, acc: 0.976190447807312)
[2025-02-13 20:33:00,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00,774][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.1355057656764984, acc: 0.9710982441902161)
[2025-02-13 20:33:00,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01,199][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.14018088579177856, acc: 0.9675675630569458)
[2025-02-13 20:33:01,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01,648][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.12444041669368744, acc: 0.9682539701461792)
[2025-02-13 20:33:01,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02,104][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.030450982972979546, acc: 1.0)
[2025-02-13 20:33:02,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02,488][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.03653986006975174, acc: 0.9887005686759949)
[2025-02-13 20:33:02,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02,897][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.06198530271649361, acc: 0.9853658676147461)
[2025-02-13 20:33:03,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03,267][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.0429227277636528, acc: 0.9950739145278931)
[2025-02-13 20:33:03,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03,674][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.02115771919488907, acc: 0.9939024448394775)
[2025-02-13 20:33:03,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04,112][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.07006165385246277, acc: 0.9857142567634583)
[2025-02-13 20:33:04,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04,514][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.1505613625049591, acc: 0.9726775884628296)
[2025-02-13 20:33:04,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04,903][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.11916819214820862, acc: 0.9745222926139832)
[2025-02-13 20:33:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05,277][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.07737341523170471, acc: 0.9837398529052734)
[2025-02-13 20:33:05,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05,701][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.0912550762295723, acc: 0.9797297120094299)
[2025-02-13 20:33:05,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06,128][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.21164903044700623, acc: 0.9441340565681458)
[2025-02-13 20:33:06,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06,561][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.11585826426744461, acc: 0.9691358208656311)
[2025-02-13 20:33:06,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06,973][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.0730271190404892, acc: 0.9855072498321533)
[2025-02-13 20:33:07,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07,366][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.2311789095401764, acc: 0.9459459185600281)
[2025-02-13 20:33:07,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07,773][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.07914991676807404, acc: 0.977142870426178)
[2025-02-13 20:33:07,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08,199][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.12393572181463242, acc: 0.9661017060279846)
[2025-02-13 20:33:08,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08,592][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.14391528069972992, acc: 0.954023003578186)
[2025-02-13 20:33:08,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08,978][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.09028124809265137, acc: 0.9715909361839294)
[2025-02-13 20:33:09,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09,400][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.1470119059085846, acc: 0.9523809552192688)
[2025-02-13 20:33:09,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09,803][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.1548670083284378, acc: 0.942307710647583)
[2025-02-13 20:33:09,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10,267][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.12519191205501556, acc: 0.9631901979446411)
[2025-02-13 20:33:10,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10,687][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.18701285123825073, acc: 0.9716312289237976)
[2025-02-13 20:33:10,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11,085][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.18524257838726044, acc: 0.9453551769256592)
[2025-02-13 20:33:11,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11,484][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.1234583705663681, acc: 0.9783783555030823)
[2025-02-13 20:33:11,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11,867][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.08988998830318451, acc: 0.9890109896659851)
[2025-02-13 20:33:12,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12,259][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.2609902322292328, acc: 0.9533678889274597)
[2025-02-13 20:33:12,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12,671][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.12439227849245071, acc: 0.9662162065505981)
[2025-02-13 20:33:12,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13,081][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.11054429411888123, acc: 0.9798657894134521)
[2025-02-13 20:33:13,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13,464][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.09987056255340576, acc: 0.9717513918876648)
[2025-02-13 20:33:13,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13,860][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.11252149939537048, acc: 0.9850746393203735)
[2025-02-13 20:33:14,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14,258][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.08182842284440994, acc: 0.9781420826911926)
[2025-02-13 20:33:14,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14,667][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.06442596018314362, acc: 0.9793103337287903)
[2025-02-13 20:33:14,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15,074][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.12286914139986038, acc: 0.9685534834861755)
[2025-02-13 20:33:15,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15,492][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.08654121309518814, acc: 0.9805194735527039)
[2025-02-13 20:33:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15,896][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.1146763265132904, acc: 0.9692307710647583)
[2025-02-13 20:33:16,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:16,316][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.03666473180055618, acc: 0.9935064911842346)
[2025-02-13 20:33:16,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:16,658][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.038908492773771286, acc: 0.9915966391563416)
[2025-02-13 20:33:16,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17,050][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.17574043571949005, acc: 0.9640718698501587)
[2025-02-13 20:33:17,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17,426][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.07875213027000427, acc: 0.9768785834312439)
[2025-02-13 20:33:17,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17,787][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.15110407769680023, acc: 0.9545454382896423)
[2025-02-13 20:33:17,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18,135][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.16894586384296417, acc: 0.9415584206581116)
[2025-02-13 20:33:18,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18,532][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.10209767520427704, acc: 0.9779005646705627)
[2025-02-13 20:33:18,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18,948][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.14863720536231995, acc: 0.9466666579246521)
[2025-02-13 20:33:19,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19,365][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.08163987845182419, acc: 0.9793814420700073)
[2025-02-13 20:33:19,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19,757][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.05044035613536835, acc: 0.9901477694511414)
[2025-02-13 20:33:19,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20,218][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.06539376825094223, acc: 0.9841269850730896)
[2025-02-13 20:33:20,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20,599][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.17691870033740997, acc: 0.9347826242446899)
[2025-02-13 20:33:20,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21,017][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.42532339692115784, acc: 0.921875)
[2025-02-13 20:33:21,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21,416][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.28770241141319275, acc: 0.9425287246704102)
[2025-02-13 20:33:21,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21,819][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.27346181869506836, acc: 0.9561403393745422)
[2025-02-13 20:33:21,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22,277][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.11157352477312088, acc: 0.9743589758872986)
[2025-02-13 20:33:22,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22,674][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.45472243428230286, acc: 0.8888888955116272)
[2025-02-13 20:33:22,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23,078][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.3999311029911041, acc: 0.8914285898208618)
[2025-02-13 20:33:23,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23,490][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.3283105492591858, acc: 0.9285714030265808)
[2025-02-13 20:33:23,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23,935][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.26411202549934387, acc: 0.95333331823349)
[2025-02-13 20:33:24,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24,311][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.04321848973631859, acc: 1.0)
[2025-02-13 20:33:24,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24,715][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.03702050819993019, acc: 1.0)
[2025-02-13 20:33:24,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25,109][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.21227240562438965, acc: 0.9428571462631226)
[2025-02-13 20:33:25,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25,563][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.2370854616165161, acc: 0.9496402740478516)
[2025-02-13 20:33:25,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25,962][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.1930716633796692, acc: 0.9526627063751221)
[2025-02-13 20:33:26,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26,360][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.2797561287879944, acc: 0.925000011920929)
[2025-02-13 20:33:26,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26,755][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.11348263919353485, acc: 0.9741935729980469)
[2025-02-13 20:33:26,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27,130][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.07506610453128815, acc: 0.9882352948188782)
[2025-02-13 20:33:27,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27,532][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.07249134033918381, acc: 0.9846153855323792)
[2025-02-13 20:33:27,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27,955][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.1600915938615799, acc: 0.9605911374092102)
[2025-02-13 20:33:28,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28,405][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.154896542429924, acc: 0.9743589758872986)
[2025-02-13 20:33:28,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28,834][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.5994953513145447, acc: 0.8993710875511169)
[2025-02-13 20:33:28,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29,218][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.1473149210214615, acc: 0.979899525642395)
[2025-02-13 20:33:29,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29,639][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.0773259848356247, acc: 0.9837837815284729)
[2025-02-13 20:33:29,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30,013][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.12245549261569977, acc: 0.9636363387107849)
[2025-02-13 20:33:30,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30,436][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.18188916146755219, acc: 0.9527027010917664)
[2025-02-13 20:33:30,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30,842][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.20851895213127136, acc: 0.9411764740943909)
[2025-02-13 20:33:30,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31,232][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.17005862295627594, acc: 0.9552238583564758)
[2025-02-13 20:33:31,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31,539][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.17701950669288635, acc: 0.9694656729698181)
[2025-02-13 20:33:31,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31,938][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.28097760677337646, acc: 0.9666666388511658)
[2025-02-13 20:33:32,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32,395][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.09534570574760437, acc: 0.9806451797485352)
[2025-02-13 20:33:32,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32,838][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.06879902631044388, acc: 0.9866666793823242)
[2025-02-13 20:33:32,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33,281][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.05708129703998566, acc: 0.9784172773361206)
[2025-02-13 20:33:33,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33,781][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.08573631197214127, acc: 0.976331353187561)
[2025-02-13 20:33:33,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34,228][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.02915634773671627, acc: 0.9937499761581421)
[2025-02-13 20:33:34,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34,653][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.01731731928884983, acc: 1.0)
[2025-02-13 20:33:34,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35,074][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.08494400978088379, acc: 0.9811320900917053)
[2025-02-13 20:33:35,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35,523][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.09155511856079102, acc: 0.9745222926139832)
[2025-02-13 20:33:35,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35,953][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.10649614781141281, acc: 0.9878048896789551)
[2025-02-13 20:33:36,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36,370][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.11012329906225204, acc: 0.9651162624359131)
[2025-02-13 20:33:36,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36,855][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.1413881629705429, acc: 0.9655172228813171)
[2025-02-13 20:33:36,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37,326][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.09743552654981613, acc: 0.9867549538612366)
[2025-02-13 20:33:37,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37,779][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.08302474766969681, acc: 0.9735099077224731)
[2025-02-13 20:33:37,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38,206][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.09604906290769577, acc: 0.9653179049491882)
[2025-02-13 20:33:38,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38,626][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.10559830814599991, acc: 0.9824561476707458)
[2025-02-13 20:33:38,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39,059][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.23152276873588562, acc: 0.951724112033844)
[2025-02-13 20:33:39,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39,476][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.11327621340751648, acc: 0.970588207244873)
[2025-02-13 20:33:39,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39,907][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.16278377175331116, acc: 0.9637681245803833)
[2025-02-13 20:33:40,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40,360][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.25299832224845886, acc: 0.9622641801834106)
[2025-02-13 20:33:40,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40,781][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.08461479842662811, acc: 0.9811320900917053)
[2025-02-13 20:33:40,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41,172][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.2693217992782593, acc: 0.9416666626930237)
[2025-02-13 20:33:41,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41,598][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.05302579700946808, acc: 1.0)
[2025-02-13 20:33:41,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41,976][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.08312197029590607, acc: 0.981249988079071)
[2025-02-13 20:33:42,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42,394][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.3230834901332855, acc: 0.9269663095474243)
[2025-02-13 20:33:42,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42,835][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.21996888518333435, acc: 0.9444444179534912)
[2025-02-13 20:33:42,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43,260][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.18206079304218292, acc: 0.9642857313156128)
[2025-02-13 20:33:43,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43,702][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.3986891806125641, acc: 0.9333333373069763)
[2025-02-13 20:33:43,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44,090][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.14869476854801178, acc: 0.9605262875556946)
[2025-02-13 20:33:44,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44,456][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.08852770924568176, acc: 0.9908257126808167)
[2025-02-13 20:33:44,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44,868][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.07243433594703674, acc: 0.9861111044883728)
[2025-02-13 20:33:45,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45,266][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.0603879950940609, acc: 0.9922480583190918)
[2025-02-13 20:33:45,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45,662][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.06466484814882278, acc: 0.9767441749572754)
[2025-02-13 20:33:45,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46,060][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.15513622760772705, acc: 0.9585798978805542)
[2025-02-13 20:33:46,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46,454][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.0362992025911808, acc: 0.9941860437393188)
[2025-02-13 20:33:46,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46,840][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.09809727966785431, acc: 0.9837398529052734)
[2025-02-13 20:33:46,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47,275][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.06236198917031288, acc: 0.9878048896789551)
[2025-02-13 20:33:47,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47,670][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.15146951377391815, acc: 0.9627329111099243)
[2025-02-13 20:33:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48,114][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.1385725438594818, acc: 0.9585798978805542)
[2025-02-13 20:33:48,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48,491][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.06115014851093292, acc: 0.9870129823684692)
[2025-02-13 20:33:48,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48,926][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.17742331326007843, acc: 0.9615384340286255)
[2025-02-13 20:33:49,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49,326][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.04778274893760681, acc: 0.9919354915618896)
[2025-02-13 20:33:49,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49,713][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.0764160305261612, acc: 0.9850000143051147)
[2025-02-13 20:33:49,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50,134][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.05132303014397621, acc: 0.988950252532959)
[2025-02-13 20:33:50,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50,532][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.0482526496052742, acc: 0.978723406791687)
[2025-02-13 20:33:50,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50,927][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.07736965268850327, acc: 0.9831932783126831)
[2025-02-13 20:33:51,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51,291][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.12100151926279068, acc: 0.9756097793579102)
[2025-02-13 20:33:51,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51,712][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.2430988997220993, acc: 0.9615384340286255)
[2025-02-13 20:33:51,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52,108][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.08371064066886902, acc: 0.9801324605941772)
[2025-02-13 20:33:52,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52,512][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.13894838094711304, acc: 0.9845361113548279)
[2025-02-13 20:33:52,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52,918][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.2423832267522812, acc: 0.9599999785423279)
[2025-02-13 20:33:53,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53,349][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.09815210849046707, acc: 0.9833333492279053)
[2025-02-13 20:33:53,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53,730][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.04834138974547386, acc: 0.9897959232330322)
[2025-02-13 20:33:53,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54,134][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.04403176158666611, acc: 0.9882352948188782)
[2025-02-13 20:33:54,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54,503][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.11118286848068237, acc: 0.9736841917037964)
[2025-02-13 20:33:54,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54,885][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.061091676354408264, acc: 0.9919999837875366)
[2025-02-13 20:33:55,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55,290][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.2608170807361603, acc: 0.9631901979446411)
[2025-02-13 20:33:55,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55,683][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.15195578336715698, acc: 0.9806451797485352)
[2025-02-13 20:33:55,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56,085][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.03922009468078613, acc: 1.0)
[2025-02-13 20:33:56,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56,473][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.09430790692567825, acc: 0.9881656765937805)
[2025-02-13 20:33:56,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56,834][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.14625883102416992, acc: 0.9685534834861755)
[2025-02-13 20:33:56,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57,210][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.15693801641464233, acc: 0.9647887349128723)
[2025-02-13 20:33:57,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57,639][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.13720464706420898, acc: 0.9726027250289917)
[2025-02-13 20:33:57,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58,082][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.1041523665189743, acc: 0.9732142686843872)
[2025-02-13 20:33:58,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58,490][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.04136167839169502, acc: 0.9777777791023254)
[2025-02-13 20:33:58,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58,892][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.020558953285217285, acc: 1.0)
[2025-02-13 20:33:59,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59,295][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.049314238131046295, acc: 0.9919354915618896)
[2025-02-13 20:33:59,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59,671][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.05761973187327385, acc: 0.9937499761581421)
[2025-02-13 20:33:59,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00,069][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.2339557409286499, acc: 0.9420289993286133)
[2025-02-13 20:34:00,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00,499][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.056472424417734146, acc: 0.9833333492279053)
[2025-02-13 20:34:00,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00,890][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.09958598762750626, acc: 0.9850746393203735)
[2025-02-13 20:34:01,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01,292][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.05096178129315376, acc: 0.9876543283462524)
[2025-02-13 20:34:01,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01,736][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.03756855055689812, acc: 0.9937106966972351)
[2025-02-13 20:34:01,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02,107][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.014668946154415607, acc: 1.0)
[2025-02-13 20:34:02,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02,514][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.07963444292545319, acc: 0.9935897588729858)
[2025-02-13 20:34:02,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02,970][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.08516518771648407, acc: 0.9887640476226807)
[2025-02-13 20:34:03,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03,382][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.017829151824116707, acc: 1.0)
[2025-02-13 20:34:03,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03,776][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.04408036172389984, acc: 0.9932432174682617)
[2025-02-13 20:34:03,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04,144][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.08831208199262619, acc: 0.9789473414421082)
[2025-02-13 20:34:04,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04,608][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.10964690893888474, acc: 0.978723406791687)
[2025-02-13 20:34:04,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05,023][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.04916663095355034, acc: 0.987730085849762)
[2025-02-13 20:34:05,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05,415][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.14044898748397827, acc: 0.9605262875556946)
[2025-02-13 20:34:05,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05,799][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.055035367608070374, acc: 0.9781420826911926)
[2025-02-13 20:34:05,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06,213][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.09500739723443985, acc: 0.9940119981765747)
[2025-02-13 20:34:06,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06,598][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.1496509313583374, acc: 0.9602272510528564)
[2025-02-13 20:34:06,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06,971][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.08091752231121063, acc: 0.9736841917037964)
[2025-02-13 20:34:07,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07,410][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.052688706666231155, acc: 0.9897959232330322)
[2025-02-13 20:34:07,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07,834][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.15114468336105347, acc: 0.9587628841400146)
[2025-02-13 20:34:07,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08,200][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.09073597937822342, acc: 0.9757575988769531)
[2025-02-13 20:34:08,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08,633][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.13594815135002136, acc: 0.9518072009086609)
[2025-02-13 20:34:08,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09,089][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.09701129794120789, acc: 0.9801324605941772)
[2025-02-13 20:34:09,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09,504][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.13464608788490295, acc: 0.9860140085220337)
[2025-02-13 20:34:09,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09,960][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.12085747718811035, acc: 0.9746835231781006)
[2025-02-13 20:34:10,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10,411][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.11647935956716537, acc: 0.9772727489471436)
[2025-02-13 20:34:10,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10,824][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.08945394307374954, acc: 0.9793103337287903)
[2025-02-13 20:34:10,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11,245][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.12191451340913773, acc: 0.9588235020637512)
[2025-02-13 20:34:11,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11,657][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.07910361140966415, acc: 0.9918032884597778)
[2025-02-13 20:34:11,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12,050][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.1675819605588913, acc: 0.9717513918876648)
[2025-02-13 20:34:12,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12,383][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.023646220564842224, acc: 1.0)
[2025-02-13 20:34:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12,766][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.051765326410532, acc: 0.989130437374115)
[2025-02-13 20:34:12,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13,209][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.007320694625377655, acc: 1.0)
[2025-02-13 20:34:13,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13,598][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.018818149343132973, acc: 1.0)
[2025-02-13 20:34:13,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13,968][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.06343507021665573, acc: 0.9797979593276978)
[2025-02-13 20:34:14,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14,374][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.027835220098495483, acc: 1.0)
[2025-02-13 20:34:14,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14,747][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.027809087187051773, acc: 0.9916666746139526)
[2025-02-13 20:34:14,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15,125][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.05134342983365059, acc: 0.9946523904800415)
[2025-02-13 20:34:15,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15,499][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.07339915633201599, acc: 0.9826589822769165)
[2025-02-13 20:34:15,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15,911][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.024339113384485245, acc: 0.9926470518112183)
[2025-02-13 20:34:16,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16,316][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.16873517632484436, acc: 0.9622641801834106)
[2025-02-13 20:34:16,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16,666][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.07411430776119232, acc: 0.991525411605835)
[2025-02-13 20:34:16,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17,072][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.026308991014957428, acc: 0.9926470518112183)
[2025-02-13 20:34:17,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17,502][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.04111533612012863, acc: 1.0)
[2025-02-13 20:34:17,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17,871][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.19307732582092285, acc: 0.9677419066429138)
[2025-02-13 20:34:18,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18,233][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.040155068039894104, acc: 0.9948979616165161)
[2025-02-13 20:34:18,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18,630][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.03615045174956322, acc: 0.9801980257034302)
[2025-02-13 20:34:18,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19,084][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.05799464136362076, acc: 0.9878787994384766)
[2025-02-13 20:34:19,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19,488][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.010516139678657055, acc: 1.0)
[2025-02-13 20:34:19,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19,924][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.023435765877366066, acc: 0.9869281053543091)
[2025-02-13 20:34:20,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20,327][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.05228816345334053, acc: 0.9847715497016907)
[2025-02-13 20:34:20,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20,748][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.01732245646417141, acc: 1.0)
[2025-02-13 20:34:20,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21,130][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.07111635059118271, acc: 0.9794871807098389)
[2025-02-13 20:34:21,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21,491][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.053313639014959335, acc: 0.9876543283462524)
[2025-02-13 20:34:21,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21,865][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.20849627256393433, acc: 0.9571428298950195)
[2025-02-13 20:34:22,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22,258][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.1821308732032776, acc: 0.9784172773361206)
[2025-02-13 20:34:22,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22,621][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.1121174618601799, acc: 0.9689922332763672)
[2025-02-13 20:34:22,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23,065][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.09116983413696289, acc: 0.9802631735801697)
[2025-02-13 20:34:23,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23,482][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.04719875380396843, acc: 0.9861111044883728)
[2025-02-13 20:34:23,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23,857][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.0847773477435112, acc: 0.9833333492279053)
[2025-02-13 20:34:23,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24,243][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.18453040719032288, acc: 0.9371069073677063)
[2025-02-13 20:34:24,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24,663][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.055280622094869614, acc: 0.9923076629638672)
[2025-02-13 20:34:24,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25,063][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.12707485258579254, acc: 0.96875)
[2025-02-13 20:34:25,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25,431][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.10654185712337494, acc: 0.9750000238418579)
[2025-02-13 20:34:25,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25,864][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.0718240737915039, acc: 0.976331353187561)
[2025-02-13 20:34:26,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26,261][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.12054388970136642, acc: 0.9753086566925049)
[2025-02-13 20:34:26,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26,653][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.09561267495155334, acc: 0.9794520735740662)
[2025-02-13 20:34:26,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27,101][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.15199707448482513, acc: 0.9553072452545166)
[2025-02-13 20:34:27,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27,514][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.06988320499658585, acc: 0.9837837815284729)
[2025-02-13 20:34:27,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27,958][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.14109820127487183, acc: 0.9651162624359131)
[2025-02-13 20:34:28,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28,403][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.26978620886802673, acc: 0.946107804775238)
[2025-02-13 20:34:28,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28,753][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.06751912087202072, acc: 0.9865771532058716)
[2025-02-13 20:34:28,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29,122][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.24026554822921753, acc: 0.977011501789093)
[2025-02-13 20:34:29,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29,506][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.06702899187803268, acc: 0.988095223903656)
[2025-02-13 20:34:29,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29,912][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.07106221467256546, acc: 0.9814814925193787)
[2025-02-13 20:34:30,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30,305][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.1135401576757431, acc: 0.9691358208656311)
[2025-02-13 20:34:30,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30,714][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.053541723638772964, acc: 1.0)
[2025-02-13 20:34:30,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31,130][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.07655047625303268, acc: 0.988304078578949)
[2025-02-13 20:34:31,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31,504][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.10055405646562576, acc: 0.9822485446929932)
[2025-02-13 20:34:31,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31,912][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.0848466232419014, acc: 0.9868420958518982)
[2025-02-13 20:34:32,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32,318][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.06395375728607178, acc: 0.9844961166381836)
[2025-02-13 20:34:32,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32,719][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.04961322247982025, acc: 0.993630588054657)
[2025-02-13 20:34:32,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33,098][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.14889955520629883, acc: 0.9677419066429138)
[2025-02-13 20:34:33,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33,459][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.07054783403873444, acc: 0.9767441749572754)
[2025-02-13 20:34:33,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33,830][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.06449208408594131, acc: 0.9832402467727661)
[2025-02-13 20:34:33,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34,189][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.1220477893948555, acc: 0.9663865566253662)
[2025-02-13 20:34:34,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34,577][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.1011224016547203, acc: 0.9795918464660645)
[2025-02-13 20:34:34,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35,004][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.051281511783599854, acc: 0.9891892075538635)
[2025-02-13 20:34:35,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35,363][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.06148834526538849, acc: 0.9868420958518982)
[2025-02-13 20:34:35,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35,747][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.013510570861399174, acc: 1.0)
[2025-02-13 20:34:35,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36,135][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.061871957033872604, acc: 0.9836065769195557)
[2025-02-13 20:34:36,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36,539][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.12312806397676468, acc: 0.9756097793579102)
[2025-02-13 20:34:36,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37,007][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.08224636316299438, acc: 0.9813664555549622)
[2025-02-13 20:34:37,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37,360][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.08932787925004959, acc: 0.9907407164573669)
[2025-02-13 20:34:37,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37,766][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.07733442634344101, acc: 0.9846153855323792)
[2025-02-13 20:34:37,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38,146][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.061234522610902786, acc: 0.9870129823684692)
[2025-02-13 20:34:38,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38,530][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.0433596707880497, acc: 0.9935897588729858)
[2025-02-13 20:34:38,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38,927][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.024981984868645668, acc: 1.0)
[2025-02-13 20:34:39,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39,305][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.14870312809944153, acc: 0.9798657894134521)
[2025-02-13 20:34:39,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39,726][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.04371105134487152, acc: 0.9935483932495117)
[2025-02-13 20:34:39,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40,123][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.041504546999931335, acc: 0.9933333396911621)
[2025-02-13 20:34:40,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40,491][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.046520888805389404, acc: 0.9803921580314636)
[2025-02-13 20:34:40,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40,866][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.024857111275196075, acc: 1.0)
[2025-02-13 20:34:41,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41,238][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.03882940858602524, acc: 0.9903846383094788)
[2025-02-13 20:34:41,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41,619][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.066330686211586, acc: 0.9738219976425171)
[2025-02-13 20:34:41,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41,987][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.051616039127111435, acc: 0.9890109896659851)
[2025-02-13 20:34:42,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42,355][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.01584254764020443, acc: 1.0)
[2025-02-13 20:34:42,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42,771][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.057609379291534424, acc: 0.9811320900917053)
[2025-02-13 20:34:42,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43,167][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.08945067226886749, acc: 0.9693877696990967)
[2025-02-13 20:34:43,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43,546][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.0298018641769886, acc: 0.9935064911842346)
[2025-02-13 20:34:43,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43,977][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.4169425964355469, acc: 0.9137930870056152)
[2025-02-13 20:34:44,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:44,399][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.04861236363649368, acc: 0.9941176176071167)
[2025-02-13 20:34:44,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:44,801][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.01845710165798664, acc: 1.0)
[2025-02-13 20:34:44,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45,148][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.1168968677520752, acc: 0.9832402467727661)
[2025-02-13 20:34:45,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45,512][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.0783352479338646, acc: 0.9753694534301758)
[2025-02-13 20:34:45,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45,867][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.03186074271798134, acc: 0.9946236610412598)
[2025-02-13 20:34:46,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46,306][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.06673336774110794, acc: 0.9817073345184326)
[2025-02-13 20:34:46,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46,694][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.03948735073208809, acc: 0.9839572310447693)
[2025-02-13 20:34:46,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47,110][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.041344206780195236, acc: 0.9878048896789551)
[2025-02-13 20:34:47,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47,501][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.045736733824014664, acc: 0.9846153855323792)
[2025-02-13 20:34:47,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47,938][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.07960515469312668, acc: 0.9781022071838379)
[2025-02-13 20:34:48,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48,357][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.026378637179732323, acc: 0.9882352948188782)
[2025-02-13 20:34:48,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48,778][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.04607006907463074, acc: 0.9934640526771545)
[2025-02-13 20:34:48,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49,237][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.022005293518304825, acc: 0.9924812316894531)
[2025-02-13 20:34:49,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49,674][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.0227265115827322, acc: 0.9931972622871399)
[2025-02-13 20:34:49,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50,052][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.00827573798596859, acc: 1.0)
[2025-02-13 20:34:50,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50,498][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.01689182221889496, acc: 0.9932885766029358)
[2025-02-13 20:34:50,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50,965][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.09525034576654434, acc: 0.9870967864990234)
[2025-02-13 20:34:51,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51,383][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.021518321707844734, acc: 1.0)
[2025-02-13 20:34:51,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51,760][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.05738382041454315, acc: 0.9751243591308594)
[2025-02-13 20:34:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52,199][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.061669956892728806, acc: 0.9860140085220337)
[2025-02-13 20:34:52,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52,660][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.0632072314620018, acc: 0.9870967864990234)
[2025-02-13 20:34:52,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53,079][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.007410313002765179, acc: 1.0)
[2025-02-13 20:34:53,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53,480][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.07078023999929428, acc: 0.989130437374115)
[2025-02-13 20:34:53,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53,911][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.03775271400809288, acc: 1.0)
[2025-02-13 20:34:54,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54,341][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.016797015443444252, acc: 1.0)
[2025-02-13 20:34:54,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54,796][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.026498522609472275, acc: 0.994350254535675)
[2025-02-13 20:34:54,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55,273][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.05284687876701355, acc: 0.9937888383865356)
[2025-02-13 20:34:55,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55,712][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.049661699682474136, acc: 0.9927007555961609)
[2025-02-13 20:34:55,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56,116][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.05810566246509552, acc: 0.9813664555549622)
[2025-02-13 20:34:56,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56,494][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.01730295456945896, acc: 1.0)
[2025-02-13 20:34:56,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56,911][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.028495920822024345, acc: 1.0)
[2025-02-13 20:34:57,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57,265][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.03640475496649742, acc: 1.0)
[2025-02-13 20:34:57,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57,662][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.06151970475912094, acc: 0.9798657894134521)
[2025-02-13 20:34:57,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58,036][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.03460916876792908, acc: 0.9870967864990234)
[2025-02-13 20:34:58,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58,474][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.023842236027121544, acc: 0.9947368502616882)
[2025-02-13 20:34:58,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58,876][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.04142334312200546, acc: 0.9934210777282715)
[2025-02-13 20:34:59,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59,287][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.05613049119710922, acc: 0.9932432174682617)
[2025-02-13 20:34:59,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59,703][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.051078181713819504, acc: 0.9815950989723206)
[2025-02-13 20:34:59,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00,097][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.034819018095731735, acc: 0.9882352948188782)
[2025-02-13 20:35:00,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00,482][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.10171714425086975, acc: 0.9685863852500916)
[2025-02-13 20:35:00,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00,937][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.03359553962945938, acc: 0.9882352948188782)
[2025-02-13 20:35:01,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01,353][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.06241822615265846, acc: 0.9823529124259949)
[2025-02-13 20:35:01,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01,761][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.01723424345254898, acc: 1.0)
[2025-02-13 20:35:01,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02,167][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.06141982227563858, acc: 0.9860140085220337)
[2025-02-13 20:35:02,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02,588][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.0195015836507082, acc: 0.9938271641731262)
[2025-02-13 20:35:02,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03,011][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.09387234598398209, acc: 0.9785714149475098)
[2025-02-13 20:35:03,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03,413][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.09470496326684952, acc: 0.9922480583190918)
[2025-02-13 20:35:03,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03,802][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.09174725413322449, acc: 0.976047933101654)
[2025-02-13 20:35:03,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04,254][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.03869489207863808, acc: 0.9936708807945251)
[2025-02-13 20:35:04,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04,643][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.08502165973186493, acc: 0.987500011920929)
[2025-02-13 20:35:04,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05,062][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.1590786725282669, acc: 0.9663865566253662)
[2025-02-13 20:35:05,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05,455][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.04419642686843872, acc: 0.9924242496490479)
[2025-02-13 20:35:05,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05,855][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.10177605599164963, acc: 0.984375)
[2025-02-13 20:35:06,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06,253][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.036734193563461304, acc: 1.0)
[2025-02-13 20:35:06,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06,648][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.034416329115629196, acc: 1.0)
[2025-02-13 20:35:06,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07,064][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.04189208149909973, acc: 0.9876543283462524)
[2025-02-13 20:35:07,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07,459][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.022404689341783524, acc: 1.0)
[2025-02-13 20:35:07,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07,889][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.056794531643390656, acc: 0.9887640476226807)
[2025-02-13 20:35:08,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08,365][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.04402619227766991, acc: 0.9797297120094299)
[2025-02-13 20:35:08,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08,749][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.035629209131002426, acc: 1.0)
[2025-02-13 20:35:08,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09,133][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.04772505536675453, acc: 0.9795918464660645)
[2025-02-13 20:35:09,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09,566][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.02926739491522312, acc: 1.0)
[2025-02-13 20:35:09,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10,012][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.13321492075920105, acc: 0.9882352948188782)
[2025-02-13 20:35:10,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10,484][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.09238685667514801, acc: 0.978723406791687)
[2025-02-13 20:35:10,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10,901][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.0723898634314537, acc: 0.9685534834861755)
[2025-02-13 20:35:11,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11,284][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.10202236473560333, acc: 0.9939758777618408)
[2025-02-13 20:35:11,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11,714][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.06782156229019165, acc: 0.9779411554336548)
[2025-02-13 20:35:11,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12,133][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.02833685837686062, acc: 1.0)
[2025-02-13 20:35:12,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12,564][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.038304705172777176, acc: 0.9939024448394775)
[2025-02-13 20:35:12,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13,015][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.03339505195617676, acc: 0.9870967864990234)
[2025-02-13 20:35:13,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13,444][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.0832415223121643, acc: 0.9784172773361206)
[2025-02-13 20:35:13,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13,866][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.016948629170656204, acc: 1.0)
[2025-02-13 20:35:14,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14,270][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.10025998950004578, acc: 0.9560439586639404)
[2025-02-13 20:35:14,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14,680][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.05800120159983635, acc: 0.9922480583190918)
[2025-02-13 20:35:14,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15,104][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.18162578344345093, acc: 0.9503546357154846)
[2025-02-13 20:35:15,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15,505][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.12324874848127365, acc: 0.9793103337287903)
[2025-02-13 20:35:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15,866][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.03703518584370613, acc: 0.9931034445762634)
[2025-02-13 20:35:16,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16,283][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.08684747666120529, acc: 0.9735099077224731)
[2025-02-13 20:35:16,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16,731][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.10778941214084625, acc: 0.9624999761581421)
[2025-02-13 20:35:16,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17,199][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.06137428805232048, acc: 0.9924242496490479)
[2025-02-13 20:35:17,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17,591][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.06449099630117416, acc: 0.9848484992980957)
[2025-02-13 20:35:17,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17,979][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.20087993144989014, acc: 0.9655172228813171)
[2025-02-13 20:35:18,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18,334][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.09115822613239288, acc: 0.9789473414421082)
[2025-02-13 20:35:18,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18,764][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.18958701193332672, acc: 0.9805825352668762)
[2025-02-13 20:35:18,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19,191][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.05030657723546028, acc: 0.9942528605461121)
[2025-02-13 20:35:19,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19,557][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.29288846254348755, acc: 0.9440559148788452)
[2025-02-13 20:35:19,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19,967][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.1369961053133011, acc: 0.9714285731315613)
[2025-02-13 20:35:20,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20,372][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.07078589498996735, acc: 0.9785714149475098)
[2025-02-13 20:35:20,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20,785][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.2266278862953186, acc: 0.954954981803894)
[2025-02-13 20:35:20,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21,247][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.10950552672147751, acc: 0.9784172773361206)
[2025-02-13 20:35:21,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21,671][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.08759310841560364, acc: 0.9811320900917053)
[2025-02-13 20:35:21,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22,039][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.04698202759027481, acc: 0.9864864945411682)
[2025-02-13 20:35:22,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22,473][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.07370390743017197, acc: 0.9860140085220337)
[2025-02-13 20:35:22,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22,879][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.021304523572325706, acc: 1.0)
[2025-02-13 20:35:23,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23,322][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.025361262261867523, acc: 0.9913793206214905)
[2025-02-13 20:35:23,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23,724][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.032316263765096664, acc: 0.9882352948188782)
[2025-02-13 20:35:23,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24,164][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.03446061909198761, acc: 1.0)
[2025-02-13 20:35:24,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24,591][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.06403444707393646, acc: 0.9849624037742615)
[2025-02-13 20:35:24,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24,960][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.09457670152187347, acc: 0.982758641242981)
[2025-02-13 20:35:25,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25,314][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.011220894753932953, acc: 1.0)
[2025-02-13 20:35:25,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25,726][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.04717228189110756, acc: 0.9867549538612366)
[2025-02-13 20:35:25,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26,069][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.03716525062918663, acc: 0.9814814925193787)
[2025-02-13 20:35:26,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26,431][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.050885383039712906, acc: 0.9916666746139526)
[2025-02-13 20:35:26,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26,817][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.07126209884881973, acc: 0.9770992398262024)
[2025-02-13 20:35:26,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27,243][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.06080494076013565, acc: 0.9866666793823242)
[2025-02-13 20:35:27,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27,660][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.032825760543346405, acc: 0.9921259880065918)
[2025-02-13 20:35:27,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28,085][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.050584472715854645, acc: 0.9896907210350037)
[2025-02-13 20:35:28,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28,526][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.1453009843826294, acc: 0.9795918464660645)
[2025-02-13 20:35:28,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28,922][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.04387336224317551, acc: 0.991150438785553)
[2025-02-13 20:35:29,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29,320][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.03901698440313339, acc: 0.9925925731658936)
[2025-02-13 20:35:29,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29,672][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.018375176936388016, acc: 1.0)
[2025-02-13 20:35:29,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30,120][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.02116512320935726, acc: 1.0)
[2025-02-13 20:35:30,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30,514][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.0261828675866127, acc: 1.0)
[2025-02-13 20:35:30,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30,860][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.015426483936607838, acc: 1.0)
[2025-02-13 20:35:31,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31,268][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.15541695058345795, acc: 0.9833333492279053)
[2025-02-13 20:35:31,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31,671][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.28157973289489746, acc: 0.9484536051750183)
[2025-02-13 20:35:31,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32,047][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.06165481358766556, acc: 0.9914529919624329)
[2025-02-13 20:35:32,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32,427][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.10150283575057983, acc: 0.9724137783050537)
[2025-02-13 20:35:32,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32,818][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.04636124521493912, acc: 0.987500011920929)
[2025-02-13 20:35:32,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33,225][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.019488831982016563, acc: 1.0)
[2025-02-13 20:35:33,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33,618][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.14533188939094543, acc: 0.9807692170143127)
[2025-02-13 20:35:33,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33,988][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.12770876288414001, acc: 0.9693251252174377)
[2025-02-13 20:35:34,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34,334][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.09209504723548889, acc: 0.9923664331436157)
[2025-02-13 20:35:34,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34,737][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.07127941399812698, acc: 0.9772727489471436)
[2025-02-13 20:35:34,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35,157][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.05898864567279816, acc: 1.0)
[2025-02-13 20:35:35,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35,541][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.11267893761396408, acc: 0.9604519605636597)
[2025-02-13 20:35:35,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35,985][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.17984503507614136, acc: 0.9417989253997803)
[2025-02-13 20:35:36,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36,404][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.0967954695224762, acc: 0.9904761910438538)
[2025-02-13 20:35:36,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36,797][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.05549188703298569, acc: 0.9927536249160767)
[2025-02-13 20:35:36,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37,187][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.03339982405304909, acc: 1.0)
[2025-02-13 20:35:37,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37,591][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.041959505528211594, acc: 0.9939758777618408)
[2025-02-13 20:35:37,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37,990][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.023218996822834015, acc: 1.0)
[2025-02-13 20:35:38,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38,352][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.022870056331157684, acc: 1.0)
[2025-02-13 20:35:38,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38,710][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.0526178777217865, acc: 0.9844961166381836)
[2025-02-13 20:35:38,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39,081][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.0898820161819458, acc: 0.9788732528686523)
[2025-02-13 20:35:39,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39,473][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.07613468915224075, acc: 0.9769230484962463)
[2025-02-13 20:35:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39,858][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.04622873291373253, acc: 0.9880239367485046)
[2025-02-13 20:35:39,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40,235][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.2164004147052765, acc: 0.9637681245803833)
[2025-02-13 20:35:40,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40,629][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.08252393454313278, acc: 0.9929577708244324)
[2025-02-13 20:35:40,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41,058][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.029222751036286354, acc: 1.0)
[2025-02-13 20:35:41,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41,517][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.02098437212407589, acc: 1.0)
[2025-02-13 20:35:41,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41,945][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.005928860977292061, acc: 1.0)
[2025-02-13 20:35:42,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42,349][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.16106194257736206, acc: 0.9615384340286255)
[2025-02-13 20:35:42,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42,796][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.05998748168349266, acc: 0.9882352948188782)
[2025-02-13 20:35:42,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43,290][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.2715691030025482, acc: 0.9217391014099121)
[2025-02-13 20:35:43,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43,717][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.0744771808385849, acc: 0.9858155846595764)
[2025-02-13 20:35:43,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44,159][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.04701279476284981, acc: 0.9916666746139526)
[2025-02-13 20:35:44,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44,593][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.01922863908112049, acc: 1.0)
[2025-02-13 20:35:44,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45,039][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.11881524324417114, acc: 0.9708737730979919)
[2025-02-13 20:35:45,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45,403][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.08134450018405914, acc: 0.982300877571106)
[2025-02-13 20:35:45,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45,808][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.034633100032806396, acc: 1.0)
[2025-02-13 20:35:45,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46,213][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.08611569553613663, acc: 0.9837837815284729)
[2025-02-13 20:35:46,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46,585][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.10737085342407227, acc: 0.9596773982048035)
[2025-02-13 20:35:46,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46,984][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.07495971769094467, acc: 0.9927007555961609)
[2025-02-13 20:35:47,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47,408][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.10341281443834305, acc: 0.9857142567634583)
[2025-02-13 20:35:47,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47,818][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.11611969023942947, acc: 0.9617834687232971)
[2025-02-13 20:35:47,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48,202][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.1690095216035843, acc: 0.981249988079071)
[2025-02-13 20:35:48,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48,591][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.13212645053863525, acc: 0.9591836929321289)
[2025-02-13 20:35:48,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48,958][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.10491471737623215, acc: 0.9679487347602844)
[2025-02-13 20:35:49,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49,367][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.17847003042697906, acc: 0.9583333134651184)
[2025-02-13 20:35:49,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49,768][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.1044631376862526, acc: 0.9777777791023254)
[2025-02-13 20:35:49,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50,190][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.15491686761379242, acc: 0.9557521939277649)
[2025-02-13 20:35:50,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50,595][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.10849412530660629, acc: 0.9798657894134521)
[2025-02-13 20:35:50,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50,992][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.24329806864261627, acc: 0.9617834687232971)
[2025-02-13 20:35:51,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51,453][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.09643730521202087, acc: 0.9817073345184326)
[2025-02-13 20:35:51,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51,881][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.03321923688054085, acc: 0.9940119981765747)
[2025-02-13 20:35:52,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52,271][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.09724171459674835, acc: 0.9800000190734863)
[2025-02-13 20:35:52,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52,603][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.11868011951446533, acc: 0.9523809552192688)
[2025-02-13 20:35:52,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53,034][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.07869322597980499, acc: 0.9851852059364319)
[2025-02-13 20:35:53,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53,432][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.09209677577018738, acc: 0.9720279574394226)
[2025-02-13 20:35:53,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53,831][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.09779425710439682, acc: 0.9714285731315613)
[2025-02-13 20:35:53,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54,219][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.1793607622385025, acc: 0.955974817276001)
[2025-02-13 20:35:54,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54,668][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.08178526908159256, acc: 0.9862068891525269)
[2025-02-13 20:35:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55,105][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.028369838371872902, acc: 0.9918032884597778)
[2025-02-13 20:35:55,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55,472][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.14659906923770905, acc: 0.9436619877815247)
[2025-02-13 20:35:55,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55,858][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.04260418564081192, acc: 0.9932432174682617)
[2025-02-13 20:35:55,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56,260][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.019164374098181725, acc: 1.0)
[2025-02-13 20:35:56,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56,669][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.06930682063102722, acc: 0.9764705896377563)
[2025-02-13 20:35:56,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57,100][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.13683846592903137, acc: 0.9780219793319702)
[2025-02-13 20:35:57,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57,478][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.09102801978588104, acc: 0.9694656729698181)
[2025-02-13 20:35:57,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57,868][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.11357609927654266, acc: 0.9844961166381836)
[2025-02-13 20:35:58,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58,263][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.1134377270936966, acc: 0.9652174115180969)
[2025-02-13 20:35:58,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58,669][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.0990467369556427, acc: 0.9838709831237793)
[2025-02-13 20:35:58,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59,086][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.07360313832759857, acc: 0.9779411554336548)
[2025-02-13 20:35:59,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59,510][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.19190727174282074, acc: 0.949999988079071)
[2025-02-13 20:35:59,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59,928][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.056636638939380646, acc: 0.9777777791023254)
[2025-02-13 20:36:00,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00,312][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.0367681086063385, acc: 0.9950980544090271)
[2025-02-13 20:36:00,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00,717][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.03727465122938156, acc: 0.9942196607589722)
[2025-02-13 20:36:00,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01,137][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.05535271018743515, acc: 0.9862068891525269)
[2025-02-13 20:36:01,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01,546][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.10866469889879227, acc: 0.976331353187561)
[2025-02-13 20:36:01,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01,976][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.041632700711488724, acc: 0.9950494766235352)
[2025-02-13 20:36:02,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02,365][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.05665209889411926, acc: 0.9845361113548279)
[2025-02-13 20:36:02,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02,772][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.065436951816082, acc: 0.988950252532959)
[2025-02-13 20:36:02,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03,191][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.0646764412522316, acc: 0.9783783555030823)
[2025-02-13 20:36:03,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03,580][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.04021928831934929, acc: 0.9881656765937805)
[2025-02-13 20:36:03,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03,983][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.07357756793498993, acc: 0.9897959232330322)
[2025-02-13 20:36:04,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04,398][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.02694648876786232, acc: 0.9951456189155579)
[2025-02-13 20:36:04,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04,837][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.054141003638505936, acc: 0.9835164546966553)
[2025-02-13 20:36:04,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05,194][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.07031433284282684, acc: 0.97826087474823)
[2025-02-13 20:36:05,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05,560][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.07607962936162949, acc: 0.9829545617103577)
[2025-02-13 20:36:05,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05,928][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.04409729316830635, acc: 0.9923076629638672)
[2025-02-13 20:36:06,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06,348][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.111809641122818, acc: 0.987261176109314)
[2025-02-13 20:36:06,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06,792][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.0718550756573677, acc: 0.9734042286872864)
[2025-02-13 20:36:06,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07,230][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.04986327141523361, acc: 0.9946523904800415)
[2025-02-13 20:36:07,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07,675][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.16572259366512299, acc: 0.9693877696990967)
[2025-02-13 20:36:07,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08,100][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.05552525818347931, acc: 0.9894179701805115)
[2025-02-13 20:36:08,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08,488][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.08123111724853516, acc: 0.9852941036224365)
[2025-02-13 20:36:08,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08,901][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.03396298363804817, acc: 0.9895287752151489)
[2025-02-13 20:36:09,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09,293][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.06361225992441177, acc: 0.9817073345184326)
[2025-02-13 20:36:09,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09,803][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.03594252094626427, acc: 1.0)
[2025-02-13 20:36:09,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10,277][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.0687413290143013, acc: 0.9736841917037964)
[2025-02-13 20:36:10,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10,666][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.08469358831644058, acc: 0.9801324605941772)
[2025-02-13 20:36:10,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11,056][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.1569576859474182, acc: 0.9520547986030579)
[2025-02-13 20:36:11,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11,434][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.392241895198822, acc: 0.9455782175064087)
[2025-02-13 20:36:11,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11,780][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.20166127383708954, acc: 0.9432623982429504)
[2025-02-13 20:36:11,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12,223][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.2589646279811859, acc: 0.9547738432884216)
[2025-02-13 20:36:12,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12,591][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.13991166651248932, acc: 0.9560439586639404)
[2025-02-13 20:36:12,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12,963][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.09100112318992615, acc: 0.977011501789093)
[2025-02-13 20:36:13,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13,347][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.13874909281730652, acc: 0.9621621370315552)
[2025-02-13 20:36:13,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13,724][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.10379140824079514, acc: 0.9801324605941772)
[2025-02-13 20:36:13,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14,113][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.07802940905094147, acc: 0.9735099077224731)
[2025-02-13 20:36:14,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14,501][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.08127812296152115, acc: 0.9894179701805115)
[2025-02-13 20:36:14,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14,942][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.04223182052373886, acc: 0.9947090148925781)
[2025-02-13 20:36:15,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15,367][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.040121082216501236, acc: 0.988950252532959)
[2025-02-13 20:36:15,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15,762][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.10585467517375946, acc: 0.9879518151283264)
[2025-02-13 20:36:15,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16,174][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.05125322937965393, acc: 0.9942528605461121)
[2025-02-13 20:36:16,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16,559][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.08901354670524597, acc: 0.9682539701461792)
[2025-02-13 20:36:16,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16,957][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.048038724809885025, acc: 0.9878787994384766)
[2025-02-13 20:36:17,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17,371][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.21657703816890717, acc: 0.9360465407371521)
[2025-02-13 20:36:17,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17,777][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.061386995017528534, acc: 0.9900990128517151)
[2025-02-13 20:36:17,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18,167][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.09999193251132965, acc: 0.9802955389022827)
[2025-02-13 20:36:18,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18,590][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.04379839822649956, acc: 0.9803921580314636)
[2025-02-13 20:36:18,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19,005][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.07364106923341751, acc: 0.9724137783050537)
[2025-02-13 20:36:19,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19,442][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.11314040422439575, acc: 0.982758641242981)
[2025-02-13 20:36:19,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19,831][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.03747434541583061, acc: 0.9913793206214905)
[2025-02-13 20:36:19,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20,234][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.08705325424671173, acc: 0.9844961166381836)
[2025-02-13 20:36:20,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20,621][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.025822635740041733, acc: 0.9935064911842346)
[2025-02-13 20:36:20,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21,015][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.16137713193893433, acc: 0.9679487347602844)
[2025-02-13 20:36:21,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21,396][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.0738728940486908, acc: 0.9765625)
[2025-02-13 20:36:21,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21,779][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.09523151814937592, acc: 0.9695122241973877)
[2025-02-13 20:36:21,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22,168][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.03197784349322319, acc: 0.9927536249160767)
[2025-02-13 20:36:22,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22,533][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.1628001630306244, acc: 0.9826589822769165)
[2025-02-13 20:36:22,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22,921][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.05386393889784813, acc: 0.9892473220825195)
[2025-02-13 20:36:23,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23,304][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.06600907444953918, acc: 0.9855072498321533)
[2025-02-13 20:36:23,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23,689][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.10508331656455994, acc: 0.9734042286872864)
[2025-02-13 20:36:23,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24,049][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.055865656584501266, acc: 0.9882352948188782)
[2025-02-13 20:36:24,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24,432][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.14582839608192444, acc: 0.9666666388511658)
[2025-02-13 20:36:24,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24,786][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.07769155502319336, acc: 0.9836065769195557)
[2025-02-13 20:36:24,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25,190][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.02478560246527195, acc: 0.9950248599052429)
[2025-02-13 20:36:25,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25,595][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.04991070553660393, acc: 0.9850746393203735)
[2025-02-13 20:36:25,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26,038][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.03714706376194954, acc: 0.9894179701805115)
[2025-02-13 20:36:26,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26,452][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.05429068207740784, acc: 0.9895833134651184)
[2025-02-13 20:36:26,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26,890][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.10681267827749252, acc: 0.9562841653823853)
[2025-02-13 20:36:27,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27,294][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.04970614239573479, acc: 0.9807692170143127)
[2025-02-13 20:36:27,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27,689][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.1462222784757614, acc: 0.9750000238418579)
[2025-02-13 20:36:27,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28,143][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.08454028517007828, acc: 0.9773755669593811)
[2025-02-13 20:36:28,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28,557][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.1023634523153305, acc: 0.976047933101654)
[2025-02-13 20:36:28,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28,919][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.11189081519842148, acc: 0.977142870426178)
[2025-02-13 20:36:29,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29,293][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.053032755851745605, acc: 0.9941176176071167)
[2025-02-13 20:36:29,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29,665][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.08041847497224808, acc: 0.9803921580314636)
[2025-02-13 20:36:29,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30,044][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.03840131685137749, acc: 0.9900000095367432)
[2025-02-13 20:36:30,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30,419][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.1267198920249939, acc: 0.9781022071838379)
[2025-02-13 20:36:30,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30,797][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.05668478086590767, acc: 0.9814814925193787)
[2025-02-13 20:36:30,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31,239][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.07099125534296036, acc: 0.9829545617103577)
[2025-02-13 20:36:31,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31,614][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.1707373708486557, acc: 0.9551281929016113)
[2025-02-13 20:36:31,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32,040][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.17319533228874207, acc: 0.9545454382896423)
[2025-02-13 20:36:32,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32,416][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.09896277636289597, acc: 0.9722222089767456)
[2025-02-13 20:36:32,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32,809][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.15492655336856842, acc: 0.9569892287254333)
[2025-02-13 20:36:32,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33,176][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.0814938172698021, acc: 0.977142870426178)
[2025-02-13 20:36:33,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33,536][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.12528099119663239, acc: 0.9731183052062988)
[2025-02-13 20:36:33,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33,907][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.01638641208410263, acc: 1.0)
[2025-02-13 20:36:34,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34,321][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.1078399047255516, acc: 0.9636363387107849)
[2025-02-13 20:36:34,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34,723][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.08739326894283295, acc: 0.9629629850387573)
[2025-02-13 20:36:34,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35,095][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.10566616803407669, acc: 0.9558011293411255)
[2025-02-13 20:36:35,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35,464][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.1582190841436386, acc: 0.9651162624359131)
[2025-02-13 20:36:35,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35,923][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.12077516317367554, acc: 0.9695122241973877)
[2025-02-13 20:36:36,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36,297][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.18149137496948242, acc: 0.9585492014884949)
[2025-02-13 20:36:36,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36,727][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.1011311337351799, acc: 0.9756097793579102)
[2025-02-13 20:36:36,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37,148][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.24738793075084686, acc: 0.9512194991111755)
[2025-02-13 20:36:37,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37,531][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.10544083267450333, acc: 0.9815950989723206)
[2025-02-13 20:36:37,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37,894][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.10314877331256866, acc: 0.9768785834312439)
[2025-02-13 20:36:38,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38,295][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.07438728958368301, acc: 0.9820359349250793)
[2025-02-13 20:36:38,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38,688][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.09135109186172485, acc: 0.9868420958518982)
[2025-02-13 20:36:38,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39,159][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.16768001019954681, acc: 0.9733333587646484)
[2025-02-13 20:36:39,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39,561][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.3279137909412384, acc: 0.9266666769981384)
[2025-02-13 20:36:39,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39,963][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.09382091462612152, acc: 0.9702380895614624)
[2025-02-13 20:36:40,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40,342][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.2240820825099945, acc: 0.9503546357154846)
[2025-02-13 20:36:40,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40,676][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.22979894280433655, acc: 0.932584285736084)
[2025-02-13 20:36:40,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41,109][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.29438814520835876, acc: 0.9448819160461426)
[2025-02-13 20:36:41,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41,488][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.16789452731609344, acc: 0.9558823704719543)
[2025-02-13 20:36:41,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41,864][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.04468340426683426, acc: 0.9903846383094788)
[2025-02-13 20:36:42,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42,279][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.11469720304012299, acc: 0.9603960514068604)
[2025-02-13 20:36:42,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42,687][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.10245534777641296, acc: 0.9763779640197754)
[2025-02-13 20:36:42,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43,103][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.030759407207369804, acc: 1.0)
[2025-02-13 20:36:43,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43,527][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.06813498586416245, acc: 0.97826087474823)
[2025-02-13 20:36:43,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43,964][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.1265510767698288, acc: 0.9801980257034302)
[2025-02-13 20:36:44,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44,344][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.05979357287287712, acc: 0.9818181991577148)
[2025-02-13 20:36:44,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44,737][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.1054551899433136, acc: 0.9629629850387573)
[2025-02-13 20:36:44,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45,137][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.15084987878799438, acc: 0.9797979593276978)
[2025-02-13 20:36:45,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45,544][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.022681983187794685, acc: 0.9916666746139526)
[2025-02-13 20:36:45,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45,919][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.12106288224458694, acc: 0.9808917045593262)
[2025-02-13 20:36:46,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46,336][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.08102870732545853, acc: 0.9814814925193787)
[2025-02-13 20:36:46,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46,771][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.18972696363925934, acc: 0.9599999785423279)
[2025-02-13 20:36:46,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47,183][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.06314880400896072, acc: 0.9841269850730896)
[2025-02-13 20:36:47,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47,554][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.11699692159891129, acc: 0.9590163826942444)
[2025-02-13 20:36:47,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47,967][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.16678304970264435, acc: 0.9593023061752319)
[2025-02-13 20:36:48,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48,351][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.06541021168231964, acc: 0.976190447807312)
[2025-02-13 20:36:48,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48,754][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.13373255729675293, acc: 0.966183602809906)
[2025-02-13 20:36:48,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49,141][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.08906573057174683, acc: 0.9895833134651184)
[2025-02-13 20:36:49,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49,525][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.054336417466402054, acc: 0.9945054650306702)
[2025-02-13 20:36:49,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49,918][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.11725948750972748, acc: 0.9836065769195557)
[2025-02-13 20:36:50,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50,336][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.06533212214708328, acc: 0.9759036302566528)
[2025-02-13 20:36:50,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50,716][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.11533807218074799, acc: 0.9824561476707458)
[2025-02-13 20:36:50,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51,090][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.2947423458099365, acc: 0.9444444179534912)
[2025-02-13 20:36:51,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51,488][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.06474229693412781, acc: 0.9712643623352051)
[2025-02-13 20:36:51,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51,876][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.08952518552541733, acc: 0.969924807548523)
[2025-02-13 20:36:52,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52,290][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.08465082198381424, acc: 0.976331353187561)
[2025-02-13 20:36:52,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52,710][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.07621341198682785, acc: 0.9682539701461792)
[2025-02-13 20:36:52,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53,178][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.06817306578159332, acc: 0.9870129823684692)
[2025-02-13 20:36:53,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53,678][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.13815267384052277, acc: 0.9753086566925049)
[2025-02-13 20:36:53,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54,107][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.17101164162158966, acc: 0.9663865566253662)
[2025-02-13 20:36:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54,566][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.12139464914798737, acc: 0.9851484894752502)
[2025-02-13 20:36:54,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54,896][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.08480298519134521, acc: 1.0)
[2025-02-13 20:36:55,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55,297][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.0960226058959961, acc: 0.95652174949646)
[2025-02-13 20:36:55,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55,689][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.09465550631284714, acc: 0.9801980257034302)
[2025-02-13 20:36:55,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56,064][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.07041393965482712, acc: 0.9814814925193787)
[2025-02-13 20:36:56,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56,416][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.09379494935274124, acc: 0.9794520735740662)
[2025-02-13 20:36:56,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56,776][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.10083013772964478, acc: 0.9753086566925049)
[2025-02-13 20:36:56,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57,166][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.06278683245182037, acc: 0.988095223903656)
[2025-02-13 20:36:57,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57,570][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.07973388582468033, acc: 0.982300877571106)
[2025-02-13 20:36:57,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57,929][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.11869870126247406, acc: 0.970370352268219)
[2025-02-13 20:36:58,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58,294][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.1710420846939087, acc: 0.9736841917037964)
[2025-02-13 20:36:58,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58,658][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.12439686059951782, acc: 0.9743589758872986)
[2025-02-13 20:36:58,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59,074][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.11385055631399155, acc: 0.9897435903549194)
[2025-02-13 20:36:59,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59,524][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.048838336020708084, acc: 0.9885714054107666)
[2025-02-13 20:36:59,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59,962][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.027964923530817032, acc: 0.9870967864990234)
[2025-02-13 20:37:00,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00,416][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.04712170735001564, acc: 0.988950252532959)
[2025-02-13 20:37:00,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00,794][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.10367273539304733, acc: 0.9896907210350037)
[2025-02-13 20:37:00,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01,232][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.05112242326140404, acc: 0.9887005686759949)
[2025-02-13 20:37:01,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01,628][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.06559495627880096, acc: 0.9908257126808167)
[2025-02-13 20:37:01,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02,065][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.07182742655277252, acc: 0.9842105507850647)
[2025-02-13 20:37:02,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02,459][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.028965376317501068, acc: 0.9932432174682617)
[2025-02-13 20:37:02,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02,874][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.027443353086709976, acc: 0.9927007555961609)
[2025-02-13 20:37:03,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03,327][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.03326937556266785, acc: 0.9929577708244324)
[2025-02-13 20:37:03,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03,737][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.0508076511323452, acc: 0.9850746393203735)
[2025-02-13 20:37:03,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04,106][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.057286374270915985, acc: 0.9831932783126831)
[2025-02-13 20:37:04,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04,496][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.07547321170568466, acc: 0.9825581312179565)
[2025-02-13 20:37:04,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04,964][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.11790955066680908, acc: 0.9741935729980469)
[2025-02-13 20:37:05,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05,374][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.04110095277428627, acc: 0.9868420958518982)
[2025-02-13 20:37:05,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05,778][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.03005068562924862, acc: 0.9928571581840515)
[2025-02-13 20:37:05,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06,141][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.08211874216794968, acc: 0.9825581312179565)
[2025-02-13 20:37:06,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06,536][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.035945966839790344, acc: 0.9920634627342224)
[2025-02-13 20:37:06,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06,908][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.059858858585357666, acc: 0.9873417615890503)
[2025-02-13 20:37:07,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07,342][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.05101075768470764, acc: 0.9851852059364319)
[2025-02-13 20:37:07,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07,734][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.0955548882484436, acc: 0.9770992398262024)
[2025-02-13 20:37:07,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08,115][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.028207994997501373, acc: 1.0)
[2025-02-13 20:37:08,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08,501][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.04461020603775978, acc: 0.9945651888847351)
[2025-02-13 20:37:08,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08,875][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.0659966841340065, acc: 0.9895833134651184)
[2025-02-13 20:37:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09,289][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.05425809696316719, acc: 0.991150438785553)
[2025-02-13 20:37:09,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09,700][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.050966352224349976, acc: 0.9861111044883728)
[2025-02-13 20:37:09,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10,083][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.1315210461616516, acc: 0.9510489702224731)
[2025-02-13 20:37:10,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10,487][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.09152336418628693, acc: 0.977011501789093)
[2025-02-13 20:37:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10,887][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.1230258047580719, acc: 0.9666666388511658)
[2025-02-13 20:37:11,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11,311][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.14676715433597565, acc: 0.9520958065986633)
[2025-02-13 20:37:11,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11,747][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.17548175156116486, acc: 0.9523809552192688)
[2025-02-13 20:37:11,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12,259][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.14466287195682526, acc: 0.9640287756919861)
[2025-02-13 20:37:12,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12,657][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.07954658567905426, acc: 0.9788359999656677)
[2025-02-13 20:37:12,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13,113][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.13404077291488647, acc: 0.9720279574394226)
[2025-02-13 20:37:13,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13,530][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.0672665387392044, acc: 0.9879518151283264)
[2025-02-13 20:37:13,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13,946][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.07060681283473969, acc: 0.9929577708244324)
[2025-02-13 20:37:14,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14,395][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.11694828420877457, acc: 0.9693877696990967)
[2025-02-13 20:37:14,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14,812][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.06900359690189362, acc: 0.9781420826911926)
[2025-02-13 20:37:14,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15,208][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.024024995043873787, acc: 1.0)
[2025-02-13 20:37:15,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15,624][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.15963251888751984, acc: 0.9629629850387573)
[2025-02-13 20:37:15,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16,029][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.12607939541339874, acc: 0.9767441749572754)
[2025-02-13 20:37:16,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16,470][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.0770689845085144, acc: 0.9857142567634583)
[2025-02-13 20:37:16,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16,876][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.10579740256071091, acc: 0.9712643623352051)
[2025-02-13 20:37:17,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17,261][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.10688742995262146, acc: 0.971222996711731)
[2025-02-13 20:37:17,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17,632][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.042319685220718384, acc: 0.9942196607589722)
[2025-02-13 20:37:17,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18,009][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.1267891675233841, acc: 0.9583333134651184)
[2025-02-13 20:37:18,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18,380][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.14584463834762573, acc: 0.9679487347602844)
[2025-02-13 20:37:18,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18,755][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.06649838387966156, acc: 0.9837398529052734)
[2025-02-13 20:37:18,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19,143][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.11045927554368973, acc: 0.9736841917037964)
[2025-02-13 20:37:19,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19,525][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.03805031627416611, acc: 0.9808917045593262)
[2025-02-13 20:37:19,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19,932][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.1422126591205597, acc: 0.9696969985961914)
[2025-02-13 20:37:20,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20,304][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.025517510250210762, acc: 1.0)
[2025-02-13 20:37:20,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20,731][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.0351976677775383, acc: 0.9857142567634583)
[2025-02-13 20:37:20,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21,153][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.04800501465797424, acc: 0.9927536249160767)
[2025-02-13 20:37:21,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21,527][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.15918685495853424, acc: 0.9593023061752319)
[2025-02-13 20:37:21,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21,898][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.17790748178958893, acc: 0.9726775884628296)
[2025-02-13 20:37:22,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22,289][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.14585497975349426, acc: 0.981249988079071)
[2025-02-13 20:37:22,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22,709][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.17816907167434692, acc: 0.9569892287254333)
[2025-02-13 20:37:22,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23,101][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.06968963146209717, acc: 0.9798657894134521)
[2025-02-13 20:37:23,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23,482][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.11461186408996582, acc: 0.9646464586257935)
[2025-02-13 20:37:23,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23,892][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.0954761952161789, acc: 0.9750000238418579)
[2025-02-13 20:37:24,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24,277][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.07012380659580231, acc: 0.9772727489471436)
[2025-02-13 20:37:24,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24,721][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.10631990432739258, acc: 0.9682539701461792)
[2025-02-13 20:37:24,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25,171][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.02215123362839222, acc: 0.9945945739746094)
[2025-02-13 20:37:25,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25,565][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.041281525045633316, acc: 0.9887005686759949)
[2025-02-13 20:37:25,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25,961][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.03649640828371048, acc: 0.993630588054657)
[2025-02-13 20:37:26,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26,359][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.07488179206848145, acc: 0.976331353187561)
[2025-02-13 20:37:26,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26,732][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.18705277144908905, acc: 0.9767441749572754)
[2025-02-13 20:37:26,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27,160][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.03906424716114998, acc: 0.994350254535675)
[2025-02-13 20:37:27,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27,527][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.08455126732587814, acc: 0.9698795080184937)
[2025-02-13 20:37:27,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27,956][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.06549210101366043, acc: 0.9858155846595764)
[2025-02-13 20:37:28,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28,391][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.02565154619514942, acc: 0.9931507110595703)
[2025-02-13 20:37:28,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28,775][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.03337691351771355, acc: 0.9939758777618408)
[2025-02-13 20:37:28,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29,239][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.06522097438573837, acc: 0.9878048896789551)
[2025-02-13 20:37:29,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29,650][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.11186415702104568, acc: 0.9776536226272583)
[2025-02-13 20:37:29,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30,082][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.17822480201721191, acc: 0.9545454382896423)
[2025-02-13 20:37:30,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30,485][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.3520488440990448, acc: 0.9178082346916199)
[2025-02-13 20:37:30,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30,884][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.14455780386924744, acc: 0.9624999761581421)
[2025-02-13 20:37:31,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31,318][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.07352945953607559, acc: 0.9888888597488403)
[2025-02-13 20:37:31,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31,708][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.17680352926254272, acc: 0.9590643048286438)
[2025-02-13 20:37:31,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32,094][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.13005590438842773, acc: 0.9753086566925049)
[2025-02-13 20:37:32,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32,510][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.252165287733078, acc: 0.9300699234008789)
[2025-02-13 20:37:32,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32,899][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.15384431183338165, acc: 0.9602272510528564)
[2025-02-13 20:37:33,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33,314][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.13983404636383057, acc: 0.9627329111099243)
[2025-02-13 20:37:33,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33,745][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.07835756242275238, acc: 0.9841269850730896)
[2025-02-13 20:37:33,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34,149][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.1933864951133728, acc: 0.9508196711540222)
[2025-02-13 20:37:34,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34,566][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.13169287145137787, acc: 0.9642857313156128)
[2025-02-13 20:37:34,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34,987][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.05330336093902588, acc: 0.9856114983558655)
[2025-02-13 20:37:35,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35,431][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.1023784950375557, acc: 0.9666666388511658)
[2025-02-13 20:37:35,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35,851][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.07219978421926498, acc: 0.9920634627342224)
[2025-02-13 20:37:36,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36,302][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.027626682072877884, acc: 0.987261176109314)
[2025-02-13 20:37:36,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36,722][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.07781915366649628, acc: 0.9774436354637146)
[2025-02-13 20:37:36,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37,140][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.15458089113235474, acc: 0.9496855139732361)
[2025-02-13 20:37:37,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37,574][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.11409863829612732, acc: 0.9740259647369385)
[2025-02-13 20:37:37,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37,958][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.048947203904390335, acc: 0.9800000190734863)
[2025-02-13 20:37:38,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38,350][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.1273145079612732, acc: 0.9798657894134521)
[2025-02-13 20:37:38,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38,736][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.07248891144990921, acc: 0.9736841917037964)
[2025-02-13 20:37:38,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39,133][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.08489877730607986, acc: 0.97826087474823)
[2025-02-13 20:37:39,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39,513][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.15814968943595886, acc: 0.9655172228813171)
[2025-02-13 20:37:39,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39,901][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.07811873406171799, acc: 0.9806451797485352)
[2025-02-13 20:37:40,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40,286][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.07324527204036713, acc: 0.9849624037742615)
[2025-02-13 20:37:40,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40,662][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.134396493434906, acc: 0.9800000190734863)
[2025-02-13 20:37:40,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41,074][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.15354637801647186, acc: 0.9683544039726257)
[2025-02-13 20:37:41,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41,503][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.1283721625804901, acc: 0.9629629850387573)
[2025-02-13 20:37:41,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41,999][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.059284139424562454, acc: 0.9863013625144958)
[2025-02-13 20:37:42,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42,368][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.031044840812683105, acc: 1.0)
[2025-02-13 20:37:42,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42,760][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.13763165473937988, acc: 0.9642857313156128)
[2025-02-13 20:37:42,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43,153][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.06618064641952515, acc: 0.9932885766029358)
[2025-02-13 20:37:43,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43,542][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.07141819596290588, acc: 0.9718309640884399)
[2025-02-13 20:37:43,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43,910][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.08835186809301376, acc: 0.9766355156898499)
[2025-02-13 20:37:44,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44,348][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.07106403261423111, acc: 0.9857819676399231)
[2025-02-13 20:37:44,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44,738][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.08141706883907318, acc: 0.9835164546966553)
[2025-02-13 20:37:44,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45,126][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.09487979859113693, acc: 0.9664429426193237)
[2025-02-13 20:37:45,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45,584][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.0706874430179596, acc: 0.9800000190734863)
[2025-02-13 20:37:45,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45,994][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.23319107294082642, acc: 0.9315789341926575)
[2025-02-13 20:37:46,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46,386][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.11262372136116028, acc: 0.9729729890823364)
[2025-02-13 20:37:46,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46,786][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.15270903706550598, acc: 0.9517543911933899)
[2025-02-13 20:37:46,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47,197][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.1359288990497589, acc: 0.9595959782600403)
[2025-02-13 20:37:47,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47,546][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.13821037113666534, acc: 0.9541284441947937)
[2025-02-13 20:37:48,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49,017][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2679, device='cuda:0') eval_epoch_loss=tensor(0.2373, device='cuda:0') eval_epoch_acc=tensor(0.9459, device='cuda:0')
[2025-02-13 20:41:49,020][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:41:49,021][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:41:49,385][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_5347_loss_0.2373497635126114/model.pt
[2025-02-13 20:41:49,392][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:41:49,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49,878][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.09689521789550781, acc: 0.9677419066429138)
[2025-02-13 20:41:50,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50,305][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.15260891616344452, acc: 0.9518072009086609)
[2025-02-13 20:41:50,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50,648][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.12593646347522736, acc: 0.9714285731315613)
[2025-02-13 20:41:50,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51,016][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.05987520143389702, acc: 0.9882352948188782)
[2025-02-13 20:41:51,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51,383][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.13343767821788788, acc: 0.9675675630569458)
[2025-02-13 20:41:51,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51,741][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.08969797939062119, acc: 0.976047933101654)
[2025-02-13 20:41:51,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52,121][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.13477133214473724, acc: 0.9444444179534912)
[2025-02-13 20:41:52,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52,519][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.13723038136959076, acc: 0.9629629850387573)
[2025-02-13 20:41:52,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52,897][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.14422979950904846, acc: 0.9791666865348816)
[2025-02-13 20:41:53,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53,251][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.1407574564218521, acc: 0.957446813583374)
[2025-02-13 20:41:53,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53,613][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.08467962592840195, acc: 0.9858155846595764)
[2025-02-13 20:41:53,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53,988][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.26410534977912903, acc: 0.9505494236946106)
[2025-02-13 20:41:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54,347][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.14214204251766205, acc: 0.9704433679580688)
[2025-02-13 20:41:54,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54,739][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.053933653980493546, acc: 0.9942528605461121)
[2025-02-13 20:41:54,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55,114][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.11486849188804626, acc: 0.9746192693710327)
[2025-02-13 20:41:55,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55,489][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.08630648255348206, acc: 0.9729729890823364)
[2025-02-13 20:41:55,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55,853][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.0889686793088913, acc: 0.9878048896789551)
[2025-02-13 20:41:55,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56,220][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.041610799729824066, acc: 0.9884393215179443)
[2025-02-13 20:41:56,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56,592][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.06256236881017685, acc: 0.9772727489471436)
[2025-02-13 20:41:56,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56,936][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.13161182403564453, acc: 0.9850746393203735)
[2025-02-13 20:41:57,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57,309][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.13865895569324493, acc: 0.97826087474823)
[2025-02-13 20:41:57,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57,680][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.056755419820547104, acc: 0.9922480583190918)
[2025-02-13 20:41:57,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58,037][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.11924930661916733, acc: 0.9830508232116699)
[2025-02-13 20:41:58,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58,401][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.10056499391794205, acc: 0.9805194735527039)
[2025-02-13 20:41:58,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58,806][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.21240049600601196, acc: 0.9599999785423279)
[2025-02-13 20:41:58,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59,235][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.05882050842046738, acc: 0.9874213933944702)
[2025-02-13 20:41:59,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59,621][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.0460967980325222, acc: 0.9920634627342224)
[2025-02-13 20:41:59,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59,982][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.011149792931973934, acc: 1.0)
[2025-02-13 20:42:00,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00,344][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.029109200462698936, acc: 1.0)
[2025-02-13 20:42:00,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00,720][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.04511982947587967, acc: 0.9729729890823364)
[2025-02-13 20:42:00,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01,099][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.043370261788368225, acc: 0.9938271641731262)
[2025-02-13 20:42:01,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01,510][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.027049286291003227, acc: 0.9947368502616882)
[2025-02-13 20:42:01,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01,882][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.054556865245103836, acc: 0.9895833134651184)
[2025-02-13 20:42:02,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02,251][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.07569076120853424, acc: 0.9925925731658936)
[2025-02-13 20:42:02,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02,620][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.05075887218117714, acc: 0.9887005686759949)
[2025-02-13 20:42:02,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02,985][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.027804197743535042, acc: 1.0)
[2025-02-13 20:42:03,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03,387][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.0383182056248188, acc: 0.9838709831237793)
[2025-02-13 20:42:03,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03,812][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.050450023263692856, acc: 0.9796954393386841)
[2025-02-13 20:42:03,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04,263][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.09018637984991074, acc: 0.9825581312179565)
[2025-02-13 20:42:04,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04,658][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.040887437760829926, acc: 0.9890109896659851)
[2025-02-13 20:42:04,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05,052][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.013853004202246666, acc: 1.0)
[2025-02-13 20:42:05,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05,442][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.024859663099050522, acc: 0.994413435459137)
[2025-02-13 20:42:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05,787][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.033263400197029114, acc: 0.9931972622871399)
[2025-02-13 20:42:05,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06,171][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.07669077813625336, acc: 0.9937106966972351)
[2025-02-13 20:42:06,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06,492][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.18214863538742065, acc: 0.9784172773361206)
[2025-02-13 20:42:06,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06,869][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.21924248337745667, acc: 0.9488636255264282)
[2025-02-13 20:42:07,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07,253][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.10865713655948639, acc: 0.9666666388511658)
[2025-02-13 20:42:07,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07,652][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.2699548900127411, acc: 0.921875)
[2025-02-13 20:42:07,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08,044][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.060394976288080215, acc: 0.9797297120094299)
[2025-02-13 20:42:08,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08,395][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.15809611976146698, acc: 0.9605262875556946)
[2025-02-13 20:42:08,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08,770][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.07650845497846603, acc: 0.9716312289237976)
[2025-02-13 20:42:08,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09,114][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.13522066175937653, acc: 0.9698795080184937)
[2025-02-13 20:42:09,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09,487][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.10520577430725098, acc: 0.977142870426178)
[2025-02-13 20:42:09,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09,861][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.2619986832141876, acc: 0.9655172228813171)
[2025-02-13 20:42:10,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10,254][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.07989248633384705, acc: 0.9722222089767456)
[2025-02-13 20:42:10,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10,628][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.0581132210791111, acc: 0.9931507110595703)
[2025-02-13 20:42:10,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11,026][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.13967584073543549, acc: 0.9754098653793335)
[2025-02-13 20:42:11,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11,418][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.16994279623031616, acc: 0.9664429426193237)
[2025-02-13 20:42:11,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11,794][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.06823109090328217, acc: 0.9826086759567261)
[2025-02-13 20:42:11,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12,163][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.06269022077322006, acc: 0.9729729890823364)
[2025-02-13 20:42:12,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12,507][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.06604622304439545, acc: 0.9890109896659851)
[2025-02-13 20:42:12,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12,846][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.225407674908638, acc: 0.9432623982429504)
[2025-02-13 20:42:12,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13,209][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.24559183418750763, acc: 0.9720279574394226)
[2025-02-13 20:42:13,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13,601][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.052465181797742844, acc: 0.9879518151283264)
[2025-02-13 20:42:13,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13,956][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.10362111032009125, acc: 0.9647887349128723)
[2025-02-13 20:42:14,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14,334][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.18851038813591003, acc: 0.982300877571106)
[2025-02-13 20:42:14,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14,710][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.11921397596597672, acc: 0.9652777910232544)
[2025-02-13 20:42:14,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15,066][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.28283095359802246, acc: 0.9281437397003174)
[2025-02-13 20:42:15,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15,433][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.175509512424469, acc: 0.9610389471054077)
[2025-02-13 20:42:15,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15,810][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.18652576208114624, acc: 0.9572192430496216)
[2025-02-13 20:42:15,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16,180][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.094732865691185, acc: 0.9679487347602844)
[2025-02-13 20:42:16,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16,536][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.16053034365177155, acc: 0.9513888955116272)
[2025-02-13 20:42:16,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16,897][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.40058642625808716, acc: 0.8978102207183838)
[2025-02-13 20:42:17,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17,271][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.4476490616798401, acc: 0.9473684430122375)
[2025-02-13 20:42:17,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17,640][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.3811381757259369, acc: 0.9312499761581421)
[2025-02-13 20:42:17,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18,013][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.10687777400016785, acc: 0.9861111044883728)
[2025-02-13 20:42:18,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18,339][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.06492637097835541, acc: 1.0)
[2025-02-13 20:42:18,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18,706][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.1010737493634224, acc: 0.9846153855323792)
[2025-02-13 20:42:18,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19,060][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.11281529814004898, acc: 0.96875)
[2025-02-13 20:42:19,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19,421][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.16047675907611847, acc: 0.9754098653793335)
[2025-02-13 20:42:19,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19,799][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.05923972278833389, acc: 0.9934640526771545)
[2025-02-13 20:42:19,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20,168][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.06754089146852493, acc: 0.9862068891525269)
[2025-02-13 20:42:20,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20,543][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.1461855173110962, acc: 0.9530201554298401)
[2025-02-13 20:42:20,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20,928][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.186426043510437, acc: 0.9675324559211731)
[2025-02-13 20:42:21,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21,296][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.09502319991588593, acc: 0.9473684430122375)
[2025-02-13 20:42:21,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21,666][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.07177741080522537, acc: 0.984375)
[2025-02-13 20:42:21,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22,022][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.1565702110528946, acc: 0.9663865566253662)
[2025-02-13 20:42:22,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22,400][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.0935380682349205, acc: 0.9677419066429138)
[2025-02-13 20:42:22,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22,775][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.08691450208425522, acc: 0.984000027179718)
[2025-02-13 20:42:22,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23,155][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.0888180211186409, acc: 0.9870967864990234)
[2025-02-13 20:42:23,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23,525][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.1257287859916687, acc: 0.9924812316894531)
[2025-02-13 20:42:23,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23,899][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.16589584946632385, acc: 0.9444444179534912)
[2025-02-13 20:42:24,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24,277][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.21630196273326874, acc: 0.939393937587738)
[2025-02-13 20:42:24,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24,650][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.20981644093990326, acc: 0.949999988079071)
[2025-02-13 20:42:24,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25,018][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.1510845273733139, acc: 0.9729729890823364)
[2025-02-13 20:42:25,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25,392][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.07493092864751816, acc: 0.9821428656578064)
[2025-02-13 20:42:25,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25,746][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.050486546009778976, acc: 0.9939024448394775)
[2025-02-13 20:42:25,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26,152][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.08093470335006714, acc: 0.9852941036224365)
[2025-02-13 20:42:26,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26,522][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.1581025868654251, acc: 0.9485294222831726)
[2025-02-13 20:42:26,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26,892][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.12318956106901169, acc: 0.9693251252174377)
[2025-02-13 20:42:27,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27,248][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.10559394955635071, acc: 0.965753436088562)
[2025-02-13 20:42:27,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27,629][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.11820875853300095, acc: 0.9670329689979553)
[2025-02-13 20:42:27,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27,984][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.12158767879009247, acc: 0.9672130942344666)
[2025-02-13 20:42:28,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28,404][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.20320358872413635, acc: 0.9666666388511658)
[2025-02-13 20:42:28,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28,805][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.06997715681791306, acc: 0.976331353187561)
[2025-02-13 20:42:28,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29,182][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.07762409746646881, acc: 0.9704142212867737)
[2025-02-13 20:42:29,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29,585][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.18481355905532837, acc: 0.949999988079071)
[2025-02-13 20:42:29,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29,963][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.10418746620416641, acc: 0.9728260636329651)
[2025-02-13 20:42:30,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30,328][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.10761042684316635, acc: 0.9893048405647278)
[2025-02-13 20:42:30,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30,710][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.08832702040672302, acc: 0.9784946441650391)
[2025-02-13 20:42:30,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31,105][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.118665911257267, acc: 0.977011501789093)
[2025-02-13 20:42:31,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31,519][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.07117319852113724, acc: 0.988950252532959)
[2025-02-13 20:42:31,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31,927][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.02211582101881504, acc: 1.0)
[2025-02-13 20:42:32,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32,285][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.0565778873860836, acc: 0.9881656765937805)
[2025-02-13 20:42:32,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32,646][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.09513062238693237, acc: 0.9724770784378052)
[2025-02-13 20:42:32,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33,022][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.056174203753471375, acc: 0.987730085849762)
[2025-02-13 20:42:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33,393][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.07717512547969818, acc: 0.9814814925193787)
[2025-02-13 20:42:33,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33,748][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.11872390657663345, acc: 0.9726775884628296)
[2025-02-13 20:42:33,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34,121][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.09400690346956253, acc: 0.9937499761581421)
[2025-02-13 20:42:34,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34,534][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.07506721466779709, acc: 0.9751243591308594)
[2025-02-13 20:42:34,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34,954][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.05240563303232193, acc: 0.9900497794151306)
[2025-02-13 20:42:35,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35,358][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.1887178272008896, acc: 0.9842932224273682)
[2025-02-13 20:42:35,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35,769][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.13934902846813202, acc: 0.9576719403266907)
[2025-02-13 20:42:35,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36,192][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.06379985809326172, acc: 0.9800994992256165)
[2025-02-13 20:42:36,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36,593][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.04725078120827675, acc: 0.9882352948188782)
[2025-02-13 20:42:36,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37,006][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.0884077399969101, acc: 0.9888268113136292)
[2025-02-13 20:42:37,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37,364][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.020173626020550728, acc: 0.9949495196342468)
[2025-02-13 20:42:37,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37,748][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.07466990500688553, acc: 0.9818181991577148)
[2025-02-13 20:42:37,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38,136][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.028801973909139633, acc: 1.0)
[2025-02-13 20:42:38,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38,510][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.009411980397999287, acc: 1.0)
[2025-02-13 20:42:38,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38,889][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.02648269198834896, acc: 0.9948979616165161)
[2025-02-13 20:42:39,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39,292][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.1510152369737625, acc: 0.970370352268219)
[2025-02-13 20:42:39,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39,685][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.16047705709934235, acc: 0.9729729890823364)
[2025-02-13 20:42:39,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40,072][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.09449553489685059, acc: 0.976190447807312)
[2025-02-13 20:42:40,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40,441][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.05294441059231758, acc: 0.9918032884597778)
[2025-02-13 20:42:40,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41,028][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.2836746871471405, acc: 0.9432989954948425)
[2025-02-13 20:42:41,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41,494][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.11929338425397873, acc: 0.9636363387107849)
[2025-02-13 20:42:41,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41,878][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.0606350302696228, acc: 0.9876543283462524)
[2025-02-13 20:42:42,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42,285][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.12342918664216995, acc: 0.975806474685669)
[2025-02-13 20:42:42,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42,665][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.20460060238838196, acc: 0.9772727489471436)
[2025-02-13 20:42:42,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43,016][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.10602068156003952, acc: 0.9659863710403442)
[2025-02-13 20:42:43,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43,381][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.11836116760969162, acc: 0.976047933101654)
[2025-02-13 20:42:43,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43,769][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.24533092975616455, acc: 0.9390243887901306)
[2025-02-13 20:42:43,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44,104][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.1326671987771988, acc: 0.9696969985961914)
[2025-02-13 20:42:44,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44,463][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.09889757633209229, acc: 0.9649122953414917)
[2025-02-13 20:42:44,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44,820][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.09359187632799149, acc: 0.9925373196601868)
[2025-02-13 20:42:44,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45,173][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.23220209777355194, acc: 0.953125)
[2025-02-13 20:42:45,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45,557][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.11377154290676117, acc: 0.9748427867889404)
[2025-02-13 20:42:45,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45,889][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.0806359127163887, acc: 0.9849624037742615)
[2025-02-13 20:42:46,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46,241][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.15790098905563354, acc: 0.9523809552192688)
[2025-02-13 20:42:46,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46,595][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.1485981047153473, acc: 0.9764705896377563)
[2025-02-13 20:42:46,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46,957][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.15662391483783722, acc: 0.9734042286872864)
[2025-02-13 20:42:47,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47,363][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.1117425262928009, acc: 0.9819819927215576)
[2025-02-13 20:42:47,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47,758][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.0684683620929718, acc: 0.984455943107605)
[2025-02-13 20:42:47,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48,159][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.11808499693870544, acc: 0.9696969985961914)
[2025-02-13 20:42:48,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48,565][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.1079799011349678, acc: 0.9717513918876648)
[2025-02-13 20:42:48,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48,925][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.2560088634490967, acc: 0.9692307710647583)
[2025-02-13 20:42:49,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49,295][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.0647742971777916, acc: 0.9879518151283264)
[2025-02-13 20:42:49,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49,658][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.230929434299469, acc: 0.9375)
[2025-02-13 20:42:49,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50,031][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.17001836001873016, acc: 0.9646464586257935)
[2025-02-13 20:42:50,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50,368][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.1451893150806427, acc: 0.9707602262496948)
[2025-02-13 20:42:50,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50,717][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.09515473246574402, acc: 0.9760765433311462)
[2025-02-13 20:42:50,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51,070][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.206782266497612, acc: 0.9478672742843628)
[2025-02-13 20:42:51,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51,450][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.16419123113155365, acc: 0.9516128897666931)
[2025-02-13 20:42:51,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51,818][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.20282447338104248, acc: 0.9563106894493103)
[2025-02-13 20:42:51,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52,214][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.2113265097141266, acc: 0.9505494236946106)
[2025-02-13 20:42:52,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52,609][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.1548660546541214, acc: 0.9585798978805542)
[2025-02-13 20:42:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53,022][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.2455904334783554, acc: 0.9490740895271301)
[2025-02-13 20:42:53,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53,391][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.17521126568317413, acc: 0.9583333134651184)
[2025-02-13 20:42:53,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53,749][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.08353718370199203, acc: 0.9942528605461121)
[2025-02-13 20:42:53,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54,105][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.09791215509176254, acc: 0.9776536226272583)
[2025-02-13 20:42:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54,462][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.0820208415389061, acc: 0.9777777791023254)
[2025-02-13 20:42:54,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54,826][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.06665770709514618, acc: 0.9938271641731262)
[2025-02-13 20:42:54,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55,228][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.05663762241601944, acc: 0.9857819676399231)
[2025-02-13 20:42:55,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55,608][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.13073532283306122, acc: 0.9685863852500916)
[2025-02-13 20:42:55,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56,006][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.1803174465894699, acc: 0.9476439952850342)
[2025-02-13 20:42:56,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56,398][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.1539989560842514, acc: 0.9835164546966553)
[2025-02-13 20:42:56,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56,801][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.14357881247997284, acc: 0.9620253443717957)
[2025-02-13 20:42:56,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57,221][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.11115694046020508, acc: 0.9756097793579102)
[2025-02-13 20:42:57,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57,599][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.05407341569662094, acc: 0.9937888383865356)
[2025-02-13 20:42:57,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57,968][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.07070066034793854, acc: 0.9790209531784058)
[2025-02-13 20:42:58,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58,375][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.12475284188985825, acc: 0.9646464586257935)
[2025-02-13 20:42:58,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58,757][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.11002573370933533, acc: 0.9681817889213562)
[2025-02-13 20:42:58,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59,142][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.06273751705884933, acc: 0.9863945841789246)
[2025-02-13 20:42:59,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59,519][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.0920267179608345, acc: 0.9759036302566528)
[2025-02-13 20:42:59,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59,943][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.11317651718854904, acc: 0.9548022747039795)
[2025-02-13 20:43:00,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00,332][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.033861808478832245, acc: 1.0)
[2025-02-13 20:43:00,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00,731][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.12382370978593826, acc: 0.966292142868042)
[2025-02-13 20:43:00,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01,114][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.12630778551101685, acc: 0.9792746305465698)
[2025-02-13 20:43:01,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01,469][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.055379074066877365, acc: 0.994535505771637)
[2025-02-13 20:43:01,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01,838][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.08739768713712692, acc: 0.9712643623352051)
[2025-02-13 20:43:01,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02,208][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.0814438909292221, acc: 0.9842932224273682)
[2025-02-13 20:43:02,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02,577][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.06055836006999016, acc: 0.9836065769195557)
[2025-02-13 20:43:02,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02,936][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.1345965564250946, acc: 0.9819276928901672)
[2025-02-13 20:43:03,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03,299][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.04193538427352905, acc: 0.9941860437393188)
[2025-02-13 20:43:03,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03,654][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.01719333417713642, acc: 1.0)
[2025-02-13 20:43:03,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04,028][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.05043339356780052, acc: 0.9893048405647278)
[2025-02-13 20:43:04,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04,397][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.04843065142631531, acc: 0.9836065769195557)
[2025-02-13 20:43:04,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04,754][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.0859174057841301, acc: 0.9821428656578064)
[2025-02-13 20:43:04,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05,117][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.06081290915608406, acc: 0.9820359349250793)
[2025-02-13 20:43:05,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05,490][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.05154271423816681, acc: 0.9834254384040833)
[2025-02-13 20:43:05,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05,879][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.05041751265525818, acc: 0.9823529124259949)
[2025-02-13 20:43:06,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06,192][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.0977024957537651, acc: 0.9865771532058716)
[2025-02-13 20:43:06,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06,555][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.07627010345458984, acc: 0.9887640476226807)
[2025-02-13 20:43:06,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06,903][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.14478786289691925, acc: 0.9677419066429138)
[2025-02-13 20:43:07,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07,242][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.04317692667245865, acc: 0.987730085849762)
[2025-02-13 20:43:07,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07,598][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.11517693847417831, acc: 0.976331353187561)
[2025-02-13 20:43:07,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07,948][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.0365995392203331, acc: 0.993630588054657)
[2025-02-13 20:43:08,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08,311][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.08250018954277039, acc: 0.9821428656578064)
[2025-02-13 20:43:08,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08,713][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.10610277950763702, acc: 0.9776536226272583)
[2025-02-13 20:43:08,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09,101][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.08028765767812729, acc: 0.9813664555549622)
[2025-02-13 20:43:09,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09,490][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.08024562895298004, acc: 0.9833333492279053)
[2025-02-13 20:43:09,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09,869][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.108134925365448, acc: 0.9776536226272583)
[2025-02-13 20:43:09,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10,247][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.1438198834657669, acc: 0.9611111283302307)
[2025-02-13 20:43:10,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10,632][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.07586564123630524, acc: 0.9918032884597778)
[2025-02-13 20:43:10,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11,013][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.21063710749149323, acc: 0.9568345546722412)
[2025-02-13 20:43:11,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11,387][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.04710421711206436, acc: 0.9861111044883728)
[2025-02-13 20:43:11,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11,754][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.059222638607025146, acc: 0.981249988079071)
[2025-02-13 20:43:11,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12,166][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.02904227003455162, acc: 0.9800000190734863)
[2025-02-13 20:43:12,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12,516][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.044842060655355453, acc: 0.9949238300323486)
[2025-02-13 20:43:12,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12,880][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.023770704865455627, acc: 0.9950248599052429)
[2025-02-13 20:43:13,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13,205][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.03812028095126152, acc: 0.9945054650306702)
[2025-02-13 20:43:13,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13,581][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.02362656034529209, acc: 0.9947916865348816)
[2025-02-13 20:43:13,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13,947][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.04201665148139, acc: 0.989130437374115)
[2025-02-13 20:43:14,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14,306][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.05576496943831444, acc: 0.9873417615890503)
[2025-02-13 20:43:14,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14,683][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.05243684723973274, acc: 0.9938650131225586)
[2025-02-13 20:43:14,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15,066][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.0636497288942337, acc: 0.9934640526771545)
[2025-02-13 20:43:15,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15,441][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.03206342086195946, acc: 0.9930555820465088)
[2025-02-13 20:43:15,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15,827][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.09536370635032654, acc: 0.9800000190734863)
[2025-02-13 20:43:15,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16,214][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.05080162733793259, acc: 0.988095223903656)
[2025-02-13 20:43:16,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16,600][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.10604539513587952, acc: 0.9851852059364319)
[2025-02-13 20:43:16,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16,982][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.0804578885436058, acc: 0.9746835231781006)
[2025-02-13 20:43:17,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17,381][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.19322291016578674, acc: 0.9815950989723206)
[2025-02-13 20:43:17,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17,760][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.032366566359996796, acc: 0.9888888597488403)
[2025-02-13 20:43:17,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18,162][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.050819385796785355, acc: 0.9878787994384766)
[2025-02-13 20:43:18,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18,569][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.04198433831334114, acc: 0.9834254384040833)
[2025-02-13 20:43:18,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18,956][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.05114728957414627, acc: 0.979899525642395)
[2025-02-13 20:43:19,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19,338][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.09170171618461609, acc: 0.9746835231781006)
[2025-02-13 20:43:19,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19,733][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.10578914731740952, acc: 0.9707317352294922)
[2025-02-13 20:43:19,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20,137][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.07005470246076584, acc: 0.9795918464660645)
[2025-02-13 20:43:20,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20,541][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.06628037244081497, acc: 0.9828571677207947)
[2025-02-13 20:43:20,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20,931][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.021591678261756897, acc: 1.0)
[2025-02-13 20:43:21,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21,312][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.07339465618133545, acc: 0.9751552939414978)
[2025-02-13 20:43:21,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21,683][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.08559371531009674, acc: 0.9826589822769165)
[2025-02-13 20:43:21,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22,047][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.03914574161171913, acc: 0.9942196607589722)
[2025-02-13 20:43:22,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22,423][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.05491998419165611, acc: 0.9941860437393188)
[2025-02-13 20:43:22,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22,800][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.046883609145879745, acc: 0.9836956262588501)
[2025-02-13 20:43:22,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23,177][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.037192996591329575, acc: 0.9943181872367859)
[2025-02-13 20:43:23,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23,567][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.03535108640789986, acc: 0.9943181872367859)
[2025-02-13 20:43:23,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23,908][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.03553035855293274, acc: 0.9880239367485046)
[2025-02-13 20:43:24,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24,313][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.06624724715948105, acc: 0.9879518151283264)
[2025-02-13 20:43:24,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24,681][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.07307639718055725, acc: 0.9826589822769165)
[2025-02-13 20:43:24,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25,057][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.0929512158036232, acc: 0.9863945841789246)
[2025-02-13 20:43:25,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25,430][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.06431908160448074, acc: 0.9857142567634583)
[2025-02-13 20:43:25,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25,788][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.05280281975865364, acc: 0.9923664331436157)
[2025-02-13 20:43:25,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26,158][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.08121951669454575, acc: 0.9788732528686523)
[2025-02-13 20:43:26,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26,536][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.06695162504911423, acc: 0.9672130942344666)
[2025-02-13 20:43:26,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26,892][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.08027589321136475, acc: 0.9837398529052734)
[2025-02-13 20:43:27,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27,252][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.09537666290998459, acc: 0.9763779640197754)
[2025-02-13 20:43:27,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27,608][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.07966554909944534, acc: 0.9813084006309509)
[2025-02-13 20:43:27,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27,963][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.05004017800092697, acc: 0.9928057789802551)
[2025-02-13 20:43:28,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28,321][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.06867881864309311, acc: 0.9844961166381836)
[2025-02-13 20:43:28,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28,717][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.08953594416379929, acc: 0.9666666388511658)
[2025-02-13 20:43:28,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29,091][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.018038185313344002, acc: 1.0)
[2025-02-13 20:43:29,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29,490][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.0651300698518753, acc: 0.9850746393203735)
[2025-02-13 20:43:29,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29,905][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.15359018743038177, acc: 0.9718309640884399)
[2025-02-13 20:43:30,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30,273][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.08654984086751938, acc: 0.9732142686843872)
[2025-02-13 20:43:30,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30,638][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.14762964844703674, acc: 0.9865771532058716)
[2025-02-13 20:43:30,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30,999][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.05524911731481552, acc: 0.9904761910438538)
[2025-02-13 20:43:31,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31,359][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.06343584507703781, acc: 0.9925925731658936)
[2025-02-13 20:43:31,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31,718][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.026861924678087234, acc: 1.0)
[2025-02-13 20:43:31,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32,094][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.08223036676645279, acc: 0.9779411554336548)
[2025-02-13 20:43:32,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32,444][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.09611816704273224, acc: 0.9579831957817078)
[2025-02-13 20:43:32,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32,799][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.07141728699207306, acc: 0.9826086759567261)
[2025-02-13 20:43:32,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33,176][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.04744420573115349, acc: 0.9935483932495117)
[2025-02-13 20:43:33,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33,560][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.06223459169268608, acc: 0.9786096215248108)
[2025-02-13 20:43:33,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33,908][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.08688870072364807, acc: 0.9791666865348816)
[2025-02-13 20:43:34,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34,294][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.07834844291210175, acc: 0.9803921580314636)
[2025-02-13 20:43:34,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34,680][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.04975210875272751, acc: 0.9901960492134094)
[2025-02-13 20:43:34,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35,050][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.027955777943134308, acc: 1.0)
[2025-02-13 20:43:35,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35,420][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.08219383656978607, acc: 0.9750000238418579)
[2025-02-13 20:43:35,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35,829][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.08456764370203018, acc: 0.9910714030265808)
[2025-02-13 20:43:35,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36,237][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.029612788930535316, acc: 0.9918032884597778)
[2025-02-13 20:43:36,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36,628][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.05777378007769585, acc: 0.9901960492134094)
[2025-02-13 20:43:36,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37,006][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.04266488552093506, acc: 0.9918699264526367)
[2025-02-13 20:43:37,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37,402][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.075653575360775, acc: 0.9615384340286255)
[2025-02-13 20:43:37,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37,783][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.05731913074851036, acc: 0.9821428656578064)
[2025-02-13 20:43:37,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38,171][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.04745054990053177, acc: 0.982300877571106)
[2025-02-13 20:43:38,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38,566][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.04482957720756531, acc: 0.9921875)
[2025-02-13 20:43:38,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38,956][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.05187324807047844, acc: 0.9923076629638672)
[2025-02-13 20:43:39,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39,346][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.03555040806531906, acc: 1.0)
[2025-02-13 20:43:39,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39,722][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.028763974085450172, acc: 1.0)
[2025-02-13 20:43:39,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40,108][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.07910080254077911, acc: 0.9935064911842346)
[2025-02-13 20:43:40,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40,465][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.058724354952573776, acc: 0.9858155846595764)
[2025-02-13 20:43:40,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40,809][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.09037069976329803, acc: 0.9775280952453613)
[2025-02-13 20:43:40,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41,191][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.057770635932683945, acc: 1.0)
[2025-02-13 20:43:41,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41,556][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.07063942402601242, acc: 0.9789473414421082)
[2025-02-13 20:43:41,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41,932][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.04497552663087845, acc: 1.0)
[2025-02-13 20:43:42,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42,354][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.03540016710758209, acc: 1.0)
[2025-02-13 20:43:42,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42,770][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.03522863611578941, acc: 1.0)
[2025-02-13 20:43:42,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43,162][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.05463409423828125, acc: 0.9930555820465088)
[2025-02-13 20:43:43,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43,536][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.13196325302124023, acc: 0.9866666793823242)
[2025-02-13 20:43:43,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43,898][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.13117744028568268, acc: 0.9662162065505981)
[2025-02-13 20:43:44,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44,273][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.09363103657960892, acc: 0.9677419066429138)
[2025-02-13 20:43:44,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44,654][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.06749141216278076, acc: 0.9727891087532043)
[2025-02-13 20:43:44,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45,044][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.07816851884126663, acc: 0.9928057789802551)
[2025-02-13 20:43:45,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45,408][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.03663066774606705, acc: 1.0)
[2025-02-13 20:43:45,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45,793][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.042696624994277954, acc: 0.9921875)
[2025-02-13 20:43:45,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46,193][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.04220418259501457, acc: 0.9906542301177979)
[2025-02-13 20:43:46,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46,568][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.0706455409526825, acc: 0.9836065769195557)
[2025-02-13 20:43:46,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46,942][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.16398456692695618, acc: 0.9536423683166504)
[2025-02-13 20:43:47,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47,327][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.06494762003421783, acc: 0.9869281053543091)
[2025-02-13 20:43:47,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47,688][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.12087994813919067, acc: 0.9714285731315613)
[2025-02-13 20:43:47,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48,047][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.09248606115579605, acc: 0.9765625)
[2025-02-13 20:43:48,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48,422][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.06312797963619232, acc: 0.9915966391563416)
[2025-02-13 20:43:48,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48,804][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.06171952560544014, acc: 0.9784172773361206)
[2025-02-13 20:43:48,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49,164][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.02520707994699478, acc: 1.0)
[2025-02-13 20:43:49,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49,523][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.1897273063659668, acc: 0.965753436088562)
[2025-02-13 20:43:49,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49,884][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.04319692403078079, acc: 0.9931972622871399)
[2025-02-13 20:43:50,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50,302][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.07535794377326965, acc: 0.9869281053543091)
[2025-02-13 20:43:50,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50,626][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.10219378024339676, acc: 0.9756097793579102)
[2025-02-13 20:43:50,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51,020][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.10499467700719833, acc: 0.9777777791023254)
[2025-02-13 20:43:51,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51,400][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.05892755463719368, acc: 0.9808917045593262)
[2025-02-13 20:43:51,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51,749][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.04239629954099655, acc: 0.9920634627342224)
[2025-02-13 20:43:51,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52,162][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.052206385880708694, acc: 0.9943820238113403)
[2025-02-13 20:43:52,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52,546][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.05921868979930878, acc: 0.9851852059364319)
[2025-02-13 20:43:52,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52,918][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.044893596321344376, acc: 0.9863013625144958)
[2025-02-13 20:43:53,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53,308][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.0850655660033226, acc: 0.9659863710403442)
[2025-02-13 20:43:53,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53,683][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.11512938141822815, acc: 0.9733333587646484)
[2025-02-13 20:43:53,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54,031][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.1927114725112915, acc: 0.9425287246704102)
[2025-02-13 20:43:54,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54,400][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.1603320688009262, acc: 0.9560439586639404)
[2025-02-13 20:43:54,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54,777][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.09769789129495621, acc: 0.9814814925193787)
[2025-02-13 20:43:54,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55,153][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.2206152230501175, acc: 0.9556962251663208)
[2025-02-13 20:43:55,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55,501][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.2855484187602997, acc: 0.945652186870575)
[2025-02-13 20:43:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55,879][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.11777806282043457, acc: 0.9757575988769531)
[2025-02-13 20:43:56,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56,258][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.06828320026397705, acc: 0.9879518151283264)
[2025-02-13 20:43:56,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56,634][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.08684695512056351, acc: 0.9888888597488403)
[2025-02-13 20:43:56,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57,022][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.30897635221481323, acc: 0.9268292784690857)
[2025-02-13 20:43:57,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57,390][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.12891648709774017, acc: 0.9763779640197754)
[2025-02-13 20:43:57,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57,736][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.06316055357456207, acc: 0.9784172773361206)
[2025-02-13 20:43:57,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58,082][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.15045350790023804, acc: 0.9696969985961914)
[2025-02-13 20:43:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58,455][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.16165415942668915, acc: 0.9777777791023254)
[2025-02-13 20:43:58,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58,821][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.10404843837022781, acc: 0.9756097793579102)
[2025-02-13 20:43:58,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59,199][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.08770904690027237, acc: 0.9793814420700073)
[2025-02-13 20:43:59,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59,552][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.14759109914302826, acc: 0.9672130942344666)
[2025-02-13 20:43:59,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59,931][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.11421285569667816, acc: 0.977142870426178)
[2025-02-13 20:44:00,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00,294][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.019225377589464188, acc: 1.0)
[2025-02-13 20:44:00,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00,648][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.06783934682607651, acc: 0.9883720874786377)
[2025-02-13 20:44:00,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01,025][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.03930802270770073, acc: 1.0)
[2025-02-13 20:44:01,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01,394][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.07116463035345078, acc: 0.9851484894752502)
[2025-02-13 20:44:01,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01,779][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.07928484678268433, acc: 0.9757575988769531)
[2025-02-13 20:44:01,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02,161][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.0741117000579834, acc: 0.9829545617103577)
[2025-02-13 20:44:02,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02,540][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.11359997093677521, acc: 0.9885057210922241)
[2025-02-13 20:44:02,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02,905][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.0703207477927208, acc: 0.9745222926139832)
[2025-02-13 20:44:03,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03,290][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.03515852242708206, acc: 0.9945945739746094)
[2025-02-13 20:44:03,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03,657][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.0968269631266594, acc: 0.9825581312179565)
[2025-02-13 20:44:03,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04,046][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.08195202052593231, acc: 0.9874213933944702)
[2025-02-13 20:44:04,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04,423][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.06225520372390747, acc: 0.9823529124259949)
[2025-02-13 20:44:04,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04,793][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.011122191324830055, acc: 1.0)
[2025-02-13 20:44:04,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05,198][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.06739957630634308, acc: 0.9752475023269653)
[2025-02-13 20:44:05,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05,576][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.10731139034032822, acc: 0.9760000109672546)
[2025-02-13 20:44:05,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05,941][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.020771147683262825, acc: 1.0)
[2025-02-13 20:44:06,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06,358][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.16345185041427612, acc: 0.9751552939414978)
[2025-02-13 20:44:06,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06,756][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.07453601062297821, acc: 0.9828571677207947)
[2025-02-13 20:44:06,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07,205][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.12547720968723297, acc: 0.9731183052062988)
[2025-02-13 20:44:07,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07,652][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.046982549130916595, acc: 0.9919354915618896)
[2025-02-13 20:44:07,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08,093][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.0852469727396965, acc: 0.9775280952453613)
[2025-02-13 20:44:08,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08,550][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.10476776957511902, acc: 0.97826087474823)
[2025-02-13 20:44:08,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08,935][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.19110378623008728, acc: 0.9558823704719543)
[2025-02-13 20:44:09,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09,327][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.11579322814941406, acc: 0.9791666865348816)
[2025-02-13 20:44:09,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09,700][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.10624869167804718, acc: 0.9649122953414917)
[2025-02-13 20:44:09,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10,079][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.036519791930913925, acc: 0.9868420958518982)
[2025-02-13 20:44:10,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10,467][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.13590724766254425, acc: 0.9647058844566345)
[2025-02-13 20:44:10,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10,810][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.10928884148597717, acc: 0.9615384340286255)
[2025-02-13 20:44:10,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11,177][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.1122284010052681, acc: 0.9740932583808899)
[2025-02-13 20:44:11,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11,535][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.029931504279375076, acc: 1.0)
[2025-02-13 20:44:11,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11,896][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.09339398145675659, acc: 0.9894179701805115)
[2025-02-13 20:44:12,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12,252][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.07673592865467072, acc: 0.9696969985961914)
[2025-02-13 20:44:12,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12,661][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.12344308197498322, acc: 0.9657142758369446)
[2025-02-13 20:44:12,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13,030][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.14190010726451874, acc: 0.9640718698501587)
[2025-02-13 20:44:13,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13,400][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.03226489946246147, acc: 1.0)
[2025-02-13 20:44:13,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13,749][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.03509889170527458, acc: 0.9926470518112183)
[2025-02-13 20:44:13,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14,108][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.03653840348124504, acc: 0.9935064911842346)
[2025-02-13 20:44:14,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14,460][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.060737207531929016, acc: 0.9925925731658936)
[2025-02-13 20:44:14,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14,842][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.07341426610946655, acc: 0.981249988079071)
[2025-02-13 20:44:14,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15,207][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.6561881303787231, acc: 0.8373494148254395)
[2025-02-13 20:44:15,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15,599][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.11926227062940598, acc: 0.9620253443717957)
[2025-02-13 20:44:15,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15,973][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.06307992339134216, acc: 0.976190447807312)
[2025-02-13 20:44:16,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16,345][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.10111291706562042, acc: 0.9602649211883545)
[2025-02-13 20:44:16,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16,752][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.04974308982491493, acc: 1.0)
[2025-02-13 20:44:16,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17,136][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.046009719371795654, acc: 0.9941520690917969)
[2025-02-13 20:44:17,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17,512][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.08078446239233017, acc: 0.9848484992980957)
[2025-02-13 20:44:17,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17,889][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.08134223520755768, acc: 0.9750000238418579)
[2025-02-13 20:44:18,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18,254][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.05587500333786011, acc: 0.9895833134651184)
[2025-02-13 20:44:18,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18,635][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.07572435587644577, acc: 0.9886363744735718)
[2025-02-13 20:44:18,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19,003][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.055342160165309906, acc: 0.9887640476226807)
[2025-02-13 20:44:19,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19,400][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.04472922161221504, acc: 0.9947916865348816)
[2025-02-13 20:44:19,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19,757][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.03133522719144821, acc: 0.991150438785553)
[2025-02-13 20:44:19,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20,157][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.19154807925224304, acc: 0.9550561904907227)
[2025-02-13 20:44:20,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20,524][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.06474082916975021, acc: 0.9753086566925049)
[2025-02-13 20:44:20,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20,887][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.10898583382368088, acc: 0.9636363387107849)
[2025-02-13 20:44:21,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21,260][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.03190144523978233, acc: 0.9921259880065918)
[2025-02-13 20:44:21,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21,647][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.13407737016677856, acc: 0.976331353187561)
[2025-02-13 20:44:21,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22,058][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.07803906500339508, acc: 0.9888268113136292)
[2025-02-13 20:44:22,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22,446][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.12186703830957413, acc: 0.9735099077224731)
[2025-02-13 20:44:22,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22,831][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.5372998714447021, acc: 0.8702290058135986)
[2025-02-13 20:44:22,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23,206][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.13191154599189758, acc: 0.9530201554298401)
[2025-02-13 20:44:23,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23,622][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.17651820182800293, acc: 0.9712643623352051)
[2025-02-13 20:44:23,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23,989][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.09400659054517746, acc: 0.9805194735527039)
[2025-02-13 20:44:24,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24,383][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.132377490401268, acc: 0.9593023061752319)
[2025-02-13 20:44:24,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24,771][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.12399408966302872, acc: 0.9550561904907227)
[2025-02-13 20:44:24,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25,150][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.23545195162296295, acc: 0.9506173133850098)
[2025-02-13 20:44:25,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25,549][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.2236742228269577, acc: 0.9417475461959839)
[2025-02-13 20:44:25,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25,890][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.11012043803930283, acc: 0.9740259647369385)
[2025-02-13 20:44:26,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26,290][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.11631445586681366, acc: 0.9520000219345093)
[2025-02-13 20:44:26,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26,690][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.11089172214269638, acc: 0.9689922332763672)
[2025-02-13 20:44:26,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27,050][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.06449773162603378, acc: 0.9890710115432739)
[2025-02-13 20:44:27,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27,444][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.026612091809511185, acc: 1.0)
[2025-02-13 20:44:27,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27,839][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.15592890977859497, acc: 0.955974817276001)
[2025-02-13 20:44:27,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28,252][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.07916751503944397, acc: 0.9870967864990234)
[2025-02-13 20:44:28,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28,604][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.04685049504041672, acc: 0.9813664555549622)
[2025-02-13 20:44:28,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29,001][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.05097741633653641, acc: 0.9937499761581421)
[2025-02-13 20:44:29,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29,416][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.021386321634054184, acc: 0.9928571581840515)
[2025-02-13 20:44:29,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29,810][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.052841875702142715, acc: 0.9891892075538635)
[2025-02-13 20:44:29,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30,219][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.14756964147090912, acc: 0.9631901979446411)
[2025-02-13 20:44:30,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30,605][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.04469415917992592, acc: 0.9916666746139526)
[2025-02-13 20:44:30,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31,009][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.038894783705472946, acc: 0.9924812316894531)
[2025-02-13 20:44:31,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31,384][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.04715882986783981, acc: 0.9876543283462524)
[2025-02-13 20:44:31,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31,743][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.04666172340512276, acc: 0.9866666793823242)
[2025-02-13 20:44:31,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32,110][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.0514170341193676, acc: 1.0)
[2025-02-13 20:44:32,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32,476][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.13483920693397522, acc: 0.9766355156898499)
[2025-02-13 20:44:32,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32,842][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.2506229877471924, acc: 0.957317054271698)
[2025-02-13 20:44:32,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33,225][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.07829280942678452, acc: 0.9941176176071167)
[2025-02-13 20:44:33,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33,586][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.11637614667415619, acc: 0.9601989984512329)
[2025-02-13 20:44:33,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33,953][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.1535751074552536, acc: 0.9595375657081604)
[2025-02-13 20:44:34,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34,336][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.02225687727332115, acc: 1.0)
[2025-02-13 20:44:34,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34,710][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.04338138550519943, acc: 0.9884393215179443)
[2025-02-13 20:44:34,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35,078][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.04609064385294914, acc: 0.9774011373519897)
[2025-02-13 20:44:35,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35,444][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.08365286141633987, acc: 0.9783783555030823)
[2025-02-13 20:44:35,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35,797][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.04058290272951126, acc: 0.9920634627342224)
[2025-02-13 20:44:35,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36,165][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.060421381145715714, acc: 0.9842932224273682)
[2025-02-13 20:44:36,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36,509][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.035367757081985474, acc: 0.9934210777282715)
[2025-02-13 20:44:36,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36,877][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.03750324621796608, acc: 1.0)
[2025-02-13 20:44:37,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37,266][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.07814202457666397, acc: 0.988095223903656)
[2025-02-13 20:44:37,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37,634][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.043399594724178314, acc: 0.9850746393203735)
[2025-02-13 20:44:37,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38,026][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.2020447552204132, acc: 0.9530201554298401)
[2025-02-13 20:44:38,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38,417][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.094161257147789, acc: 0.9815950989723206)
[2025-02-13 20:44:38,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38,795][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.28019285202026367, acc: 0.9363057613372803)
[2025-02-13 20:44:38,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39,211][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.21008215844631195, acc: 0.9398496150970459)
[2025-02-13 20:44:39,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39,596][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.18329906463623047, acc: 0.9389312863349915)
[2025-02-13 20:44:39,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39,953][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.1292341649532318, acc: 0.9664429426193237)
[2025-02-13 20:44:40,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40,343][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.7207161784172058, acc: 0.9011628031730652)
[2025-02-13 20:44:40,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40,726][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.12867441773414612, acc: 0.960629940032959)
[2025-02-13 20:44:40,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41,100][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.2522728443145752, acc: 0.935251772403717)
[2025-02-13 20:44:41,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41,482][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.22480441629886627, acc: 0.9506173133850098)
[2025-02-13 20:44:41,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41,858][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.057033471763134, acc: 1.0)
[2025-02-13 20:44:42,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42,243][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.1458539366722107, acc: 0.982758641242981)
[2025-02-13 20:44:42,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42,639][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.1537766456604004, acc: 0.9363057613372803)
[2025-02-13 20:44:42,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43,025][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.11730163544416428, acc: 0.9668874144554138)
[2025-02-13 20:44:43,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43,413][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.04386119917035103, acc: 0.9939393997192383)
[2025-02-13 20:44:43,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43,794][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.15112978219985962, acc: 0.9577465057373047)
[2025-02-13 20:44:43,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44,181][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.05938965827226639, acc: 0.9873417615890503)
[2025-02-13 20:44:44,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44,536][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.03993147611618042, acc: 1.0)
[2025-02-13 20:44:44,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44,915][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.02814199961721897, acc: 1.0)
[2025-02-13 20:44:45,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45,282][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.030477125197649002, acc: 1.0)
[2025-02-13 20:44:45,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45,704][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.042921219021081924, acc: 0.9926470518112183)
[2025-02-13 20:44:45,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46,067][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.037987977266311646, acc: 0.9918032884597778)
[2025-02-13 20:44:46,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46,468][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.09142729640007019, acc: 0.9679999947547913)
[2025-02-13 20:44:46,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46,809][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.039028994739055634, acc: 1.0)
[2025-02-13 20:44:46,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47,150][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.02450210228562355, acc: 0.9893048405647278)
[2025-02-13 20:44:47,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47,517][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.13036689162254333, acc: 0.9904761910438538)
[2025-02-13 20:44:47,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47,903][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.06031017005443573, acc: 0.9934640526771545)
[2025-02-13 20:44:48,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48,249][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.044388964772224426, acc: 0.9929577708244324)
[2025-02-13 20:44:48,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48,641][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.09358090907335281, acc: 0.9917355179786682)
[2025-02-13 20:44:48,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48,980][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.01642702706158161, acc: 1.0)
[2025-02-13 20:44:49,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49,343][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.04886670038104057, acc: 0.9798657894134521)
[2025-02-13 20:44:49,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49,691][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.03412749990820885, acc: 1.0)
[2025-02-13 20:44:49,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50,058][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.04512461647391319, acc: 0.9878787994384766)
[2025-02-13 20:44:50,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50,516][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.12487442046403885, acc: 0.9694656729698181)
[2025-02-13 20:44:50,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50,883][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.03363504633307457, acc: 0.9922480583190918)
[2025-02-13 20:44:51,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51,229][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.03262357786297798, acc: 0.9919354915618896)
[2025-02-13 20:44:51,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51,597][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.07026195526123047, acc: 0.9738562107086182)
[2025-02-13 20:44:51,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51,974][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.02426350861787796, acc: 0.9939024448394775)
[2025-02-13 20:44:52,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52,320][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.1204693540930748, acc: 0.9679999947547913)
[2025-02-13 20:44:52,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52,672][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.052947476506233215, acc: 0.9897959232330322)
[2025-02-13 20:44:52,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53,037][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.15807273983955383, acc: 0.9694656729698181)
[2025-02-13 20:44:53,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53,382][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.032018501311540604, acc: 0.9924812316894531)
[2025-02-13 20:44:53,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53,757][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.089346744120121, acc: 0.9824561476707458)
[2025-02-13 20:44:53,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54,126][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.12089796364307404, acc: 0.9800000190734863)
[2025-02-13 20:44:54,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54,503][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.13348868489265442, acc: 0.9681528806686401)
[2025-02-13 20:44:54,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54,869][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.39793580770492554, acc: 0.926174521446228)
[2025-02-13 20:44:55,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55,246][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.14321453869342804, acc: 0.9784172773361206)
[2025-02-13 20:44:55,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55,641][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.13566698133945465, acc: 0.9710144996643066)
[2025-02-13 20:44:55,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56,045][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.027846362441778183, acc: 0.9930070042610168)
[2025-02-13 20:44:56,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56,398][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.15436406433582306, acc: 0.96875)
[2025-02-13 20:44:56,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56,798][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.08166088908910751, acc: 0.9917355179786682)
[2025-02-13 20:44:56,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57,190][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.11708793044090271, acc: 0.9821428656578064)
[2025-02-13 20:44:57,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57,610][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.07265938073396683, acc: 0.9759036302566528)
[2025-02-13 20:44:57,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57,947][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.084468312561512, acc: 0.9805194735527039)
[2025-02-13 20:44:58,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58,353][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.12656332552433014, acc: 0.988095223903656)
[2025-02-13 20:44:58,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58,732][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.0675370842218399, acc: 0.9819276928901672)
[2025-02-13 20:44:58,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59,120][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.07859174907207489, acc: 0.9864864945411682)
[2025-02-13 20:44:59,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59,497][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.09532803297042847, acc: 0.9740259647369385)
[2025-02-13 20:44:59,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59,854][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.08501412719488144, acc: 0.98591548204422)
[2025-02-13 20:44:59,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00,214][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.12133270502090454, acc: 0.9589040875434875)
[2025-02-13 20:45:00,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00,584][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.074346624314785, acc: 0.9790209531784058)
[2025-02-13 20:45:00,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00,959][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.13823066651821136, acc: 0.9719101190567017)
[2025-02-13 20:45:01,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01,318][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.06439854204654694, acc: 0.9846153855323792)
[2025-02-13 20:45:01,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01,666][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.06963984668254852, acc: 0.9830508232116699)
[2025-02-13 20:45:01,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02,030][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.12256600707769394, acc: 0.98591548204422)
[2025-02-13 20:45:02,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02,413][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.09339819103479385, acc: 0.9774436354637146)
[2025-02-13 20:45:02,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02,745][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.06035066768527031, acc: 0.9908257126808167)
[2025-02-13 20:45:02,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03,115][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.04278375953435898, acc: 1.0)
[2025-02-13 20:45:03,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03,447][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.06641245633363724, acc: 0.9886363744735718)
[2025-02-13 20:45:03,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03,802][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.026217781007289886, acc: 1.0)
[2025-02-13 20:45:03,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04,178][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.08044523000717163, acc: 0.9830508232116699)
[2025-02-13 20:45:04,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04,542][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.17665818333625793, acc: 0.9462365508079529)
[2025-02-13 20:45:04,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04,912][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.20065495371818542, acc: 0.9532163739204407)
[2025-02-13 20:45:05,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05,263][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.08323312550783157, acc: 0.9861111044883728)
[2025-02-13 20:45:05,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05,638][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.03776761516928673, acc: 0.9945054650306702)
[2025-02-13 20:45:05,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05,996][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.061868056654930115, acc: 0.9864864945411682)
[2025-02-13 20:45:06,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06,409][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.09494302421808243, acc: 0.9738562107086182)
[2025-02-13 20:45:06,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06,809][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.08373613655567169, acc: 0.9820359349250793)
[2025-02-13 20:45:06,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07,188][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.017421243712306023, acc: 1.0)
[2025-02-13 20:45:07,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07,575][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.027134567499160767, acc: 0.9816513657569885)
[2025-02-13 20:45:07,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07,970][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.03779500350356102, acc: 0.9937888383865356)
[2025-02-13 20:45:08,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08,350][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.13736554980278015, acc: 0.9625668525695801)
[2025-02-13 20:45:08,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08,721][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.048728715628385544, acc: 0.9841269850730896)
[2025-02-13 20:45:08,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09,044][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.070770263671875, acc: 0.9772727489471436)
[2025-02-13 20:45:09,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09,388][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.034719519317150116, acc: 0.9887640476226807)
[2025-02-13 20:45:09,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09,748][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.07515395432710648, acc: 0.9720670580863953)
[2025-02-13 20:45:09,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10,095][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.08339537680149078, acc: 0.9727272987365723)
[2025-02-13 20:45:10,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10,440][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.03473503515124321, acc: 0.9888888597488403)
[2025-02-13 20:45:10,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10,778][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.052385058254003525, acc: 0.9821428656578064)
[2025-02-13 20:45:10,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11,149][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.03981681168079376, acc: 0.9829059839248657)
[2025-02-13 20:45:11,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11,482][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.042244017124176025, acc: 0.9901960492134094)
[2025-02-13 20:45:11,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11,834][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.046350061893463135, acc: 0.9769230484962463)
[2025-02-13 20:45:11,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12,200][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.07329722493886948, acc: 0.9841269850730896)
[2025-02-13 20:45:12,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12,562][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.2590462863445282, acc: 0.9607843160629272)
[2025-02-13 20:45:12,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12,922][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.027001652866601944, acc: 0.988095223903656)
[2025-02-13 20:45:13,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13,292][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.04380057752132416, acc: 0.9833333492279053)
[2025-02-13 20:45:13,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13,658][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.1656198799610138, acc: 0.9645389914512634)
[2025-02-13 20:45:13,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13,951][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.05591706931591034, acc: 0.9807692170143127)
[2025-02-13 20:45:14,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14,322][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.027981458231806755, acc: 1.0)
[2025-02-13 20:45:14,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14,717][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.07241512089967728, acc: 0.9714285731315613)
[2025-02-13 20:45:14,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15,109][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.09437765926122665, acc: 0.9825581312179565)
[2025-02-13 20:45:15,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15,486][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.030120475217700005, acc: 0.9938650131225586)
[2025-02-13 20:45:15,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15,833][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.017090201377868652, acc: 1.0)
[2025-02-13 20:45:15,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16,183][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.04296098276972771, acc: 0.9895833134651184)
[2025-02-13 20:45:16,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16,555][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.06111101061105728, acc: 0.9937106966972351)
[2025-02-13 20:45:16,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16,939][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.0494692362844944, acc: 0.9886363744735718)
[2025-02-13 20:45:17,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17,311][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.03772859647870064, acc: 0.9821428656578064)
[2025-02-13 20:45:17,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17,674][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.020406194031238556, acc: 1.0)
[2025-02-13 20:45:17,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18,026][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.028773298487067223, acc: 0.9876543283462524)
[2025-02-13 20:45:18,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18,403][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.15813834965229034, acc: 0.969072163105011)
[2025-02-13 20:45:18,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18,808][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.02069060504436493, acc: 1.0)
[2025-02-13 20:45:18,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19,193][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.21120905876159668, acc: 0.9646017551422119)
[2025-02-13 20:45:19,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19,581][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.09911159425973892, acc: 0.9718309640884399)
[2025-02-13 20:45:19,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19,951][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.0996389165520668, acc: 0.9797297120094299)
[2025-02-13 20:45:20,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20,350][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.15962035953998566, acc: 0.9607843160629272)
[2025-02-13 20:45:20,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20,754][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.29114678502082825, acc: 0.9583333134651184)
[2025-02-13 20:45:20,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21,155][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.1877703219652176, acc: 0.9805194735527039)
[2025-02-13 20:45:21,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21,648][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.11693156510591507, acc: 0.9635036587715149)
[2025-02-13 20:45:21,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22,061][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.08902118355035782, acc: 0.9933775067329407)
[2025-02-13 20:45:22,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22,415][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.11670750379562378, acc: 0.9652174115180969)
[2025-02-13 20:45:22,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22,775][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.048082828521728516, acc: 0.9921875)
[2025-02-13 20:45:22,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23,181][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.07206002622842789, acc: 0.9940828680992126)
[2025-02-13 20:45:23,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23,560][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.06691277772188187, acc: 0.9785714149475098)
[2025-02-13 20:45:23,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23,948][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.08475705981254578, acc: 0.9923664331436157)
[2025-02-13 20:45:24,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24,335][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.1221461147069931, acc: 0.9602649211883545)
[2025-02-13 20:45:24,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24,692][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.03154784440994263, acc: 1.0)
[2025-02-13 20:45:24,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25,081][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.1631285399198532, acc: 0.9580419659614563)
[2025-02-13 20:45:25,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25,466][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.051810625940561295, acc: 0.9925373196601868)
[2025-02-13 20:45:25,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25,848][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.04943973571062088, acc: 0.9850746393203735)
[2025-02-13 20:45:25,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26,225][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.07861742377281189, acc: 0.982758641242981)
[2025-02-13 20:45:26,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26,616][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.3671710789203644, acc: 0.9294871687889099)
[2025-02-13 20:45:26,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27,013][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.119988813996315, acc: 0.9873417615890503)
[2025-02-13 20:45:27,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27,398][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.05426798760890961, acc: 0.9903846383094788)
[2025-02-13 20:45:27,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27,759][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.32499149441719055, acc: 0.9222221970558167)
[2025-02-13 20:45:27,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28,139][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.24815192818641663, acc: 0.9144737124443054)
[2025-02-13 20:45:28,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28,514][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.10727094113826752, acc: 0.9821428656578064)
[2025-02-13 20:45:28,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28,874][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.377356618642807, acc: 0.9459459185600281)
[2025-02-13 20:45:29,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29,249][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.18223974108695984, acc: 0.9435483813285828)
[2025-02-13 20:45:29,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29,635][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.2861422896385193, acc: 0.942148745059967)
[2025-02-13 20:45:29,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30,013][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.1320858597755432, acc: 0.9572649598121643)
[2025-02-13 20:45:30,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30,387][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.28373193740844727, acc: 0.9064748287200928)
[2025-02-13 20:45:30,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30,717][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.26149168610572815, acc: 0.9558823704719543)
[2025-02-13 20:45:30,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31,089][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.18911521136760712, acc: 0.9357143044471741)
[2025-02-13 20:45:31,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31,455][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.0443120114505291, acc: 0.9918032884597778)
[2025-02-13 20:45:31,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31,817][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.2782200872898102, acc: 0.9386503100395203)
[2025-02-13 20:45:31,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32,216][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.1409199982881546, acc: 0.9647887349128723)
[2025-02-13 20:45:32,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32,605][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.055703770369291306, acc: 0.9924812316894531)
[2025-02-13 20:45:32,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32,963][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.3565501570701599, acc: 0.931034505367279)
[2025-02-13 20:45:33,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33,352][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.14545166492462158, acc: 0.9627329111099243)
[2025-02-13 20:45:33,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33,724][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.08122209459543228, acc: 0.988095223903656)
[2025-02-13 20:45:33,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34,118][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.07046526670455933, acc: 0.984000027179718)
[2025-02-13 20:45:34,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34,494][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.0541003979742527, acc: 0.9815950989723206)
[2025-02-13 20:45:34,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34,945][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.1410195231437683, acc: 0.9744898080825806)
[2025-02-13 20:45:35,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35,382][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.024449670687317848, acc: 0.9940828680992126)
[2025-02-13 20:45:35,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35,752][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.09161624312400818, acc: 0.9652777910232544)
[2025-02-13 20:45:35,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36,124][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.17567075788974762, acc: 0.9567307829856873)
[2025-02-13 20:45:36,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36,485][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.1728142499923706, acc: 0.9512194991111755)
[2025-02-13 20:45:36,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36,848][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.04263051226735115, acc: 0.9888888597488403)
[2025-02-13 20:45:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37,215][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.11016152799129486, acc: 0.9702970385551453)
[2025-02-13 20:45:37,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37,585][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.17537692189216614, acc: 0.9523809552192688)
[2025-02-13 20:45:37,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37,969][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.41516613960266113, acc: 0.9342105388641357)
[2025-02-13 20:45:38,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38,343][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.17077206075191498, acc: 0.9622641801834106)
[2025-02-13 20:45:38,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38,703][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.03360394760966301, acc: 0.9900990128517151)
[2025-02-13 20:45:38,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39,024][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.14201267063617706, acc: 0.9814814925193787)
[2025-02-13 20:45:39,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39,375][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.15933701395988464, acc: 0.9750000238418579)
[2025-02-13 20:45:39,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39,695][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.16113008558750153, acc: 0.9756097793579102)
[2025-02-13 20:45:39,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40,084][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.1274590790271759, acc: 0.9657142758369446)
[2025-02-13 20:45:40,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40,458][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.16659711301326752, acc: 0.957446813583374)
[2025-02-13 20:45:40,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40,824][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.012163590639829636, acc: 1.0)
[2025-02-13 20:45:40,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41,193][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.14655090868473053, acc: 0.9722222089767456)
[2025-02-13 20:45:41,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41,534][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.04694569855928421, acc: 0.9862068891525269)
[2025-02-13 20:45:41,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41,882][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.04195944592356682, acc: 0.9942857027053833)
[2025-02-13 20:45:42,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42,266][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.028924310579895973, acc: 0.9921259880065918)
[2025-02-13 20:45:42,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42,641][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.07228849828243256, acc: 0.9807692170143127)
[2025-02-13 20:45:42,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43,018][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.07016085088253021, acc: 0.97826087474823)
[2025-02-13 20:45:43,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43,380][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.10994406789541245, acc: 0.9791666865348816)
[2025-02-13 20:45:43,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43,751][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.06345460563898087, acc: 0.9800000190734863)
[2025-02-13 20:45:43,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44,114][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.14027787744998932, acc: 0.9677419066429138)
[2025-02-13 20:45:44,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44,474][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.10134891420602798, acc: 0.9921875)
[2025-02-13 20:45:44,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44,842][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.03599518910050392, acc: 0.9894737005233765)
[2025-02-13 20:45:44,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45,191][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.1581386774778366, acc: 0.9754601120948792)
[2025-02-13 20:45:45,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45,608][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.15113839507102966, acc: 0.9657142758369446)
[2025-02-13 20:45:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45,995][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.032050199806690216, acc: 0.9926470518112183)
[2025-02-13 20:45:46,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46,424][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.0514381006360054, acc: 0.9878048896789551)
[2025-02-13 20:45:46,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46,867][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.03823540359735489, acc: 0.9869281053543091)
[2025-02-13 20:45:47,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47,295][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.10561908781528473, acc: 0.9700000286102295)
[2025-02-13 20:45:47,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47,657][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.10346818715333939, acc: 0.9813664555549622)
[2025-02-13 20:45:47,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48,021][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.11905892938375473, acc: 0.9894737005233765)
[2025-02-13 20:45:48,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48,412][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.06422680616378784, acc: 0.989130437374115)
[2025-02-13 20:45:48,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48,820][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.09741512686014175, acc: 0.9847715497016907)
[2025-02-13 20:45:48,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49,222][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.05805209279060364, acc: 0.9838709831237793)
[2025-02-13 20:45:49,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49,616][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.04680503159761429, acc: 0.9885057210922241)
[2025-02-13 20:45:49,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50,017][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.08985153585672379, acc: 0.9757575988769531)
[2025-02-13 20:45:50,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50,430][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.09425541758537292, acc: 0.9719101190567017)
[2025-02-13 20:45:50,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50,868][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.13663406670093536, acc: 0.9682539701461792)
[2025-02-13 20:45:51,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51,250][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.09990736097097397, acc: 0.9756097793579102)
[2025-02-13 20:45:51,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51,641][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.042908504605293274, acc: 0.9888888597488403)
[2025-02-13 20:45:51,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52,044][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.058253996074199677, acc: 0.9923076629638672)
[2025-02-13 20:45:52,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52,466][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.10275181382894516, acc: 0.9779005646705627)
[2025-02-13 20:45:52,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52,814][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.18643949925899506, acc: 0.9772727489471436)
[2025-02-13 20:45:52,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53,232][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.06716696172952652, acc: 0.9846938848495483)
[2025-02-13 20:45:53,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53,625][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.047572046518325806, acc: 0.9887640476226807)
[2025-02-13 20:45:53,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54,023][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.030725928023457527, acc: 0.9917355179786682)
[2025-02-13 20:45:54,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54,424][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.07705257087945938, acc: 0.9710982441902161)
[2025-02-13 20:45:54,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54,822][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.054715730249881744, acc: 0.9866666793823242)
[2025-02-13 20:45:54,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55,216][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.09781821072101593, acc: 0.9772727489471436)
[2025-02-13 20:45:55,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55,589][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.08667493611574173, acc: 0.9841269850730896)
[2025-02-13 20:45:55,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55,951][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.040292978286743164, acc: 0.990338146686554)
[2025-02-13 20:45:56,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56,272][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.06157553195953369, acc: 0.98591548204422)
[2025-02-13 20:45:56,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56,629][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.03582913428544998, acc: 1.0)
[2025-02-13 20:45:56,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56,973][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.11972996592521667, acc: 0.9669421315193176)
[2025-02-13 20:45:57,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57,390][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.08184432983398438, acc: 0.9740259647369385)
[2025-02-13 20:45:57,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57,775][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.211089625954628, acc: 0.9479768872261047)
[2025-02-13 20:45:57,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58,157][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.08961419016122818, acc: 0.9801324605941772)
[2025-02-13 20:45:58,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58,536][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.12077197432518005, acc: 0.9679999947547913)
[2025-02-13 20:45:58,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58,968][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.08635944128036499, acc: 0.970588207244873)
[2025-02-13 20:45:59,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59,349][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.10061025619506836, acc: 0.9720279574394226)
[2025-02-13 20:45:59,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59,756][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.10362236201763153, acc: 0.9849624037742615)
[2025-02-13 20:45:59,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00,123][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.1710634082555771, acc: 0.9620253443717957)
[2025-02-13 20:46:00,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00,499][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.19896775484085083, acc: 0.9642857313156128)
[2025-02-13 20:46:00,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00,863][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.15073974430561066, acc: 0.9515151381492615)
[2025-02-13 20:46:01,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01,239][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.18858665227890015, acc: 0.9438202381134033)
[2025-02-13 20:46:01,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01,625][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.14008034765720367, acc: 0.960629940032959)
[2025-02-13 20:46:01,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02,005][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.1386883556842804, acc: 0.954954981803894)
[2025-02-13 20:46:02,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02,356][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.06255688518285751, acc: 0.9729729890823364)
[2025-02-13 20:46:02,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02,656][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.04492553323507309, acc: 1.0)
[2025-02-13 20:46:02,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02,987][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.05318938195705414, acc: 1.0)
[2025-02-13 20:46:03,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03,310][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.03649863973259926, acc: 1.0)
[2025-02-13 20:46:03,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03,633][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.09912300109863281, acc: 0.9682539701461792)
[2025-02-13 20:46:03,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03,956][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.0192245040088892, acc: 1.0)
[2025-02-13 20:46:04,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04,308][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.06528038531541824, acc: 0.9605262875556946)
[2025-02-13 20:46:04,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04,633][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.1858777403831482, acc: 0.9772727489471436)
[2025-02-13 20:46:04,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04,972][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.011526866815984249, acc: 1.0)
[2025-02-13 20:46:05,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05,320][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.08271641284227371, acc: 0.9672130942344666)
[2025-02-13 20:46:05,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05,643][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.6080889105796814, acc: 0.8983050584793091)
[2025-02-13 20:46:05,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05,978][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.07914233207702637, acc: 0.9775280952453613)
[2025-02-13 20:46:06,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06,323][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.019407669082283974, acc: 1.0)
[2025-02-13 20:46:06,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06,691][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.07793937623500824, acc: 0.98591548204422)
[2025-02-13 20:46:06,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07,027][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.038299474865198135, acc: 0.9848484992980957)
[2025-02-13 20:46:07,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07,375][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.036683354526758194, acc: 1.0)
[2025-02-13 20:46:07,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07,718][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.06202688440680504, acc: 0.9900990128517151)
[2025-02-13 20:46:07,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08,079][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.08774933218955994, acc: 0.9696969985961914)
[2025-02-13 20:46:08,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08,390][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.19619891047477722, acc: 0.9552238583564758)
[2025-02-13 20:46:08,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08,773][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.28510981798171997, acc: 0.9389671087265015)
[2025-02-13 20:46:08,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09,159][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.3000239431858063, acc: 0.9382022619247437)
[2025-02-13 20:46:09,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09,552][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.14174233376979828, acc: 0.957446813583374)
[2025-02-13 20:46:09,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09,925][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.11362672597169876, acc: 0.9558823704719543)
[2025-02-13 20:46:10,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:10,302][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.2009718120098114, acc: 0.9414634108543396)
[2025-02-13 20:46:10,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:10,662][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.09940608590841293, acc: 0.9735682606697083)
[2025-02-13 20:46:10,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11,033][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.28974682092666626, acc: 0.9247311949729919)
[2025-02-13 20:46:11,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11,406][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.08936968445777893, acc: 0.963302731513977)
[2025-02-13 20:46:11,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11,773][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.12356185168027878, acc: 0.9731183052062988)
[2025-02-13 20:46:11,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12,157][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.10256526619195938, acc: 0.961904764175415)
[2025-02-13 20:46:12,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12,530][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.18520714342594147, acc: 0.9569377899169922)
[2025-02-13 20:46:12,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12,919][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.28114819526672363, acc: 0.965753436088562)
[2025-02-13 20:46:13,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13,298][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.2177201360464096, acc: 0.9523809552192688)
[2025-02-13 20:46:13,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13,689][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.12296576797962189, acc: 0.9512194991111755)
[2025-02-13 20:46:13,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14,055][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.07909964770078659, acc: 0.9650349617004395)
[2025-02-13 20:46:14,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14,424][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.24001272022724152, acc: 0.9306358098983765)
[2025-02-13 20:46:14,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14,813][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.2943098247051239, acc: 0.9034090638160706)
[2025-02-13 20:46:14,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15,204][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.19327403604984283, acc: 0.9599999785423279)
[2025-02-13 20:46:15,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15,594][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.20346154272556305, acc: 0.9469026327133179)
[2025-02-13 20:46:15,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15,972][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.1375165581703186, acc: 0.9637305736541748)
[2025-02-13 20:46:16,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16,328][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.09273747354745865, acc: 0.9675324559211731)
[2025-02-13 20:46:16,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16,694][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.19644127786159515, acc: 0.9819819927215576)
[2025-02-13 20:46:16,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17,047][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.02653900906443596, acc: 0.9870129823684692)
[2025-02-13 20:46:17,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17,412][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.017517924308776855, acc: 1.0)
[2025-02-13 20:46:17,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17,808][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.3272208571434021, acc: 0.9589040875434875)
[2025-02-13 20:46:17,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18,213][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.11312796920537949, acc: 0.9567901492118835)
[2025-02-13 20:46:18,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18,603][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.1427219659090042, acc: 0.9647887349128723)
[2025-02-13 20:46:18,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18,996][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.18055616319179535, acc: 0.9312169551849365)
[2025-02-13 20:46:19,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19,415][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.08367857336997986, acc: 0.991525411605835)
[2025-02-13 20:46:19,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19,806][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.19607090950012207, acc: 0.9440993666648865)
[2025-02-13 20:46:19,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20,161][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.12465004622936249, acc: 0.9644669890403748)
[2025-02-13 20:46:20,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20,550][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.16253887116909027, acc: 0.9481481313705444)
[2025-02-13 20:46:20,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20,947][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.1666908711194992, acc: 0.9407894611358643)
[2025-02-13 20:46:21,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21,429][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.4019966721534729, acc: 0.8702702522277832)
[2025-02-13 20:46:21,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21,836][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.233615443110466, acc: 0.929411768913269)
[2025-02-13 20:46:21,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22,329][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.17302453517913818, acc: 0.9485714435577393)
[2025-02-13 20:46:22,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22,664][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.1943311095237732, acc: 0.9467455744743347)
[2025-02-13 20:46:22,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23,007][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.24478088319301605, acc: 0.9166666865348816)
[2025-02-13 20:46:23,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23,381][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.177388533949852, acc: 0.9554139971733093)
[2025-02-13 20:46:23,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23,743][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.27876219153404236, acc: 0.9419354796409607)
[2025-02-13 20:46:23,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24,121][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.1616837978363037, acc: 0.9789473414421082)
[2025-02-13 20:46:24,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24,486][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.18070916831493378, acc: 0.9197860956192017)
[2025-02-13 20:46:24,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24,876][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.051066987216472626, acc: 0.9940828680992126)
[2025-02-13 20:46:25,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25,277][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.17896603047847748, acc: 0.9548022747039795)
[2025-02-13 20:46:25,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25,696][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.11120755225419998, acc: 0.9696969985961914)
[2025-02-13 20:46:25,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26,055][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.12450098991394043, acc: 0.977011501789093)
[2025-02-13 20:46:26,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26,421][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.15844844281673431, acc: 0.9594594836235046)
[2025-02-13 20:46:26,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26,783][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.0820627212524414, acc: 0.9731543660163879)
[2025-02-13 20:46:26,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27,162][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.05193394050002098, acc: 0.9896373152732849)
[2025-02-13 20:46:27,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27,525][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.13992486894130707, acc: 0.9800000190734863)
[2025-02-13 20:46:27,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27,915][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.09859946370124817, acc: 0.9722222089767456)
[2025-02-13 20:46:28,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28,310][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.07155539095401764, acc: 0.9836065769195557)
[2025-02-13 20:46:28,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28,684][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.14532943069934845, acc: 0.9607843160629272)
[2025-02-13 20:46:28,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29,048][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.16577504575252533, acc: 0.9513513445854187)
[2025-02-13 20:46:29,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29,404][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.0556269995868206, acc: 0.9828571677207947)
[2025-02-13 20:46:29,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29,800][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.07861596345901489, acc: 0.9839572310447693)
[2025-02-13 20:46:29,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30,176][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.032961901277303696, acc: 0.993630588054657)
[2025-02-13 20:46:30,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30,534][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.048397619277238846, acc: 0.9878787994384766)
[2025-02-13 20:46:30,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30,936][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.13007767498493195, acc: 0.9552238583564758)
[2025-02-13 20:46:31,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31,303][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.23159073293209076, acc: 0.9318181872367859)
[2025-02-13 20:46:31,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31,671][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.07149768620729446, acc: 0.9815950989723206)
[2025-02-13 20:46:31,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32,062][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.15942831337451935, acc: 0.9746192693710327)
[2025-02-13 20:46:32,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32,413][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.10921242833137512, acc: 0.9627329111099243)
[2025-02-13 20:46:32,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32,830][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.258530855178833, acc: 0.9415204524993896)
[2025-02-13 20:46:32,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33,263][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.1574489176273346, acc: 0.9387755393981934)
[2025-02-13 20:46:33,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33,665][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.20126710832118988, acc: 0.9743589758872986)
[2025-02-13 20:46:33,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34,038][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.1492862105369568, acc: 0.9583333134651184)
[2025-02-13 20:46:34,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34,394][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.17759189009666443, acc: 0.9375)
[2025-02-13 20:46:34,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34,759][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.051251914352178574, acc: 0.9909090995788574)
[2025-02-13 20:46:34,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35,115][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.20198465883731842, acc: 0.9662162065505981)
[2025-02-13 20:46:35,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35,495][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.33073610067367554, acc: 0.939393937587738)
[2025-02-13 20:46:35,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35,866][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.14024662971496582, acc: 0.9842519760131836)
[2025-02-13 20:46:36,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36,250][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.053393829613924026, acc: 0.9829059839248657)
[2025-02-13 20:46:36,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36,615][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.13009247183799744, acc: 0.9669421315193176)
[2025-02-13 20:46:36,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36,958][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.06525520980358124, acc: 0.9826086759567261)
[2025-02-13 20:46:37,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37,324][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.2140156328678131, acc: 0.953125)
[2025-02-13 20:46:37,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37,672][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.07446799427270889, acc: 0.989130437374115)
[2025-02-13 20:46:37,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38,043][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.13757479190826416, acc: 0.9738562107086182)
[2025-02-13 20:46:38,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38,419][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.10833729803562164, acc: 0.9695122241973877)
[2025-02-13 20:46:38,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38,776][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.1257968246936798, acc: 0.9860140085220337)
[2025-02-13 20:46:38,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39,159][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.06053352728486061, acc: 0.9919999837875366)
[2025-02-13 20:46:39,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39,504][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.22543178498744965, acc: 0.9596773982048035)
[2025-02-13 20:46:39,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39,886][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.14734584093093872, acc: 0.9580838084220886)
[2025-02-13 20:46:40,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40,268][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.04292857274413109, acc: 0.9841269850730896)
[2025-02-13 20:46:40,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40,667][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.0741698145866394, acc: 0.9837837815284729)
[2025-02-13 20:46:40,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41,030][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.049972787499427795, acc: 1.0)
[2025-02-13 20:46:41,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41,399][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.24449844658374786, acc: 0.9527559280395508)
[2025-02-13 20:46:41,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41,771][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.04490872099995613, acc: 0.9919354915618896)
[2025-02-13 20:46:41,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42,147][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.09218268096446991, acc: 0.9743589758872986)
[2025-02-13 20:46:42,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42,515][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.052633922547101974, acc: 0.9915966391563416)
[2025-02-13 20:46:42,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42,954][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.2286665439605713, acc: 0.9532710313796997)
[2025-02-13 20:46:43,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43,323][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.1472461074590683, acc: 0.9602649211883545)
[2025-02-13 20:46:43,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43,666][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.011267643421888351, acc: 1.0)
[2025-02-13 20:46:43,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44,021][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.09280113875865936, acc: 0.9831932783126831)
[2025-02-13 20:46:44,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44,393][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.19087301194667816, acc: 0.9585798978805542)
[2025-02-13 20:46:44,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44,743][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.13576656579971313, acc: 0.9626865386962891)
[2025-02-13 20:46:44,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45,120][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.25660139322280884, acc: 0.9320388436317444)
[2025-02-13 20:46:45,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45,475][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.09896999597549438, acc: 0.9743589758872986)
[2025-02-13 20:46:45,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45,855][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.07429881393909454, acc: 0.9855769276618958)
[2025-02-13 20:46:45,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46,206][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.20518143475055695, acc: 0.9421965479850769)
[2025-02-13 20:46:46,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46,575][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.05428113788366318, acc: 0.9922480583190918)
[2025-02-13 20:46:46,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46,930][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.16060464084148407, acc: 0.970059871673584)
[2025-02-13 20:46:47,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47,274][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.13606256246566772, acc: 0.9740259647369385)
[2025-02-13 20:46:47,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47,624][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.14582844078540802, acc: 0.9693251252174377)
[2025-02-13 20:46:47,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47,973][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.14802473783493042, acc: 0.9523809552192688)
[2025-02-13 20:46:48,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48,357][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.11664316058158875, acc: 0.9923076629638672)
[2025-02-13 20:46:48,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48,699][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.20294693112373352, acc: 0.95652174949646)
[2025-02-13 20:46:48,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49,078][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.08300882577896118, acc: 0.9885714054107666)
[2025-02-13 20:46:49,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49,448][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.13368935883045197, acc: 0.9696969985961914)
[2025-02-13 20:46:49,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49,849][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.13336101174354553, acc: 0.965753436088562)
[2025-02-13 20:46:50,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50,281][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.2405620664358139, acc: 0.9398906826972961)
[2025-02-13 20:46:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50,627][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.14854839444160461, acc: 0.9638554453849792)
[2025-02-13 20:46:50,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50,996][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.10332141071557999, acc: 0.9842519760131836)
[2025-02-13 20:46:51,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51,377][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.1029958575963974, acc: 0.9724137783050537)
[2025-02-13 20:46:51,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51,751][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.1337565779685974, acc: 0.9867549538612366)
[2025-02-13 20:46:51,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52,126][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.05382421240210533, acc: 0.9860140085220337)
[2025-02-13 20:46:52,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52,464][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.05250518396496773, acc: 0.993630588054657)
[2025-02-13 20:46:52,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52,841][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.13626180589199066, acc: 0.9828571677207947)
[2025-02-13 20:46:52,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53,220][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.11133549362421036, acc: 0.9814814925193787)
[2025-02-13 20:46:53,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53,605][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.11999788880348206, acc: 0.9723756909370422)
[2025-02-13 20:46:53,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53,963][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.12952809035778046, acc: 0.976331353187561)
[2025-02-13 20:46:54,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54,417][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.09357836842536926, acc: 0.9683544039726257)
[2025-02-13 20:46:54,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54,821][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.046000346541404724, acc: 0.9936708807945251)
[2025-02-13 20:46:54,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55,189][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.13961637020111084, acc: 0.9736841917037964)
[2025-02-13 20:46:55,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55,559][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.09323502331972122, acc: 0.988095223903656)
[2025-02-13 20:46:55,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55,941][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.21265427768230438, acc: 0.9363057613372803)
[2025-02-13 20:46:56,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56,318][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.18450041115283966, acc: 0.9370078444480896)
[2025-02-13 20:46:56,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56,680][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.30177247524261475, acc: 0.9487179517745972)
[2025-02-13 20:46:56,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57,069][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.2400030791759491, acc: 0.9379844665527344)
[2025-02-13 20:46:57,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57,436][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.12954068183898926, acc: 0.9817073345184326)
[2025-02-13 20:46:57,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57,822][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.13212604820728302, acc: 0.9694656729698181)
[2025-02-13 20:46:57,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58,192][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.07177933305501938, acc: 0.9927007555961609)
[2025-02-13 20:46:58,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58,548][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.11223948001861572, acc: 0.9767441749572754)
[2025-02-13 20:46:58,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58,930][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.16785679757595062, acc: 0.9731183052062988)
[2025-02-13 20:46:59,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59,276][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.13833796977996826, acc: 0.9819276928901672)
[2025-02-13 20:46:59,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59,631][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.0863272100687027, acc: 0.9640287756919861)
[2025-02-13 20:46:59,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59,999][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.06331789493560791, acc: 0.9890109896659851)
[2025-02-13 20:47:00,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00,340][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.09008540213108063, acc: 0.984375)
[2025-02-13 20:47:00,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00,688][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.06451287120580673, acc: 0.9679487347602844)
[2025-02-13 20:47:00,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01,059][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.024174174293875694, acc: 0.9939393997192383)
[2025-02-13 20:47:01,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01,423][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.10739985853433609, acc: 0.9691358208656311)
[2025-02-13 20:47:01,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01,853][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.08182985335588455, acc: 0.9817073345184326)
[2025-02-13 20:47:01,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02,226][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.04733825474977493, acc: 0.9945054650306702)
[2025-02-13 20:47:02,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02,596][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.0713554248213768, acc: 0.9803921580314636)
[2025-02-13 20:47:02,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02,961][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.17494326829910278, acc: 0.95333331823349)
[2025-02-13 20:47:03,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03,308][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.335360050201416, acc: 0.9468085169792175)
[2025-02-13 20:47:03,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03,726][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.25833019614219666, acc: 0.9583333134651184)
[2025-02-13 20:47:03,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04,138][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.1894405633211136, acc: 0.9457364082336426)
[2025-02-13 20:47:04,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04,542][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.3304847776889801, acc: 0.9197530746459961)
[2025-02-13 20:47:04,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04,937][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.278190553188324, acc: 0.9304347634315491)
[2025-02-13 20:47:05,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05,320][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.246608704328537, acc: 0.9516128897666931)
[2025-02-13 20:47:05,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05,671][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.10358776897192001, acc: 0.9694656729698181)
[2025-02-13 20:47:05,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06,051][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.11209653317928314, acc: 0.9766082167625427)
[2025-02-13 20:47:06,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06,411][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.19433286786079407, acc: 0.9673202633857727)
[2025-02-13 20:47:06,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06,832][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.15359553694725037, acc: 0.9510489702224731)
[2025-02-13 20:47:06,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07,229][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.14296954870224, acc: 0.9714285731315613)
[2025-02-13 20:47:07,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07,592][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.14205273985862732, acc: 0.9593495726585388)
[2025-02-13 20:47:07,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07,965][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.0876414030790329, acc: 0.9707317352294922)
[2025-02-13 20:47:08,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08,343][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.11657974869012833, acc: 0.9646464586257935)
[2025-02-13 20:47:08,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08,714][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.05858490616083145, acc: 0.9918032884597778)
[2025-02-13 20:47:08,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09,068][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.08710361272096634, acc: 0.9716981053352356)
[2025-02-13 20:47:09,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09,430][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.05606059730052948, acc: 0.9814814925193787)
[2025-02-13 20:47:09,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09,815][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.16629911959171295, acc: 0.9715909361839294)
[2025-02-13 20:47:09,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10,182][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.10205206274986267, acc: 0.9646017551422119)
[2025-02-13 20:47:10,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10,549][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.05792485177516937, acc: 0.9894179701805115)
[2025-02-13 20:47:10,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10,881][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.07123562693595886, acc: 0.9846153855323792)
[2025-02-13 20:47:11,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11,237][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.1276313215494156, acc: 0.9665071964263916)
[2025-02-13 20:47:11,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11,557][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.08983160555362701, acc: 0.961904764175415)
[2025-02-13 20:47:11,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11,909][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.08021274209022522, acc: 0.9767441749572754)
[2025-02-13 20:47:12,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12,261][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.11756919324398041, acc: 0.9916666746139526)
[2025-02-13 20:47:12,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12,627][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.08987348526716232, acc: 0.9928057789802551)
[2025-02-13 20:47:12,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13,010][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.028600165620446205, acc: 1.0)
[2025-02-13 20:47:13,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13,404][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.28257834911346436, acc: 0.939393937587738)
[2025-02-13 20:47:13,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13,741][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.0942830741405487, acc: 0.984375)
[2025-02-13 20:47:13,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14,086][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.06530624628067017, acc: 0.9940476417541504)
[2025-02-13 20:47:14,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14,482][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.14256450533866882, acc: 0.9679144620895386)
[2025-02-13 20:47:14,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14,891][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.12782317399978638, acc: 0.9575757384300232)
[2025-02-13 20:47:15,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15,256][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.066579669713974, acc: 0.9743589758872986)
[2025-02-13 20:47:15,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15,601][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.040531571954488754, acc: 0.9909090995788574)
[2025-02-13 20:47:15,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15,938][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.0666302740573883, acc: 0.9887005686759949)
[2025-02-13 20:47:16,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16,306][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.09789345413446426, acc: 0.9758453965187073)
[2025-02-13 20:47:16,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16,652][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.10079497843980789, acc: 0.9897959232330322)
[2025-02-13 20:47:16,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17,008][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.045001156628131866, acc: 0.9873417615890503)
[2025-02-13 20:47:17,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17,387][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.0397220104932785, acc: 0.9870967864990234)
[2025-02-13 20:47:17,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17,749][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.07315216213464737, acc: 0.9869281053543091)
[2025-02-13 20:47:17,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18,102][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.06114772707223892, acc: 0.977011501789093)
[2025-02-13 20:47:18,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18,467][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.02615790255367756, acc: 0.9925373196601868)
[2025-02-13 20:47:18,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18,809][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.1423056572675705, acc: 0.9638554453849792)
[2025-02-13 20:47:18,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19,184][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.2959073483943939, acc: 0.9479166865348816)
[2025-02-13 20:47:19,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19,553][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.08135972917079926, acc: 0.9729729890823364)
[2025-02-13 20:47:19,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19,926][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.11893455684185028, acc: 0.9720670580863953)
[2025-02-13 20:47:20,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20,290][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.12542106211185455, acc: 0.97826087474823)
[2025-02-13 20:47:20,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20,712][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.046948306262493134, acc: 0.987500011920929)
[2025-02-13 20:47:20,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21,081][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.12219639122486115, acc: 0.978723406791687)
[2025-02-13 20:47:21,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21,449][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.20635709166526794, acc: 0.9414634108543396)
[2025-02-13 20:47:21,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21,833][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.1688329130411148, acc: 0.9734042286872864)
[2025-02-13 20:47:21,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22,153][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.0716715008020401, acc: 0.9766082167625427)
[2025-02-13 20:47:22,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22,545][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.09704241901636124, acc: 0.9670329689979553)
[2025-02-13 20:47:22,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22,938][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.06029786914587021, acc: 0.9942528605461121)
[2025-02-13 20:47:23,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23,300][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.24203038215637207, acc: 0.9388889074325562)
[2025-02-13 20:47:23,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23,678][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.0685478150844574, acc: 0.976190447807312)
[2025-02-13 20:47:23,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24,060][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.1364632248878479, acc: 0.9664804339408875)
[2025-02-13 20:47:24,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24,457][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.200382798910141, acc: 0.9491525292396545)
[2025-02-13 20:47:24,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24,816][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.08039751648902893, acc: 0.9735099077224731)
[2025-02-13 20:47:24,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25,210][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.08833660185337067, acc: 0.9740932583808899)
[2025-02-13 20:47:25,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25,581][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.01669553481042385, acc: 1.0)
[2025-02-13 20:47:25,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25,976][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.05745571106672287, acc: 0.9933333396911621)
[2025-02-13 20:47:26,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26,350][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.041974157094955444, acc: 0.9931972622871399)
[2025-02-13 20:47:26,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26,724][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.09936024248600006, acc: 0.9798657894134521)
[2025-02-13 20:47:26,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27,100][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.08568377047777176, acc: 0.9820359349250793)
[2025-02-13 20:47:27,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27,468][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.0865597277879715, acc: 0.976331353187561)
[2025-02-13 20:47:27,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27,860][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.04636310786008835, acc: 0.9839572310447693)
[2025-02-13 20:47:28,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28,257][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.11330699175596237, acc: 0.9728260636329651)
[2025-02-13 20:47:28,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28,677][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.22999726235866547, acc: 0.9580838084220886)
[2025-02-13 20:47:28,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29,045][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.026836354285478592, acc: 0.9933775067329407)
[2025-02-13 20:47:29,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29,414][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.00955122709274292, acc: 1.0)
[2025-02-13 20:47:29,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29,820][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.1527872085571289, acc: 0.9536423683166504)
[2025-02-13 20:47:29,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30,244][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.009332169778645039, acc: 1.0)
[2025-02-13 20:47:30,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30,696][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.061954565346241, acc: 0.9720279574394226)
[2025-02-13 20:47:30,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31,081][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.0669860690832138, acc: 0.9850746393203735)
[2025-02-13 20:47:31,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31,487][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.08648475259542465, acc: 0.9829545617103577)
[2025-02-13 20:47:31,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31,881][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.024859590455889702, acc: 1.0)
[2025-02-13 20:47:32,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32,264][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.031658221036195755, acc: 0.9940828680992126)
[2025-02-13 20:47:32,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32,656][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.04047425463795662, acc: 0.9866666793823242)
[2025-02-13 20:47:32,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33,051][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.026074159890413284, acc: 0.9937888383865356)
[2025-02-13 20:47:33,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33,407][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.028599854558706284, acc: 0.993630588054657)
[2025-02-13 20:47:33,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33,799][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.06879109889268875, acc: 0.9875776171684265)
[2025-02-13 20:47:33,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34,177][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.03313518315553665, acc: 0.9876543283462524)
[2025-02-13 20:47:34,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34,564][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.021045608446002007, acc: 0.9935897588729858)
[2025-02-13 20:47:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34,935][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.022011125460267067, acc: 1.0)
[2025-02-13 20:47:35,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35,308][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.03043695166707039, acc: 0.9938650131225586)
[2025-02-13 20:47:35,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35,664][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.013655963353812695, acc: 1.0)
[2025-02-13 20:47:35,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36,028][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.02536744251847267, acc: 0.9934210777282715)
[2025-02-13 20:47:36,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36,413][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.05007852241396904, acc: 0.9751552939414978)
[2025-02-13 20:47:36,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36,789][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.013864638283848763, acc: 1.0)
[2025-02-13 20:47:36,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37,141][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.00795657653361559, acc: 1.0)
[2025-02-13 20:47:37,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37,510][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.0683133527636528, acc: 0.9886363744735718)
[2025-02-13 20:47:37,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37,895][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.01236842293292284, acc: 0.9938650131225586)
[2025-02-13 20:47:38,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38,277][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.011164121329784393, acc: 1.0)
[2025-02-13 20:47:38,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38,644][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.012684876099228859, acc: 1.0)
[2025-02-13 20:47:38,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39,010][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.06781191378831863, acc: 0.9798657894134521)
[2025-02-13 20:47:39,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39,364][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.05218241363763809, acc: 0.9837398529052734)
[2025-02-13 20:47:39,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39,724][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.08634044229984283, acc: 0.9716981053352356)
[2025-02-13 20:47:39,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40,085][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.02472289465367794, acc: 1.0)
[2025-02-13 20:47:40,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40,436][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.059075720608234406, acc: 0.9846153855323792)
[2025-02-13 20:47:40,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40,804][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.0636662170290947, acc: 0.9776119589805603)
[2025-02-13 20:47:40,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41,171][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.03348628804087639, acc: 0.9852941036224365)
[2025-02-13 20:47:41,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41,538][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.04856690391898155, acc: 0.9861111044883728)
[2025-02-13 20:47:41,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41,894][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.029969796538352966, acc: 0.991304337978363)
[2025-02-13 20:47:42,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42,266][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.09522422403097153, acc: 0.9727272987365723)
[2025-02-13 20:47:42,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42,624][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.10450190305709839, acc: 0.9684210419654846)
[2025-02-13 20:47:42,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42,983][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.026482580229640007, acc: 0.9930555820465088)
[2025-02-13 20:47:43,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43,349][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.0182021651417017, acc: 1.0)
[2025-02-13 20:47:43,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43,701][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.015002365224063396, acc: 1.0)
[2025-02-13 20:47:43,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44,068][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.06323818117380142, acc: 0.9924242496490479)
[2025-02-13 20:47:44,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44,440][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.03794654831290245, acc: 0.9925373196601868)
[2025-02-13 20:47:44,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44,870][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.015339692123234272, acc: 1.0)
[2025-02-13 20:47:45,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45,246][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.04619434475898743, acc: 1.0)
[2025-02-13 20:47:45,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45,641][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.09669095277786255, acc: 0.9818181991577148)
[2025-02-13 20:47:45,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46,085][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.0768246203660965, acc: 0.9736841917037964)
[2025-02-13 20:47:46,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46,460][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.04383116215467453, acc: 1.0)
[2025-02-13 20:47:46,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46,809][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.21934039890766144, acc: 0.9496402740478516)
[2025-02-13 20:47:46,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47,141][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.08757142722606659, acc: 0.9624060392379761)
[2025-02-13 20:47:47,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47,536][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.11304126679897308, acc: 0.9722222089767456)
[2025-02-13 20:47:47,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47,890][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.09425206482410431, acc: 0.9868420958518982)
[2025-02-13 20:47:48,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48,252][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.08436665683984756, acc: 0.9736841917037964)
[2025-02-13 20:47:48,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48,633][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.022293126210570335, acc: 1.0)
[2025-02-13 20:47:48,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48,993][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.019792985171079636, acc: 0.994413435459137)
[2025-02-13 20:47:49,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49,375][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.032616861164569855, acc: 0.9910314083099365)
[2025-02-13 20:47:49,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49,740][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.02693331427872181, acc: 0.9886363744735718)
[2025-02-13 20:47:49,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50,092][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.09318053722381592, acc: 0.9794520735740662)
[2025-02-13 20:47:50,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50,465][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.021388627588748932, acc: 0.9905213117599487)
[2025-02-13 20:47:50,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50,833][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.11716089397668839, acc: 0.9770992398262024)
[2025-02-13 20:47:50,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51,216][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.09652931243181229, acc: 0.9792746305465698)
[2025-02-13 20:47:51,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51,580][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.04502752795815468, acc: 0.9881656765937805)
[2025-02-13 20:47:51,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51,949][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.07643579691648483, acc: 0.9649122953414917)
[2025-02-13 20:47:52,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52,331][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.04759281873703003, acc: 0.9895833134651184)
[2025-02-13 20:47:52,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52,747][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.030222421512007713, acc: 1.0)
[2025-02-13 20:47:52,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53,140][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.04656127095222473, acc: 0.9837837815284729)
[2025-02-13 20:47:53,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53,529][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.032525334507226944, acc: 0.9946236610412598)
[2025-02-13 20:47:53,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53,891][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.14174020290374756, acc: 0.9664429426193237)
[2025-02-13 20:47:54,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54,236][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.027030693367123604, acc: 0.9931034445762634)
[2025-02-13 20:47:54,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54,627][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.01912715658545494, acc: 1.0)
[2025-02-13 20:47:54,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54,986][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.021511230617761612, acc: 0.9934210777282715)
[2025-02-13 20:47:55,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55,350][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.06363918632268906, acc: 0.9822221994400024)
[2025-02-13 20:47:55,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55,739][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.026638440787792206, acc: 0.9932432174682617)
[2025-02-13 20:47:55,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56,119][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.03630360588431358, acc: 0.9904761910438538)
[2025-02-13 20:47:56,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56,489][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.036020588129758835, acc: 0.987261176109314)
[2025-02-13 20:47:56,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56,863][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.053609319031238556, acc: 0.987500011920929)
[2025-02-13 20:47:57,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57,252][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.13014698028564453, acc: 0.9672130942344666)
[2025-02-13 20:47:57,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57,608][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.06985335797071457, acc: 0.9838709831237793)
[2025-02-13 20:47:57,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57,976][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.06781662255525589, acc: 0.9811320900917053)
[2025-02-13 20:47:58,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58,350][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.03338073194026947, acc: 0.9890109896659851)
[2025-02-13 20:47:58,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58,719][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.09093764424324036, acc: 0.9813664555549622)
[2025-02-13 20:47:58,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59,087][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.1062990203499794, acc: 0.9871794581413269)
[2025-02-13 20:47:59,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59,490][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.04291175305843353, acc: 0.9905660152435303)
[2025-02-13 20:47:59,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59,884][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.01058546919375658, acc: 1.0)
[2025-02-13 20:48:00,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00,267][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.14518465101718903, acc: 0.9716312289237976)
[2025-02-13 20:48:00,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00,634][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.0795050859451294, acc: 0.9712643623352051)
[2025-02-13 20:48:00,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00,985][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.031369660049676895, acc: 0.9926470518112183)
[2025-02-13 20:48:01,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01,352][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.010250928811728954, acc: 1.0)
[2025-02-13 20:48:01,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01,712][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.061586491763591766, acc: 0.9824561476707458)
[2025-02-13 20:48:01,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02,081][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.03326263651251793, acc: 0.9930070042610168)
[2025-02-13 20:48:02,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02,451][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.03118506632745266, acc: 0.9937106966972351)
[2025-02-13 20:48:02,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02,786][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.05307633429765701, acc: 0.9869281053543091)
[2025-02-13 20:48:02,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03,167][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.025081822648644447, acc: 0.9938650131225586)
[2025-02-13 20:48:03,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03,605][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.048272524029016495, acc: 0.9842519760131836)
[2025-02-13 20:48:03,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03,998][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.06694607436656952, acc: 0.9826589822769165)
[2025-02-13 20:48:04,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04,386][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.04796016588807106, acc: 0.9931507110595703)
[2025-02-13 20:48:04,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04,778][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.11503107845783234, acc: 0.9834254384040833)
[2025-02-13 20:48:04,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05,164][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.04222872108221054, acc: 0.9945651888847351)
[2025-02-13 20:48:05,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05,522][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.18535345792770386, acc: 0.9651162624359131)
[2025-02-13 20:48:05,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05,863][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.1546480804681778, acc: 0.9411764740943909)
[2025-02-13 20:48:06,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06,243][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.030123084783554077, acc: 0.9930555820465088)
[2025-02-13 20:48:06,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06,613][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.04244699701666832, acc: 0.9848484992980957)
[2025-02-13 20:48:06,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06,987][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.1169368326663971, acc: 0.9735449552536011)
[2025-02-13 20:48:07,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07,364][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.10636741667985916, acc: 0.970059871673584)
[2025-02-13 20:48:07,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07,758][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.20404282212257385, acc: 0.9595375657081604)
[2025-02-13 20:48:07,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08,140][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.10695762932300568, acc: 0.9754601120948792)
[2025-02-13 20:48:08,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08,501][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.030786432325839996, acc: 1.0)
[2025-02-13 20:48:08,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08,919][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.12190602719783783, acc: 0.9800000190734863)
[2025-02-13 20:48:09,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09,347][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.04401051998138428, acc: 0.9808917045593262)
[2025-02-13 20:48:09,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09,713][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.05138276889920235, acc: 0.9878787994384766)
[2025-02-13 20:48:09,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10,096][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.0690394714474678, acc: 0.9824561476707458)
[2025-02-13 20:48:10,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10,495][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.13813795149326324, acc: 0.9832402467727661)
[2025-02-13 20:48:10,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10,855][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.07869362831115723, acc: 0.9727891087532043)
[2025-02-13 20:48:10,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11,244][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.0676577240228653, acc: 0.9928057789802551)
[2025-02-13 20:48:11,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11,618][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.053441841155290604, acc: 0.9806451797485352)
[2025-02-13 20:48:11,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11,981][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.076141856610775, acc: 0.9881656765937805)
[2025-02-13 20:48:12,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12,382][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.051206666976213455, acc: 0.9808917045593262)
[2025-02-13 20:48:12,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12,767][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.0361582413315773, acc: 0.991525411605835)
[2025-02-13 20:48:12,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13,157][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.12096896022558212, acc: 0.9704142212867737)
[2025-02-13 20:48:13,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13,548][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.05503065884113312, acc: 0.9922480583190918)
[2025-02-13 20:48:13,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13,942][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.07125040143728256, acc: 0.9698795080184937)
[2025-02-13 20:48:14,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14,330][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.09011659771203995, acc: 0.9702380895614624)
[2025-02-13 20:48:14,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14,756][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.172218456864357, acc: 0.9593023061752319)
[2025-02-13 20:48:14,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15,129][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.15853703022003174, acc: 0.9607843160629272)
[2025-02-13 20:48:15,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15,484][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.15218010544776917, acc: 0.982758641242981)
[2025-02-13 20:48:15,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15,842][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.3064250946044922, acc: 0.9677419066429138)
[2025-02-13 20:48:15,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16,224][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.08918382972478867, acc: 0.9663865566253662)
[2025-02-13 20:48:16,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16,596][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.12996168434619904, acc: 0.9613259434700012)
[2025-02-13 20:48:16,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16,920][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.06970565766096115, acc: 0.9837398529052734)
[2025-02-13 20:48:17,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17,271][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.03777536004781723, acc: 0.9919999837875366)
[2025-02-13 20:48:17,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17,649][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.13282720744609833, acc: 0.9674418568611145)
[2025-02-13 20:48:17,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18,019][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.22074230015277863, acc: 0.9684210419654846)
[2025-02-13 20:48:18,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18,405][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.12205418199300766, acc: 0.9624999761581421)
[2025-02-13 20:48:18,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18,776][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.02273492142558098, acc: 1.0)
[2025-02-13 20:48:18,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19,156][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.03267921879887581, acc: 0.9945054650306702)
[2025-02-13 20:48:19,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19,523][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.19249393045902252, acc: 0.9595375657081604)
[2025-02-13 20:48:19,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19,891][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.053412310779094696, acc: 0.9936708807945251)
[2025-02-13 20:48:20,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20,268][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.18038320541381836, acc: 0.9853658676147461)
[2025-02-13 20:48:20,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20,654][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.057331427931785583, acc: 0.9892473220825195)
[2025-02-13 20:48:20,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21,023][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.04909689351916313, acc: 0.9939393997192383)
[2025-02-13 20:48:21,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21,393][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.09119461476802826, acc: 0.9810126423835754)
[2025-02-13 20:48:21,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21,772][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.1251584142446518, acc: 0.9615384340286255)
[2025-02-13 20:48:21,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22,149][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.06722979992628098, acc: 0.9826086759567261)
[2025-02-13 20:48:22,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22,496][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.1443108171224594, acc: 0.9593023061752319)
[2025-02-13 20:48:22,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22,860][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.13707105815410614, acc: 0.9652174115180969)
[2025-02-13 20:48:23,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23,233][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.1197972297668457, acc: 0.9814814925193787)
[2025-02-13 20:48:23,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23,603][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.1902473270893097, acc: 0.9666666388511658)
[2025-02-13 20:48:23,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23,965][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.09121093899011612, acc: 0.9851484894752502)
[2025-02-13 20:48:24,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24,393][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.08312695473432541, acc: 0.9760765433311462)
[2025-02-13 20:48:24,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24,813][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.05771489441394806, acc: 0.9850746393203735)
[2025-02-13 20:48:24,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25,186][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.05585912615060806, acc: 0.9931034445762634)
[2025-02-13 20:48:25,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25,554][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.07949736714363098, acc: 0.9887640476226807)
[2025-02-13 20:48:25,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25,911][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.022002605721354485, acc: 1.0)
[2025-02-13 20:48:26,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26,318][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.06635664403438568, acc: 0.9904761910438538)
[2025-02-13 20:48:26,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26,692][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.14015063643455505, acc: 0.9696969985961914)
[2025-02-13 20:48:26,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27,053][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.030099356546998024, acc: 0.9948186278343201)
[2025-02-13 20:48:27,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27,432][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.046106934547424316, acc: 0.9881656765937805)
[2025-02-13 20:48:27,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27,875][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.06335413455963135, acc: 0.9855769276618958)
[2025-02-13 20:48:28,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28,258][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.11756708472967148, acc: 0.9649999737739563)
[2025-02-13 20:48:28,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28,652][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.048032570630311966, acc: 0.9888888597488403)
[2025-02-13 20:48:28,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29,059][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.1297396719455719, acc: 0.9551281929016113)
[2025-02-13 20:48:29,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29,473][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.11693218350410461, acc: 0.9878048896789551)
[2025-02-13 20:48:29,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29,973][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.18191081285476685, acc: 0.9416666626930237)
[2025-02-13 20:48:30,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30,415][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.1285821497440338, acc: 0.9689922332763672)
[2025-02-13 20:48:30,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30,806][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.03749025613069534, acc: 0.9935483932495117)
[2025-02-13 20:48:30,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31,190][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.17990568280220032, acc: 0.9731543660163879)
[2025-02-13 20:48:31,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31,573][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.05523082986474037, acc: 0.9868420958518982)
[2025-02-13 20:48:31,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31,957][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.07605155557394028, acc: 0.9772727489471436)
[2025-02-13 20:48:32,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32,344][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.1280083954334259, acc: 0.9637681245803833)
[2025-02-13 20:48:32,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32,721][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.22689023613929749, acc: 0.9534883499145508)
[2025-02-13 20:48:32,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33,094][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.2767059803009033, acc: 0.9328858852386475)
[2025-02-13 20:48:33,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33,471][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.2744019329547882, acc: 0.9285714030265808)
[2025-02-13 20:48:33,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33,843][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.12321346253156662, acc: 0.9683544039726257)
[2025-02-13 20:48:34,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34,297][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.18027682602405548, acc: 0.9638554453849792)
[2025-02-13 20:48:34,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34,740][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.14051736891269684, acc: 0.9664804339408875)
[2025-02-13 20:48:34,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35,131][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.1769413948059082, acc: 0.9615384340286255)
[2025-02-13 20:48:35,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35,508][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.13774770498275757, acc: 0.9627659320831299)
[2025-02-13 20:48:35,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35,951][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.0679839625954628, acc: 0.9832402467727661)
[2025-02-13 20:48:36,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36,376][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.1054275631904602, acc: 0.9776536226272583)
[2025-02-13 20:48:36,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36,769][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.11556800454854965, acc: 0.9681528806686401)
[2025-02-13 20:48:36,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37,145][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.05378594994544983, acc: 0.993630588054657)
[2025-02-13 20:48:37,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37,578][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.07473713904619217, acc: 0.9806451797485352)
[2025-02-13 20:48:37,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37,956][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.2346418797969818, acc: 0.9624999761581421)
[2025-02-13 20:48:38,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38,296][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.18094509840011597, acc: 0.9459459185600281)
[2025-02-13 20:48:38,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38,639][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.5721116065979004, acc: 0.8838709592819214)
[2025-02-13 20:48:38,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38,993][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.16456478834152222, acc: 0.9545454382896423)
[2025-02-13 20:48:39,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39,351][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.04046396166086197, acc: 0.9942196607589722)
[2025-02-13 20:48:39,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39,740][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.08554527163505554, acc: 0.97826087474823)
[2025-02-13 20:48:39,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40,120][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.07116959989070892, acc: 0.9865771532058716)
[2025-02-13 20:48:40,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40,516][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.15257418155670166, acc: 0.9605262875556946)
[2025-02-13 20:48:40,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40,901][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.19808754324913025, acc: 0.9563106894493103)
[2025-02-13 20:48:41,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41,287][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.08640865236520767, acc: 0.9751552939414978)
[2025-02-13 20:48:41,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41,695][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.08158484846353531, acc: 0.9903846383094788)
[2025-02-13 20:48:41,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42,080][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.08133763074874878, acc: 0.9811320900917053)
[2025-02-13 20:48:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42,510][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.05106697231531143, acc: 0.9854369163513184)
[2025-02-13 20:48:42,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42,935][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.040617264807224274, acc: 0.9950980544090271)
[2025-02-13 20:48:43,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43,326][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.08568306267261505, acc: 0.9888888597488403)
[2025-02-13 20:48:43,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43,696][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.1825646460056305, acc: 0.9605911374092102)
[2025-02-13 20:48:43,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44,074][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.05493902042508125, acc: 0.9852941036224365)
[2025-02-13 20:48:44,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44,500][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.11314919590950012, acc: 0.96875)
[2025-02-13 20:48:44,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44,872][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.24574442207813263, acc: 0.9548386931419373)
[2025-02-13 20:48:45,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45,266][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.08873064070940018, acc: 0.984455943107605)
[2025-02-13 20:48:45,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45,649][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.048748794943094254, acc: 0.9884393215179443)
[2025-02-13 20:48:45,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46,089][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.14823412895202637, acc: 0.9663865566253662)
[2025-02-13 20:48:46,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46,450][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.021028079092502594, acc: 1.0)
[2025-02-13 20:48:46,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46,812][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.07030782848596573, acc: 0.9754601120948792)
[2025-02-13 20:48:46,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47,191][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.07558469474315643, acc: 0.9781420826911926)
[2025-02-13 20:48:47,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47,568][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.15505196154117584, acc: 0.9733333587646484)
[2025-02-13 20:48:47,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47,973][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.20107552409172058, acc: 0.946107804775238)
[2025-02-13 20:48:48,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48,345][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.042821455746889114, acc: 0.9816513657569885)
[2025-02-13 20:48:48,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48,698][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.05876254290342331, acc: 0.9864864945411682)
[2025-02-13 20:48:48,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49,051][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.009486569091677666, acc: 1.0)
[2025-02-13 20:48:49,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49,394][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.08992375433444977, acc: 0.9846938848495483)
[2025-02-13 20:48:49,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49,726][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.03467696160078049, acc: 1.0)
[2025-02-13 20:48:49,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50,126][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.02862738072872162, acc: 0.9946808218955994)
[2025-02-13 20:48:50,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50,521][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.06869190186262131, acc: 0.9902912378311157)
[2025-02-13 20:48:50,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50,925][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.09858235716819763, acc: 0.9720670580863953)
[2025-02-13 20:48:51,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51,295][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.0239388570189476, acc: 1.0)
[2025-02-13 20:48:51,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51,669][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.09199148416519165, acc: 0.9735449552536011)
[2025-02-13 20:48:51,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52,030][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.10045759379863739, acc: 0.9759036302566528)
[2025-02-13 20:48:52,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52,390][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.03730342164635658, acc: 0.9926470518112183)
[2025-02-13 20:48:52,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52,791][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.1485876441001892, acc: 0.9626168012619019)
[2025-02-13 20:48:52,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53,139][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.05966109037399292, acc: 0.9803921580314636)
[2025-02-13 20:48:53,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53,508][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.050539951771497726, acc: 1.0)
[2025-02-13 20:48:53,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53,890][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.1197376698255539, acc: 0.9750000238418579)
[2025-02-13 20:48:54,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54,351][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.03376829996705055, acc: 0.9944444298744202)
[2025-02-13 20:48:54,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54,750][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.047257114201784134, acc: 0.9946236610412598)
[2025-02-13 20:48:54,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55,107][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.054552916437387466, acc: 0.9857142567634583)
[2025-02-13 20:48:55,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55,477][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.051614295691251755, acc: 0.9866666793823242)
[2025-02-13 20:48:55,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55,852][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.10754046589136124, acc: 0.9652174115180969)
[2025-02-13 20:48:55,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56,250][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.07625342160463333, acc: 0.9710144996643066)
[2025-02-13 20:48:56,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56,654][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.049995724111795425, acc: 0.9865771532058716)
[2025-02-13 20:48:56,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57,042][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.0834827795624733, acc: 0.9714285731315613)
[2025-02-13 20:48:57,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57,431][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.0260658860206604, acc: 0.9928571581840515)
[2025-02-13 20:48:57,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57,827][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.011432652361690998, acc: 1.0)
[2025-02-13 20:48:57,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58,166][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.12469460070133209, acc: 0.9861111044883728)
[2025-02-13 20:48:58,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58,526][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.07707084715366364, acc: 0.9766082167625427)
[2025-02-13 20:48:58,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58,904][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.028627512976527214, acc: 1.0)
[2025-02-13 20:48:59,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59,272][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.2100212424993515, acc: 0.9420289993286133)
[2025-02-13 20:48:59,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59,625][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.08087964355945587, acc: 0.9852941036224365)
[2025-02-13 20:48:59,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59,996][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.12722273170948029, acc: 0.9696969985961914)
[2025-02-13 20:49:00,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00,366][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.29836902022361755, acc: 0.94017094373703)
[2025-02-13 20:49:00,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00,688][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.2747597098350525, acc: 0.9402984976768494)
[2025-02-13 20:49:00,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01,051][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.15635977685451508, acc: 0.9610389471054077)
[2025-02-13 20:49:01,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01,473][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.06346532702445984, acc: 0.9935483932495117)
[2025-02-13 20:49:01,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01,859][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.09536036849021912, acc: 0.9642857313156128)
[2025-02-13 20:49:01,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02,259][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.16289719939231873, acc: 0.9646017551422119)
[2025-02-13 20:49:02,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02,621][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.08781265467405319, acc: 0.9938271641731262)
[2025-02-13 20:49:02,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03,003][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.09246156364679337, acc: 0.9768785834312439)
[2025-02-13 20:49:03,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03,336][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.18712720274925232, acc: 0.9513888955116272)
[2025-02-13 20:49:03,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03,693][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.10645084083080292, acc: 0.970588207244873)
[2025-02-13 20:49:03,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04,040][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.0904654860496521, acc: 0.9840425252914429)
[2025-02-13 20:49:04,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04,403][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.23873239755630493, acc: 0.9425287246704102)
[2025-02-13 20:49:04,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04,770][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.34109166264533997, acc: 0.9226804375648499)
[2025-02-13 20:49:04,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05,194][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.10146268457174301, acc: 0.9797979593276978)
[2025-02-13 20:49:05,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05,583][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.09515953809022903, acc: 0.9751552939414978)
[2025-02-13 20:49:05,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05,953][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.07809466868638992, acc: 0.9794520735740662)
[2025-02-13 20:49:06,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06,342][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.12987323105335236, acc: 0.9666666388511658)
[2025-02-13 20:49:06,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06,747][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.13808321952819824, acc: 0.969072163105011)
[2025-02-13 20:49:06,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07,138][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.17481057345867157, acc: 0.9447852969169617)
[2025-02-13 20:49:07,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07,523][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.07078996300697327, acc: 0.9833333492279053)
[2025-02-13 20:49:07,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07,902][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.06189121678471565, acc: 0.9845361113548279)
[2025-02-13 20:49:08,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08,299][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.14790743589401245, acc: 0.9708737730979919)
[2025-02-13 20:49:08,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08,670][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.10550713539123535, acc: 0.9701492786407471)
[2025-02-13 20:49:08,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09,041][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.03044777177274227, acc: 1.0)
[2025-02-13 20:49:09,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09,403][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.06588078290224075, acc: 0.9851852059364319)
[2025-02-13 20:49:09,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09,767][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.1870623528957367, acc: 0.9753086566925049)
[2025-02-13 20:49:09,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10,149][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.06900478899478912, acc: 0.9780219793319702)
[2025-02-13 20:49:10,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10,538][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.10094234347343445, acc: 0.9764705896377563)
[2025-02-13 20:49:10,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10,917][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.029307205229997635, acc: 1.0)
[2025-02-13 20:49:11,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11,301][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.026320522651076317, acc: 0.9940119981765747)
[2025-02-13 20:49:11,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11,689][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.06945707648992538, acc: 0.9921259880065918)
[2025-02-13 20:49:11,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12,065][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.06241043657064438, acc: 0.9935064911842346)
[2025-02-13 20:49:12,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12,433][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.03345808386802673, acc: 0.9945945739746094)
[2025-02-13 20:49:12,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12,811][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.06528130918741226, acc: 0.9869281053543091)
[2025-02-13 20:49:12,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13,195][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.21334156394004822, acc: 0.9513888955116272)
[2025-02-13 20:49:13,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13,566][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.13913972675800323, acc: 0.9723756909370422)
[2025-02-13 20:49:13,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13,912][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.022937070578336716, acc: 0.9917355179786682)
[2025-02-13 20:49:14,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14,300][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.12185550481081009, acc: 0.9695431590080261)
[2025-02-13 20:49:14,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14,659][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.11273276805877686, acc: 0.9683544039726257)
[2025-02-13 20:49:14,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15,005][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.14318375289440155, acc: 0.9710144996643066)
[2025-02-13 20:49:15,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15,355][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.21212778985500336, acc: 0.9618320465087891)
[2025-02-13 20:49:15,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15,739][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.10733196884393692, acc: 0.9723756909370422)
[2025-02-13 20:49:15,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16,128][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.06519406288862228, acc: 0.9817073345184326)
[2025-02-13 20:49:16,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16,505][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.2014022022485733, acc: 0.9725274443626404)
[2025-02-13 20:49:16,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16,885][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.017807163298130035, acc: 1.0)
[2025-02-13 20:49:17,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17,269][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.20832324028015137, acc: 0.9537572264671326)
[2025-02-13 20:49:17,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17,655][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.032262083142995834, acc: 0.9905660152435303)
[2025-02-13 20:49:17,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18,039][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.06423019617795944, acc: 0.9739130139350891)
[2025-02-13 20:49:18,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18,410][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.048074375838041306, acc: 0.9910714030265808)
[2025-02-13 20:49:18,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18,781][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.20285961031913757, acc: 0.965753436088562)
[2025-02-13 20:49:18,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19,166][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.019706249237060547, acc: 1.0)
[2025-02-13 20:49:19,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19,555][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.5679641962051392, acc: 0.9090909361839294)
[2025-02-13 20:49:19,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19,953][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.08683571964502335, acc: 0.9723756909370422)
[2025-02-13 20:49:20,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20,389][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.07558288425207138, acc: 0.9857142567634583)
[2025-02-13 20:49:20,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20,802][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.04445742815732956, acc: 0.9824561476707458)
[2025-02-13 20:49:20,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21,202][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.13059675693511963, acc: 0.9729729890823364)
[2025-02-13 20:49:21,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21,582][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.010271156206727028, acc: 1.0)
[2025-02-13 20:49:21,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21,957][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.14789873361587524, acc: 0.97826087474823)
[2025-02-13 20:49:22,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22,346][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.16074973344802856, acc: 0.9637681245803833)
[2025-02-13 20:49:22,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22,710][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.04082449525594711, acc: 0.9904761910438538)
[2025-02-13 20:49:22,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23,055][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.2457524687051773, acc: 0.9541984796524048)
[2025-02-13 20:49:23,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23,424][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.14865745604038239, acc: 0.9426751732826233)
[2025-02-13 20:49:23,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23,796][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.09549310058355331, acc: 0.9644669890403748)
[2025-02-13 20:49:23,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24,177][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.15698915719985962, acc: 0.9603524208068848)
[2025-02-13 20:49:24,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24,552][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.10419187694787979, acc: 0.9848484992980957)
[2025-02-13 20:49:24,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24,922][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.12824125587940216, acc: 0.9766355156898499)
[2025-02-13 20:49:25,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25,301][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.10381776094436646, acc: 0.9698275923728943)
[2025-02-13 20:49:25,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25,660][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.11933989822864532, acc: 0.9864253401756287)
[2025-02-13 20:49:25,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26,035][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.1154160425066948, acc: 0.977477490901947)
[2025-02-13 20:49:26,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26,395][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.15949131548404694, acc: 0.9766355156898499)
[2025-02-13 20:49:26,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26,759][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.12382856756448746, acc: 0.9638554453849792)
[2025-02-13 20:49:26,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27,204][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.05599762871861458, acc: 0.9842932224273682)
[2025-02-13 20:49:27,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27,589][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.06337669491767883, acc: 0.9947643876075745)
[2025-02-13 20:49:27,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27,976][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.04910380393266678, acc: 0.9874213933944702)
[2025-02-13 20:49:28,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28,373][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.12772998213768005, acc: 0.9696969985961914)
[2025-02-13 20:49:28,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28,763][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.06986068934202194, acc: 0.9939758777618408)
[2025-02-13 20:49:28,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29,170][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.1119086742401123, acc: 0.9684684872627258)
[2025-02-13 20:49:29,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29,581][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.0831911712884903, acc: 0.9798387289047241)
[2025-02-13 20:49:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30,000][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.09582968801259995, acc: 0.9866666793823242)
[2025-02-13 20:49:30,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30,403][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.061507806181907654, acc: 0.9890109896659851)
[2025-02-13 20:49:30,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30,801][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.062013886868953705, acc: 0.9796954393386841)
[2025-02-13 20:49:30,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31,181][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.04626062884926796, acc: 0.9901477694511414)
[2025-02-13 20:49:31,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31,533][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.034463949501514435, acc: 1.0)
[2025-02-13 20:49:31,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31,880][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.05984882637858391, acc: 0.9853658676147461)
[2025-02-13 20:49:32,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32,281][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.03491542860865593, acc: 0.9902912378311157)
[2025-02-13 20:49:32,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32,647][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.0448092557489872, acc: 0.9956896305084229)
[2025-02-13 20:49:32,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33,023][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.08545977622270584, acc: 0.98591548204422)
[2025-02-13 20:49:33,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33,398][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.060941461473703384, acc: 0.9908257126808167)
[2025-02-13 20:49:33,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33,766][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.1129254549741745, acc: 0.9757575988769531)
[2025-02-13 20:49:34,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34,256][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.07776687294244766, acc: 0.9754098653793335)
[2025-02-13 20:49:34,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34,569][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.23215654492378235, acc: 0.9548872113227844)
[2025-02-13 20:49:34,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34,933][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.07425453513860703, acc: 0.9842105507850647)
[2025-02-13 20:49:35,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35,290][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.11670400947332382, acc: 0.9783783555030823)
[2025-02-13 20:49:35,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35,655][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.13028137385845184, acc: 0.9868420958518982)
[2025-02-13 20:49:35,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36,014][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.12755878269672394, acc: 0.979899525642395)
[2025-02-13 20:49:36,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36,376][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.0989026427268982, acc: 0.9784946441650391)
[2025-02-13 20:49:36,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36,720][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.07848662883043289, acc: 0.9797297120094299)
[2025-02-13 20:49:36,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37,097][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.117164246737957, acc: 0.9631901979446411)
[2025-02-13 20:49:37,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37,483][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.14573897421360016, acc: 0.9793814420700073)
[2025-02-13 20:49:37,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37,871][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.09937531501054764, acc: 0.9709302186965942)
[2025-02-13 20:49:38,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38,254][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.11612052470445633, acc: 0.9751552939414978)
[2025-02-13 20:49:38,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38,713][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.20733077824115753, acc: 0.9680851101875305)
[2025-02-13 20:49:38,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39,101][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.09188731014728546, acc: 0.9707602262496948)
[2025-02-13 20:49:39,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39,466][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.1113046258687973, acc: 0.9655172228813171)
[2025-02-13 20:49:39,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39,825][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.1278379112482071, acc: 0.9733333587646484)
[2025-02-13 20:49:39,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40,189][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.07231143862009048, acc: 0.9852216839790344)
[2025-02-13 20:49:40,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40,549][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.10126519203186035, acc: 0.9814814925193787)
[2025-02-13 20:49:40,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40,901][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.15334822237491608, acc: 0.9578313231468201)
[2025-02-13 20:49:41,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41,268][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.07039468735456467, acc: 0.9842932224273682)
[2025-02-13 20:49:41,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41,636][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.13809865713119507, acc: 0.9781420826911926)
[2025-02-13 20:49:41,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42,020][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.08243583887815475, acc: 0.9756097793579102)
[2025-02-13 20:49:42,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42,424][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.1368984878063202, acc: 0.949438214302063)
[2025-02-13 20:49:42,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42,838][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.2618359327316284, acc: 0.9411764740943909)
[2025-02-13 20:49:42,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43,238][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.08160357922315598, acc: 0.9801980257034302)
[2025-02-13 20:49:43,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43,613][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.10156521946191788, acc: 0.9874213933944702)
[2025-02-13 20:49:43,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43,952][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.1171569749712944, acc: 0.9677419066429138)
[2025-02-13 20:49:44,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44,318][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.1666671633720398, acc: 0.9731183052062988)
[2025-02-13 20:49:44,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44,700][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.08113235980272293, acc: 0.9832402467727661)
[2025-02-13 20:49:44,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45,072][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.18759307265281677, acc: 0.9548386931419373)
[2025-02-13 20:49:45,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45,439][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.04571832716464996, acc: 0.9863013625144958)
[2025-02-13 20:49:45,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45,813][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.1113414615392685, acc: 0.9811320900917053)
[2025-02-13 20:49:45,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46,184][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.13649117946624756, acc: 0.9757575988769531)
[2025-02-13 20:49:46,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46,576][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.10857053101062775, acc: 0.965753436088562)
[2025-02-13 20:49:46,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46,962][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.230624258518219, acc: 0.9435897469520569)
[2025-02-13 20:49:47,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47,347][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.08566149324178696, acc: 0.9798657894134521)
[2025-02-13 20:49:47,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47,707][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.0896189734339714, acc: 0.9945054650306702)
[2025-02-13 20:49:47,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48,091][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.14153212308883667, acc: 0.9594594836235046)
[2025-02-13 20:49:48,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48,490][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.07220321148633957, acc: 0.9818181991577148)
[2025-02-13 20:49:48,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48,830][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.07792935520410538, acc: 0.9777777791023254)
[2025-02-13 20:49:48,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49,172][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.03018921986222267, acc: 0.9927536249160767)
[2025-02-13 20:49:49,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49,546][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.04874066635966301, acc: 0.9887005686759949)
[2025-02-13 20:49:49,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49,913][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.09659460932016373, acc: 0.9722222089767456)
[2025-02-13 20:49:50,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50,294][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.09953733533620834, acc: 0.9657142758369446)
[2025-02-13 20:49:50,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50,654][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.0693470686674118, acc: 0.9807692170143127)
[2025-02-13 20:49:50,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51,007][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.08048132061958313, acc: 0.9820359349250793)
[2025-02-13 20:49:51,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51,375][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.04518476873636246, acc: 0.987261176109314)
[2025-02-13 20:49:51,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51,744][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.039117325097322464, acc: 0.9933775067329407)
[2025-02-13 20:49:51,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52,119][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.08062268048524857, acc: 0.9828571677207947)
[2025-02-13 20:49:52,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52,502][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.050821416079998016, acc: 0.9892473220825195)
[2025-02-13 20:49:52,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52,877][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.08520623296499252, acc: 0.9821428656578064)
[2025-02-13 20:49:52,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53,215][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.15433482825756073, acc: 0.9750000238418579)
[2025-02-13 20:49:53,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53,611][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.08610903471708298, acc: 0.9791666865348816)
[2025-02-13 20:49:53,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53,972][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.08142033219337463, acc: 0.9689440727233887)
[2025-02-13 20:49:54,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54,382][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.054991722106933594, acc: 0.9949238300323486)
[2025-02-13 20:49:54,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54,746][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.09642769396305084, acc: 0.9835164546966553)
[2025-02-13 20:49:54,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55,126][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.09099508821964264, acc: 0.969924807548523)
[2025-02-13 20:49:55,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55,465][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.2660103738307953, acc: 0.9399999976158142)
[2025-02-13 20:49:55,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55,824][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.16242942214012146, acc: 0.9560975432395935)
[2025-02-13 20:49:55,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56,178][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.07624133676290512, acc: 0.981249988079071)
[2025-02-13 20:49:56,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56,542][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.09507955610752106, acc: 0.9780219793319702)
[2025-02-13 20:49:56,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56,938][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.11675642430782318, acc: 0.9580838084220886)
[2025-02-13 20:49:57,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57,345][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.0438818484544754, acc: 0.987261176109314)
[2025-02-13 20:49:57,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57,757][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.1947934776544571, acc: 0.9601770043373108)
[2025-02-13 20:49:57,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58,149][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.0992959588766098, acc: 0.9759036302566528)
[2025-02-13 20:49:58,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58,537][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.11038463562726974, acc: 0.9677419066429138)
[2025-02-13 20:49:58,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58,908][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.13489973545074463, acc: 0.9668049812316895)
[2025-02-13 20:49:59,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59,316][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.07957394421100616, acc: 0.9788359999656677)
[2025-02-13 20:49:59,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59,700][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.14359958469867706, acc: 0.9694322943687439)
[2025-02-13 20:49:59,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00,064][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.04769891873002052, acc: 0.9941176176071167)
[2025-02-13 20:50:00,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00,446][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.09832044690847397, acc: 0.9806451797485352)
[2025-02-13 20:50:00,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00,822][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.10706087946891785, acc: 0.9710144996643066)
[2025-02-13 20:50:00,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01,218][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.06205297261476517, acc: 0.9863636493682861)
[2025-02-13 20:50:01,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01,610][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.12304268032312393, acc: 0.9715909361839294)
[2025-02-13 20:50:01,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01,999][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.18421979248523712, acc: 0.970588207244873)
[2025-02-13 20:50:02,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02,400][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.18744809925556183, acc: 0.9363636374473572)
[2025-02-13 20:50:02,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02,773][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.13364535570144653, acc: 0.9683544039726257)
[2025-02-13 20:50:02,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03,142][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.08014523983001709, acc: 0.961904764175415)
[2025-02-13 20:50:03,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03,462][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.10634811967611313, acc: 0.9578947424888611)
[2025-02-13 20:50:03,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03,841][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.14477548003196716, acc: 0.9627659320831299)
[2025-02-13 20:50:03,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04,219][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.06414289772510529, acc: 0.9847715497016907)
[2025-02-13 20:50:04,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04,602][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.13459248840808868, acc: 0.9836065769195557)
[2025-02-13 20:50:04,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04,970][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.08780180662870407, acc: 0.9754098653793335)
[2025-02-13 20:50:05,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05,344][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.09928308427333832, acc: 0.9800994992256165)
[2025-02-13 20:50:05,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05,712][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.20630377531051636, acc: 0.9424460530281067)
[2025-02-13 20:50:05,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06,094][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.09471062570810318, acc: 0.9783549904823303)
[2025-02-13 20:50:06,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06,462][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.04564020782709122, acc: 0.9928571581840515)
[2025-02-13 20:50:06,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06,821][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.060898877680301666, acc: 1.0)
[2025-02-13 20:50:06,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07,214][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.059931956231594086, acc: 0.9818181991577148)
[2025-02-13 20:50:07,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07,599][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.17034272849559784, acc: 0.9387755393981934)
[2025-02-13 20:50:07,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08,013][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.02835271693766117, acc: 0.9927007555961609)
[2025-02-13 20:50:08,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08,370][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.03166768699884415, acc: 0.9930555820465088)
[2025-02-13 20:50:08,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08,715][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.06614513695240021, acc: 0.9856114983558655)
[2025-02-13 20:50:08,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09,094][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.03172624483704567, acc: 0.9878048896789551)
[2025-02-13 20:50:09,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09,474][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.03822290524840355, acc: 0.9919999837875366)
[2025-02-13 20:50:09,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09,853][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.06418716907501221, acc: 0.9811320900917053)
[2025-02-13 20:50:09,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10,211][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.026174182072281837, acc: 0.991525411605835)
[2025-02-13 20:50:10,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10,580][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.06403100490570068, acc: 0.9901960492134094)
[2025-02-13 20:50:10,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10,961][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.06540826708078384, acc: 0.9900000095367432)
[2025-02-13 20:50:11,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11,328][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.09232723712921143, acc: 0.9847328066825867)
[2025-02-13 20:50:11,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11,687][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.04102544113993645, acc: 0.9925925731658936)
[2025-02-13 20:50:11,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12,054][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.057146232575178146, acc: 0.982300877571106)
[2025-02-13 20:50:12,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12,407][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.010856868699193, acc: 1.0)
[2025-02-13 20:50:12,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12,808][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.023214226588606834, acc: 0.9921875)
[2025-02-13 20:50:12,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13,207][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.12997189164161682, acc: 0.9708737730979919)
[2025-02-13 20:50:13,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13,558][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.08100856840610504, acc: 0.9885057210922241)
[2025-02-13 20:50:13,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13,898][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.1562570035457611, acc: 0.9685039520263672)
[2025-02-13 20:50:14,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14,267][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.061434973031282425, acc: 0.9760000109672546)
[2025-02-13 20:50:14,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14,661][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.11366136372089386, acc: 0.982758641242981)
[2025-02-13 20:50:14,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15,025][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.05904659628868103, acc: 0.9883720874786377)
[2025-02-13 20:50:15,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15,407][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.08902063965797424, acc: 0.971222996711731)
[2025-02-13 20:50:15,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15,762][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.05124200880527496, acc: 0.9907407164573669)
[2025-02-13 20:50:15,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16,114][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.12398146837949753, acc: 0.9803921580314636)
[2025-02-13 20:50:16,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16,518][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.07284720242023468, acc: 0.9791666865348816)
[2025-02-13 20:50:16,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16,867][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.23608779907226562, acc: 0.9439252614974976)
[2025-02-13 20:50:16,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17,230][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.09006098657846451, acc: 0.970370352268219)
[2025-02-13 20:50:17,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17,596][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.050564397126436234, acc: 0.9933775067329407)
[2025-02-13 20:50:17,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17,961][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.07096158713102341, acc: 0.9829545617103577)
[2025-02-13 20:50:18,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18,324][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.11143706738948822, acc: 0.9791666865348816)
[2025-02-13 20:50:18,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18,685][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.20026734471321106, acc: 0.949999988079071)
[2025-02-13 20:50:18,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19,020][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.061917662620544434, acc: 0.9865771532058716)
[2025-02-13 20:50:19,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19,417][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.079111747443676, acc: 0.9904761910438538)
[2025-02-13 20:50:19,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19,778][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.15023621916770935, acc: 0.9481481313705444)
[2025-02-13 20:50:19,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20,126][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.4148726761341095, acc: 0.9179104566574097)
[2025-02-13 20:50:20,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20,513][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.1070617064833641, acc: 0.9652174115180969)
[2025-02-13 20:50:20,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20,900][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.15928490459918976, acc: 0.95652174949646)
[2025-02-13 20:50:21,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21,283][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.1695100963115692, acc: 0.9482758641242981)
[2025-02-13 20:50:21,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21,717][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.04753657802939415, acc: 1.0)
[2025-02-13 20:50:21,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22,102][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.06086207181215286, acc: 0.9909090995788574)
[2025-02-13 20:50:22,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22,478][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.11898620426654816, acc: 0.9743589758872986)
[2025-02-13 20:50:22,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22,861][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.06545504927635193, acc: 0.9847328066825867)
[2025-02-13 20:50:23,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23,256][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.13236887753009796, acc: 0.9756097793579102)
[2025-02-13 20:50:23,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23,630][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.17299818992614746, acc: 0.9774436354637146)
[2025-02-13 20:50:23,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24,026][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.06872795522212982, acc: 0.9810126423835754)
[2025-02-13 20:50:24,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24,393][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.15338951349258423, acc: 0.9655172228813171)
[2025-02-13 20:50:24,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24,830][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.17475856840610504, acc: 0.9386503100395203)
[2025-02-13 20:50:24,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25,189][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.1550201177597046, acc: 0.9537572264671326)
[2025-02-13 20:50:25,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25,555][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.19112206995487213, acc: 0.9527027010917664)
[2025-02-13 20:50:25,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25,920][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.1130247488617897, acc: 0.965753436088562)
[2025-02-13 20:50:26,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26,279][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.0875643715262413, acc: 0.9797297120094299)
[2025-02-13 20:50:26,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26,649][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.14383193850517273, acc: 0.9724137783050537)
[2025-02-13 20:50:26,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27,020][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.04765027388930321, acc: 1.0)
[2025-02-13 20:50:27,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27,391][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.20892123878002167, acc: 0.959770143032074)
[2025-02-13 20:50:27,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27,745][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.1706438511610031, acc: 0.9681528806686401)
[2025-02-13 20:50:27,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28,082][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.18365982174873352, acc: 0.9370629191398621)
[2025-02-13 20:50:28,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28,447][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.13011996448040009, acc: 0.9726775884628296)
[2025-02-13 20:50:28,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28,803][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.1599213033914566, acc: 0.9685534834861755)
[2025-02-13 20:50:28,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29,147][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.12774428725242615, acc: 0.9736841917037964)
[2025-02-13 20:50:29,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29,486][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.23190200328826904, acc: 0.9586777091026306)
[2025-02-13 20:50:29,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29,850][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.2002192884683609, acc: 0.9568345546722412)
[2025-02-13 20:50:29,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30,218][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.14625205099582672, acc: 0.948387086391449)
[2025-02-13 20:50:30,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30,591][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.09217517822980881, acc: 0.9873417615890503)
[2025-02-13 20:50:30,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31,020][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.19149385392665863, acc: 0.9482758641242981)
[2025-02-13 20:50:31,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31,450][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.09538928419351578, acc: 0.9759036302566528)
[2025-02-13 20:50:31,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31,816][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.1861155778169632, acc: 0.9807692170143127)
[2025-02-13 20:50:31,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32,154][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.1320357322692871, acc: 0.9856114983558655)
[2025-02-13 20:50:32,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32,511][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.20512202382087708, acc: 0.9510489702224731)
[2025-02-13 20:50:32,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32,864][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.13597321510314941, acc: 0.9679487347602844)
[2025-02-13 20:50:33,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33,272][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.17192651331424713, acc: 0.9611111283302307)
[2025-02-13 20:50:33,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33,656][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.17853350937366486, acc: 0.9536423683166504)
[2025-02-13 20:50:33,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34,024][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.08048394322395325, acc: 0.9852941036224365)
[2025-02-13 20:50:34,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34,386][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.1287335753440857, acc: 0.9857142567634583)
[2025-02-13 20:50:34,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34,765][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.1632472276687622, acc: 0.9594594836235046)
[2025-02-13 20:50:34,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35,134][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.0689607635140419, acc: 0.9819276928901672)
[2025-02-13 20:50:35,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35,496][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.02820379100739956, acc: 1.0)
[2025-02-13 20:50:35,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35,892][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.03942728415131569, acc: 0.994350254535675)
[2025-02-13 20:50:36,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36,265][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.03683391585946083, acc: 0.9932432174682617)
[2025-02-13 20:50:36,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36,634][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.022775858640670776, acc: 1.0)
[2025-02-13 20:50:36,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36,984][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.021028103306889534, acc: 1.0)
[2025-02-13 20:50:37,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37,361][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.08664415031671524, acc: 0.9790576100349426)
[2025-02-13 20:50:37,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37,747][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.07490276545286179, acc: 0.9901960492134094)
[2025-02-13 20:50:37,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38,141][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.06913826614618301, acc: 0.9777777791023254)
[2025-02-13 20:50:38,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38,505][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.06029482185840607, acc: 0.9950494766235352)
[2025-02-13 20:50:38,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38,868][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.04962042346596718, acc: 0.9850746393203735)
[2025-02-13 20:50:39,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39,222][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.06377052515745163, acc: 0.9815950989723206)
[2025-02-13 20:50:39,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39,598][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.02628270536661148, acc: 0.9898989796638489)
[2025-02-13 20:50:39,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40,031][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.040307942777872086, acc: 0.9947643876075745)
[2025-02-13 20:50:40,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40,397][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.07313300669193268, acc: 0.9790576100349426)
[2025-02-13 20:50:40,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40,744][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.020106321200728416, acc: 1.0)
[2025-02-13 20:50:40,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41,078][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.02084384486079216, acc: 1.0)
[2025-02-13 20:50:41,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41,438][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.04656747356057167, acc: 0.9948186278343201)
[2025-02-13 20:50:41,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41,815][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.05178030580282211, acc: 0.9910714030265808)
[2025-02-13 20:50:41,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42,213][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.05714087188243866, acc: 0.9893617033958435)
[2025-02-13 20:50:42,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42,580][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.0654325857758522, acc: 0.9857142567634583)
[2025-02-13 20:50:42,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43,051][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.034973494708538055, acc: 0.9950000047683716)
[2025-02-13 20:50:43,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43,427][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.029068347066640854, acc: 1.0)
[2025-02-13 20:50:43,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43,815][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.037370551377534866, acc: 0.9941176176071167)
[2025-02-13 20:50:43,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44,241][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.04121784493327141, acc: 0.9890109896659851)
[2025-02-13 20:50:44,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44,606][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.05652240291237831, acc: 0.9942857027053833)
[2025-02-13 20:50:44,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44,944][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.06595895439386368, acc: 0.9732142686843872)
[2025-02-13 20:50:45,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45,321][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.025203710421919823, acc: 1.0)
[2025-02-13 20:50:45,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45,701][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.06120606139302254, acc: 0.9935897588729858)
[2025-02-13 20:50:45,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46,094][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.0422164648771286, acc: 0.9947368502616882)
[2025-02-13 20:50:46,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46,469][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.05433472990989685, acc: 0.988304078578949)
[2025-02-13 20:50:46,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46,855][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.22265203297138214, acc: 0.9440559148788452)
[2025-02-13 20:50:46,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47,215][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.08735478669404984, acc: 0.9753086566925049)
[2025-02-13 20:50:47,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47,599][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.06869444996118546, acc: 0.9881656765937805)
[2025-02-13 20:50:47,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47,973][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.05302513763308525, acc: 0.9931034445762634)
[2025-02-13 20:50:48,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48,342][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.1540689915418625, acc: 0.984000027179718)
[2025-02-13 20:50:48,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48,683][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.12689051032066345, acc: 0.9885714054107666)
[2025-02-13 20:50:48,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49,047][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.04361845552921295, acc: 0.9829545617103577)
[2025-02-13 20:50:49,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49,411][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.08614351600408554, acc: 0.988095223903656)
[2025-02-13 20:50:49,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49,794][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.03301754593849182, acc: 0.9940119981765747)
[2025-02-13 20:50:49,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50,168][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.029664376750588417, acc: 0.9941520690917969)
[2025-02-13 20:50:50,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50,531][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.018445398658514023, acc: 0.9935483932495117)
[2025-02-13 20:50:50,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50,881][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.06964908540248871, acc: 0.9862068891525269)
[2025-02-13 20:50:51,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51,295][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.025215310975909233, acc: 0.9936708807945251)
[2025-02-13 20:50:51,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51,676][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.028181025758385658, acc: 0.9934640526771545)
[2025-02-13 20:50:51,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52,060][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.01681661792099476, acc: 1.0)
[2025-02-13 20:50:52,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52,431][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.09323624521493912, acc: 0.9726775884628296)
[2025-02-13 20:50:52,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52,795][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.04275035858154297, acc: 0.9928057789802551)
[2025-02-13 20:50:52,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53,154][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.03797672316431999, acc: 0.978723406791687)
[2025-02-13 20:50:53,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53,539][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.13844533264636993, acc: 0.9659090638160706)
[2025-02-13 20:50:53,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53,909][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.04621148481965065, acc: 0.9871794581413269)
[2025-02-13 20:50:54,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54,317][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.013542547821998596, acc: 1.0)
[2025-02-13 20:50:54,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54,722][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.01228408981114626, acc: 1.0)
[2025-02-13 20:50:54,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55,070][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.07677243649959564, acc: 0.9931972622871399)
[2025-02-13 20:50:55,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55,409][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.040562890470027924, acc: 0.9942857027053833)
[2025-02-13 20:50:55,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55,782][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.01288575865328312, acc: 1.0)
[2025-02-13 20:50:55,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56,141][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.08869428932666779, acc: 0.9756097793579102)
[2025-02-13 20:50:56,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56,548][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.04218670725822449, acc: 0.9939758777618408)
[2025-02-13 20:50:56,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56,935][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.03899418190121651, acc: 0.9880239367485046)
[2025-02-13 20:50:57,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57,271][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.04561937600374222, acc: 1.0)
[2025-02-13 20:50:57,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57,689][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.06255850195884705, acc: 0.9784172773361206)
[2025-02-13 20:50:57,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58,112][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.04403858631849289, acc: 0.9930070042610168)
[2025-02-13 20:50:58,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58,486][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.059811387211084366, acc: 0.9888888597488403)
[2025-02-13 20:50:58,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58,848][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.0825747475028038, acc: 0.9865771532058716)
[2025-02-13 20:50:58,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59,215][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.09424630552530289, acc: 0.9777777791023254)
[2025-02-13 20:50:59,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59,574][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.07452943921089172, acc: 0.9887640476226807)
[2025-02-13 20:50:59,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59,952][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.04711049050092697, acc: 0.981249988079071)
[2025-02-13 20:51:00,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00,321][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.06651653349399567, acc: 0.9865771532058716)
[2025-02-13 20:51:00,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00,722][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.07998314499855042, acc: 0.9807692170143127)
[2025-02-13 20:51:00,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01,119][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.03474125266075134, acc: 1.0)
[2025-02-13 20:51:01,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01,471][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.07317778468132019, acc: 0.9825581312179565)
[2025-02-13 20:51:01,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01,806][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.032845258712768555, acc: 1.0)
[2025-02-13 20:51:01,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02,150][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.03441287949681282, acc: 1.0)
[2025-02-13 20:51:02,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02,549][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.02991071157157421, acc: 0.9942857027053833)
[2025-02-13 20:51:02,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02,957][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.054419826716184616, acc: 0.9823529124259949)
[2025-02-13 20:51:03,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03,358][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.03299981355667114, acc: 0.9930555820465088)
[2025-02-13 20:51:03,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03,767][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.1586909145116806, acc: 0.9577465057373047)
[2025-02-13 20:51:03,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04,185][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.05545925721526146, acc: 0.9931972622871399)
[2025-02-13 20:51:04,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04,586][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.08441544324159622, acc: 0.9781022071838379)
[2025-02-13 20:51:04,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04,957][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.07001426815986633, acc: 0.970802903175354)
[2025-02-13 20:51:05,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05,377][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.015436974354088306, acc: 1.0)
[2025-02-13 20:51:05,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05,769][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.17726415395736694, acc: 0.9457364082336426)
[2025-02-13 20:51:05,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06,149][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.06643738597631454, acc: 0.9879518151283264)
[2025-02-13 20:51:06,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06,519][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.15416738390922546, acc: 0.9593495726585388)
[2025-02-13 20:51:06,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06,882][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.04692930355668068, acc: 1.0)
[2025-02-13 20:51:07,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07,236][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.10722561180591583, acc: 0.9855072498321533)
[2025-02-13 20:51:07,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07,613][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.23741203546524048, acc: 0.9479166865348816)
[2025-02-13 20:51:07,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07,979][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.18896286189556122, acc: 0.9577465057373047)
[2025-02-13 20:51:08,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08,359][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.1309349089860916, acc: 0.9644970297813416)
[2025-02-13 20:51:08,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08,732][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.09483001381158829, acc: 0.9797297120094299)
[2025-02-13 20:51:08,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09,102][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.07994090020656586, acc: 0.9664804339408875)
[2025-02-13 20:51:09,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09,465][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.06486049294471741, acc: 0.9861111044883728)
[2025-02-13 20:51:09,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09,822][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.12474612891674042, acc: 0.9617834687232971)
[2025-02-13 20:51:09,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10,163][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.09711937606334686, acc: 0.9731543660163879)
[2025-02-13 20:51:10,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10,559][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.04061928763985634, acc: 1.0)
[2025-02-13 20:51:10,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10,957][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.0811987817287445, acc: 0.9857819676399231)
[2025-02-13 20:51:11,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11,356][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.10904271900653839, acc: 0.9848484992980957)
[2025-02-13 20:51:11,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11,728][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.13809379935264587, acc: 0.9698795080184937)
[2025-02-13 20:51:11,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12,105][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.1175321415066719, acc: 0.9733333587646484)
[2025-02-13 20:51:12,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12,507][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.14281460642814636, acc: 0.9617834687232971)
[2025-02-13 20:51:12,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12,864][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.041917312890291214, acc: 0.9939024448394775)
[2025-02-13 20:51:13,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13,240][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.089060477912426, acc: 0.9780219793319702)
[2025-02-13 20:51:13,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13,621][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.10439225286245346, acc: 0.9689119458198547)
[2025-02-13 20:51:13,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13,985][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.11653055250644684, acc: 0.9731543660163879)
[2025-02-13 20:51:14,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14,371][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.08248837292194366, acc: 0.9692307710647583)
[2025-02-13 20:51:14,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14,757][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.08200377225875854, acc: 0.9938271641731262)
[2025-02-13 20:51:14,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15,135][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.06699107587337494, acc: 0.987500011920929)
[2025-02-13 20:51:15,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15,539][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.06549187749624252, acc: 0.9775280952453613)
[2025-02-13 20:51:15,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15,946][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.07244646549224854, acc: 0.9714285731315613)
[2025-02-13 20:51:16,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16,356][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.0687750056385994, acc: 0.9714285731315613)
[2025-02-13 20:51:16,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16,741][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.14495523273944855, acc: 0.9668508172035217)
[2025-02-13 20:51:16,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17,113][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.1259993016719818, acc: 0.9714285731315613)
[2025-02-13 20:51:17,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17,517][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.0733548253774643, acc: 0.9826589822769165)
[2025-02-13 20:51:17,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17,894][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.03992186114192009, acc: 0.9830508232116699)
[2025-02-13 20:51:18,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18,257][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.04136062040925026, acc: 0.9813084006309509)
[2025-02-13 20:51:18,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18,651][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.01869313046336174, acc: 0.9939758777618408)
[2025-02-13 20:51:18,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19,056][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.020281145349144936, acc: 0.9940476417541504)
[2025-02-13 20:51:19,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19,393][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.009562503546476364, acc: 1.0)
[2025-02-13 20:51:19,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19,800][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.0288856141269207, acc: 0.9943820238113403)
[2025-02-13 20:51:19,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20,252][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.06157898157835007, acc: 0.9862068891525269)
[2025-02-13 20:51:20,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20,655][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.017687156796455383, acc: 1.0)
[2025-02-13 20:51:20,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21,031][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.11905796825885773, acc: 0.9696969985961914)
[2025-02-13 20:51:21,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21,402][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.04861730709671974, acc: 0.9882352948188782)
[2025-02-13 20:51:21,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21,785][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.023876914754509926, acc: 1.0)
[2025-02-13 20:51:21,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22,153][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.07771661877632141, acc: 0.9674796462059021)
[2025-02-13 20:51:22,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22,510][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.009859884157776833, acc: 1.0)
[2025-02-13 20:51:22,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22,878][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.004333478398621082, acc: 1.0)
[2025-02-13 20:51:23,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23,273][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.008176238276064396, acc: 1.0)
[2025-02-13 20:51:23,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23,630][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.012885033152997494, acc: 1.0)
[2025-02-13 20:51:23,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24,008][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.0416480116546154, acc: 0.9873417615890503)
[2025-02-13 20:51:24,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24,380][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.010161096230149269, acc: 1.0)
[2025-02-13 20:51:24,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24,745][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.04579097032546997, acc: 0.9886363744735718)
[2025-02-13 20:51:24,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25,115][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.07838735729455948, acc: 0.9882352948188782)
[2025-02-13 20:51:25,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25,479][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.012405818328261375, acc: 1.0)
[2025-02-13 20:51:25,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25,844][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.043650396168231964, acc: 0.9932885766029358)
[2025-02-13 20:51:25,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26,206][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.025382723659276962, acc: 1.0)
[2025-02-13 20:51:26,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26,574][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.026631200686097145, acc: 1.0)
[2025-02-13 20:51:26,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27,018][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.06960426270961761, acc: 0.9738562107086182)
[2025-02-13 20:51:27,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27,423][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.06151444464921951, acc: 0.982758641242981)
[2025-02-13 20:51:27,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27,830][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.029260054230690002, acc: 0.9949238300323486)
[2025-02-13 20:51:27,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28,210][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.12434261292219162, acc: 0.9734042286872864)
[2025-02-13 20:51:28,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28,585][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.10050329566001892, acc: 0.9788732528686523)
[2025-02-13 20:51:28,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28,971][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.0770062729716301, acc: 0.9852941036224365)
[2025-02-13 20:51:29,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29,372][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.0856151133775711, acc: 0.9886363744735718)
[2025-02-13 20:51:29,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29,752][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.036358341574668884, acc: 0.9906542301177979)
[2025-02-13 20:51:29,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30,147][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.07616112381219864, acc: 0.9797297120094299)
[2025-02-13 20:51:30,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30,532][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.1628779023885727, acc: 0.9652174115180969)
[2025-02-13 20:51:30,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30,972][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.060915756970644, acc: 0.9933333396911621)
[2025-02-13 20:51:31,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31,350][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.04814770817756653, acc: 0.9869281053543091)
[2025-02-13 20:51:31,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31,758][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.1320759654045105, acc: 0.9677419066429138)
[2025-02-13 20:51:31,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32,147][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.05811213701963425, acc: 0.9814814925193787)
[2025-02-13 20:51:32,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32,556][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.08450217545032501, acc: 0.9838709831237793)
[2025-02-13 20:51:32,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32,925][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.08762621879577637, acc: 0.9759615659713745)
[2025-02-13 20:51:33,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33,299][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.08947973698377609, acc: 0.9791666865348816)
[2025-02-13 20:51:33,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33,681][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.07295051217079163, acc: 0.9852216839790344)
[2025-02-13 20:51:33,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34,043][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.11460115760564804, acc: 0.9884393215179443)
[2025-02-13 20:51:34,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34,443][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.08412165939807892, acc: 0.9851484894752502)
[2025-02-13 20:51:34,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34,913][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.07157424092292786, acc: 0.9710982441902161)
[2025-02-13 20:51:35,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35,339][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.1394762545824051, acc: 0.978723406791687)
[2025-02-13 20:51:35,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35,827][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.1409757137298584, acc: 0.9710982441902161)
[2025-02-13 20:51:36,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36,308][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.09156754612922668, acc: 0.9837837815284729)
[2025-02-13 20:51:36,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36,769][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.05757573992013931, acc: 0.983146071434021)
[2025-02-13 20:51:36,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37,204][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.06750501692295074, acc: 0.9804878234863281)
[2025-02-13 20:51:37,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37,623][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.08155816048383713, acc: 0.984455943107605)
[2025-02-13 20:51:37,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38,084][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.059983640909194946, acc: 0.9850746393203735)
[2025-02-13 20:51:38,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38,511][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.1342289298772812, acc: 0.9901960492134094)
[2025-02-13 20:51:38,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38,968][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.07106436789035797, acc: 0.9896373152732849)
[2025-02-13 20:51:39,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39,435][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.1732960343360901, acc: 0.9714285731315613)
[2025-02-13 20:51:39,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39,915][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.07349754124879837, acc: 0.9836065769195557)
[2025-02-13 20:51:40,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40,309][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.04742204770445824, acc: 0.984375)
[2025-02-13 20:51:40,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40,660][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.21301275491714478, acc: 0.9337016344070435)
[2025-02-13 20:51:40,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41,053][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.14182111620903015, acc: 0.9707317352294922)
[2025-02-13 20:51:41,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41,536][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.09976502507925034, acc: 0.9806763529777527)
[2025-02-13 20:51:41,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41,915][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.24571146070957184, acc: 0.9477124214172363)
[2025-02-13 20:51:42,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42,344][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.09102334827184677, acc: 0.9783783555030823)
[2025-02-13 20:51:42,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42,727][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.12386754900217056, acc: 0.9658536314964294)
[2025-02-13 20:51:42,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43,118][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.08815495669841766, acc: 0.9742268323898315)
[2025-02-13 20:51:43,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43,499][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.05659397318959236, acc: 0.9845361113548279)
[2025-02-13 20:51:43,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43,928][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.15185153484344482, acc: 0.9695431590080261)
[2025-02-13 20:51:44,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44,362][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.039459120482206345, acc: 0.9880239367485046)
[2025-02-13 20:51:44,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44,763][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.12730373442173004, acc: 0.9656862616539001)
[2025-02-13 20:51:44,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45,172][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.08045995980501175, acc: 0.9743589758872986)
[2025-02-13 20:51:45,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45,579][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.09494543075561523, acc: 0.9824561476707458)
[2025-02-13 20:51:45,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46,023][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.0868799164891243, acc: 0.9878787994384766)
[2025-02-13 20:51:46,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46,443][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.03726568445563316, acc: 0.9882352948188782)
[2025-02-13 20:51:46,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46,833][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.07978423684835434, acc: 0.976190447807312)
[2025-02-13 20:51:46,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47,214][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.13284431397914886, acc: 0.9653465151786804)
[2025-02-13 20:51:47,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47,530][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.18413379788398743, acc: 0.96875)
[2025-02-13 20:51:47,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47,915][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.04539775475859642, acc: 0.9886363744735718)
[2025-02-13 20:51:48,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48,316][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.040633708238601685, acc: 0.9895833134651184)
[2025-02-13 20:51:48,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48,718][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.0808730497956276, acc: 0.9947090148925781)
[2025-02-13 20:51:48,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49,114][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.12278441339731216, acc: 0.9820359349250793)
[2025-02-13 20:51:49,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49,498][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.1557183414697647, acc: 0.9640718698501587)
[2025-02-13 20:51:49,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49,869][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.17435304820537567, acc: 0.9743589758872986)
[2025-02-13 20:51:50,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50,259][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.14366570115089417, acc: 0.9680851101875305)
[2025-02-13 20:51:50,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50,640][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.0619855597615242, acc: 0.9892473220825195)
[2025-02-13 20:51:50,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51,014][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.23160462081432343, acc: 0.9520547986030579)
[2025-02-13 20:51:51,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51,387][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.28797683119773865, acc: 0.931034505367279)
[2025-02-13 20:51:51,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51,781][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.143373042345047, acc: 0.9464285969734192)
[2025-02-13 20:51:51,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52,137][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.3056236505508423, acc: 0.9448275566101074)
[2025-02-13 20:51:52,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52,468][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.03321300819516182, acc: 1.0)
[2025-02-13 20:51:52,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52,817][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.1364867091178894, acc: 0.9681528806686401)
[2025-02-13 20:51:52,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53,172][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.16631343960762024, acc: 0.9469026327133179)
[2025-02-13 20:51:53,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53,541][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.1147642582654953, acc: 0.9724137783050537)
[2025-02-13 20:51:53,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53,888][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.1973009556531906, acc: 0.9609375)
[2025-02-13 20:51:54,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54,326][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.1404803842306137, acc: 0.9569892287254333)
[2025-02-13 20:51:54,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54,756][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.09840492904186249, acc: 0.982300877571106)
[2025-02-13 20:51:54,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55,131][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.12308704107999802, acc: 0.9642857313156128)
[2025-02-13 20:51:55,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55,498][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.0774030014872551, acc: 0.991304337978363)
[2025-02-13 20:51:55,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55,846][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.07317138463258743, acc: 0.9873417615890503)
[2025-02-13 20:51:55,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56,244][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.11218030005693436, acc: 0.9513888955116272)
[2025-02-13 20:51:56,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56,608][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.09145217388868332, acc: 0.9714285731315613)
[2025-02-13 20:51:56,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56,997][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.12356672435998917, acc: 0.9849624037742615)
[2025-02-13 20:51:57,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57,389][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.27331095933914185, acc: 0.9568965435028076)
[2025-02-13 20:51:57,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57,776][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.09951041638851166, acc: 0.9750000238418579)
[2025-02-13 20:51:57,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58,167][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.0693022683262825, acc: 0.985401451587677)
[2025-02-13 20:51:58,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58,537][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.07411752641201019, acc: 0.984000027179718)
[2025-02-13 20:51:58,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58,964][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.12009932845830917, acc: 0.9829059839248657)
[2025-02-13 20:51:59,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59,373][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.21743741631507874, acc: 0.9541284441947937)
[2025-02-13 20:51:59,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59,796][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.09099417924880981, acc: 0.9791666865348816)
[2025-02-13 20:51:59,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00,210][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.06352026760578156, acc: 0.9743589758872986)
[2025-02-13 20:52:00,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00,599][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.0779380053281784, acc: 0.9927536249160767)
[2025-02-13 20:52:00,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00,950][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.06270046532154083, acc: 0.9801324605941772)
[2025-02-13 20:52:01,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01,361][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.024071959778666496, acc: 1.0)
[2025-02-13 20:52:01,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01,750][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.014887056313455105, acc: 1.0)
[2025-02-13 20:52:01,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02,146][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.06427672505378723, acc: 0.9904761910438538)
[2025-02-13 20:52:02,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02,554][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.028654763475060463, acc: 1.0)
[2025-02-13 20:52:02,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02,895][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.048194222152233124, acc: 0.9865771532058716)
[2025-02-13 20:52:03,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03,257][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.21827024221420288, acc: 0.9516128897666931)
[2025-02-13 20:52:03,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03,656][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.21699248254299164, acc: 0.9396551847457886)
[2025-02-13 20:52:03,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04,036][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.1620853692293167, acc: 0.9652777910232544)
[2025-02-13 20:52:04,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04,522][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.09191935509443283, acc: 0.9844961166381836)
[2025-02-13 20:52:04,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04,926][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.08923130482435226, acc: 0.9913793206214905)
[2025-02-13 20:52:05,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05,325][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.1029607504606247, acc: 0.9738562107086182)
[2025-02-13 20:52:05,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05,722][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.1190677359700203, acc: 0.9829059839248657)
[2025-02-13 20:52:05,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06,156][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.06870033591985703, acc: 0.9785714149475098)
[2025-02-13 20:52:06,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06,584][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.1261235922574997, acc: 0.9694656729698181)
[2025-02-13 20:52:06,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07,005][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.07322893291711807, acc: 0.9810126423835754)
[2025-02-13 20:52:07,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07,409][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.06277009844779968, acc: 0.9933775067329407)
[2025-02-13 20:52:07,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07,796][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.25575387477874756, acc: 0.9265536665916443)
[2025-02-13 20:52:07,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08,207][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.22687038779258728, acc: 0.9587628841400146)
[2025-02-13 20:52:08,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08,662][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.20903673768043518, acc: 0.9581151604652405)
[2025-02-13 20:52:08,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09,130][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.21711032092571259, acc: 0.9679487347602844)
[2025-02-13 20:52:09,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09,615][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.1124751940369606, acc: 0.9720930457115173)
[2025-02-13 20:52:09,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10,068][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.28964993357658386, acc: 0.9184549450874329)
[2025-02-13 20:52:10,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10,529][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.12663091719150543, acc: 0.9629629850387573)
[2025-02-13 20:52:10,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11,019][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.14955280721187592, acc: 0.9710744023323059)
[2025-02-13 20:52:11,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11,522][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.04335605353116989, acc: 1.0)
[2025-02-13 20:52:11,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11,958][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.17725050449371338, acc: 0.9594594836235046)
[2025-02-13 20:52:12,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12,317][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.01771393232047558, acc: 1.0)
[2025-02-13 20:52:12,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12,686][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.20193465054035187, acc: 0.9603960514068604)
[2025-02-13 20:52:12,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13,051][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.08057896792888641, acc: 0.9819819927215576)
[2025-02-13 20:52:13,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13,442][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.1122731864452362, acc: 0.9681528806686401)
[2025-02-13 20:52:13,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13,834][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.23851852118968964, acc: 0.9466666579246521)
[2025-02-13 20:52:13,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14,212][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.039885442703962326, acc: 0.9941860437393188)
[2025-02-13 20:52:14,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14,657][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.06999768316745758, acc: 0.9794520735740662)
[2025-02-13 20:52:14,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15,073][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.07060737907886505, acc: 0.9870967864990234)
[2025-02-13 20:52:15,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15,535][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.10528446733951569, acc: 0.9894737005233765)
[2025-02-13 20:52:15,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15,952][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.04596630483865738, acc: 0.9887005686759949)
[2025-02-13 20:52:16,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16,309][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.04762662947177887, acc: 0.9818181991577148)
[2025-02-13 20:52:16,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16,705][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.22670570015907288, acc: 0.9740259647369385)
[2025-02-13 20:52:16,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17,092][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.08135338872671127, acc: 0.9831932783126831)
[2025-02-13 20:52:17,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17,470][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.04659424349665642, acc: 0.9896373152732849)
[2025-02-13 20:52:17,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17,856][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.11112474650144577, acc: 0.9634146094322205)
[2025-02-13 20:52:18,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18,270][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.05991654098033905, acc: 1.0)
[2025-02-13 20:52:18,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18,683][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.03272973746061325, acc: 1.0)
[2025-02-13 20:52:18,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19,091][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.08057229220867157, acc: 0.9935483932495117)
[2025-02-13 20:52:19,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19,454][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.07026642560958862, acc: 0.9834254384040833)
[2025-02-13 20:52:19,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19,904][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.07246766984462738, acc: 0.9791666865348816)
[2025-02-13 20:52:20,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20,320][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.05793954059481621, acc: 0.9870967864990234)
[2025-02-13 20:52:20,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20,698][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.20635773241519928, acc: 0.9790576100349426)
[2025-02-13 20:52:20,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21,108][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.01403104979544878, acc: 1.0)
[2025-02-13 20:52:21,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21,503][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.02895524725317955, acc: 0.993630588054657)
[2025-02-13 20:52:21,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21,876][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.10846807062625885, acc: 0.9729729890823364)
[2025-02-13 20:52:22,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22,272][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.047478318214416504, acc: 0.9925373196601868)
[2025-02-13 20:52:22,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22,692][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.12982475757598877, acc: 0.9679487347602844)
[2025-02-13 20:52:22,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23,083][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.06240195780992508, acc: 0.991150438785553)
[2025-02-13 20:52:23,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23,500][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.17055557668209076, acc: 0.9629629850387573)
[2025-02-13 20:52:23,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23,895][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.29965564608573914, acc: 0.9430379867553711)
[2025-02-13 20:52:24,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24,291][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.0434286929666996, acc: 0.9939758777618408)
[2025-02-13 20:52:24,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24,741][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.06357089430093765, acc: 0.9925925731658936)
[2025-02-13 20:52:24,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25,143][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.14635181427001953, acc: 0.9583333134651184)
[2025-02-13 20:52:25,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25,612][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.08763990551233292, acc: 0.9879518151283264)
[2025-02-13 20:52:25,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26,021][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.04256662353873253, acc: 0.9855072498321533)
[2025-02-13 20:52:26,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26,465][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.19955122470855713, acc: 0.9523809552192688)
[2025-02-13 20:52:26,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26,916][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.07005322724580765, acc: 1.0)
[2025-02-13 20:52:27,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27,295][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.07582955062389374, acc: 0.98591548204422)
[2025-02-13 20:52:27,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27,706][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.058553796261548996, acc: 0.9890109896659851)
[2025-02-13 20:52:27,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28,083][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.06979159265756607, acc: 0.9775280952453613)
[2025-02-13 20:52:28,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28,534][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.07829475402832031, acc: 1.0)
[2025-02-13 20:52:28,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28,967][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.08149808645248413, acc: 0.9876543283462524)
[2025-02-13 20:52:29,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29,385][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.07079963386058807, acc: 1.0)
[2025-02-13 20:52:29,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29,777][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.131686732172966, acc: 0.9753086566925049)
[2025-02-13 20:52:29,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30,136][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.18709851801395416, acc: 0.984375)
[2025-02-13 20:52:30,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30,491][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.11028650403022766, acc: 0.9682539701461792)
[2025-02-13 20:52:30,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30,828][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.10641475766897202, acc: 0.9682539701461792)
[2025-02-13 20:52:30,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31,142][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.07899800688028336, acc: 0.9701492786407471)
[2025-02-13 20:52:31,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31,514][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.14746315777301788, acc: 0.9367088675498962)
[2025-02-13 20:52:31,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31,876][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.3139680027961731, acc: 0.9777777791023254)
[2025-02-13 20:52:32,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32,231][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.20441249012947083, acc: 0.9599999785423279)
[2025-02-13 20:52:32,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32,591][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.14440682530403137, acc: 0.9624999761581421)
[2025-02-13 20:52:32,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32,945][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.16953687369823456, acc: 0.9577465057373047)
[2025-02-13 20:52:33,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33,289][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.2170047014951706, acc: 0.9333333373069763)
[2025-02-13 20:52:33,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33,649][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.09275058656930923, acc: 1.0)
[2025-02-13 20:52:33,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34,021][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.11729015409946442, acc: 0.9586206674575806)
[2025-02-13 20:52:34,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34,413][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.06634889543056488, acc: 0.9880239367485046)
[2025-02-13 20:52:34,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34,789][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.10694915056228638, acc: 0.9624413251876831)
[2025-02-13 20:52:34,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35,193][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.022057127207517624, acc: 0.9948453903198242)
[2025-02-13 20:52:35,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35,565][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.08599218726158142, acc: 0.9826589822769165)
[2025-02-13 20:52:35,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35,941][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.10759295523166656, acc: 0.9885714054107666)
[2025-02-13 20:52:36,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36,332][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.026949651539325714, acc: 0.9945651888847351)
[2025-02-13 20:52:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36,712][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.08282788097858429, acc: 0.9768785834312439)
[2025-02-13 20:52:36,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37,094][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.050495896488428116, acc: 0.994535505771637)
[2025-02-13 20:52:37,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37,473][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.2466440200805664, acc: 0.948387086391449)
[2025-02-13 20:52:37,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37,859][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.16365596652030945, acc: 0.9615384340286255)
[2025-02-13 20:52:38,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38,238][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.0713738426566124, acc: 0.9888888597488403)
[2025-02-13 20:52:38,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38,646][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.42062559723854065, acc: 0.9337016344070435)
[2025-02-13 20:52:38,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39,050][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.18822965025901794, acc: 0.9457364082336426)
[2025-02-13 20:52:39,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39,431][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.0750582367181778, acc: 0.9871794581413269)
[2025-02-13 20:52:39,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39,808][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.13403114676475525, acc: 0.9788732528686523)
[2025-02-13 20:52:39,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40,212][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.15183304250240326, acc: 0.9569892287254333)
[2025-02-13 20:52:40,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40,622][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.11036373674869537, acc: 0.9576271176338196)
[2025-02-13 20:52:40,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40,989][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.24092595279216766, acc: 0.9513513445854187)
[2025-02-13 20:52:41,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41,389][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.05448704585433006, acc: 0.9819276928901672)
[2025-02-13 20:52:41,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41,770][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.08467921614646912, acc: 0.9898989796638489)
[2025-02-13 20:52:41,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42,218][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.04318787157535553, acc: 0.9895287752151489)
[2025-02-13 20:52:42,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42,638][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.05179984122514725, acc: 0.9894179701805115)
[2025-02-13 20:52:42,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43,032][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.03168666735291481, acc: 0.995121955871582)
[2025-02-13 20:52:43,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43,420][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.06541517376899719, acc: 0.9740259647369385)
[2025-02-13 20:52:43,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43,812][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.05117085203528404, acc: 0.9884393215179443)
[2025-02-13 20:52:43,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44,196][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.08223945647478104, acc: 0.9747899174690247)
[2025-02-13 20:52:44,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44,566][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.07395841181278229, acc: 0.9797979593276978)
[2025-02-13 20:52:44,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44,959][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.1021132543683052, acc: 0.9698492288589478)
[2025-02-13 20:52:45,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45,315][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.1499471813440323, acc: 0.9593495726585388)
[2025-02-13 20:52:45,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45,733][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.15148665010929108, acc: 0.9554139971733093)
[2025-02-13 20:52:45,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46,103][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.1995496153831482, acc: 0.9550561904907227)
[2025-02-13 20:52:46,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46,464][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.1743374466896057, acc: 0.9589040875434875)
[2025-02-13 20:52:46,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46,826][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.11641141772270203, acc: 0.9724137783050537)
[2025-02-13 20:52:46,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47,187][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.06251446902751923, acc: 0.9868420958518982)
[2025-02-13 20:52:47,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47,613][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.11376398056745529, acc: 0.9727891087532043)
[2025-02-13 20:52:47,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48,029][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.04850573465228081, acc: 0.9942196607589722)
[2025-02-13 20:52:48,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48,406][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.1466129720211029, acc: 0.9671052694320679)
[2025-02-13 20:52:48,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48,766][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.051652248948812485, acc: 0.9929078221321106)
[2025-02-13 20:52:48,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49,159][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.12683673202991486, acc: 0.9655172228813171)
[2025-02-13 20:52:49,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49,566][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.1073806881904602, acc: 0.9800000190734863)
[2025-02-13 20:52:49,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49,963][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.08052586764097214, acc: 0.9870967864990234)
[2025-02-13 20:52:50,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50,350][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.1190711036324501, acc: 0.9844961166381836)
[2025-02-13 20:52:50,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50,718][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.10649517923593521, acc: 0.9645389914512634)
[2025-02-13 20:52:50,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51,079][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.11325053870677948, acc: 0.9612902998924255)
[2025-02-13 20:52:51,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51,453][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.1202273815870285, acc: 0.9821428656578064)
[2025-02-13 20:52:51,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51,837][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.08471794426441193, acc: 0.9696969985961914)
[2025-02-13 20:52:51,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52,221][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.06747446209192276, acc: 0.9849624037742615)
[2025-02-13 20:52:52,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52,612][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.07990714907646179, acc: 0.9829059839248657)
[2025-02-13 20:52:52,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52,992][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.04340178892016411, acc: 0.9895833134651184)
[2025-02-13 20:52:53,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53,401][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.15613164007663727, acc: 0.9661017060279846)
[2025-02-13 20:52:53,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53,795][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.12405037879943848, acc: 0.9647887349128723)
[2025-02-13 20:52:53,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54,194][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.1527252197265625, acc: 0.9551281929016113)
[2025-02-13 20:52:54,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54,651][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.04518166929483414, acc: 0.9930070042610168)
[2025-02-13 20:52:54,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55,055][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.12728285789489746, acc: 0.9637681245803833)
[2025-02-13 20:52:55,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55,443][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.13894324004650116, acc: 0.9640718698501587)
[2025-02-13 20:52:55,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55,813][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.07798243314027786, acc: 0.9928057789802551)
[2025-02-13 20:52:55,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56,211][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.05500543862581253, acc: 0.991525411605835)
[2025-02-13 20:52:56,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56,649][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.05258527770638466, acc: 1.0)
[2025-02-13 20:52:56,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57,049][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.02705433778464794, acc: 1.0)
[2025-02-13 20:52:57,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57,455][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.06821344047784805, acc: 0.9861111044883728)
[2025-02-13 20:52:57,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57,834][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.03949235379695892, acc: 1.0)
[2025-02-13 20:52:57,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58,244][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.076009102165699, acc: 0.9751552939414978)
[2025-02-13 20:52:58,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58,646][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.06869196146726608, acc: 0.9927007555961609)
[2025-02-13 20:52:58,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59,039][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.08945106714963913, acc: 0.9629629850387573)
[2025-02-13 20:52:59,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59,514][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.03949517384171486, acc: 0.9924812316894531)
[2025-02-13 20:52:59,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59,879][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.09141167998313904, acc: 0.9629629850387573)
[2025-02-13 20:53:00,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00,249][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.058867812156677246, acc: 0.9909909963607788)
[2025-02-13 20:53:00,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00,645][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.17456939816474915, acc: 0.9681528806686401)
[2025-02-13 20:53:00,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01,046][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.16062642633914948, acc: 0.963302731513977)
[2025-02-13 20:53:01,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01,434][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.19680653512477875, acc: 0.9541984796524048)
[2025-02-13 20:53:01,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01,795][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.11582516878843307, acc: 0.970588207244873)
[2025-02-13 20:53:01,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02,168][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.13885581493377686, acc: 0.960629940032959)
[2025-02-13 20:53:02,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02,562][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.09488378465175629, acc: 0.9928571581840515)
[2025-02-13 20:53:02,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02,964][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.05810065567493439, acc: 0.9931034445762634)
[2025-02-13 20:53:03,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03,377][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.07024870067834854, acc: 0.9805825352668762)
[2025-02-13 20:53:03,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03,795][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.05747301131486893, acc: 0.9923076629638672)
[2025-02-13 20:53:03,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04,171][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.07701674848794937, acc: 0.9892473220825195)
[2025-02-13 20:53:04,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04,579][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.03861917555332184, acc: 1.0)
[2025-02-13 20:53:04,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04,990][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.07460512965917587, acc: 0.9918032884597778)
[2025-02-13 20:53:05,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05,373][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.07322218269109726, acc: 0.9847328066825867)
[2025-02-13 20:53:05,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05,746][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.2254270315170288, acc: 0.970802903175354)
[2025-02-13 20:53:05,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06,112][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.03325074166059494, acc: 1.0)
[2025-02-13 20:53:06,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06,497][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.07718177139759064, acc: 0.9794520735740662)
[2025-02-13 20:53:06,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06,868][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.3371608555316925, acc: 0.9534883499145508)
[2025-02-13 20:53:07,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:15,351][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2740, device='cuda:0') eval_epoch_loss=tensor(0.2422, device='cuda:0') eval_epoch_acc=tensor(0.9465, device='cuda:0')
[2025-02-13 20:57:15,353][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:57:15,353][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:57:15,600][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_7130_loss_0.24219220876693726/model.pt
[2025-02-13 20:57:15,603][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:57:15,604][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9465179443359375
[2025-02-13 20:57:15,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16,029][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.3498584032058716, acc: 0.9133333563804626)
[2025-02-13 20:57:16,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16,388][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.09581973403692245, acc: 0.9811320900917053)
[2025-02-13 20:57:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16,754][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.0188435111194849, acc: 1.0)
[2025-02-13 20:57:16,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17,107][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.10586794465780258, acc: 0.9849624037742615)
[2025-02-13 20:57:17,557][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.1320, train_epoch_loss=0.1240, epoch time 3774.5675823166966s
[2025-02-13 20:57:17,558][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 20:57:17,558][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 20:57:17,558][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 20:57:17,558][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 20:57:17,558][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 20:57:17,562][root][INFO] - Key: avg_train_prep, Value: 1.2417622804641724
[2025-02-13 20:57:17,565][root][INFO] - Key: avg_train_loss, Value: 0.21260732412338257
[2025-02-13 20:57:17,565][root][INFO] - Key: avg_train_acc, Value: 0.9513086080551147
[2025-02-13 20:57:17,565][root][INFO] - Key: avg_eval_prep, Value: 1.3098177909851074
[2025-02-13 20:57:17,566][root][INFO] - Key: avg_eval_loss, Value: 0.2689000070095062
[2025-02-13 20:57:17,566][root][INFO] - Key: avg_eval_acc, Value: 0.9385845065116882
[2025-02-13 20:57:17,566][root][INFO] - Key: avg_epoch_time, Value: 3786.5626051574945
[2025-02-13 20:57:17,567][root][INFO] - Key: avg_checkpoint_time, Value: 0.3452558880671859
