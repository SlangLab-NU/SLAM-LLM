[2025-02-17 10:35:15,812][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 2, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 2, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False, 'test_flag': True}
[2025-02-17 10:35:15,813][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-17 10:35:15,813][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme', 'identifier': 'psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2'}
[2025-02-17 10:35:15,813][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-17_10-35-14.txt', 'log_interval': 5}
[2025-02-17 10:35:42,878][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-17 10:35:48,011][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-17 10:35:48,013][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-17 10:35:48,015][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-17 10:35:48,016][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-17 10:35:50,084][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-17 10:35:50,086][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-17 10:35:50,086][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-17 10:35:50,087][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-17 10:35:59,635][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-17 10:35:59,638][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-17 10:35:59,638][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-17 10:35:59,755][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-17 10:35:59,757][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-17 10:35:59,957][slam_llm.utils.train_utils][INFO] - --> Module dual
[2025-02-17 10:35:59,958][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2025-02-17 10:35:59,958][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-17 10:35:59,964][slam_llm.utils.train_utils][INFO] - --> asr has 346.244736 Million params

[2025-02-17 10:36:02,416][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-17 10:36:06,180][root][INFO] - --> Training Set Length = 2298
[2025-02-17 10:36:06,201][root][INFO] - --> Validation Set Length = 341
[2025-02-17 10:36:06,201][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-17 10:36:06,202][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-17 10:36:09,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:09,980][root][INFO] - Training Epoch: 1/2, step 0/1149 completed (loss: 10.503989219665527, acc: 0.0)
[2025-02-17 10:36:10,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:10,395][root][INFO] - Training Epoch: 1/2, step 1/1149 completed (loss: 8.089451789855957, acc: 0.0)
[2025-02-17 10:36:10,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:10,767][root][INFO] - Training Epoch: 1/2, step 2/1149 completed (loss: 7.75893497467041, acc: 0.0)
[2025-02-17 10:36:11,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:11,297][root][INFO] - Training Epoch: 1/2, step 3/1149 completed (loss: 9.244865417480469, acc: 0.0)
[2025-02-17 10:36:11,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:11,867][root][INFO] - Training Epoch: 1/2, step 4/1149 completed (loss: 8.786263465881348, acc: 0.0)
[2025-02-17 10:36:12,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:12,338][root][INFO] - Training Epoch: 1/2, step 5/1149 completed (loss: 7.62589693069458, acc: 0.0)
[2025-02-17 10:36:12,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:12,742][root][INFO] - Training Epoch: 1/2, step 6/1149 completed (loss: 8.974688529968262, acc: 0.0)
[2025-02-17 10:36:12,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:13,164][root][INFO] - Training Epoch: 1/2, step 7/1149 completed (loss: 6.873147487640381, acc: 0.03703703731298447)
[2025-02-17 10:36:13,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:13,712][root][INFO] - Training Epoch: 1/2, step 8/1149 completed (loss: 7.1622490882873535, acc: 0.0)
[2025-02-17 10:36:13,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:14,155][root][INFO] - Training Epoch: 1/2, step 9/1149 completed (loss: 7.147192478179932, acc: 0.0)
[2025-02-17 10:36:14,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:14,572][root][INFO] - Training Epoch: 1/2, step 10/1149 completed (loss: 7.609593868255615, acc: 0.0)
[2025-02-17 10:36:14,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:15,036][root][INFO] - Training Epoch: 1/2, step 11/1149 completed (loss: 7.805996894836426, acc: 0.0)
[2025-02-17 10:36:15,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:15,469][root][INFO] - Training Epoch: 1/2, step 12/1149 completed (loss: 7.609196662902832, acc: 0.03846153989434242)
[2025-02-17 10:36:15,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:15,897][root][INFO] - Training Epoch: 1/2, step 13/1149 completed (loss: 7.78368616104126, acc: 0.0)
[2025-02-17 10:36:16,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:16,284][root][INFO] - Training Epoch: 1/2, step 14/1149 completed (loss: 7.923176288604736, acc: 0.0)
[2025-02-17 10:36:16,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:16,678][root][INFO] - Training Epoch: 1/2, step 15/1149 completed (loss: 7.488100051879883, acc: 0.0)
[2025-02-17 10:36:16,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:17,119][root][INFO] - Training Epoch: 1/2, step 16/1149 completed (loss: 9.574042320251465, acc: 0.0)
[2025-02-17 10:36:17,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:17,574][root][INFO] - Training Epoch: 1/2, step 17/1149 completed (loss: 8.752517700195312, acc: 0.0)
[2025-02-17 10:36:17,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:17,993][root][INFO] - Training Epoch: 1/2, step 18/1149 completed (loss: 6.888546466827393, acc: 0.0)
[2025-02-17 10:36:18,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:18,377][root][INFO] - Training Epoch: 1/2, step 19/1149 completed (loss: 7.191513538360596, acc: 0.07692307978868484)
[2025-02-17 10:36:18,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:18,794][root][INFO] - Training Epoch: 1/2, step 20/1149 completed (loss: 8.037299156188965, acc: 0.0)
[2025-02-17 10:36:18,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:19,184][root][INFO] - Training Epoch: 1/2, step 21/1149 completed (loss: 7.392660617828369, acc: 0.0)
[2025-02-17 10:36:19,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:19,686][root][INFO] - Training Epoch: 1/2, step 22/1149 completed (loss: 6.897879123687744, acc: 0.0)
[2025-02-17 10:36:19,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:20,134][root][INFO] - Training Epoch: 1/2, step 23/1149 completed (loss: 6.24915885925293, acc: 0.03703703731298447)
[2025-02-17 10:36:20,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:20,577][root][INFO] - Training Epoch: 1/2, step 24/1149 completed (loss: 6.873291015625, acc: 0.0)
[2025-02-17 10:36:20,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:20,956][root][INFO] - Training Epoch: 1/2, step 25/1149 completed (loss: 7.5363030433654785, acc: 0.0833333358168602)
[2025-02-17 10:36:21,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:21,374][root][INFO] - Training Epoch: 1/2, step 26/1149 completed (loss: 6.2465500831604, acc: 0.0)
[2025-02-17 10:36:21,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:21,775][root][INFO] - Training Epoch: 1/2, step 27/1149 completed (loss: 6.618411540985107, acc: 0.0625)
[2025-02-17 10:36:22,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:22,238][root][INFO] - Training Epoch: 1/2, step 28/1149 completed (loss: 6.438183307647705, acc: 0.032258063554763794)
[2025-02-17 10:36:22,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:22,640][root][INFO] - Training Epoch: 1/2, step 29/1149 completed (loss: 7.145474910736084, acc: 0.0)
[2025-02-17 10:36:22,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:23,013][root][INFO] - Training Epoch: 1/2, step 30/1149 completed (loss: 6.261370658874512, acc: 0.0)
[2025-02-17 10:36:23,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:23,421][root][INFO] - Training Epoch: 1/2, step 31/1149 completed (loss: 5.095848560333252, acc: 0.18518517911434174)
[2025-02-17 10:36:23,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:23,803][root][INFO] - Training Epoch: 1/2, step 32/1149 completed (loss: 8.74558162689209, acc: 0.0)
[2025-02-17 10:36:24,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:24,214][root][INFO] - Training Epoch: 1/2, step 33/1149 completed (loss: 7.037909507751465, acc: 0.0)
[2025-02-17 10:36:24,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:24,587][root][INFO] - Training Epoch: 1/2, step 34/1149 completed (loss: 5.984795093536377, acc: 0.0)
[2025-02-17 10:36:24,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:25,011][root][INFO] - Training Epoch: 1/2, step 35/1149 completed (loss: 7.058692455291748, acc: 0.0)
[2025-02-17 10:36:25,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:25,444][root][INFO] - Training Epoch: 1/2, step 36/1149 completed (loss: 5.600701332092285, acc: 0.0555555559694767)
[2025-02-17 10:36:25,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:25,837][root][INFO] - Training Epoch: 1/2, step 37/1149 completed (loss: 5.1813249588012695, acc: 0.0555555559694767)
[2025-02-17 10:36:26,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:26,215][root][INFO] - Training Epoch: 1/2, step 38/1149 completed (loss: 6.575638294219971, acc: 0.0)
[2025-02-17 10:36:26,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:26,644][root][INFO] - Training Epoch: 1/2, step 39/1149 completed (loss: 5.992114543914795, acc: 0.0)
[2025-02-17 10:36:26,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:27,022][root][INFO] - Training Epoch: 1/2, step 40/1149 completed (loss: 6.568124294281006, acc: 0.0833333358168602)
[2025-02-17 10:36:27,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:27,398][root][INFO] - Training Epoch: 1/2, step 41/1149 completed (loss: 6.5462517738342285, acc: 0.0)
[2025-02-17 10:36:27,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:27,826][root][INFO] - Training Epoch: 1/2, step 42/1149 completed (loss: 5.482880592346191, acc: 0.0)
[2025-02-17 10:36:27,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:28,202][root][INFO] - Training Epoch: 1/2, step 43/1149 completed (loss: 5.908867359161377, acc: 0.0625)
[2025-02-17 10:36:28,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:28,556][root][INFO] - Training Epoch: 1/2, step 44/1149 completed (loss: 5.764154434204102, acc: 0.0)
[2025-02-17 10:36:28,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:28,922][root][INFO] - Training Epoch: 1/2, step 45/1149 completed (loss: 5.457518577575684, acc: 0.07692307978868484)
[2025-02-17 10:36:29,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:29,342][root][INFO] - Training Epoch: 1/2, step 46/1149 completed (loss: 6.5385894775390625, acc: 0.09090909361839294)
[2025-02-17 10:36:29,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:29,765][root][INFO] - Training Epoch: 1/2, step 47/1149 completed (loss: 6.122143745422363, acc: 0.10000000149011612)
[2025-02-17 10:36:29,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:30,148][root][INFO] - Training Epoch: 1/2, step 48/1149 completed (loss: 8.612701416015625, acc: 0.0)
[2025-02-17 10:36:30,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:30,552][root][INFO] - Training Epoch: 1/2, step 49/1149 completed (loss: 5.296870708465576, acc: 0.1818181872367859)
[2025-02-17 10:36:30,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:30,912][root][INFO] - Training Epoch: 1/2, step 50/1149 completed (loss: 5.458588123321533, acc: 0.0)
[2025-02-17 10:36:31,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:31,296][root][INFO] - Training Epoch: 1/2, step 51/1149 completed (loss: 5.30750846862793, acc: 0.10000000149011612)
[2025-02-17 10:36:31,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:31,730][root][INFO] - Training Epoch: 1/2, step 52/1149 completed (loss: 4.915608882904053, acc: 0.054054055362939835)
[2025-02-17 10:36:31,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:32,139][root][INFO] - Training Epoch: 1/2, step 53/1149 completed (loss: 5.139214515686035, acc: 0.02777777798473835)
[2025-02-17 10:36:32,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:32,543][root][INFO] - Training Epoch: 1/2, step 54/1149 completed (loss: 4.875681400299072, acc: 0.02380952425301075)
[2025-02-17 10:36:33,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:33,797][root][INFO] - Training Epoch: 1/2, step 55/1149 completed (loss: 3.929781198501587, acc: 0.22274881601333618)
[2025-02-17 10:36:33,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:34,203][root][INFO] - Training Epoch: 1/2, step 56/1149 completed (loss: 5.162768840789795, acc: 0.10000000149011612)
[2025-02-17 10:36:34,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:34,598][root][INFO] - Training Epoch: 1/2, step 57/1149 completed (loss: 5.460666179656982, acc: 0.1304347813129425)
[2025-02-17 10:36:34,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:34,980][root][INFO] - Training Epoch: 1/2, step 58/1149 completed (loss: 4.294723987579346, acc: 0.21276596188545227)
[2025-02-17 10:36:35,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:35,441][root][INFO] - Training Epoch: 1/2, step 59/1149 completed (loss: 5.36009407043457, acc: 0.0555555559694767)
[2025-02-17 10:36:35,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:35,847][root][INFO] - Training Epoch: 1/2, step 60/1149 completed (loss: 4.404250144958496, acc: 0.11594203114509583)
[2025-02-17 10:36:36,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:36,246][root][INFO] - Training Epoch: 1/2, step 61/1149 completed (loss: 5.795302867889404, acc: 0.0833333358168602)
[2025-02-17 10:36:36,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:36,654][root][INFO] - Training Epoch: 1/2, step 62/1149 completed (loss: 5.418060779571533, acc: 0.0)
[2025-02-17 10:36:36,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:37,070][root][INFO] - Training Epoch: 1/2, step 63/1149 completed (loss: 5.718794822692871, acc: 0.0)
[2025-02-17 10:36:37,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:37,439][root][INFO] - Training Epoch: 1/2, step 64/1149 completed (loss: 5.0230326652526855, acc: 0.0)
[2025-02-17 10:36:37,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:37,884][root][INFO] - Training Epoch: 1/2, step 65/1149 completed (loss: 4.899409770965576, acc: 0.0)
[2025-02-17 10:36:38,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:38,303][root][INFO] - Training Epoch: 1/2, step 66/1149 completed (loss: 5.163182735443115, acc: 0.09090909361839294)
[2025-02-17 10:36:38,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:38,746][root][INFO] - Training Epoch: 1/2, step 67/1149 completed (loss: 6.64462423324585, acc: 0.0)
[2025-02-17 10:36:38,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:39,147][root][INFO] - Training Epoch: 1/2, step 68/1149 completed (loss: 4.241775035858154, acc: 0.2068965584039688)
[2025-02-17 10:36:39,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:39,536][root][INFO] - Training Epoch: 1/2, step 69/1149 completed (loss: 4.866568565368652, acc: 0.09375)
[2025-02-17 10:36:39,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:39,925][root][INFO] - Training Epoch: 1/2, step 70/1149 completed (loss: 4.857077121734619, acc: 0.11538461595773697)
[2025-02-17 10:36:40,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:40,308][root][INFO] - Training Epoch: 1/2, step 71/1149 completed (loss: 4.203702449798584, acc: 0.17142857611179352)
[2025-02-17 10:36:40,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:40,758][root][INFO] - Training Epoch: 1/2, step 72/1149 completed (loss: 3.93143892288208, acc: 0.19148936867713928)
[2025-02-17 10:36:40,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:41,146][root][INFO] - Training Epoch: 1/2, step 73/1149 completed (loss: 4.314809322357178, acc: 0.25)
[2025-02-17 10:36:41,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:41,519][root][INFO] - Training Epoch: 1/2, step 74/1149 completed (loss: 4.616298198699951, acc: 0.02857142873108387)
[2025-02-17 10:36:41,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:41,895][root][INFO] - Training Epoch: 1/2, step 75/1149 completed (loss: 5.327600002288818, acc: 0.0833333358168602)
[2025-02-17 10:36:42,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:42,294][root][INFO] - Training Epoch: 1/2, step 76/1149 completed (loss: 4.862042427062988, acc: 0.1041666641831398)
[2025-02-17 10:36:42,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:42,673][root][INFO] - Training Epoch: 1/2, step 77/1149 completed (loss: 2.9897541999816895, acc: 0.41025641560554504)
[2025-02-17 10:36:42,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:43,048][root][INFO] - Training Epoch: 1/2, step 78/1149 completed (loss: 5.294307231903076, acc: 0.0)
[2025-02-17 10:36:43,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:43,499][root][INFO] - Training Epoch: 1/2, step 79/1149 completed (loss: 4.1540679931640625, acc: 0.1538461595773697)
[2025-02-17 10:36:43,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:43,938][root][INFO] - Training Epoch: 1/2, step 80/1149 completed (loss: 4.943068981170654, acc: 0.1666666716337204)
[2025-02-17 10:36:44,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:44,381][root][INFO] - Training Epoch: 1/2, step 81/1149 completed (loss: 4.343773365020752, acc: 0.0714285746216774)
[2025-02-17 10:36:44,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:44,769][root][INFO] - Training Epoch: 1/2, step 82/1149 completed (loss: 4.296467304229736, acc: 0.1764705926179886)
[2025-02-17 10:36:44,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:45,198][root][INFO] - Training Epoch: 1/2, step 83/1149 completed (loss: 3.732151508331299, acc: 0.2631579041481018)
[2025-02-17 10:36:45,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:45,703][root][INFO] - Training Epoch: 1/2, step 84/1149 completed (loss: 3.94821834564209, acc: 0.25641027092933655)
[2025-02-17 10:36:45,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:46,133][root][INFO] - Training Epoch: 1/2, step 85/1149 completed (loss: 4.52700138092041, acc: 0.07692307978868484)
[2025-02-17 10:36:46,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:46,585][root][INFO] - Training Epoch: 1/2, step 86/1149 completed (loss: 3.8252086639404297, acc: 0.25531914830207825)
[2025-02-17 10:36:46,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:47,062][root][INFO] - Training Epoch: 1/2, step 87/1149 completed (loss: 4.746550559997559, acc: 0.21153846383094788)
[2025-02-17 10:36:47,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:47,444][root][INFO] - Training Epoch: 1/2, step 88/1149 completed (loss: 4.188248157501221, acc: 0.1666666716337204)
[2025-02-17 10:36:47,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:47,892][root][INFO] - Training Epoch: 1/2, step 89/1149 completed (loss: 3.504290819168091, acc: 0.44897958636283875)
[2025-02-17 10:36:48,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:48,336][root][INFO] - Training Epoch: 1/2, step 90/1149 completed (loss: 3.8216140270233154, acc: 0.20000000298023224)
[2025-02-17 10:36:48,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:48,737][root][INFO] - Training Epoch: 1/2, step 91/1149 completed (loss: 4.491661548614502, acc: 0.1964285671710968)
[2025-02-17 10:36:48,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:49,121][root][INFO] - Training Epoch: 1/2, step 92/1149 completed (loss: 4.475713729858398, acc: 0.20000000298023224)
[2025-02-17 10:36:49,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:49,565][root][INFO] - Training Epoch: 1/2, step 93/1149 completed (loss: 5.338425636291504, acc: 0.0)
[2025-02-17 10:36:49,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:50,033][root][INFO] - Training Epoch: 1/2, step 94/1149 completed (loss: 3.7696268558502197, acc: 0.23076923191547394)
[2025-02-17 10:36:50,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:50,449][root][INFO] - Training Epoch: 1/2, step 95/1149 completed (loss: 4.322996616363525, acc: 0.2142857164144516)
[2025-02-17 10:36:50,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:50,825][root][INFO] - Training Epoch: 1/2, step 96/1149 completed (loss: 3.2853262424468994, acc: 0.1818181872367859)
[2025-02-17 10:36:50,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:51,204][root][INFO] - Training Epoch: 1/2, step 97/1149 completed (loss: 4.441742420196533, acc: 0.11764705926179886)
[2025-02-17 10:36:51,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:51,642][root][INFO] - Training Epoch: 1/2, step 98/1149 completed (loss: 4.159400463104248, acc: 0.07692307978868484)
[2025-02-17 10:36:51,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:52,145][root][INFO] - Training Epoch: 1/2, step 99/1149 completed (loss: 4.325905799865723, acc: 0.08695652335882187)
[2025-02-17 10:36:52,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:52,624][root][INFO] - Training Epoch: 1/2, step 100/1149 completed (loss: 4.7421135902404785, acc: 0.09090909361839294)
[2025-02-17 10:36:52,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:53,067][root][INFO] - Training Epoch: 1/2, step 101/1149 completed (loss: 3.6855416297912598, acc: 0.2857142984867096)
[2025-02-17 10:36:53,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:53,477][root][INFO] - Training Epoch: 1/2, step 102/1149 completed (loss: 3.557218313217163, acc: 0.1666666716337204)
[2025-02-17 10:36:53,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:53,921][root][INFO] - Training Epoch: 1/2, step 103/1149 completed (loss: 3.912607431411743, acc: 0.31111112236976624)
[2025-02-17 10:36:54,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:54,367][root][INFO] - Training Epoch: 1/2, step 104/1149 completed (loss: 3.515097141265869, acc: 0.20000000298023224)
[2025-02-17 10:36:54,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:54,791][root][INFO] - Training Epoch: 1/2, step 105/1149 completed (loss: 4.313657760620117, acc: 0.1764705926179886)
[2025-02-17 10:36:54,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:55,223][root][INFO] - Training Epoch: 1/2, step 106/1149 completed (loss: 4.003166198730469, acc: 0.22033898532390594)
[2025-02-17 10:36:55,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:55,739][root][INFO] - Training Epoch: 1/2, step 107/1149 completed (loss: 3.969287157058716, acc: 0.2527472674846649)
[2025-02-17 10:36:55,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:56,183][root][INFO] - Training Epoch: 1/2, step 108/1149 completed (loss: 4.383159637451172, acc: 0.20000000298023224)
[2025-02-17 10:36:56,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:56,574][root][INFO] - Training Epoch: 1/2, step 109/1149 completed (loss: 3.339245557785034, acc: 0.4166666567325592)
[2025-02-17 10:36:56,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:56,951][root][INFO] - Training Epoch: 1/2, step 110/1149 completed (loss: 4.286640644073486, acc: 0.1111111119389534)
[2025-02-17 10:36:57,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:57,381][root][INFO] - Training Epoch: 1/2, step 111/1149 completed (loss: 4.205312728881836, acc: 0.0)
[2025-02-17 10:36:57,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:36:57,834][root][INFO] - Training Epoch: 1/2, step 112/1149 completed (loss: 3.38493275642395, acc: 0.4000000059604645)
[2025-02-17 10:36:59,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:00,554][root][INFO] - Training Epoch: 1/2, step 113/1149 completed (loss: 2.7572336196899414, acc: 0.43816253542900085)
[2025-02-17 10:37:01,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:01,502][root][INFO] - Training Epoch: 1/2, step 114/1149 completed (loss: 3.2296228408813477, acc: 0.32319390773773193)
[2025-02-17 10:37:02,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:02,670][root][INFO] - Training Epoch: 1/2, step 115/1149 completed (loss: 3.4747934341430664, acc: 0.27040815353393555)
[2025-02-17 10:37:03,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:03,324][root][INFO] - Training Epoch: 1/2, step 116/1149 completed (loss: 3.2298669815063477, acc: 0.3737373650074005)
[2025-02-17 10:37:03,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:03,812][root][INFO] - Training Epoch: 1/2, step 117/1149 completed (loss: 3.562624454498291, acc: 0.2857142984867096)
[2025-02-17 10:37:04,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:04,400][root][INFO] - Training Epoch: 1/2, step 118/1149 completed (loss: 3.604553699493408, acc: 0.2933333218097687)
[2025-02-17 10:37:04,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:04,830][root][INFO] - Training Epoch: 1/2, step 119/1149 completed (loss: 3.4860024452209473, acc: 0.2786885201931)
[2025-02-17 10:37:05,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:05,229][root][INFO] - Training Epoch: 1/2, step 120/1149 completed (loss: 3.3773739337921143, acc: 0.1666666716337204)
[2025-02-17 10:37:05,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:05,801][root][INFO] - Training Epoch: 1/2, step 121/1149 completed (loss: 3.2576446533203125, acc: 0.2604166567325592)
[2025-02-17 10:37:06,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:06,281][root][INFO] - Training Epoch: 1/2, step 122/1149 completed (loss: 3.2399046421051025, acc: 0.4027777910232544)
[2025-02-17 10:37:06,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:06,728][root][INFO] - Training Epoch: 1/2, step 123/1149 completed (loss: 4.714565277099609, acc: 0.125)
[2025-02-17 10:37:06,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:07,158][root][INFO] - Training Epoch: 1/2, step 124/1149 completed (loss: 4.253647327423096, acc: 0.15789473056793213)
[2025-02-17 10:37:07,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:07,658][root][INFO] - Training Epoch: 1/2, step 125/1149 completed (loss: 2.972376585006714, acc: 0.20000000298023224)
[2025-02-17 10:37:07,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:08,061][root][INFO] - Training Epoch: 1/2, step 126/1149 completed (loss: 3.6160757541656494, acc: 0.2142857164144516)
[2025-02-17 10:37:08,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:08,512][root][INFO] - Training Epoch: 1/2, step 127/1149 completed (loss: 3.3319878578186035, acc: 0.3636363744735718)
[2025-02-17 10:37:08,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:08,955][root][INFO] - Training Epoch: 1/2, step 128/1149 completed (loss: 3.1070728302001953, acc: 0.23529411852359772)
[2025-02-17 10:37:09,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:09,449][root][INFO] - Training Epoch: 1/2, step 129/1149 completed (loss: 2.484517812728882, acc: 0.38297873735427856)
[2025-02-17 10:37:09,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:09,851][root][INFO] - Training Epoch: 1/2, step 130/1149 completed (loss: 2.7928569316864014, acc: 0.31578946113586426)
[2025-02-17 10:37:10,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:10,296][root][INFO] - Training Epoch: 1/2, step 131/1149 completed (loss: 3.222118854522705, acc: 0.20000000298023224)
[2025-02-17 10:37:10,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:10,729][root][INFO] - Training Epoch: 1/2, step 132/1149 completed (loss: 3.6328940391540527, acc: 0.3448275923728943)
[2025-02-17 10:37:10,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:11,105][root][INFO] - Training Epoch: 1/2, step 133/1149 completed (loss: 2.7903635501861572, acc: 0.40740740299224854)
[2025-02-17 10:37:11,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:11,502][root][INFO] - Training Epoch: 1/2, step 134/1149 completed (loss: 3.0296339988708496, acc: 0.24137930572032928)
[2025-02-17 10:37:11,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:11,919][root][INFO] - Training Epoch: 1/2, step 135/1149 completed (loss: 3.175265073776245, acc: 0.35483869910240173)
[2025-02-17 10:37:12,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:12,275][root][INFO] - Training Epoch: 1/2, step 136/1149 completed (loss: 2.3975775241851807, acc: 0.4166666567325592)
[2025-02-17 10:37:12,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:12,654][root][INFO] - Training Epoch: 1/2, step 137/1149 completed (loss: 3.8438987731933594, acc: 0.23076923191547394)
[2025-02-17 10:37:12,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:13,044][root][INFO] - Training Epoch: 1/2, step 138/1149 completed (loss: 3.5876388549804688, acc: 0.17391304671764374)
[2025-02-17 10:37:13,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:13,450][root][INFO] - Training Epoch: 1/2, step 139/1149 completed (loss: 2.9206197261810303, acc: 0.3076923191547394)
[2025-02-17 10:37:13,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:13,808][root][INFO] - Training Epoch: 1/2, step 140/1149 completed (loss: 2.7339537143707275, acc: 0.2666666805744171)
[2025-02-17 10:37:13,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:14,211][root][INFO] - Training Epoch: 1/2, step 141/1149 completed (loss: 4.069330215454102, acc: 0.2222222238779068)
[2025-02-17 10:37:14,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:14,674][root][INFO] - Training Epoch: 1/2, step 142/1149 completed (loss: 4.086811065673828, acc: 0.11764705926179886)
[2025-02-17 10:37:14,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:15,107][root][INFO] - Training Epoch: 1/2, step 143/1149 completed (loss: 3.334043502807617, acc: 0.29411765933036804)
[2025-02-17 10:37:15,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:15,505][root][INFO] - Training Epoch: 1/2, step 144/1149 completed (loss: 2.690403461456299, acc: 0.39024388790130615)
[2025-02-17 10:37:15,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:15,929][root][INFO] - Training Epoch: 1/2, step 145/1149 completed (loss: 3.1539876461029053, acc: 0.1818181872367859)
[2025-02-17 10:37:16,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:16,363][root][INFO] - Training Epoch: 1/2, step 146/1149 completed (loss: 3.2290143966674805, acc: 0.32967033982276917)
[2025-02-17 10:37:16,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:16,779][root][INFO] - Training Epoch: 1/2, step 147/1149 completed (loss: 3.410154104232788, acc: 0.2211538404226303)
[2025-02-17 10:37:16,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:17,162][root][INFO] - Training Epoch: 1/2, step 148/1149 completed (loss: 3.5506815910339355, acc: 0.18333333730697632)
[2025-02-17 10:37:17,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:17,520][root][INFO] - Training Epoch: 1/2, step 149/1149 completed (loss: 3.7854936122894287, acc: 0.28947368264198303)
[2025-02-17 10:37:17,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:17,894][root][INFO] - Training Epoch: 1/2, step 150/1149 completed (loss: 3.4940812587738037, acc: 0.22077922523021698)
[2025-02-17 10:37:18,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:18,296][root][INFO] - Training Epoch: 1/2, step 151/1149 completed (loss: 3.522514581680298, acc: 0.19298245012760162)
[2025-02-17 10:37:18,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:18,750][root][INFO] - Training Epoch: 1/2, step 152/1149 completed (loss: 2.8891875743865967, acc: 0.359375)
[2025-02-17 10:37:18,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:19,169][root][INFO] - Training Epoch: 1/2, step 153/1149 completed (loss: 3.1086597442626953, acc: 0.30821916460990906)
[2025-02-17 10:37:19,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:19,527][root][INFO] - Training Epoch: 1/2, step 154/1149 completed (loss: 3.454514265060425, acc: 0.2222222238779068)
[2025-02-17 10:37:19,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:19,887][root][INFO] - Training Epoch: 1/2, step 155/1149 completed (loss: 3.6924173831939697, acc: 0.0833333358168602)
[2025-02-17 10:37:20,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:20,260][root][INFO] - Training Epoch: 1/2, step 156/1149 completed (loss: 2.9890196323394775, acc: 0.2857142984867096)
[2025-02-17 10:37:20,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:20,626][root][INFO] - Training Epoch: 1/2, step 157/1149 completed (loss: 3.607278347015381, acc: 0.4000000059604645)
[2025-02-17 10:37:20,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:21,006][root][INFO] - Training Epoch: 1/2, step 158/1149 completed (loss: 2.4510843753814697, acc: 0.23076923191547394)
[2025-02-17 10:37:21,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:21,396][root][INFO] - Training Epoch: 1/2, step 159/1149 completed (loss: 3.182217836380005, acc: 0.30000001192092896)
[2025-02-17 10:37:21,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:21,850][root][INFO] - Training Epoch: 1/2, step 160/1149 completed (loss: 2.9954724311828613, acc: 0.20000000298023224)
[2025-02-17 10:37:22,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:22,253][root][INFO] - Training Epoch: 1/2, step 161/1149 completed (loss: 3.0111916065216064, acc: 0.3636363744735718)
[2025-02-17 10:37:22,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:22,644][root][INFO] - Training Epoch: 1/2, step 162/1149 completed (loss: 2.745828151702881, acc: 0.20000000298023224)
[2025-02-17 10:37:22,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:23,031][root][INFO] - Training Epoch: 1/2, step 163/1149 completed (loss: 3.3781819343566895, acc: 0.2702702581882477)
[2025-02-17 10:37:23,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:23,401][root][INFO] - Training Epoch: 1/2, step 164/1149 completed (loss: 3.63492751121521, acc: 0.1666666716337204)
[2025-02-17 10:37:23,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:23,790][root][INFO] - Training Epoch: 1/2, step 165/1149 completed (loss: 2.905409097671509, acc: 0.2857142984867096)
[2025-02-17 10:37:23,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:24,148][root][INFO] - Training Epoch: 1/2, step 166/1149 completed (loss: 2.940736770629883, acc: 0.30000001192092896)
[2025-02-17 10:37:24,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:24,503][root][INFO] - Training Epoch: 1/2, step 167/1149 completed (loss: 2.778841972351074, acc: 0.1666666716337204)
[2025-02-17 10:37:24,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:24,919][root][INFO] - Training Epoch: 1/2, step 168/1149 completed (loss: 2.37941312789917, acc: 0.46666666865348816)
[2025-02-17 10:37:25,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:25,373][root][INFO] - Training Epoch: 1/2, step 169/1149 completed (loss: 3.2120437622070312, acc: 0.23076923191547394)
[2025-02-17 10:37:25,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:25,796][root][INFO] - Training Epoch: 1/2, step 170/1149 completed (loss: 3.0817434787750244, acc: 0.3529411852359772)
[2025-02-17 10:37:25,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:26,165][root][INFO] - Training Epoch: 1/2, step 171/1149 completed (loss: 2.0392353534698486, acc: 0.5)
[2025-02-17 10:37:26,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:26,533][root][INFO] - Training Epoch: 1/2, step 172/1149 completed (loss: 3.17535400390625, acc: 0.23076923191547394)
[2025-02-17 10:37:26,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:26,906][root][INFO] - Training Epoch: 1/2, step 173/1149 completed (loss: 3.177572250366211, acc: 0.20000000298023224)
[2025-02-17 10:37:27,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:27,301][root][INFO] - Training Epoch: 1/2, step 174/1149 completed (loss: 2.6383659839630127, acc: 0.20000000298023224)
[2025-02-17 10:37:27,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:27,833][root][INFO] - Training Epoch: 1/2, step 175/1149 completed (loss: 3.1088221073150635, acc: 0.2750000059604645)
[2025-02-17 10:37:28,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:28,206][root][INFO] - Training Epoch: 1/2, step 176/1149 completed (loss: 2.430924892425537, acc: 0.3968254029750824)
[2025-02-17 10:37:28,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:28,576][root][INFO] - Training Epoch: 1/2, step 177/1149 completed (loss: 2.6758179664611816, acc: 0.4000000059604645)
[2025-02-17 10:37:29,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:29,634][root][INFO] - Training Epoch: 1/2, step 178/1149 completed (loss: 2.7506213188171387, acc: 0.3733333349227905)
[2025-02-17 10:37:29,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:30,043][root][INFO] - Training Epoch: 1/2, step 179/1149 completed (loss: 2.6291632652282715, acc: 0.42748090624809265)
[2025-02-17 10:37:30,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:30,805][root][INFO] - Training Epoch: 1/2, step 180/1149 completed (loss: 2.723212718963623, acc: 0.3962264060974121)
[2025-02-17 10:37:31,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:31,345][root][INFO] - Training Epoch: 1/2, step 181/1149 completed (loss: 2.8860597610473633, acc: 0.3125)
[2025-02-17 10:37:31,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:32,079][root][INFO] - Training Epoch: 1/2, step 182/1149 completed (loss: 2.522174119949341, acc: 0.44897958636283875)
[2025-02-17 10:37:32,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:32,513][root][INFO] - Training Epoch: 1/2, step 183/1149 completed (loss: 2.5341575145721436, acc: 0.38823530077934265)
[2025-02-17 10:37:32,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:33,249][root][INFO] - Training Epoch: 1/2, step 184/1149 completed (loss: 2.459264039993286, acc: 0.4285714328289032)
[2025-02-17 10:37:33,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:33,611][root][INFO] - Training Epoch: 1/2, step 185/1149 completed (loss: 2.6348204612731934, acc: 0.3888888955116272)
[2025-02-17 10:37:34,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:34,540][root][INFO] - Training Epoch: 1/2, step 186/1149 completed (loss: 2.944302558898926, acc: 0.27586206793785095)
[2025-02-17 10:37:34,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:34,885][root][INFO] - Training Epoch: 1/2, step 187/1149 completed (loss: 2.7888872623443604, acc: 0.2857142984867096)
[2025-02-17 10:37:35,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:35,299][root][INFO] - Training Epoch: 1/2, step 188/1149 completed (loss: 3.4161276817321777, acc: 0.20000000298023224)
[2025-02-17 10:37:35,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:35,686][root][INFO] - Training Epoch: 1/2, step 189/1149 completed (loss: 2.949164867401123, acc: 0.32692307233810425)
[2025-02-17 10:37:35,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:36,097][root][INFO] - Training Epoch: 1/2, step 190/1149 completed (loss: 2.646531820297241, acc: 0.3333333432674408)
[2025-02-17 10:37:36,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:36,511][root][INFO] - Training Epoch: 1/2, step 191/1149 completed (loss: 3.0745646953582764, acc: 0.1666666716337204)
[2025-02-17 10:37:36,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:36,879][root][INFO] - Training Epoch: 1/2, step 192/1149 completed (loss: 3.1146626472473145, acc: 0.2222222238779068)
[2025-02-17 10:37:37,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:37,267][root][INFO] - Training Epoch: 1/2, step 193/1149 completed (loss: 3.1220173835754395, acc: 0.1927710771560669)
[2025-02-17 10:37:37,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:37,682][root][INFO] - Training Epoch: 1/2, step 194/1149 completed (loss: 3.1523544788360596, acc: 0.1111111119389534)
[2025-02-17 10:37:37,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:38,110][root][INFO] - Training Epoch: 1/2, step 195/1149 completed (loss: 2.754533290863037, acc: 0.3400000035762787)
[2025-02-17 10:37:38,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:38,536][root][INFO] - Training Epoch: 1/2, step 196/1149 completed (loss: 3.042574167251587, acc: 0.2542372941970825)
[2025-02-17 10:37:38,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:38,953][root][INFO] - Training Epoch: 1/2, step 197/1149 completed (loss: 2.776528835296631, acc: 0.3205128312110901)
[2025-02-17 10:37:39,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:39,332][root][INFO] - Training Epoch: 1/2, step 198/1149 completed (loss: 3.186631679534912, acc: 0.1666666716337204)
[2025-02-17 10:37:39,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:39,716][root][INFO] - Training Epoch: 1/2, step 199/1149 completed (loss: 2.9992246627807617, acc: 0.13513512909412384)
[2025-02-17 10:37:39,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:40,097][root][INFO] - Training Epoch: 1/2, step 200/1149 completed (loss: 3.07378888130188, acc: 0.1666666716337204)
[2025-02-17 10:37:40,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:40,557][root][INFO] - Training Epoch: 1/2, step 201/1149 completed (loss: 3.5613579750061035, acc: 0.25)
[2025-02-17 10:37:40,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:41,006][root][INFO] - Training Epoch: 1/2, step 202/1149 completed (loss: 2.4203193187713623, acc: 0.30000001192092896)
[2025-02-17 10:37:41,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:41,358][root][INFO] - Training Epoch: 1/2, step 203/1149 completed (loss: 2.9112284183502197, acc: 0.25)
[2025-02-17 10:37:41,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:41,792][root][INFO] - Training Epoch: 1/2, step 204/1149 completed (loss: 2.4615185260772705, acc: 0.25)
[2025-02-17 10:37:41,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:42,167][root][INFO] - Training Epoch: 1/2, step 205/1149 completed (loss: 2.3918561935424805, acc: 0.1818181872367859)
[2025-02-17 10:37:42,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:42,542][root][INFO] - Training Epoch: 1/2, step 206/1149 completed (loss: 2.2688465118408203, acc: 0.4117647111415863)
[2025-02-17 10:37:42,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:42,929][root][INFO] - Training Epoch: 1/2, step 207/1149 completed (loss: 3.0242888927459717, acc: 0.25925925374031067)
[2025-02-17 10:37:43,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:43,389][root][INFO] - Training Epoch: 1/2, step 208/1149 completed (loss: 2.9600412845611572, acc: 0.23076923191547394)
[2025-02-17 10:37:43,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:43,861][root][INFO] - Training Epoch: 1/2, step 209/1149 completed (loss: 2.9469892978668213, acc: 0.3684210479259491)
[2025-02-17 10:37:44,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:44,260][root][INFO] - Training Epoch: 1/2, step 210/1149 completed (loss: 3.3202078342437744, acc: 0.3333333432674408)
[2025-02-17 10:37:44,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:44,682][root][INFO] - Training Epoch: 1/2, step 211/1149 completed (loss: 3.6260221004486084, acc: 0.0714285746216774)
[2025-02-17 10:37:44,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:45,116][root][INFO] - Training Epoch: 1/2, step 212/1149 completed (loss: 2.537402391433716, acc: 0.25)
[2025-02-17 10:37:45,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:45,536][root][INFO] - Training Epoch: 1/2, step 213/1149 completed (loss: 2.1028223037719727, acc: 0.4615384638309479)
[2025-02-17 10:37:45,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:45,981][root][INFO] - Training Epoch: 1/2, step 214/1149 completed (loss: 2.819200277328491, acc: 0.25)
[2025-02-17 10:37:46,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:46,411][root][INFO] - Training Epoch: 1/2, step 215/1149 completed (loss: 2.524881601333618, acc: 0.3333333432674408)
[2025-02-17 10:37:46,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:46,837][root][INFO] - Training Epoch: 1/2, step 216/1149 completed (loss: 2.5699713230133057, acc: 0.2857142984867096)
[2025-02-17 10:37:47,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:47,263][root][INFO] - Training Epoch: 1/2, step 217/1149 completed (loss: 2.3422720432281494, acc: 0.3333333432674408)
[2025-02-17 10:37:47,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:47,707][root][INFO] - Training Epoch: 1/2, step 218/1149 completed (loss: 3.3460068702697754, acc: 0.29411765933036804)
[2025-02-17 10:37:47,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:48,072][root][INFO] - Training Epoch: 1/2, step 219/1149 completed (loss: 2.888230323791504, acc: 0.2800000011920929)
[2025-02-17 10:37:48,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:48,450][root][INFO] - Training Epoch: 1/2, step 220/1149 completed (loss: 2.9350037574768066, acc: 0.4117647111415863)
[2025-02-17 10:37:48,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:48,887][root][INFO] - Training Epoch: 1/2, step 221/1149 completed (loss: 2.8635623455047607, acc: 0.25)
[2025-02-17 10:37:49,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:49,359][root][INFO] - Training Epoch: 1/2, step 222/1149 completed (loss: 3.126736640930176, acc: 0.1944444477558136)
[2025-02-17 10:37:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:49,828][root][INFO] - Training Epoch: 1/2, step 223/1149 completed (loss: 3.4123034477233887, acc: 0.2380952388048172)
[2025-02-17 10:37:50,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:50,260][root][INFO] - Training Epoch: 1/2, step 224/1149 completed (loss: 2.6915488243103027, acc: 0.3103448152542114)
[2025-02-17 10:37:50,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:50,662][root][INFO] - Training Epoch: 1/2, step 225/1149 completed (loss: 3.1980698108673096, acc: 0.25)
[2025-02-17 10:37:50,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:51,076][root][INFO] - Training Epoch: 1/2, step 226/1149 completed (loss: 2.8941121101379395, acc: 0.4285714328289032)
[2025-02-17 10:37:51,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:51,509][root][INFO] - Training Epoch: 1/2, step 227/1149 completed (loss: 3.031420946121216, acc: 0.2222222238779068)
[2025-02-17 10:37:51,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:51,914][root][INFO] - Training Epoch: 1/2, step 228/1149 completed (loss: 2.5297415256500244, acc: 0.375)
[2025-02-17 10:37:52,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:52,276][root][INFO] - Training Epoch: 1/2, step 229/1149 completed (loss: 2.4300832748413086, acc: 0.47058823704719543)
[2025-02-17 10:37:52,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:52,645][root][INFO] - Training Epoch: 1/2, step 230/1149 completed (loss: 2.5753915309906006, acc: 0.4615384638309479)
[2025-02-17 10:37:52,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:53,009][root][INFO] - Training Epoch: 1/2, step 231/1149 completed (loss: 2.233689308166504, acc: 0.4444444477558136)
[2025-02-17 10:37:53,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:53,460][root][INFO] - Training Epoch: 1/2, step 232/1149 completed (loss: 2.8169305324554443, acc: 0.3333333432674408)
[2025-02-17 10:37:53,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:53,918][root][INFO] - Training Epoch: 1/2, step 233/1149 completed (loss: 2.634781837463379, acc: 0.3137255012989044)
[2025-02-17 10:37:54,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:54,321][root][INFO] - Training Epoch: 1/2, step 234/1149 completed (loss: 3.0035958290100098, acc: 0.3050847351551056)
[2025-02-17 10:37:54,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:54,747][root][INFO] - Training Epoch: 1/2, step 235/1149 completed (loss: 2.549931287765503, acc: 0.390625)
[2025-02-17 10:37:54,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:55,180][root][INFO] - Training Epoch: 1/2, step 236/1149 completed (loss: 2.1443490982055664, acc: 0.5)
[2025-02-17 10:37:55,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:55,612][root][INFO] - Training Epoch: 1/2, step 237/1149 completed (loss: 2.705379009246826, acc: 0.3199999928474426)
[2025-02-17 10:37:56,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:56,429][root][INFO] - Training Epoch: 1/2, step 238/1149 completed (loss: 2.5424976348876953, acc: 0.35384616255760193)
[2025-02-17 10:37:56,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:56,834][root][INFO] - Training Epoch: 1/2, step 239/1149 completed (loss: 2.6521522998809814, acc: 0.30882352590560913)
[2025-02-17 10:37:57,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:57,273][root][INFO] - Training Epoch: 1/2, step 240/1149 completed (loss: 2.66001558303833, acc: 0.34375)
[2025-02-17 10:37:57,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:57,655][root][INFO] - Training Epoch: 1/2, step 241/1149 completed (loss: 2.4205307960510254, acc: 0.44186046719551086)
[2025-02-17 10:37:57,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:58,101][root][INFO] - Training Epoch: 1/2, step 242/1149 completed (loss: 2.527472972869873, acc: 0.41860464215278625)
[2025-02-17 10:37:58,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:58,510][root][INFO] - Training Epoch: 1/2, step 243/1149 completed (loss: 3.0022530555725098, acc: 0.3333333432674408)
[2025-02-17 10:37:58,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:58,880][root][INFO] - Training Epoch: 1/2, step 244/1149 completed (loss: 2.467745542526245, acc: 0.4545454680919647)
[2025-02-17 10:37:59,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:59,328][root][INFO] - Training Epoch: 1/2, step 245/1149 completed (loss: 2.5357284545898438, acc: 0.3076923191547394)
[2025-02-17 10:37:59,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:37:59,727][root][INFO] - Training Epoch: 1/2, step 246/1149 completed (loss: 1.9492981433868408, acc: 0.625)
[2025-02-17 10:37:59,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:00,142][root][INFO] - Training Epoch: 1/2, step 247/1149 completed (loss: 2.979550838470459, acc: 0.3636363744735718)
[2025-02-17 10:38:00,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:00,613][root][INFO] - Training Epoch: 1/2, step 248/1149 completed (loss: 2.9855241775512695, acc: 0.3055555522441864)
[2025-02-17 10:38:00,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:01,074][root][INFO] - Training Epoch: 1/2, step 249/1149 completed (loss: 3.050628900527954, acc: 0.21978022158145905)
[2025-02-17 10:38:01,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:01,517][root][INFO] - Training Epoch: 1/2, step 250/1149 completed (loss: 2.9592232704162598, acc: 0.25)
[2025-02-17 10:38:01,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:02,016][root][INFO] - Training Epoch: 1/2, step 251/1149 completed (loss: 2.3492231369018555, acc: 0.40625)
[2025-02-17 10:38:02,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:02,405][root][INFO] - Training Epoch: 1/2, step 252/1149 completed (loss: 2.6511216163635254, acc: 0.2716049253940582)
[2025-02-17 10:38:02,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:02,794][root][INFO] - Training Epoch: 1/2, step 253/1149 completed (loss: 2.6256866455078125, acc: 0.3076923191547394)
[2025-02-17 10:38:03,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:03,258][root][INFO] - Training Epoch: 1/2, step 254/1149 completed (loss: 2.7883617877960205, acc: 0.20720720291137695)
[2025-02-17 10:38:03,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:03,635][root][INFO] - Training Epoch: 1/2, step 255/1149 completed (loss: 2.705130100250244, acc: 0.28070175647735596)
[2025-02-17 10:38:03,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:04,099][root][INFO] - Training Epoch: 1/2, step 256/1149 completed (loss: 2.692587375640869, acc: 0.29870128631591797)
[2025-02-17 10:38:04,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:04,520][root][INFO] - Training Epoch: 1/2, step 257/1149 completed (loss: 2.7472589015960693, acc: 0.32203391194343567)
[2025-02-17 10:38:04,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:04,934][root][INFO] - Training Epoch: 1/2, step 258/1149 completed (loss: 2.2716901302337646, acc: 0.421875)
[2025-02-17 10:38:05,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:05,334][root][INFO] - Training Epoch: 1/2, step 259/1149 completed (loss: 2.5201644897460938, acc: 0.25)
[2025-02-17 10:38:05,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:05,761][root][INFO] - Training Epoch: 1/2, step 260/1149 completed (loss: 2.9795167446136475, acc: 0.2857142984867096)
[2025-02-17 10:38:05,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:06,148][root][INFO] - Training Epoch: 1/2, step 261/1149 completed (loss: 2.21995210647583, acc: 0.5)
[2025-02-17 10:38:06,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:06,564][root][INFO] - Training Epoch: 1/2, step 262/1149 completed (loss: 2.198561191558838, acc: 0.4000000059604645)
[2025-02-17 10:38:06,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:06,930][root][INFO] - Training Epoch: 1/2, step 263/1149 completed (loss: 2.0308573246002197, acc: 0.4615384638309479)
[2025-02-17 10:38:07,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:07,297][root][INFO] - Training Epoch: 1/2, step 264/1149 completed (loss: 3.323277473449707, acc: 0.0625)
[2025-02-17 10:38:07,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:07,659][root][INFO] - Training Epoch: 1/2, step 265/1149 completed (loss: 2.4477298259735107, acc: 0.3125)
[2025-02-17 10:38:07,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:08,095][root][INFO] - Training Epoch: 1/2, step 266/1149 completed (loss: 1.9251351356506348, acc: 0.4545454680919647)
[2025-02-17 10:38:08,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:08,542][root][INFO] - Training Epoch: 1/2, step 267/1149 completed (loss: 2.8549325466156006, acc: 0.25)
[2025-02-17 10:38:08,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:08,920][root][INFO] - Training Epoch: 1/2, step 268/1149 completed (loss: 1.6716796159744263, acc: 0.5384615659713745)
[2025-02-17 10:38:09,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:09,350][root][INFO] - Training Epoch: 1/2, step 269/1149 completed (loss: 2.6169843673706055, acc: 0.3181818127632141)
[2025-02-17 10:38:09,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:09,727][root][INFO] - Training Epoch: 1/2, step 270/1149 completed (loss: 2.501744031906128, acc: 0.46666666865348816)
[2025-02-17 10:38:09,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:10,080][root][INFO] - Training Epoch: 1/2, step 271/1149 completed (loss: 2.6192467212677, acc: 0.1818181872367859)
[2025-02-17 10:38:10,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:10,523][root][INFO] - Training Epoch: 1/2, step 272/1149 completed (loss: 2.3407225608825684, acc: 0.4000000059604645)
[2025-02-17 10:38:10,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:10,999][root][INFO] - Training Epoch: 1/2, step 273/1149 completed (loss: 3.0737788677215576, acc: 0.25)
[2025-02-17 10:38:11,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:11,373][root][INFO] - Training Epoch: 1/2, step 274/1149 completed (loss: 2.774071455001831, acc: 0.3571428656578064)
[2025-02-17 10:38:11,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:11,780][root][INFO] - Training Epoch: 1/2, step 275/1149 completed (loss: 1.9237319231033325, acc: 0.375)
[2025-02-17 10:38:12,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:12,229][root][INFO] - Training Epoch: 1/2, step 276/1149 completed (loss: 2.4285829067230225, acc: 0.3076923191547394)
[2025-02-17 10:38:12,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:12,645][root][INFO] - Training Epoch: 1/2, step 277/1149 completed (loss: 2.4172492027282715, acc: 0.30000001192092896)
[2025-02-17 10:38:12,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:13,036][root][INFO] - Training Epoch: 1/2, step 278/1149 completed (loss: 3.0293052196502686, acc: 0.27272728085517883)
[2025-02-17 10:38:13,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:13,409][root][INFO] - Training Epoch: 1/2, step 279/1149 completed (loss: 2.8348116874694824, acc: 0.4000000059604645)
[2025-02-17 10:38:13,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:13,745][root][INFO] - Training Epoch: 1/2, step 280/1149 completed (loss: 2.613562822341919, acc: 0.4166666567325592)
[2025-02-17 10:38:13,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:14,126][root][INFO] - Training Epoch: 1/2, step 281/1149 completed (loss: 3.22196888923645, acc: 0.2142857164144516)
[2025-02-17 10:38:14,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:14,535][root][INFO] - Training Epoch: 1/2, step 282/1149 completed (loss: 2.789107322692871, acc: 0.2222222238779068)
[2025-02-17 10:38:14,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:14,956][root][INFO] - Training Epoch: 1/2, step 283/1149 completed (loss: 3.2955262660980225, acc: 0.23076923191547394)
[2025-02-17 10:38:15,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:15,348][root][INFO] - Training Epoch: 1/2, step 284/1149 completed (loss: 4.012596130371094, acc: 0.11764705926179886)
[2025-02-17 10:38:15,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:15,751][root][INFO] - Training Epoch: 1/2, step 285/1149 completed (loss: 3.4429752826690674, acc: 0.10000000149011612)
[2025-02-17 10:38:16,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:16,330][root][INFO] - Training Epoch: 1/2, step 286/1149 completed (loss: 2.3107776641845703, acc: 0.380952388048172)
[2025-02-17 10:38:17,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:17,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:18,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:18,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:19,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:19,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:20,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:20,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:21,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:21,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:22,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:23,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:23,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:24,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:24,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:24,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:25,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:26,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:26,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:27,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:27,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:28,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:28,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:29,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:29,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:30,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:30,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:31,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:31,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:32,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:33,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:33,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:33,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:34,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:34,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:35,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:35,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:36,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:36,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:37,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:38,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:38,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:39,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:39,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:40,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:40,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:41,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:41,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:42,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:42,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:43,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:43,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:44,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:44,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:45,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:45,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:46,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:46,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:47,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:48,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:48,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:49,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:49,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:50,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:50,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:51,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:51,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:52,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:52,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:53,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:54,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:54,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:55,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:55,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:56,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:56,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:57,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:57,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:58,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:58,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:58,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:38:59,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:00,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:00,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:00,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:01,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:01,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:02,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:02,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:03,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:03,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:04,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:04,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:05,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:06,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:06,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:07,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:07,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:08,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:08,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:08,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:09,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:09,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:10,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:10,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:11,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:11,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:12,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:12,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:13,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:13,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:14,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:14,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:15,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:15,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:16,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:16,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:17,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:17,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:18,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:19,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:19,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:20,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:20,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:21,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:21,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:22,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:22,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:23,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:23,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:24,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:24,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:25,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:25,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:26,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:26,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:27,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:27,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:28,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:28,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:29,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:29,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:30,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:30,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:31,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:31,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:32,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:32,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:33,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:33,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:34,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:34,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:35,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:35,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:36,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:37,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:37,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:38,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:38,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:39,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:39,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:40,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:40,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:41,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:41,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:42,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:42,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:43,595][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(14.8385, device='cuda:0') eval_epoch_loss=tensor(2.6972, device='cuda:0') eval_epoch_acc=tensor(0.3015, device='cuda:0')
[2025-02-17 10:39:43,596][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 10:39:43,597][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 10:39:47,090][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_1_step_287_loss_2.697222948074341/model.pt
[2025-02-17 10:39:47,097][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 10:39:47,098][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.697222948074341
[2025-02-17 10:39:47,099][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.3014988303184509
[2025-02-17 10:39:47,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:47,596][root][INFO] - Training Epoch: 1/2, step 287/1149 completed (loss: 2.5984609127044678, acc: 0.2666666805744171)
[2025-02-17 10:39:47,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:47,986][root][INFO] - Training Epoch: 1/2, step 288/1149 completed (loss: 1.9863348007202148, acc: 0.44736841320991516)
[2025-02-17 10:39:48,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:48,429][root][INFO] - Training Epoch: 1/2, step 289/1149 completed (loss: 2.385755777359009, acc: 0.4270833432674408)
[2025-02-17 10:39:48,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:48,807][root][INFO] - Training Epoch: 1/2, step 290/1149 completed (loss: 2.9878041744232178, acc: 0.2857142984867096)
[2025-02-17 10:39:49,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:49,241][root][INFO] - Training Epoch: 1/2, step 291/1149 completed (loss: 2.5811986923217773, acc: 0.3452380895614624)
[2025-02-17 10:39:49,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:49,714][root][INFO] - Training Epoch: 1/2, step 292/1149 completed (loss: 2.420778512954712, acc: 0.37142857909202576)
[2025-02-17 10:39:49,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:50,098][root][INFO] - Training Epoch: 1/2, step 293/1149 completed (loss: 2.521214008331299, acc: 0.3333333432674408)
[2025-02-17 10:39:50,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:50,488][root][INFO] - Training Epoch: 1/2, step 294/1149 completed (loss: 2.4241764545440674, acc: 0.4354838728904724)
[2025-02-17 10:39:50,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:50,859][root][INFO] - Training Epoch: 1/2, step 295/1149 completed (loss: 2.0315136909484863, acc: 0.375)
[2025-02-17 10:39:51,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:51,271][root][INFO] - Training Epoch: 1/2, step 296/1149 completed (loss: 3.0761947631835938, acc: 0.3333333432674408)
[2025-02-17 10:39:51,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:51,629][root][INFO] - Training Epoch: 1/2, step 297/1149 completed (loss: 3.062469959259033, acc: 0.23076923191547394)
[2025-02-17 10:39:51,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:52,035][root][INFO] - Training Epoch: 1/2, step 298/1149 completed (loss: 1.892073392868042, acc: 0.5)
[2025-02-17 10:39:52,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:52,466][root][INFO] - Training Epoch: 1/2, step 299/1149 completed (loss: 2.6763319969177246, acc: 0.23076923191547394)
[2025-02-17 10:39:52,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:52,844][root][INFO] - Training Epoch: 1/2, step 300/1149 completed (loss: 3.409071445465088, acc: 0.20000000298023224)
[2025-02-17 10:39:52,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:53,205][root][INFO] - Training Epoch: 1/2, step 301/1149 completed (loss: 2.4560608863830566, acc: 0.2857142984867096)
[2025-02-17 10:39:53,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:53,586][root][INFO] - Training Epoch: 1/2, step 302/1149 completed (loss: 2.1568470001220703, acc: 0.5)
[2025-02-17 10:39:53,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:53,960][root][INFO] - Training Epoch: 1/2, step 303/1149 completed (loss: 2.7298145294189453, acc: 0.40909090638160706)
[2025-02-17 10:39:54,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:54,335][root][INFO] - Training Epoch: 1/2, step 304/1149 completed (loss: 2.2578284740448, acc: 0.3191489279270172)
[2025-02-17 10:39:54,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:54,694][root][INFO] - Training Epoch: 1/2, step 305/1149 completed (loss: 2.322674036026001, acc: 0.4166666567325592)
[2025-02-17 10:39:54,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:55,068][root][INFO] - Training Epoch: 1/2, step 306/1149 completed (loss: 2.951150417327881, acc: 0.2631579041481018)
[2025-02-17 10:39:55,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:55,436][root][INFO] - Training Epoch: 1/2, step 307/1149 completed (loss: 2.7027344703674316, acc: 0.3684210479259491)
[2025-02-17 10:39:55,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:55,795][root][INFO] - Training Epoch: 1/2, step 308/1149 completed (loss: 1.9327247142791748, acc: 0.47058823704719543)
[2025-02-17 10:39:55,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:56,175][root][INFO] - Training Epoch: 1/2, step 309/1149 completed (loss: 2.610086441040039, acc: 0.35087719559669495)
[2025-02-17 10:39:56,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:56,533][root][INFO] - Training Epoch: 1/2, step 310/1149 completed (loss: 2.3493027687072754, acc: 0.5384615659713745)
[2025-02-17 10:39:56,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:56,884][root][INFO] - Training Epoch: 1/2, step 311/1149 completed (loss: 1.8399772644042969, acc: 0.7333333492279053)
[2025-02-17 10:39:57,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:57,227][root][INFO] - Training Epoch: 1/2, step 312/1149 completed (loss: 1.7702113389968872, acc: 0.6000000238418579)
[2025-02-17 10:39:57,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:57,575][root][INFO] - Training Epoch: 1/2, step 313/1149 completed (loss: 1.1796014308929443, acc: 0.75)
[2025-02-17 10:39:57,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:57,996][root][INFO] - Training Epoch: 1/2, step 314/1149 completed (loss: 1.7279518842697144, acc: 0.625)
[2025-02-17 10:39:58,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:58,356][root][INFO] - Training Epoch: 1/2, step 315/1149 completed (loss: 2.9655282497406006, acc: 0.1818181872367859)
[2025-02-17 10:39:58,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:39:58,752][root][INFO] - Training Epoch: 1/2, step 316/1149 completed (loss: 2.185908317565918, acc: 0.4399999976158142)
[2025-02-17 10:39:59,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:00,413][root][INFO] - Training Epoch: 1/2, step 317/1149 completed (loss: 2.654752254486084, acc: 0.3265306055545807)
[2025-02-17 10:40:00,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:00,850][root][INFO] - Training Epoch: 1/2, step 318/1149 completed (loss: 3.111435651779175, acc: 0.24137930572032928)
[2025-02-17 10:40:01,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:01,285][root][INFO] - Training Epoch: 1/2, step 319/1149 completed (loss: 2.2895119190216064, acc: 0.47999998927116394)
[2025-02-17 10:40:01,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:01,770][root][INFO] - Training Epoch: 1/2, step 320/1149 completed (loss: 2.7329623699188232, acc: 0.3272727131843567)
[2025-02-17 10:40:02,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:02,271][root][INFO] - Training Epoch: 1/2, step 321/1149 completed (loss: 2.441380023956299, acc: 0.35483869910240173)
[2025-02-17 10:40:02,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:02,683][root][INFO] - Training Epoch: 1/2, step 322/1149 completed (loss: 2.5693085193634033, acc: 0.3870967626571655)
[2025-02-17 10:40:03,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:03,312][root][INFO] - Training Epoch: 1/2, step 323/1149 completed (loss: 2.1930365562438965, acc: 0.4444444477558136)
[2025-02-17 10:40:03,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:03,923][root][INFO] - Training Epoch: 1/2, step 324/1149 completed (loss: 2.538336753845215, acc: 0.35483869910240173)
[2025-02-17 10:40:04,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:04,340][root][INFO] - Training Epoch: 1/2, step 325/1149 completed (loss: 2.469666004180908, acc: 0.25925925374031067)
[2025-02-17 10:40:04,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:04,762][root][INFO] - Training Epoch: 1/2, step 326/1149 completed (loss: 1.776820182800293, acc: 0.5517241358757019)
[2025-02-17 10:40:04,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:05,163][root][INFO] - Training Epoch: 1/2, step 327/1149 completed (loss: 2.681250810623169, acc: 0.46666666865348816)
[2025-02-17 10:40:05,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:05,543][root][INFO] - Training Epoch: 1/2, step 328/1149 completed (loss: 3.1341841220855713, acc: 0.09090909361839294)
[2025-02-17 10:40:05,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:05,945][root][INFO] - Training Epoch: 1/2, step 329/1149 completed (loss: 1.7790682315826416, acc: 0.6000000238418579)
[2025-02-17 10:40:06,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:06,364][root][INFO] - Training Epoch: 1/2, step 330/1149 completed (loss: 2.484851598739624, acc: 0.3125)
[2025-02-17 10:40:06,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:06,725][root][INFO] - Training Epoch: 1/2, step 331/1149 completed (loss: 2.727741241455078, acc: 0.1538461595773697)
[2025-02-17 10:40:06,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:07,122][root][INFO] - Training Epoch: 1/2, step 332/1149 completed (loss: 2.740346670150757, acc: 0.1818181872367859)
[2025-02-17 10:40:07,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:07,565][root][INFO] - Training Epoch: 1/2, step 333/1149 completed (loss: 1.9363248348236084, acc: 0.4736842215061188)
[2025-02-17 10:40:07,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:07,995][root][INFO] - Training Epoch: 1/2, step 334/1149 completed (loss: 2.3385655879974365, acc: 0.4000000059604645)
[2025-02-17 10:40:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:08,381][root][INFO] - Training Epoch: 1/2, step 335/1149 completed (loss: 2.69913911819458, acc: 0.20000000298023224)
[2025-02-17 10:40:08,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:08,823][root][INFO] - Training Epoch: 1/2, step 336/1149 completed (loss: 2.2680296897888184, acc: 0.4375)
[2025-02-17 10:40:09,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:09,275][root][INFO] - Training Epoch: 1/2, step 337/1149 completed (loss: 2.0995471477508545, acc: 0.375)
[2025-02-17 10:40:09,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:09,732][root][INFO] - Training Epoch: 1/2, step 338/1149 completed (loss: 2.3215880393981934, acc: 0.3287671208381653)
[2025-02-17 10:40:09,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:10,208][root][INFO] - Training Epoch: 1/2, step 339/1149 completed (loss: 2.015974998474121, acc: 0.4137931168079376)
[2025-02-17 10:40:10,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:11,305][root][INFO] - Training Epoch: 1/2, step 340/1149 completed (loss: 2.836099624633789, acc: 0.31496062874794006)
[2025-02-17 10:40:11,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:11,759][root][INFO] - Training Epoch: 1/2, step 341/1149 completed (loss: 2.0200748443603516, acc: 0.5789473652839661)
[2025-02-17 10:40:11,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:12,222][root][INFO] - Training Epoch: 1/2, step 342/1149 completed (loss: 2.0582144260406494, acc: 0.4375)
[2025-02-17 10:40:12,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:12,620][root][INFO] - Training Epoch: 1/2, step 343/1149 completed (loss: 1.3779045343399048, acc: 0.75)
[2025-02-17 10:40:12,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:12,980][root][INFO] - Training Epoch: 1/2, step 344/1149 completed (loss: 2.512040376663208, acc: 0.3125)
[2025-02-17 10:40:13,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:13,369][root][INFO] - Training Epoch: 1/2, step 345/1149 completed (loss: 2.5073442459106445, acc: 0.3636363744735718)
[2025-02-17 10:40:13,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:13,807][root][INFO] - Training Epoch: 1/2, step 346/1149 completed (loss: 2.6830341815948486, acc: 0.38461539149284363)
[2025-02-17 10:40:14,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:14,264][root][INFO] - Training Epoch: 1/2, step 347/1149 completed (loss: 2.4167535305023193, acc: 0.3333333432674408)
[2025-02-17 10:40:14,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:14,681][root][INFO] - Training Epoch: 1/2, step 348/1149 completed (loss: 2.9344396591186523, acc: 0.3243243098258972)
[2025-02-17 10:40:14,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:15,271][root][INFO] - Training Epoch: 1/2, step 349/1149 completed (loss: 2.098612070083618, acc: 0.5131579041481018)
[2025-02-17 10:40:15,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:15,685][root][INFO] - Training Epoch: 1/2, step 350/1149 completed (loss: 2.5324065685272217, acc: 0.47058823704719543)
[2025-02-17 10:40:15,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:16,129][root][INFO] - Training Epoch: 1/2, step 351/1149 completed (loss: 2.64156436920166, acc: 0.34285715222358704)
[2025-02-17 10:40:16,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:16,550][root][INFO] - Training Epoch: 1/2, step 352/1149 completed (loss: 2.777419090270996, acc: 0.3529411852359772)
[2025-02-17 10:40:16,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:16,935][root][INFO] - Training Epoch: 1/2, step 353/1149 completed (loss: 3.1718227863311768, acc: 0.20000000298023224)
[2025-02-17 10:40:17,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:17,965][root][INFO] - Training Epoch: 1/2, step 354/1149 completed (loss: 2.7860069274902344, acc: 0.34939759969711304)
[2025-02-17 10:40:18,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:18,439][root][INFO] - Training Epoch: 1/2, step 355/1149 completed (loss: 3.1146228313446045, acc: 0.1666666716337204)
[2025-02-17 10:40:18,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:19,117][root][INFO] - Training Epoch: 1/2, step 356/1149 completed (loss: 2.594106674194336, acc: 0.30588236451148987)
[2025-02-17 10:40:19,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:19,488][root][INFO] - Training Epoch: 1/2, step 357/1149 completed (loss: 2.969266653060913, acc: 0.18000000715255737)
[2025-02-17 10:40:19,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:19,859][root][INFO] - Training Epoch: 1/2, step 358/1149 completed (loss: 2.5049526691436768, acc: 0.2142857164144516)
[2025-02-17 10:40:20,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:20,238][root][INFO] - Training Epoch: 1/2, step 359/1149 completed (loss: 1.8900583982467651, acc: 0.5151515007019043)
[2025-02-17 10:40:20,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:20,616][root][INFO] - Training Epoch: 1/2, step 360/1149 completed (loss: 1.6274399757385254, acc: 0.4545454680919647)
[2025-02-17 10:40:20,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:20,975][root][INFO] - Training Epoch: 1/2, step 361/1149 completed (loss: 1.8580442667007446, acc: 0.6153846383094788)
[2025-02-17 10:40:21,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:21,335][root][INFO] - Training Epoch: 1/2, step 362/1149 completed (loss: 1.9192497730255127, acc: 0.5)
[2025-02-17 10:40:21,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:21,703][root][INFO] - Training Epoch: 1/2, step 363/1149 completed (loss: 2.751459836959839, acc: 0.27272728085517883)
[2025-02-17 10:40:21,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:22,068][root][INFO] - Training Epoch: 1/2, step 364/1149 completed (loss: 2.8891358375549316, acc: 0.1428571492433548)
[2025-02-17 10:40:22,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:22,442][root][INFO] - Training Epoch: 1/2, step 365/1149 completed (loss: 2.3202335834503174, acc: 0.4285714328289032)
[2025-02-17 10:40:22,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:22,805][root][INFO] - Training Epoch: 1/2, step 366/1149 completed (loss: 2.084719181060791, acc: 0.4000000059604645)
[2025-02-17 10:40:22,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:23,186][root][INFO] - Training Epoch: 1/2, step 367/1149 completed (loss: 3.0166964530944824, acc: 0.2222222238779068)
[2025-02-17 10:40:23,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:23,569][root][INFO] - Training Epoch: 1/2, step 368/1149 completed (loss: 3.059614419937134, acc: 0.2589927911758423)
[2025-02-17 10:40:23,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:23,968][root][INFO] - Training Epoch: 1/2, step 369/1149 completed (loss: 2.7007291316986084, acc: 0.3333333432674408)
[2025-02-17 10:40:24,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:24,327][root][INFO] - Training Epoch: 1/2, step 370/1149 completed (loss: 2.7943360805511475, acc: 0.2606382966041565)
[2025-02-17 10:40:24,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:24,712][root][INFO] - Training Epoch: 1/2, step 371/1149 completed (loss: 2.951964855194092, acc: 0.24528302252292633)
[2025-02-17 10:40:24,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:25,234][root][INFO] - Training Epoch: 1/2, step 372/1149 completed (loss: 2.7989695072174072, acc: 0.2467532455921173)
[2025-02-17 10:40:25,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:25,639][root][INFO] - Training Epoch: 1/2, step 373/1149 completed (loss: 2.8790299892425537, acc: 0.27108433842658997)
[2025-02-17 10:40:25,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:26,127][root][INFO] - Training Epoch: 1/2, step 374/1149 completed (loss: 2.502269983291626, acc: 0.3598484992980957)
[2025-02-17 10:40:26,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:26,667][root][INFO] - Training Epoch: 1/2, step 375/1149 completed (loss: 2.611269950866699, acc: 0.2899628281593323)
[2025-02-17 10:40:26,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:27,091][root][INFO] - Training Epoch: 1/2, step 376/1149 completed (loss: 2.6262640953063965, acc: 0.26153847575187683)
[2025-02-17 10:40:27,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:27,535][root][INFO] - Training Epoch: 1/2, step 377/1149 completed (loss: 2.4938995838165283, acc: 0.33112582564353943)
[2025-02-17 10:40:27,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:27,919][root][INFO] - Training Epoch: 1/2, step 378/1149 completed (loss: 3.5729663372039795, acc: 0.3333333432674408)
[2025-02-17 10:40:28,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:28,291][root][INFO] - Training Epoch: 1/2, step 379/1149 completed (loss: 1.2480520009994507, acc: 0.800000011920929)
[2025-02-17 10:40:28,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:28,705][root][INFO] - Training Epoch: 1/2, step 380/1149 completed (loss: 2.359280586242676, acc: 0.3333333432674408)
[2025-02-17 10:40:29,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:29,333][root][INFO] - Training Epoch: 1/2, step 381/1149 completed (loss: 2.6823036670684814, acc: 0.3035714328289032)
[2025-02-17 10:40:29,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:30,224][root][INFO] - Training Epoch: 1/2, step 382/1149 completed (loss: 2.4352831840515137, acc: 0.4109589159488678)
[2025-02-17 10:40:30,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:30,821][root][INFO] - Training Epoch: 1/2, step 383/1149 completed (loss: 2.5750741958618164, acc: 0.37735849618911743)
[2025-02-17 10:40:31,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:31,799][root][INFO] - Training Epoch: 1/2, step 384/1149 completed (loss: 2.38879656791687, acc: 0.3814432919025421)
[2025-02-17 10:40:32,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:32,526][root][INFO] - Training Epoch: 1/2, step 385/1149 completed (loss: 2.604980707168579, acc: 0.2857142984867096)
[2025-02-17 10:40:32,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:33,264][root][INFO] - Training Epoch: 1/2, step 386/1149 completed (loss: 2.0061705112457275, acc: 0.515625)
[2025-02-17 10:40:33,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:33,689][root][INFO] - Training Epoch: 1/2, step 387/1149 completed (loss: 2.07904052734375, acc: 0.4761904776096344)
[2025-02-17 10:40:34,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:34,497][root][INFO] - Training Epoch: 1/2, step 388/1149 completed (loss: 2.234086275100708, acc: 0.4141414165496826)
[2025-02-17 10:40:35,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:35,569][root][INFO] - Training Epoch: 1/2, step 389/1149 completed (loss: 1.9631810188293457, acc: 0.5079365372657776)
[2025-02-17 10:40:36,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:36,582][root][INFO] - Training Epoch: 1/2, step 390/1149 completed (loss: 2.1625092029571533, acc: 0.3529411852359772)
[2025-02-17 10:40:36,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:36,887][root][INFO] - Training Epoch: 1/2, step 391/1149 completed (loss: 1.7695763111114502, acc: 0.5454545617103577)
[2025-02-17 10:40:37,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:37,251][root][INFO] - Training Epoch: 1/2, step 392/1149 completed (loss: 2.6589677333831787, acc: 0.4285714328289032)
[2025-02-17 10:40:37,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:37,628][root][INFO] - Training Epoch: 1/2, step 393/1149 completed (loss: 2.1738781929016113, acc: 0.2142857164144516)
[2025-02-17 10:40:37,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:38,000][root][INFO] - Training Epoch: 1/2, step 394/1149 completed (loss: 3.069021701812744, acc: 0.125)
[2025-02-17 10:40:38,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:38,381][root][INFO] - Training Epoch: 1/2, step 395/1149 completed (loss: 1.9971485137939453, acc: 0.4583333432674408)
[2025-02-17 10:40:38,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:38,833][root][INFO] - Training Epoch: 1/2, step 396/1149 completed (loss: 3.198657751083374, acc: 0.1599999964237213)
[2025-02-17 10:40:39,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:39,220][root][INFO] - Training Epoch: 1/2, step 397/1149 completed (loss: 2.733628988265991, acc: 0.302325576543808)
[2025-02-17 10:40:39,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:39,657][root][INFO] - Training Epoch: 1/2, step 398/1149 completed (loss: 2.428408622741699, acc: 0.4285714328289032)
[2025-02-17 10:40:39,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:40,052][root][INFO] - Training Epoch: 1/2, step 399/1149 completed (loss: 2.518559455871582, acc: 0.2888889014720917)
[2025-02-17 10:40:40,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:40,439][root][INFO] - Training Epoch: 1/2, step 400/1149 completed (loss: 3.1590306758880615, acc: 0.21153846383094788)
[2025-02-17 10:40:40,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:40,891][root][INFO] - Training Epoch: 1/2, step 401/1149 completed (loss: 2.333807945251465, acc: 0.39393940567970276)
[2025-02-17 10:40:41,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:41,359][root][INFO] - Training Epoch: 1/2, step 402/1149 completed (loss: 3.0200259685516357, acc: 0.25531914830207825)
[2025-02-17 10:40:41,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:41,786][root][INFO] - Training Epoch: 1/2, step 403/1149 completed (loss: 2.453639268875122, acc: 0.4367816150188446)
[2025-02-17 10:40:41,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:42,194][root][INFO] - Training Epoch: 1/2, step 404/1149 completed (loss: 2.7443692684173584, acc: 0.3333333432674408)
[2025-02-17 10:40:42,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:42,562][root][INFO] - Training Epoch: 1/2, step 405/1149 completed (loss: 2.707346200942993, acc: 0.3529411852359772)
[2025-02-17 10:40:42,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:42,946][root][INFO] - Training Epoch: 1/2, step 406/1149 completed (loss: 2.424687385559082, acc: 0.36000001430511475)
[2025-02-17 10:40:43,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:43,301][root][INFO] - Training Epoch: 1/2, step 407/1149 completed (loss: 3.4563708305358887, acc: 0.1538461595773697)
[2025-02-17 10:40:43,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:43,762][root][INFO] - Training Epoch: 1/2, step 408/1149 completed (loss: 2.677212953567505, acc: 0.2702702581882477)
[2025-02-17 10:40:43,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:44,202][root][INFO] - Training Epoch: 1/2, step 409/1149 completed (loss: 2.3654353618621826, acc: 0.42592594027519226)
[2025-02-17 10:40:44,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:44,649][root][INFO] - Training Epoch: 1/2, step 410/1149 completed (loss: 2.6681225299835205, acc: 0.30434781312942505)
[2025-02-17 10:40:44,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:45,088][root][INFO] - Training Epoch: 1/2, step 411/1149 completed (loss: 2.479114532470703, acc: 0.34415584802627563)
[2025-02-17 10:40:45,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:45,603][root][INFO] - Training Epoch: 1/2, step 412/1149 completed (loss: 2.3849151134490967, acc: 0.3986014127731323)
[2025-02-17 10:40:45,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:46,035][root][INFO] - Training Epoch: 1/2, step 413/1149 completed (loss: 2.411673069000244, acc: 0.37837839126586914)
[2025-02-17 10:40:46,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:46,526][root][INFO] - Training Epoch: 1/2, step 414/1149 completed (loss: 2.4038851261138916, acc: 0.34375)
[2025-02-17 10:40:46,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:46,944][root][INFO] - Training Epoch: 1/2, step 415/1149 completed (loss: 2.618011474609375, acc: 0.30882352590560913)
[2025-02-17 10:40:47,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:47,403][root][INFO] - Training Epoch: 1/2, step 416/1149 completed (loss: 2.349271535873413, acc: 0.4350649416446686)
[2025-02-17 10:40:47,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:47,868][root][INFO] - Training Epoch: 1/2, step 417/1149 completed (loss: 2.34132719039917, acc: 0.40163934230804443)
[2025-02-17 10:40:48,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:48,275][root][INFO] - Training Epoch: 1/2, step 418/1149 completed (loss: 2.68833065032959, acc: 0.23893804848194122)
[2025-02-17 10:40:48,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:48,664][root][INFO] - Training Epoch: 1/2, step 419/1149 completed (loss: 2.425431728363037, acc: 0.3402777910232544)
[2025-02-17 10:40:48,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:49,058][root][INFO] - Training Epoch: 1/2, step 420/1149 completed (loss: 2.7891252040863037, acc: 0.30000001192092896)
[2025-02-17 10:40:49,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:49,425][root][INFO] - Training Epoch: 1/2, step 421/1149 completed (loss: 1.9577676057815552, acc: 0.5833333134651184)
[2025-02-17 10:40:49,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:49,782][root][INFO] - Training Epoch: 1/2, step 422/1149 completed (loss: 2.6652424335479736, acc: 0.2857142984867096)
[2025-02-17 10:40:49,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:50,146][root][INFO] - Training Epoch: 1/2, step 423/1149 completed (loss: 2.1202688217163086, acc: 0.3333333432674408)
[2025-02-17 10:40:50,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:50,557][root][INFO] - Training Epoch: 1/2, step 424/1149 completed (loss: 2.879298448562622, acc: 0.3125)
[2025-02-17 10:40:50,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:50,966][root][INFO] - Training Epoch: 1/2, step 425/1149 completed (loss: 2.29553484916687, acc: 0.25)
[2025-02-17 10:40:51,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:51,369][root][INFO] - Training Epoch: 1/2, step 426/1149 completed (loss: 2.1610448360443115, acc: 0.3636363744735718)
[2025-02-17 10:40:51,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:51,756][root][INFO] - Training Epoch: 1/2, step 427/1149 completed (loss: 2.02248215675354, acc: 0.3333333432674408)
[2025-02-17 10:40:52,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:52,661][root][INFO] - Training Epoch: 1/2, step 428/1149 completed (loss: 2.189591646194458, acc: 0.3448275923728943)
[2025-02-17 10:40:52,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:53,049][root][INFO] - Training Epoch: 1/2, step 429/1149 completed (loss: 2.4864025115966797, acc: 0.3488371968269348)
[2025-02-17 10:40:53,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:53,459][root][INFO] - Training Epoch: 1/2, step 430/1149 completed (loss: 2.4636380672454834, acc: 0.31707316637039185)
[2025-02-17 10:40:53,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:53,826][root][INFO] - Training Epoch: 1/2, step 431/1149 completed (loss: 2.0737874507904053, acc: 0.42424243688583374)
[2025-02-17 10:40:53,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:54,201][root][INFO] - Training Epoch: 1/2, step 432/1149 completed (loss: 2.203390121459961, acc: 0.380952388048172)
[2025-02-17 10:40:54,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:54,595][root][INFO] - Training Epoch: 1/2, step 433/1149 completed (loss: 2.2809319496154785, acc: 0.4318181872367859)
[2025-02-17 10:40:54,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:55,235][root][INFO] - Training Epoch: 1/2, step 434/1149 completed (loss: 2.1599836349487305, acc: 0.5)
[2025-02-17 10:40:55,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:55,654][root][INFO] - Training Epoch: 1/2, step 435/1149 completed (loss: 2.1001248359680176, acc: 0.4693877696990967)
[2025-02-17 10:40:55,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:56,083][root][INFO] - Training Epoch: 1/2, step 436/1149 completed (loss: 2.4375548362731934, acc: 0.3392857015132904)
[2025-02-17 10:40:56,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:56,474][root][INFO] - Training Epoch: 1/2, step 437/1149 completed (loss: 1.8617421388626099, acc: 0.5)
[2025-02-17 10:40:56,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:56,939][root][INFO] - Training Epoch: 1/2, step 438/1149 completed (loss: 1.6984785795211792, acc: 0.5454545617103577)
[2025-02-17 10:40:57,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:57,345][root][INFO] - Training Epoch: 1/2, step 439/1149 completed (loss: 0.9468602538108826, acc: 0.8181818127632141)
[2025-02-17 10:40:57,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:57,776][root][INFO] - Training Epoch: 1/2, step 440/1149 completed (loss: 1.5935097932815552, acc: 0.6153846383094788)
[2025-02-17 10:40:57,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:58,217][root][INFO] - Training Epoch: 1/2, step 441/1149 completed (loss: 1.6225128173828125, acc: 0.5)
[2025-02-17 10:40:58,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:58,594][root][INFO] - Training Epoch: 1/2, step 442/1149 completed (loss: 1.5776690244674683, acc: 0.27272728085517883)
[2025-02-17 10:40:58,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:58,980][root][INFO] - Training Epoch: 1/2, step 443/1149 completed (loss: 2.0545613765716553, acc: 0.2857142984867096)
[2025-02-17 10:40:59,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:59,415][root][INFO] - Training Epoch: 1/2, step 444/1149 completed (loss: 2.1437594890594482, acc: 0.4545454680919647)
[2025-02-17 10:40:59,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:40:59,844][root][INFO] - Training Epoch: 1/2, step 445/1149 completed (loss: 2.6839473247528076, acc: 0.2195121943950653)
[2025-02-17 10:41:00,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:00,703][root][INFO] - Training Epoch: 1/2, step 446/1149 completed (loss: 2.525197982788086, acc: 0.34343433380126953)
[2025-02-17 10:41:00,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:01,258][root][INFO] - Training Epoch: 1/2, step 447/1149 completed (loss: 1.7970085144042969, acc: 0.4941176474094391)
[2025-02-17 10:41:01,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:01,758][root][INFO] - Training Epoch: 1/2, step 448/1149 completed (loss: 2.647397518157959, acc: 0.29032257199287415)
[2025-02-17 10:41:02,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:02,375][root][INFO] - Training Epoch: 1/2, step 449/1149 completed (loss: 2.0107274055480957, acc: 0.45783132314682007)
[2025-02-17 10:41:02,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:02,849][root][INFO] - Training Epoch: 1/2, step 450/1149 completed (loss: 2.639979600906372, acc: 0.34939759969711304)
[2025-02-17 10:41:03,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:03,226][root][INFO] - Training Epoch: 1/2, step 451/1149 completed (loss: 1.9583369493484497, acc: 0.3636363744735718)
[2025-02-17 10:41:03,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:03,611][root][INFO] - Training Epoch: 1/2, step 452/1149 completed (loss: 2.2787773609161377, acc: 0.3199999928474426)
[2025-02-17 10:41:03,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:04,027][root][INFO] - Training Epoch: 1/2, step 453/1149 completed (loss: 2.397111654281616, acc: 0.3571428656578064)
[2025-02-17 10:41:04,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:04,382][root][INFO] - Training Epoch: 1/2, step 454/1149 completed (loss: 2.328871011734009, acc: 0.4166666567325592)
[2025-02-17 10:41:04,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:04,757][root][INFO] - Training Epoch: 1/2, step 455/1149 completed (loss: 2.269174337387085, acc: 0.3541666567325592)
[2025-02-17 10:41:04,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:05,115][root][INFO] - Training Epoch: 1/2, step 456/1149 completed (loss: 2.088167190551758, acc: 0.4375)
[2025-02-17 10:41:05,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:05,496][root][INFO] - Training Epoch: 1/2, step 457/1149 completed (loss: 0.8311401605606079, acc: 0.8181818127632141)
[2025-02-17 10:41:05,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:05,835][root][INFO] - Training Epoch: 1/2, step 458/1149 completed (loss: 2.105721950531006, acc: 0.5)
[2025-02-17 10:41:05,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:06,201][root][INFO] - Training Epoch: 1/2, step 459/1149 completed (loss: 1.906420350074768, acc: 0.5)
[2025-02-17 10:41:06,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:06,608][root][INFO] - Training Epoch: 1/2, step 460/1149 completed (loss: 2.2768750190734863, acc: 0.375)
[2025-02-17 10:41:06,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:06,984][root][INFO] - Training Epoch: 1/2, step 461/1149 completed (loss: 2.962512493133545, acc: 0.26760563254356384)
[2025-02-17 10:41:07,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:07,347][root][INFO] - Training Epoch: 1/2, step 462/1149 completed (loss: 2.060838222503662, acc: 0.44897958636283875)
[2025-02-17 10:41:07,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:07,702][root][INFO] - Training Epoch: 1/2, step 463/1149 completed (loss: 2.468576669692993, acc: 0.3658536672592163)
[2025-02-17 10:41:07,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:08,089][root][INFO] - Training Epoch: 1/2, step 464/1149 completed (loss: 2.285987138748169, acc: 0.40963855385780334)
[2025-02-17 10:41:08,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:08,562][root][INFO] - Training Epoch: 1/2, step 465/1149 completed (loss: 1.7991589307785034, acc: 0.5670102834701538)
[2025-02-17 10:41:08,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:09,083][root][INFO] - Training Epoch: 1/2, step 466/1149 completed (loss: 2.0360841751098633, acc: 0.49462366104125977)
[2025-02-17 10:41:09,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:09,529][root][INFO] - Training Epoch: 1/2, step 467/1149 completed (loss: 2.045698404312134, acc: 0.5360000133514404)
[2025-02-17 10:41:09,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:09,934][root][INFO] - Training Epoch: 1/2, step 468/1149 completed (loss: 2.196493625640869, acc: 0.42307692766189575)
[2025-02-17 10:41:10,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:10,449][root][INFO] - Training Epoch: 1/2, step 469/1149 completed (loss: 1.857053518295288, acc: 0.43589743971824646)
[2025-02-17 10:41:10,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:10,786][root][INFO] - Training Epoch: 1/2, step 470/1149 completed (loss: 1.1415685415267944, acc: 0.875)
[2025-02-17 10:41:10,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:11,131][root][INFO] - Training Epoch: 1/2, step 471/1149 completed (loss: 2.6549603939056396, acc: 0.4545454680919647)
[2025-02-17 10:41:11,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:11,481][root][INFO] - Training Epoch: 1/2, step 472/1149 completed (loss: 1.9122244119644165, acc: 0.5)
[2025-02-17 10:41:11,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:11,827][root][INFO] - Training Epoch: 1/2, step 473/1149 completed (loss: 2.600295066833496, acc: 0.5)
[2025-02-17 10:41:11,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:12,187][root][INFO] - Training Epoch: 1/2, step 474/1149 completed (loss: 2.5822768211364746, acc: 0.20000000298023224)
[2025-02-17 10:41:12,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:12,537][root][INFO] - Training Epoch: 1/2, step 475/1149 completed (loss: 2.846924066543579, acc: 0.4166666567325592)
[2025-02-17 10:41:12,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:12,900][root][INFO] - Training Epoch: 1/2, step 476/1149 completed (loss: 2.0931077003479004, acc: 0.5714285969734192)
[2025-02-17 10:41:13,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:13,287][root][INFO] - Training Epoch: 1/2, step 477/1149 completed (loss: 1.9203771352767944, acc: 0.3499999940395355)
[2025-02-17 10:41:13,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:13,646][root][INFO] - Training Epoch: 1/2, step 478/1149 completed (loss: 2.7369675636291504, acc: 0.4615384638309479)
[2025-02-17 10:41:13,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:14,017][root][INFO] - Training Epoch: 1/2, step 479/1149 completed (loss: 2.497507333755493, acc: 0.27272728085517883)
[2025-02-17 10:41:14,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:14,392][root][INFO] - Training Epoch: 1/2, step 480/1149 completed (loss: 1.5234121084213257, acc: 0.5714285969734192)
[2025-02-17 10:41:14,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:14,772][root][INFO] - Training Epoch: 1/2, step 481/1149 completed (loss: 2.4068682193756104, acc: 0.30434781312942505)
[2025-02-17 10:41:14,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:15,139][root][INFO] - Training Epoch: 1/2, step 482/1149 completed (loss: 1.9837770462036133, acc: 0.5652173757553101)
[2025-02-17 10:41:15,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:15,503][root][INFO] - Training Epoch: 1/2, step 483/1149 completed (loss: 2.690824508666992, acc: 0.4285714328289032)
[2025-02-17 10:41:15,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:15,995][root][INFO] - Training Epoch: 1/2, step 484/1149 completed (loss: 1.8031607866287231, acc: 0.4838709533214569)
[2025-02-17 10:41:16,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:16,603][root][INFO] - Training Epoch: 1/2, step 485/1149 completed (loss: 2.7311625480651855, acc: 0.25806450843811035)
[2025-02-17 10:41:16,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:16,965][root][INFO] - Training Epoch: 1/2, step 486/1149 completed (loss: 1.9140602350234985, acc: 0.46666666865348816)
[2025-02-17 10:41:17,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:17,546][root][INFO] - Training Epoch: 1/2, step 487/1149 completed (loss: 2.234567165374756, acc: 0.37931033968925476)
[2025-02-17 10:41:17,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:17,917][root][INFO] - Training Epoch: 1/2, step 488/1149 completed (loss: 1.7465436458587646, acc: 0.375)
[2025-02-17 10:41:18,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:18,286][root][INFO] - Training Epoch: 1/2, step 489/1149 completed (loss: 1.839656949043274, acc: 0.38461539149284363)
[2025-02-17 10:41:18,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:18,638][root][INFO] - Training Epoch: 1/2, step 490/1149 completed (loss: 1.6797033548355103, acc: 0.5714285969734192)
[2025-02-17 10:41:18,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:19,067][root][INFO] - Training Epoch: 1/2, step 491/1149 completed (loss: 2.5795392990112305, acc: 0.4166666567325592)
[2025-02-17 10:41:19,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:19,435][root][INFO] - Training Epoch: 1/2, step 492/1149 completed (loss: 2.832714080810547, acc: 0.20000000298023224)
[2025-02-17 10:41:19,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:19,796][root][INFO] - Training Epoch: 1/2, step 493/1149 completed (loss: 2.7504239082336426, acc: 0.375)
[2025-02-17 10:41:19,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:20,141][root][INFO] - Training Epoch: 1/2, step 494/1149 completed (loss: 2.1181578636169434, acc: 0.5)
[2025-02-17 10:41:20,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:20,504][root][INFO] - Training Epoch: 1/2, step 495/1149 completed (loss: 1.9456857442855835, acc: 0.4000000059604645)
[2025-02-17 10:41:20,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:20,951][root][INFO] - Training Epoch: 1/2, step 496/1149 completed (loss: 2.3227524757385254, acc: 0.5)
[2025-02-17 10:41:21,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:21,312][root][INFO] - Training Epoch: 1/2, step 497/1149 completed (loss: 1.9923930168151855, acc: 0.5454545617103577)
[2025-02-17 10:41:21,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:21,751][root][INFO] - Training Epoch: 1/2, step 498/1149 completed (loss: 2.4047608375549316, acc: 0.2631579041481018)
[2025-02-17 10:41:21,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:22,121][root][INFO] - Training Epoch: 1/2, step 499/1149 completed (loss: 2.6646888256073, acc: 0.1666666716337204)
[2025-02-17 10:41:22,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:22,521][root][INFO] - Training Epoch: 1/2, step 500/1149 completed (loss: 2.5984644889831543, acc: 0.20000000298023224)
[2025-02-17 10:41:22,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:22,900][root][INFO] - Training Epoch: 1/2, step 501/1149 completed (loss: 2.240875005722046, acc: 0.5185185074806213)
[2025-02-17 10:41:23,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:23,280][root][INFO] - Training Epoch: 1/2, step 502/1149 completed (loss: 2.56036114692688, acc: 0.35555556416511536)
[2025-02-17 10:41:23,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:23,656][root][INFO] - Training Epoch: 1/2, step 503/1149 completed (loss: 2.267305850982666, acc: 0.43478259444236755)
[2025-02-17 10:41:23,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:24,042][root][INFO] - Training Epoch: 1/2, step 504/1149 completed (loss: 1.4037870168685913, acc: 0.5925925970077515)
[2025-02-17 10:41:24,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:24,403][root][INFO] - Training Epoch: 1/2, step 505/1149 completed (loss: 1.4169834852218628, acc: 0.5714285969734192)
[2025-02-17 10:41:24,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:24,744][root][INFO] - Training Epoch: 1/2, step 506/1149 completed (loss: 1.2036229372024536, acc: 0.6666666865348816)
[2025-02-17 10:41:24,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:25,086][root][INFO] - Training Epoch: 1/2, step 507/1149 completed (loss: 1.4976881742477417, acc: 0.5625)
[2025-02-17 10:41:25,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:25,450][root][INFO] - Training Epoch: 1/2, step 508/1149 completed (loss: 1.4660587310791016, acc: 0.4615384638309479)
[2025-02-17 10:41:25,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:25,796][root][INFO] - Training Epoch: 1/2, step 509/1149 completed (loss: 1.3777508735656738, acc: 0.6666666865348816)
[2025-02-17 10:41:25,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:26,157][root][INFO] - Training Epoch: 1/2, step 510/1149 completed (loss: 2.2829065322875977, acc: 0.2777777910232544)
[2025-02-17 10:41:26,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:26,518][root][INFO] - Training Epoch: 1/2, step 511/1149 completed (loss: 2.258355140686035, acc: 0.38461539149284363)
[2025-02-17 10:41:26,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:26,878][root][INFO] - Training Epoch: 1/2, step 512/1149 completed (loss: 2.269989252090454, acc: 0.3181818127632141)
[2025-02-17 10:41:27,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:27,244][root][INFO] - Training Epoch: 1/2, step 513/1149 completed (loss: 2.4200198650360107, acc: 0.37142857909202576)
[2025-02-17 10:41:27,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:27,606][root][INFO] - Training Epoch: 1/2, step 514/1149 completed (loss: 2.409256935119629, acc: 0.38235294818878174)
[2025-02-17 10:41:27,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:27,966][root][INFO] - Training Epoch: 1/2, step 515/1149 completed (loss: 2.290769577026367, acc: 0.4444444477558136)
[2025-02-17 10:41:28,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:28,322][root][INFO] - Training Epoch: 1/2, step 516/1149 completed (loss: 1.8096128702163696, acc: 0.4545454680919647)
[2025-02-17 10:41:28,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:28,772][root][INFO] - Training Epoch: 1/2, step 517/1149 completed (loss: 2.388123035430908, acc: 0.44186046719551086)
[2025-02-17 10:41:29,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:29,401][root][INFO] - Training Epoch: 1/2, step 518/1149 completed (loss: 2.024641513824463, acc: 0.44594594836235046)
[2025-02-17 10:41:29,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:29,841][root][INFO] - Training Epoch: 1/2, step 519/1149 completed (loss: 2.5561838150024414, acc: 0.34375)
[2025-02-17 10:41:30,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:30,240][root][INFO] - Training Epoch: 1/2, step 520/1149 completed (loss: 2.018183469772339, acc: 0.48275861144065857)
[2025-02-17 10:41:30,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:30,789][root][INFO] - Training Epoch: 1/2, step 521/1149 completed (loss: 2.301065683364868, acc: 0.38461539149284363)
[2025-02-17 10:41:30,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:31,212][root][INFO] - Training Epoch: 1/2, step 522/1149 completed (loss: 1.9652527570724487, acc: 0.5555555820465088)
[2025-02-17 10:41:31,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:31,625][root][INFO] - Training Epoch: 1/2, step 523/1149 completed (loss: 2.464777946472168, acc: 0.3333333432674408)
[2025-02-17 10:41:31,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:31,996][root][INFO] - Training Epoch: 1/2, step 524/1149 completed (loss: 2.6728415489196777, acc: 0.29411765933036804)
[2025-02-17 10:41:32,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:32,370][root][INFO] - Training Epoch: 1/2, step 525/1149 completed (loss: 1.7814905643463135, acc: 0.5714285969734192)
[2025-02-17 10:41:32,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:32,735][root][INFO] - Training Epoch: 1/2, step 526/1149 completed (loss: 2.726144552230835, acc: 0.375)
[2025-02-17 10:41:32,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:33,164][root][INFO] - Training Epoch: 1/2, step 527/1149 completed (loss: 2.899270534515381, acc: 0.2542372941970825)
[2025-02-17 10:41:33,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:33,591][root][INFO] - Training Epoch: 1/2, step 528/1149 completed (loss: 2.998790979385376, acc: 0.3611111044883728)
[2025-02-17 10:41:33,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:33,965][root][INFO] - Training Epoch: 1/2, step 529/1149 completed (loss: 1.9021620750427246, acc: 0.5)
[2025-02-17 10:41:34,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:34,819][root][INFO] - Training Epoch: 1/2, step 530/1149 completed (loss: 2.6496806144714355, acc: 0.26923078298568726)
[2025-02-17 10:41:35,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:35,279][root][INFO] - Training Epoch: 1/2, step 531/1149 completed (loss: 2.364948034286499, acc: 0.2857142984867096)
[2025-02-17 10:41:35,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:35,720][root][INFO] - Training Epoch: 1/2, step 532/1149 completed (loss: 2.482706069946289, acc: 0.2888889014720917)
[2025-02-17 10:41:35,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:36,116][root][INFO] - Training Epoch: 1/2, step 533/1149 completed (loss: 2.2021985054016113, acc: 0.5)
[2025-02-17 10:41:36,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:36,554][root][INFO] - Training Epoch: 1/2, step 534/1149 completed (loss: 2.646225929260254, acc: 0.4313725531101227)
[2025-02-17 10:41:36,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:36,988][root][INFO] - Training Epoch: 1/2, step 535/1149 completed (loss: 2.0558106899261475, acc: 0.47826087474823)
[2025-02-17 10:41:37,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:37,481][root][INFO] - Training Epoch: 1/2, step 536/1149 completed (loss: 2.133470296859741, acc: 0.4047619104385376)
[2025-02-17 10:41:37,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:37,845][root][INFO] - Training Epoch: 1/2, step 537/1149 completed (loss: 1.010825753211975, acc: 0.75)
[2025-02-17 10:41:38,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:38,263][root][INFO] - Training Epoch: 1/2, step 538/1149 completed (loss: 1.2724946737289429, acc: 0.6153846383094788)
[2025-02-17 10:41:38,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:38,647][root][INFO] - Training Epoch: 1/2, step 539/1149 completed (loss: 2.527923583984375, acc: 0.3333333432674408)
[2025-02-17 10:41:38,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:39,094][root][INFO] - Training Epoch: 1/2, step 540/1149 completed (loss: 0.8994221091270447, acc: 0.9090909361839294)
[2025-02-17 10:41:39,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:39,523][root][INFO] - Training Epoch: 1/2, step 541/1149 completed (loss: 2.012514352798462, acc: 0.3636363744735718)
[2025-02-17 10:41:39,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:39,895][root][INFO] - Training Epoch: 1/2, step 542/1149 completed (loss: 1.8878085613250732, acc: 0.42105263471603394)
[2025-02-17 10:41:40,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:40,351][root][INFO] - Training Epoch: 1/2, step 543/1149 completed (loss: 0.7372186183929443, acc: 0.8461538553237915)
[2025-02-17 10:41:40,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:40,751][root][INFO] - Training Epoch: 1/2, step 544/1149 completed (loss: 1.7387934923171997, acc: 0.4444444477558136)
[2025-02-17 10:41:40,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:41,126][root][INFO] - Training Epoch: 1/2, step 545/1149 completed (loss: 1.8768967390060425, acc: 0.5833333134651184)
[2025-02-17 10:41:41,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:41,470][root][INFO] - Training Epoch: 1/2, step 546/1149 completed (loss: 1.6671403646469116, acc: 0.5333333611488342)
[2025-02-17 10:41:41,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:41,892][root][INFO] - Training Epoch: 1/2, step 547/1149 completed (loss: 2.8882012367248535, acc: 0.31111112236976624)
[2025-02-17 10:41:42,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:42,334][root][INFO] - Training Epoch: 1/2, step 548/1149 completed (loss: 2.3723866939544678, acc: 0.4285714328289032)
[2025-02-17 10:41:42,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:42,749][root][INFO] - Training Epoch: 1/2, step 549/1149 completed (loss: 1.2750802040100098, acc: 0.8181818127632141)
[2025-02-17 10:41:42,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:43,168][root][INFO] - Training Epoch: 1/2, step 550/1149 completed (loss: 2.420441150665283, acc: 0.3571428656578064)
[2025-02-17 10:41:43,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:43,560][root][INFO] - Training Epoch: 1/2, step 551/1149 completed (loss: 0.9628547430038452, acc: 0.625)
[2025-02-17 10:41:43,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:43,926][root][INFO] - Training Epoch: 1/2, step 552/1149 completed (loss: 2.131632089614868, acc: 0.4117647111415863)
[2025-02-17 10:41:44,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:44,285][root][INFO] - Training Epoch: 1/2, step 553/1149 completed (loss: 1.3620980978012085, acc: 0.6666666865348816)
[2025-02-17 10:41:44,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:44,650][root][INFO] - Training Epoch: 1/2, step 554/1149 completed (loss: 1.2755224704742432, acc: 0.4545454680919647)
[2025-02-17 10:41:44,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:44,990][root][INFO] - Training Epoch: 1/2, step 555/1149 completed (loss: 2.0731282234191895, acc: 0.5)
[2025-02-17 10:41:45,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:45,359][root][INFO] - Training Epoch: 1/2, step 556/1149 completed (loss: 2.9139702320098877, acc: 0.21052631735801697)
[2025-02-17 10:41:45,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:45,734][root][INFO] - Training Epoch: 1/2, step 557/1149 completed (loss: 2.5428547859191895, acc: 0.3928571343421936)
[2025-02-17 10:41:45,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:46,159][root][INFO] - Training Epoch: 1/2, step 558/1149 completed (loss: 2.110538959503174, acc: 0.5)
[2025-02-17 10:41:46,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:46,573][root][INFO] - Training Epoch: 1/2, step 559/1149 completed (loss: 1.990004301071167, acc: 0.375)
[2025-02-17 10:41:46,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:47,026][root][INFO] - Training Epoch: 1/2, step 560/1149 completed (loss: 2.0205130577087402, acc: 0.5199999809265137)
[2025-02-17 10:41:47,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:47,440][root][INFO] - Training Epoch: 1/2, step 561/1149 completed (loss: 2.197740077972412, acc: 0.4736842215061188)
[2025-02-17 10:41:47,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:47,907][root][INFO] - Training Epoch: 1/2, step 562/1149 completed (loss: 1.9189356565475464, acc: 0.5263158082962036)
[2025-02-17 10:41:48,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:48,412][root][INFO] - Training Epoch: 1/2, step 563/1149 completed (loss: 2.609576940536499, acc: 0.328125)
[2025-02-17 10:41:48,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:48,787][root][INFO] - Training Epoch: 1/2, step 564/1149 completed (loss: 2.2787082195281982, acc: 0.5)
[2025-02-17 10:41:48,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:49,179][root][INFO] - Training Epoch: 1/2, step 565/1149 completed (loss: 2.598841428756714, acc: 0.3333333432674408)
[2025-02-17 10:41:49,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:49,631][root][INFO] - Training Epoch: 1/2, step 566/1149 completed (loss: 2.4877328872680664, acc: 0.2631579041481018)
[2025-02-17 10:41:49,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:50,048][root][INFO] - Training Epoch: 1/2, step 567/1149 completed (loss: 2.582490921020508, acc: 0.21052631735801697)
[2025-02-17 10:41:50,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:50,415][root][INFO] - Training Epoch: 1/2, step 568/1149 completed (loss: 2.591038703918457, acc: 0.1538461595773697)
[2025-02-17 10:41:50,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:50,788][root][INFO] - Training Epoch: 1/2, step 569/1149 completed (loss: 2.2926344871520996, acc: 0.3333333432674408)
[2025-02-17 10:41:51,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:51,224][root][INFO] - Training Epoch: 1/2, step 570/1149 completed (loss: 2.8167080879211426, acc: 0.2142857164144516)
[2025-02-17 10:41:51,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:51,596][root][INFO] - Training Epoch: 1/2, step 571/1149 completed (loss: 2.0545926094055176, acc: 0.38461539149284363)
[2025-02-17 10:41:51,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:51,946][root][INFO] - Training Epoch: 1/2, step 572/1149 completed (loss: 2.419020652770996, acc: 0.2950819730758667)
[2025-02-17 10:41:52,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:52,303][root][INFO] - Training Epoch: 1/2, step 573/1149 completed (loss: 2.478076219558716, acc: 0.34328359365463257)
[2025-02-17 10:41:53,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:53,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:54,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:54,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:55,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:55,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:56,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:56,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:57,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:57,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:57,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:58,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:58,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:59,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:41:59,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:00,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:00,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:01,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:01,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:02,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:02,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:03,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:03,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:04,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:04,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:05,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:05,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:06,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:06,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:06,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:07,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:07,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:08,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:08,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:09,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:09,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:10,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:10,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:11,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:11,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:12,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:12,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:13,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:13,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:14,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:14,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:15,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:15,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:16,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:16,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:17,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:17,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:18,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:18,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:19,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:19,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:20,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:20,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:21,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:21,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:21,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:22,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:22,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:23,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:23,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:23,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:24,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:24,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:25,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:26,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:26,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:26,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:27,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:27,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:28,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:28,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:29,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:29,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:30,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:30,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:31,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:31,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:32,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:32,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:33,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:33,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:33,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:34,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:34,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:35,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:35,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:36,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:36,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:37,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:37,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:38,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:38,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:39,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:39,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:40,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:40,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:41,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:41,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:42,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:42,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:43,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:43,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:44,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:44,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:45,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:46,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:46,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:47,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:47,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:48,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:48,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:49,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:49,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:50,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:50,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:50,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:51,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:51,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:52,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:52,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:53,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:53,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:54,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:54,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:55,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:56,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:56,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:56,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:57,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:58,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:58,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:59,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:42:59,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:00,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:00,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:01,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:01,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:01,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:02,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:02,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:03,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:03,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:04,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:04,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:05,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:05,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:06,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:06,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:07,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:07,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:07,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:08,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:08,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:09,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:09,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:10,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:10,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:11,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:11,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:11,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:12,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:12,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:13,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:13,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:14,121][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(8.8264, device='cuda:0') eval_epoch_loss=tensor(2.1778, device='cuda:0') eval_epoch_acc=tensor(0.4194, device='cuda:0')
[2025-02-17 10:43:14,123][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 10:43:14,123][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 10:43:18,138][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_1_step_574_loss_2.1777522563934326/model.pt
[2025-02-17 10:43:18,155][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 10:43:18,157][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.1777522563934326
[2025-02-17 10:43:18,158][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.4193848967552185
[2025-02-17 10:43:18,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:18,727][root][INFO] - Training Epoch: 1/2, step 574/1149 completed (loss: 2.6267597675323486, acc: 0.28947368264198303)
[2025-02-17 10:43:18,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:19,168][root][INFO] - Training Epoch: 1/2, step 575/1149 completed (loss: 2.7356696128845215, acc: 0.26530611515045166)
[2025-02-17 10:43:19,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:19,539][root][INFO] - Training Epoch: 1/2, step 576/1149 completed (loss: 2.3197484016418457, acc: 0.4117647111415863)
[2025-02-17 10:43:19,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:19,922][root][INFO] - Training Epoch: 1/2, step 577/1149 completed (loss: 2.0988004207611084, acc: 0.44999998807907104)
[2025-02-17 10:43:20,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:20,398][root][INFO] - Training Epoch: 1/2, step 578/1149 completed (loss: 2.5385966300964355, acc: 0.3835616409778595)
[2025-02-17 10:43:20,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:20,790][root][INFO] - Training Epoch: 1/2, step 579/1149 completed (loss: 2.4954326152801514, acc: 0.28409090638160706)
[2025-02-17 10:43:20,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:21,199][root][INFO] - Training Epoch: 1/2, step 580/1149 completed (loss: 2.5712788105010986, acc: 0.33076924085617065)
[2025-02-17 10:43:21,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:21,590][root][INFO] - Training Epoch: 1/2, step 581/1149 completed (loss: 2.559075117111206, acc: 0.296875)
[2025-02-17 10:43:21,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:21,934][root][INFO] - Training Epoch: 1/2, step 582/1149 completed (loss: 1.642985224723816, acc: 0.4444444477558136)
[2025-02-17 10:43:22,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:22,360][root][INFO] - Training Epoch: 1/2, step 583/1149 completed (loss: 1.3920669555664062, acc: 0.692307710647583)
[2025-02-17 10:43:22,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:22,716][root][INFO] - Training Epoch: 1/2, step 584/1149 completed (loss: 2.008388042449951, acc: 0.5185185074806213)
[2025-02-17 10:43:22,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:23,155][root][INFO] - Training Epoch: 1/2, step 585/1149 completed (loss: 2.938988208770752, acc: 0.3333333432674408)
[2025-02-17 10:43:23,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:23,569][root][INFO] - Training Epoch: 1/2, step 586/1149 completed (loss: 2.3186612129211426, acc: 0.30000001192092896)
[2025-02-17 10:43:23,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:23,970][root][INFO] - Training Epoch: 1/2, step 587/1149 completed (loss: 1.749132752418518, acc: 0.5416666865348816)
[2025-02-17 10:43:24,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:24,572][root][INFO] - Training Epoch: 1/2, step 588/1149 completed (loss: 1.6593352556228638, acc: 0.6279069781303406)
[2025-02-17 10:43:24,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:24,934][root][INFO] - Training Epoch: 1/2, step 589/1149 completed (loss: 1.3390008211135864, acc: 0.75)
[2025-02-17 10:43:25,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:25,370][root][INFO] - Training Epoch: 1/2, step 590/1149 completed (loss: 1.9188487529754639, acc: 0.44999998807907104)
[2025-02-17 10:43:25,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:25,961][root][INFO] - Training Epoch: 1/2, step 591/1149 completed (loss: 1.9945151805877686, acc: 0.5324675440788269)
[2025-02-17 10:43:26,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:26,362][root][INFO] - Training Epoch: 1/2, step 592/1149 completed (loss: 2.0059502124786377, acc: 0.6000000238418579)
[2025-02-17 10:43:26,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:26,750][root][INFO] - Training Epoch: 1/2, step 593/1149 completed (loss: 2.644378185272217, acc: 0.3488371968269348)
[2025-02-17 10:43:26,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:27,197][root][INFO] - Training Epoch: 1/2, step 594/1149 completed (loss: 1.8802658319473267, acc: 0.529411792755127)
[2025-02-17 10:43:27,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:27,571][root][INFO] - Training Epoch: 1/2, step 595/1149 completed (loss: 2.694434881210327, acc: 0.30000001192092896)
[2025-02-17 10:43:27,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:28,030][root][INFO] - Training Epoch: 1/2, step 596/1149 completed (loss: 2.3175814151763916, acc: 0.37037035822868347)
[2025-02-17 10:43:28,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:28,390][root][INFO] - Training Epoch: 1/2, step 597/1149 completed (loss: 1.686032772064209, acc: 0.6363636255264282)
[2025-02-17 10:43:28,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:28,767][root][INFO] - Training Epoch: 1/2, step 598/1149 completed (loss: 2.439786911010742, acc: 0.4444444477558136)
[2025-02-17 10:43:28,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:29,180][root][INFO] - Training Epoch: 1/2, step 599/1149 completed (loss: 2.309471368789673, acc: 0.48275861144065857)
[2025-02-17 10:43:29,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:29,599][root][INFO] - Training Epoch: 1/2, step 600/1149 completed (loss: 2.348139524459839, acc: 0.3499999940395355)
[2025-02-17 10:43:29,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:29,964][root][INFO] - Training Epoch: 1/2, step 601/1149 completed (loss: 2.210609197616577, acc: 0.25)
[2025-02-17 10:43:30,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:30,367][root][INFO] - Training Epoch: 1/2, step 602/1149 completed (loss: 1.9122250080108643, acc: 0.5)
[2025-02-17 10:43:30,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:30,779][root][INFO] - Training Epoch: 1/2, step 603/1149 completed (loss: 2.469639778137207, acc: 0.39393940567970276)
[2025-02-17 10:43:30,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:31,175][root][INFO] - Training Epoch: 1/2, step 604/1149 completed (loss: 1.7012449502944946, acc: 0.6666666865348816)
[2025-02-17 10:43:31,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:31,566][root][INFO] - Training Epoch: 1/2, step 605/1149 completed (loss: 1.478927731513977, acc: 0.6097561120986938)
[2025-02-17 10:43:31,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:31,950][root][INFO] - Training Epoch: 1/2, step 606/1149 completed (loss: 1.3780875205993652, acc: 0.7058823704719543)
[2025-02-17 10:43:32,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:32,362][root][INFO] - Training Epoch: 1/2, step 607/1149 completed (loss: 2.1263418197631836, acc: 0.47058823704719543)
[2025-02-17 10:43:32,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:32,765][root][INFO] - Training Epoch: 1/2, step 608/1149 completed (loss: 2.7506582736968994, acc: 0.3333333432674408)
[2025-02-17 10:43:32,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:33,145][root][INFO] - Training Epoch: 1/2, step 609/1149 completed (loss: 2.298551559448242, acc: 0.4117647111415863)
[2025-02-17 10:43:33,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:33,594][root][INFO] - Training Epoch: 1/2, step 610/1149 completed (loss: 1.7428737878799438, acc: 0.5526315569877625)
[2025-02-17 10:43:33,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:33,985][root][INFO] - Training Epoch: 1/2, step 611/1149 completed (loss: 1.8169281482696533, acc: 0.5652173757553101)
[2025-02-17 10:43:34,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:34,355][root][INFO] - Training Epoch: 1/2, step 612/1149 completed (loss: 1.7939268350601196, acc: 0.5555555820465088)
[2025-02-17 10:43:34,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:34,768][root][INFO] - Training Epoch: 1/2, step 613/1149 completed (loss: 1.0199916362762451, acc: 0.75)
[2025-02-17 10:43:34,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:35,159][root][INFO] - Training Epoch: 1/2, step 614/1149 completed (loss: 0.6796399354934692, acc: 0.625)
[2025-02-17 10:43:35,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:35,497][root][INFO] - Training Epoch: 1/2, step 615/1149 completed (loss: 1.6665271520614624, acc: 0.5454545617103577)
[2025-02-17 10:43:35,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:35,887][root][INFO] - Training Epoch: 1/2, step 616/1149 completed (loss: 2.5127975940704346, acc: 0.3888888955116272)
[2025-02-17 10:43:36,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:36,323][root][INFO] - Training Epoch: 1/2, step 617/1149 completed (loss: 2.3334381580352783, acc: 0.42424243688583374)
[2025-02-17 10:43:36,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:36,821][root][INFO] - Training Epoch: 1/2, step 618/1149 completed (loss: 2.2673447132110596, acc: 0.4615384638309479)
[2025-02-17 10:43:36,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:37,183][root][INFO] - Training Epoch: 1/2, step 619/1149 completed (loss: 1.6967853307724, acc: 0.5)
[2025-02-17 10:43:37,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:37,561][root][INFO] - Training Epoch: 1/2, step 620/1149 completed (loss: 2.415041208267212, acc: 0.31707316637039185)
[2025-02-17 10:43:37,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:37,977][root][INFO] - Training Epoch: 1/2, step 621/1149 completed (loss: 1.6851167678833008, acc: 0.5476190447807312)
[2025-02-17 10:43:38,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:38,348][root][INFO] - Training Epoch: 1/2, step 622/1149 completed (loss: 2.3444128036499023, acc: 0.3272727131843567)
[2025-02-17 10:43:38,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:38,718][root][INFO] - Training Epoch: 1/2, step 623/1149 completed (loss: 2.7365264892578125, acc: 0.21739129722118378)
[2025-02-17 10:43:38,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:39,125][root][INFO] - Training Epoch: 1/2, step 624/1149 completed (loss: 2.6065032482147217, acc: 0.31343284249305725)
[2025-02-17 10:43:39,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:39,468][root][INFO] - Training Epoch: 1/2, step 625/1149 completed (loss: 2.4949240684509277, acc: 0.22580644488334656)
[2025-02-17 10:43:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:39,839][root][INFO] - Training Epoch: 1/2, step 626/1149 completed (loss: 0.6312645673751831, acc: 0.75)
[2025-02-17 10:43:40,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:40,214][root][INFO] - Training Epoch: 1/2, step 627/1149 completed (loss: 0.8739548921585083, acc: 0.75)
[2025-02-17 10:43:40,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:40,526][root][INFO] - Training Epoch: 1/2, step 628/1149 completed (loss: 0.8832051157951355, acc: 0.7692307829856873)
[2025-02-17 10:43:40,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:40,878][root][INFO] - Training Epoch: 1/2, step 629/1149 completed (loss: 2.623103380203247, acc: 0.27272728085517883)
[2025-02-17 10:43:41,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:41,229][root][INFO] - Training Epoch: 1/2, step 630/1149 completed (loss: 1.0177547931671143, acc: 0.692307710647583)
[2025-02-17 10:43:41,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:41,592][root][INFO] - Training Epoch: 1/2, step 631/1149 completed (loss: 2.3476827144622803, acc: 0.4444444477558136)
[2025-02-17 10:43:41,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:41,951][root][INFO] - Training Epoch: 1/2, step 632/1149 completed (loss: 1.8872318267822266, acc: 0.46666666865348816)
[2025-02-17 10:43:42,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:42,305][root][INFO] - Training Epoch: 1/2, step 633/1149 completed (loss: 2.3629369735717773, acc: 0.375)
[2025-02-17 10:43:42,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:42,715][root][INFO] - Training Epoch: 1/2, step 634/1149 completed (loss: 2.0026302337646484, acc: 0.44186046719551086)
[2025-02-17 10:43:42,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:43,058][root][INFO] - Training Epoch: 1/2, step 635/1149 completed (loss: 1.6181138753890991, acc: 0.5416666865348816)
[2025-02-17 10:43:43,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:43,409][root][INFO] - Training Epoch: 1/2, step 636/1149 completed (loss: 1.2098021507263184, acc: 0.7142857313156128)
[2025-02-17 10:43:43,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:43,797][root][INFO] - Training Epoch: 1/2, step 637/1149 completed (loss: 1.8621972799301147, acc: 0.4819277226924896)
[2025-02-17 10:43:43,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:44,158][root][INFO] - Training Epoch: 1/2, step 638/1149 completed (loss: 2.741194725036621, acc: 0.2916666567325592)
[2025-02-17 10:43:44,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:44,512][root][INFO] - Training Epoch: 1/2, step 639/1149 completed (loss: 1.9727994203567505, acc: 0.4285714328289032)
[2025-02-17 10:43:44,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:44,864][root][INFO] - Training Epoch: 1/2, step 640/1149 completed (loss: 1.973759412765503, acc: 0.4761904776096344)
[2025-02-17 10:43:44,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:45,212][root][INFO] - Training Epoch: 1/2, step 641/1149 completed (loss: 2.044224500656128, acc: 0.6000000238418579)
[2025-02-17 10:43:45,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:45,617][root][INFO] - Training Epoch: 1/2, step 642/1149 completed (loss: 1.6383427381515503, acc: 0.6363636255264282)
[2025-02-17 10:43:45,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:45,952][root][INFO] - Training Epoch: 1/2, step 643/1149 completed (loss: 0.7665315866470337, acc: 0.7647058963775635)
[2025-02-17 10:43:46,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:46,300][root][INFO] - Training Epoch: 1/2, step 644/1149 completed (loss: 3.6282026767730713, acc: 0.2222222238779068)
[2025-02-17 10:43:46,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:46,663][root][INFO] - Training Epoch: 1/2, step 645/1149 completed (loss: 2.784343719482422, acc: 0.2222222238779068)
[2025-02-17 10:43:46,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:47,004][root][INFO] - Training Epoch: 1/2, step 646/1149 completed (loss: 3.4389007091522217, acc: 0.0833333358168602)
[2025-02-17 10:43:47,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:47,353][root][INFO] - Training Epoch: 1/2, step 647/1149 completed (loss: 3.923349142074585, acc: 0.27272728085517883)
[2025-02-17 10:43:47,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:47,714][root][INFO] - Training Epoch: 1/2, step 648/1149 completed (loss: 2.746405839920044, acc: 0.375)
[2025-02-17 10:43:47,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:48,067][root][INFO] - Training Epoch: 1/2, step 649/1149 completed (loss: 2.471477746963501, acc: 0.3333333432674408)
[2025-02-17 10:43:48,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:48,443][root][INFO] - Training Epoch: 1/2, step 650/1149 completed (loss: 2.9139373302459717, acc: 0.30000001192092896)
[2025-02-17 10:43:48,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:48,778][root][INFO] - Training Epoch: 1/2, step 651/1149 completed (loss: 2.389918327331543, acc: 0.5454545617103577)
[2025-02-17 10:43:48,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:49,150][root][INFO] - Training Epoch: 1/2, step 652/1149 completed (loss: 2.836872100830078, acc: 0.3076923191547394)
[2025-02-17 10:43:49,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:49,527][root][INFO] - Training Epoch: 1/2, step 653/1149 completed (loss: 2.2683587074279785, acc: 0.4000000059604645)
[2025-02-17 10:43:49,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:49,861][root][INFO] - Training Epoch: 1/2, step 654/1149 completed (loss: 2.2754275798797607, acc: 0.2857142984867096)
[2025-02-17 10:43:50,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:50,214][root][INFO] - Training Epoch: 1/2, step 655/1149 completed (loss: 1.4842008352279663, acc: 0.5)
[2025-02-17 10:43:50,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:50,561][root][INFO] - Training Epoch: 1/2, step 656/1149 completed (loss: 1.96087646484375, acc: 0.5)
[2025-02-17 10:43:50,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:50,896][root][INFO] - Training Epoch: 1/2, step 657/1149 completed (loss: 1.7435146570205688, acc: 0.5)
[2025-02-17 10:43:51,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:51,244][root][INFO] - Training Epoch: 1/2, step 658/1149 completed (loss: 2.998922348022461, acc: 0.25)
[2025-02-17 10:43:51,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:51,598][root][INFO] - Training Epoch: 1/2, step 659/1149 completed (loss: 1.9168280363082886, acc: 0.46666666865348816)
[2025-02-17 10:43:51,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:51,971][root][INFO] - Training Epoch: 1/2, step 660/1149 completed (loss: 1.3724912405014038, acc: 0.6875)
[2025-02-17 10:43:52,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:52,387][root][INFO] - Training Epoch: 1/2, step 661/1149 completed (loss: 1.7760319709777832, acc: 0.5)
[2025-02-17 10:43:52,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:52,756][root][INFO] - Training Epoch: 1/2, step 662/1149 completed (loss: 2.352961301803589, acc: 0.42424243688583374)
[2025-02-17 10:43:52,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:53,134][root][INFO] - Training Epoch: 1/2, step 663/1149 completed (loss: 2.215366840362549, acc: 0.3448275923728943)
[2025-02-17 10:43:53,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:53,549][root][INFO] - Training Epoch: 1/2, step 664/1149 completed (loss: 2.10858154296875, acc: 0.4166666567325592)
[2025-02-17 10:43:53,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:53,992][root][INFO] - Training Epoch: 1/2, step 665/1149 completed (loss: 1.8117583990097046, acc: 0.5333333611488342)
[2025-02-17 10:43:54,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:54,385][root][INFO] - Training Epoch: 1/2, step 666/1149 completed (loss: 2.7616379261016846, acc: 0.2222222238779068)
[2025-02-17 10:43:54,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:54,732][root][INFO] - Training Epoch: 1/2, step 667/1149 completed (loss: 2.1565539836883545, acc: 0.4285714328289032)
[2025-02-17 10:43:54,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:55,114][root][INFO] - Training Epoch: 1/2, step 668/1149 completed (loss: 2.207045793533325, acc: 0.3333333432674408)
[2025-02-17 10:43:55,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:55,644][root][INFO] - Training Epoch: 1/2, step 669/1149 completed (loss: 1.3110084533691406, acc: 0.6666666865348816)
[2025-02-17 10:43:55,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:56,053][root][INFO] - Training Epoch: 1/2, step 670/1149 completed (loss: 2.216578722000122, acc: 0.5)
[2025-02-17 10:43:56,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:56,430][root][INFO] - Training Epoch: 1/2, step 671/1149 completed (loss: 2.1671040058135986, acc: 0.4285714328289032)
[2025-02-17 10:43:56,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:56,829][root][INFO] - Training Epoch: 1/2, step 672/1149 completed (loss: 2.2192461490631104, acc: 0.27272728085517883)
[2025-02-17 10:43:57,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:57,241][root][INFO] - Training Epoch: 1/2, step 673/1149 completed (loss: 2.566075563430786, acc: 0.3589743673801422)
[2025-02-17 10:43:57,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:57,675][root][INFO] - Training Epoch: 1/2, step 674/1149 completed (loss: 2.607656955718994, acc: 0.4032258093357086)
[2025-02-17 10:43:57,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:58,038][root][INFO] - Training Epoch: 1/2, step 675/1149 completed (loss: 2.3610143661499023, acc: 0.3199999928474426)
[2025-02-17 10:43:58,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:58,416][root][INFO] - Training Epoch: 1/2, step 676/1149 completed (loss: 2.8728158473968506, acc: 0.26530611515045166)
[2025-02-17 10:43:58,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:58,820][root][INFO] - Training Epoch: 1/2, step 677/1149 completed (loss: 2.2821314334869385, acc: 0.3777777850627899)
[2025-02-17 10:43:58,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:59,176][root][INFO] - Training Epoch: 1/2, step 678/1149 completed (loss: 2.8381521701812744, acc: 0.32307693362236023)
[2025-02-17 10:43:59,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:59,547][root][INFO] - Training Epoch: 1/2, step 679/1149 completed (loss: 1.8724106550216675, acc: 0.5)
[2025-02-17 10:43:59,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:43:59,943][root][INFO] - Training Epoch: 1/2, step 680/1149 completed (loss: 2.3037493228912354, acc: 0.4444444477558136)
[2025-02-17 10:44:00,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:00,380][root][INFO] - Training Epoch: 1/2, step 681/1149 completed (loss: 1.4137264490127563, acc: 0.5)
[2025-02-17 10:44:00,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:00,734][root][INFO] - Training Epoch: 1/2, step 682/1149 completed (loss: 2.6506540775299072, acc: 0.3499999940395355)
[2025-02-17 10:44:00,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:01,090][root][INFO] - Training Epoch: 1/2, step 683/1149 completed (loss: 2.350541830062866, acc: 0.31578946113586426)
[2025-02-17 10:44:01,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:01,464][root][INFO] - Training Epoch: 1/2, step 684/1149 completed (loss: 2.1648879051208496, acc: 0.42424243688583374)
[2025-02-17 10:44:01,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:01,829][root][INFO] - Training Epoch: 1/2, step 685/1149 completed (loss: 3.208371162414551, acc: 0.18000000715255737)
[2025-02-17 10:44:01,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:02,193][root][INFO] - Training Epoch: 1/2, step 686/1149 completed (loss: 2.320063591003418, acc: 0.4166666567325592)
[2025-02-17 10:44:02,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:02,541][root][INFO] - Training Epoch: 1/2, step 687/1149 completed (loss: 1.3635541200637817, acc: 0.7058823704719543)
[2025-02-17 10:44:02,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:02,916][root][INFO] - Training Epoch: 1/2, step 688/1149 completed (loss: 2.6169612407684326, acc: 0.2926829159259796)
[2025-02-17 10:44:03,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:03,276][root][INFO] - Training Epoch: 1/2, step 689/1149 completed (loss: 2.3588638305664062, acc: 0.28947368264198303)
[2025-02-17 10:44:03,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:03,637][root][INFO] - Training Epoch: 1/2, step 690/1149 completed (loss: 2.4758832454681396, acc: 0.25925925374031067)
[2025-02-17 10:44:03,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:03,992][root][INFO] - Training Epoch: 1/2, step 691/1149 completed (loss: 2.096994161605835, acc: 0.4166666567325592)
[2025-02-17 10:44:04,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:04,365][root][INFO] - Training Epoch: 1/2, step 692/1149 completed (loss: 2.245055675506592, acc: 0.5625)
[2025-02-17 10:44:04,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:04,789][root][INFO] - Training Epoch: 1/2, step 693/1149 completed (loss: 2.9990367889404297, acc: 0.1764705926179886)
[2025-02-17 10:44:04,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:05,147][root][INFO] - Training Epoch: 1/2, step 694/1149 completed (loss: 1.540937900543213, acc: 0.75)
[2025-02-17 10:44:05,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:05,539][root][INFO] - Training Epoch: 1/2, step 695/1149 completed (loss: 1.7372052669525146, acc: 0.5)
[2025-02-17 10:44:05,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:05,926][root][INFO] - Training Epoch: 1/2, step 696/1149 completed (loss: 1.604776382446289, acc: 0.5454545617103577)
[2025-02-17 10:44:06,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:06,340][root][INFO] - Training Epoch: 1/2, step 697/1149 completed (loss: 1.8632465600967407, acc: 0.4285714328289032)
[2025-02-17 10:44:06,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:06,857][root][INFO] - Training Epoch: 1/2, step 698/1149 completed (loss: 1.4972823858261108, acc: 0.5416666865348816)
[2025-02-17 10:44:07,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:07,218][root][INFO] - Training Epoch: 1/2, step 699/1149 completed (loss: 1.432407021522522, acc: 0.75)
[2025-02-17 10:44:07,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:07,592][root][INFO] - Training Epoch: 1/2, step 700/1149 completed (loss: 2.8598220348358154, acc: 0.36000001430511475)
[2025-02-17 10:44:07,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:07,963][root][INFO] - Training Epoch: 1/2, step 701/1149 completed (loss: 1.1849642992019653, acc: 0.6111111044883728)
[2025-02-17 10:44:08,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:08,334][root][INFO] - Training Epoch: 1/2, step 702/1149 completed (loss: 2.2253801822662354, acc: 0.36666667461395264)
[2025-02-17 10:44:08,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:08,695][root][INFO] - Training Epoch: 1/2, step 703/1149 completed (loss: 2.3374435901641846, acc: 0.4444444477558136)
[2025-02-17 10:44:08,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:09,046][root][INFO] - Training Epoch: 1/2, step 704/1149 completed (loss: 1.7830549478530884, acc: 0.5833333134651184)
[2025-02-17 10:44:09,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:09,450][root][INFO] - Training Epoch: 1/2, step 705/1149 completed (loss: 2.4955036640167236, acc: 0.24242424964904785)
[2025-02-17 10:44:09,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:09,806][root][INFO] - Training Epoch: 1/2, step 706/1149 completed (loss: 1.259627103805542, acc: 0.6666666865348816)
[2025-02-17 10:44:09,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:10,165][root][INFO] - Training Epoch: 1/2, step 707/1149 completed (loss: 1.2453252077102661, acc: 0.5)
[2025-02-17 10:44:10,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:10,504][root][INFO] - Training Epoch: 1/2, step 708/1149 completed (loss: 2.7193071842193604, acc: 0.4166666567325592)
[2025-02-17 10:44:10,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:10,874][root][INFO] - Training Epoch: 1/2, step 709/1149 completed (loss: 3.0503814220428467, acc: 0.2142857164144516)
[2025-02-17 10:44:11,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:11,269][root][INFO] - Training Epoch: 1/2, step 710/1149 completed (loss: 2.555755615234375, acc: 0.29411765933036804)
[2025-02-17 10:44:11,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:11,659][root][INFO] - Training Epoch: 1/2, step 711/1149 completed (loss: 2.6904244422912598, acc: 0.31578946113586426)
[2025-02-17 10:44:11,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:12,225][root][INFO] - Training Epoch: 1/2, step 712/1149 completed (loss: 1.9846206903457642, acc: 0.43421053886413574)
[2025-02-17 10:44:12,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:12,629][root][INFO] - Training Epoch: 1/2, step 713/1149 completed (loss: 2.3132834434509277, acc: 0.43589743971824646)
[2025-02-17 10:44:12,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:12,991][root][INFO] - Training Epoch: 1/2, step 714/1149 completed (loss: 2.1243598461151123, acc: 0.36734694242477417)
[2025-02-17 10:44:13,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:13,377][root][INFO] - Training Epoch: 1/2, step 715/1149 completed (loss: 2.406151533126831, acc: 0.3255814015865326)
[2025-02-17 10:44:13,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:13,730][root][INFO] - Training Epoch: 1/2, step 716/1149 completed (loss: 2.4727070331573486, acc: 0.2857142984867096)
[2025-02-17 10:44:13,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:14,158][root][INFO] - Training Epoch: 1/2, step 717/1149 completed (loss: 2.1901471614837646, acc: 0.4642857015132904)
[2025-02-17 10:44:14,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:14,554][root][INFO] - Training Epoch: 1/2, step 718/1149 completed (loss: 0.5454935431480408, acc: 0.8181818127632141)
[2025-02-17 10:44:14,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:14,979][root][INFO] - Training Epoch: 1/2, step 719/1149 completed (loss: 1.0864644050598145, acc: 0.692307710647583)
[2025-02-17 10:44:15,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:15,322][root][INFO] - Training Epoch: 1/2, step 720/1149 completed (loss: 1.264554738998413, acc: 0.5)
[2025-02-17 10:44:15,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:15,663][root][INFO] - Training Epoch: 1/2, step 721/1149 completed (loss: 2.0384953022003174, acc: 0.4166666567325592)
[2025-02-17 10:44:15,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:16,086][root][INFO] - Training Epoch: 1/2, step 722/1149 completed (loss: 1.9783120155334473, acc: 0.40909090638160706)
[2025-02-17 10:44:16,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:16,473][root][INFO] - Training Epoch: 1/2, step 723/1149 completed (loss: 2.4107494354248047, acc: 0.3684210479259491)
[2025-02-17 10:44:16,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:16,836][root][INFO] - Training Epoch: 1/2, step 724/1149 completed (loss: 2.3743135929107666, acc: 0.4000000059604645)
[2025-02-17 10:44:17,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:17,232][root][INFO] - Training Epoch: 1/2, step 725/1149 completed (loss: 1.8056061267852783, acc: 0.47999998927116394)
[2025-02-17 10:44:17,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:17,588][root][INFO] - Training Epoch: 1/2, step 726/1149 completed (loss: 1.699402093887329, acc: 0.6000000238418579)
[2025-02-17 10:44:17,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:17,972][root][INFO] - Training Epoch: 1/2, step 727/1149 completed (loss: 2.655749797821045, acc: 0.3484848439693451)
[2025-02-17 10:44:18,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:18,317][root][INFO] - Training Epoch: 1/2, step 728/1149 completed (loss: 1.5368810892105103, acc: 0.6000000238418579)
[2025-02-17 10:44:18,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:18,692][root][INFO] - Training Epoch: 1/2, step 729/1149 completed (loss: 2.8169546127319336, acc: 0.29032257199287415)
[2025-02-17 10:44:18,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:19,080][root][INFO] - Training Epoch: 1/2, step 730/1149 completed (loss: 1.9208331108093262, acc: 0.40909090638160706)
[2025-02-17 10:44:19,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:19,440][root][INFO] - Training Epoch: 1/2, step 731/1149 completed (loss: 2.798687219619751, acc: 0.27272728085517883)
[2025-02-17 10:44:19,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:19,789][root][INFO] - Training Epoch: 1/2, step 732/1149 completed (loss: 0.9497118592262268, acc: 0.6153846383094788)
[2025-02-17 10:44:19,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:20,142][root][INFO] - Training Epoch: 1/2, step 733/1149 completed (loss: 0.9634383916854858, acc: 0.7272727489471436)
[2025-02-17 10:44:20,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:20,482][root][INFO] - Training Epoch: 1/2, step 734/1149 completed (loss: 1.013260006904602, acc: 0.692307710647583)
[2025-02-17 10:44:20,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:20,875][root][INFO] - Training Epoch: 1/2, step 735/1149 completed (loss: 1.170877456665039, acc: 0.699999988079071)
[2025-02-17 10:44:21,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:21,232][root][INFO] - Training Epoch: 1/2, step 736/1149 completed (loss: 1.3438555002212524, acc: 0.5)
[2025-02-17 10:44:21,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:21,601][root][INFO] - Training Epoch: 1/2, step 737/1149 completed (loss: 1.7073947191238403, acc: 0.5714285969734192)
[2025-02-17 10:44:21,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:21,948][root][INFO] - Training Epoch: 1/2, step 738/1149 completed (loss: 1.3508524894714355, acc: 0.5454545617103577)
[2025-02-17 10:44:22,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:22,374][root][INFO] - Training Epoch: 1/2, step 739/1149 completed (loss: 1.8314247131347656, acc: 0.4285714328289032)
[2025-02-17 10:44:22,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:23,063][root][INFO] - Training Epoch: 1/2, step 740/1149 completed (loss: 2.444448232650757, acc: 0.37931033968925476)
[2025-02-17 10:44:23,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:23,562][root][INFO] - Training Epoch: 1/2, step 741/1149 completed (loss: 1.7745733261108398, acc: 0.5510203838348389)
[2025-02-17 10:44:24,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:24,447][root][INFO] - Training Epoch: 1/2, step 742/1149 completed (loss: 1.4971246719360352, acc: 0.6000000238418579)
[2025-02-17 10:44:24,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:24,827][root][INFO] - Training Epoch: 1/2, step 743/1149 completed (loss: 2.2245054244995117, acc: 0.3888888955116272)
[2025-02-17 10:44:24,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:25,191][root][INFO] - Training Epoch: 1/2, step 744/1149 completed (loss: 1.9947394132614136, acc: 0.44897958636283875)
[2025-02-17 10:44:25,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:25,627][root][INFO] - Training Epoch: 1/2, step 745/1149 completed (loss: 2.2059085369110107, acc: 0.4390243887901306)
[2025-02-17 10:44:25,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:26,011][root][INFO] - Training Epoch: 1/2, step 746/1149 completed (loss: 1.9391111135482788, acc: 0.4871794879436493)
[2025-02-17 10:44:26,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:26,383][root][INFO] - Training Epoch: 1/2, step 747/1149 completed (loss: 1.5994709730148315, acc: 0.6470588445663452)
[2025-02-17 10:44:26,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:26,780][root][INFO] - Training Epoch: 1/2, step 748/1149 completed (loss: 1.5370697975158691, acc: 0.5925925970077515)
[2025-02-17 10:44:26,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:27,147][root][INFO] - Training Epoch: 1/2, step 749/1149 completed (loss: 0.3593314290046692, acc: 0.75)
[2025-02-17 10:44:27,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:27,518][root][INFO] - Training Epoch: 1/2, step 750/1149 completed (loss: 0.6339840292930603, acc: 0.9090909361839294)
[2025-02-17 10:44:27,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:27,895][root][INFO] - Training Epoch: 1/2, step 751/1149 completed (loss: 0.7372545003890991, acc: 0.7857142686843872)
[2025-02-17 10:44:28,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:28,251][root][INFO] - Training Epoch: 1/2, step 752/1149 completed (loss: 0.47123631834983826, acc: 0.8333333134651184)
[2025-02-17 10:44:28,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:28,660][root][INFO] - Training Epoch: 1/2, step 753/1149 completed (loss: 1.1084272861480713, acc: 0.6363636255264282)
[2025-02-17 10:44:28,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:29,028][root][INFO] - Training Epoch: 1/2, step 754/1149 completed (loss: 2.1815528869628906, acc: 0.46666666865348816)
[2025-02-17 10:44:29,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:29,490][root][INFO] - Training Epoch: 1/2, step 755/1149 completed (loss: 2.793349027633667, acc: 0.3333333432674408)
[2025-02-17 10:44:29,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:29,903][root][INFO] - Training Epoch: 1/2, step 756/1149 completed (loss: 2.2037816047668457, acc: 0.42592594027519226)
[2025-02-17 10:44:30,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:30,327][root][INFO] - Training Epoch: 1/2, step 757/1149 completed (loss: 1.9317258596420288, acc: 0.5121951103210449)
[2025-02-17 10:44:30,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:30,765][root][INFO] - Training Epoch: 1/2, step 758/1149 completed (loss: 2.3289172649383545, acc: 0.45348837971687317)
[2025-02-17 10:44:31,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:31,352][root][INFO] - Training Epoch: 1/2, step 759/1149 completed (loss: 1.6620630025863647, acc: 0.5679012537002563)
[2025-02-17 10:44:31,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:31,769][root][INFO] - Training Epoch: 1/2, step 760/1149 completed (loss: 1.6464383602142334, acc: 0.546875)
[2025-02-17 10:44:31,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:32,203][root][INFO] - Training Epoch: 1/2, step 761/1149 completed (loss: 2.0321481227874756, acc: 0.4202898442745209)
[2025-02-17 10:44:32,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:32,542][root][INFO] - Training Epoch: 1/2, step 762/1149 completed (loss: 2.5070948600769043, acc: 0.40740740299224854)
[2025-02-17 10:44:33,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:33,625][root][INFO] - Training Epoch: 1/2, step 763/1149 completed (loss: 1.6446276903152466, acc: 0.5639097690582275)
[2025-02-17 10:44:33,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:34,172][root][INFO] - Training Epoch: 1/2, step 764/1149 completed (loss: 1.708173394203186, acc: 0.607594907283783)
[2025-02-17 10:44:34,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:34,583][root][INFO] - Training Epoch: 1/2, step 765/1149 completed (loss: 1.4700480699539185, acc: 0.625)
[2025-02-17 10:44:34,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:34,987][root][INFO] - Training Epoch: 1/2, step 766/1149 completed (loss: 0.26235172152519226, acc: 0.9090909361839294)
[2025-02-17 10:44:35,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:35,397][root][INFO] - Training Epoch: 1/2, step 767/1149 completed (loss: 1.7496901750564575, acc: 0.47058823704719543)
[2025-02-17 10:44:35,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:35,715][root][INFO] - Training Epoch: 1/2, step 768/1149 completed (loss: 1.2936904430389404, acc: 0.6875)
[2025-02-17 10:44:35,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:36,074][root][INFO] - Training Epoch: 1/2, step 769/1149 completed (loss: 0.4150172173976898, acc: 0.9166666865348816)
[2025-02-17 10:44:36,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:36,432][root][INFO] - Training Epoch: 1/2, step 770/1149 completed (loss: 1.7229256629943848, acc: 0.4000000059604645)
[2025-02-17 10:44:36,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:36,776][root][INFO] - Training Epoch: 1/2, step 771/1149 completed (loss: 1.8330131769180298, acc: 0.529411792755127)
[2025-02-17 10:44:36,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:37,144][root][INFO] - Training Epoch: 1/2, step 772/1149 completed (loss: 1.095696210861206, acc: 0.6666666865348816)
[2025-02-17 10:44:37,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:37,512][root][INFO] - Training Epoch: 1/2, step 773/1149 completed (loss: 2.045459747314453, acc: 0.5)
[2025-02-17 10:44:37,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:37,900][root][INFO] - Training Epoch: 1/2, step 774/1149 completed (loss: 0.6969268321990967, acc: 0.8333333134651184)
[2025-02-17 10:44:38,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:38,290][root][INFO] - Training Epoch: 1/2, step 775/1149 completed (loss: 2.272850275039673, acc: 0.38461539149284363)
[2025-02-17 10:44:38,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:38,667][root][INFO] - Training Epoch: 1/2, step 776/1149 completed (loss: 0.8410838842391968, acc: 0.699999988079071)
[2025-02-17 10:44:38,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:39,026][root][INFO] - Training Epoch: 1/2, step 777/1149 completed (loss: 1.0815142393112183, acc: 0.75)
[2025-02-17 10:44:39,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:39,384][root][INFO] - Training Epoch: 1/2, step 778/1149 completed (loss: 1.0571304559707642, acc: 0.75)
[2025-02-17 10:44:39,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:39,758][root][INFO] - Training Epoch: 1/2, step 779/1149 completed (loss: 1.654752254486084, acc: 0.25)
[2025-02-17 10:44:39,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:40,073][root][INFO] - Training Epoch: 1/2, step 780/1149 completed (loss: 0.8484691977500916, acc: 0.75)
[2025-02-17 10:44:40,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:40,422][root][INFO] - Training Epoch: 1/2, step 781/1149 completed (loss: 2.1910765171051025, acc: 0.4444444477558136)
[2025-02-17 10:44:40,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:40,850][root][INFO] - Training Epoch: 1/2, step 782/1149 completed (loss: 2.853400230407715, acc: 0.38461539149284363)
[2025-02-17 10:44:41,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:41,251][root][INFO] - Training Epoch: 1/2, step 783/1149 completed (loss: 2.707399606704712, acc: 0.2857142984867096)
[2025-02-17 10:44:41,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:41,665][root][INFO] - Training Epoch: 1/2, step 784/1149 completed (loss: 2.4018399715423584, acc: 0.3877550959587097)
[2025-02-17 10:44:41,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:42,014][root][INFO] - Training Epoch: 1/2, step 785/1149 completed (loss: 2.4305410385131836, acc: 0.35185185074806213)
[2025-02-17 10:44:42,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:42,417][root][INFO] - Training Epoch: 1/2, step 786/1149 completed (loss: 2.2209320068359375, acc: 0.4878048896789551)
[2025-02-17 10:44:42,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:42,980][root][INFO] - Training Epoch: 1/2, step 787/1149 completed (loss: 1.951572299003601, acc: 0.5157894492149353)
[2025-02-17 10:44:43,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:43,409][root][INFO] - Training Epoch: 1/2, step 788/1149 completed (loss: 2.5643696784973145, acc: 0.3827160596847534)
[2025-02-17 10:44:43,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:43,768][root][INFO] - Training Epoch: 1/2, step 789/1149 completed (loss: 2.3275136947631836, acc: 0.4637681245803833)
[2025-02-17 10:44:43,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:44,197][root][INFO] - Training Epoch: 1/2, step 790/1149 completed (loss: 2.1418299674987793, acc: 0.47422680258750916)
[2025-02-17 10:44:44,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:44,630][root][INFO] - Training Epoch: 1/2, step 791/1149 completed (loss: 2.557190418243408, acc: 0.38297873735427856)
[2025-02-17 10:44:44,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:45,067][root][INFO] - Training Epoch: 1/2, step 792/1149 completed (loss: 1.5311940908432007, acc: 0.625)
[2025-02-17 10:44:45,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:45,492][root][INFO] - Training Epoch: 1/2, step 793/1149 completed (loss: 3.102217435836792, acc: 0.31578946113586426)
[2025-02-17 10:44:45,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:45,892][root][INFO] - Training Epoch: 1/2, step 794/1149 completed (loss: 1.015933632850647, acc: 0.7142857313156128)
[2025-02-17 10:44:46,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:46,259][root][INFO] - Training Epoch: 1/2, step 795/1149 completed (loss: 1.910064458847046, acc: 0.6000000238418579)
[2025-02-17 10:44:46,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:46,650][root][INFO] - Training Epoch: 1/2, step 796/1149 completed (loss: 2.1368093490600586, acc: 0.375)
[2025-02-17 10:44:46,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:47,067][root][INFO] - Training Epoch: 1/2, step 797/1149 completed (loss: 1.6722978353500366, acc: 0.5925925970077515)
[2025-02-17 10:44:47,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:47,506][root][INFO] - Training Epoch: 1/2, step 798/1149 completed (loss: 0.5233176350593567, acc: 0.800000011920929)
[2025-02-17 10:44:47,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:47,889][root][INFO] - Training Epoch: 1/2, step 799/1149 completed (loss: 2.348714590072632, acc: 0.3333333432674408)
[2025-02-17 10:44:48,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:48,462][root][INFO] - Training Epoch: 1/2, step 800/1149 completed (loss: 2.1090543270111084, acc: 0.5)
[2025-02-17 10:44:48,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:48,853][root][INFO] - Training Epoch: 1/2, step 801/1149 completed (loss: 1.7655870914459229, acc: 0.5)
[2025-02-17 10:44:49,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:49,226][root][INFO] - Training Epoch: 1/2, step 802/1149 completed (loss: 1.7259190082550049, acc: 0.574999988079071)
[2025-02-17 10:44:49,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:49,637][root][INFO] - Training Epoch: 1/2, step 803/1149 completed (loss: 1.9860055446624756, acc: 0.48571428656578064)
[2025-02-17 10:44:49,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:49,997][root][INFO] - Training Epoch: 1/2, step 804/1149 completed (loss: 1.4156999588012695, acc: 0.6363636255264282)
[2025-02-17 10:44:50,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:50,376][root][INFO] - Training Epoch: 1/2, step 805/1149 completed (loss: 0.7735220193862915, acc: 0.7272727489471436)
[2025-02-17 10:44:50,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:50,794][root][INFO] - Training Epoch: 1/2, step 806/1149 completed (loss: 1.565845012664795, acc: 0.6363636255264282)
[2025-02-17 10:44:50,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:51,193][root][INFO] - Training Epoch: 1/2, step 807/1149 completed (loss: 1.4386941194534302, acc: 0.7272727489471436)
[2025-02-17 10:44:51,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:51,599][root][INFO] - Training Epoch: 1/2, step 808/1149 completed (loss: 0.4841318130493164, acc: 0.8461538553237915)
[2025-02-17 10:44:51,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:52,001][root][INFO] - Training Epoch: 1/2, step 809/1149 completed (loss: 0.9979557394981384, acc: 0.8333333134651184)
[2025-02-17 10:44:52,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:52,428][root][INFO] - Training Epoch: 1/2, step 810/1149 completed (loss: 1.8120841979980469, acc: 0.5)
[2025-02-17 10:44:52,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:52,784][root][INFO] - Training Epoch: 1/2, step 811/1149 completed (loss: 1.0491504669189453, acc: 0.7692307829856873)
[2025-02-17 10:44:52,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:53,158][root][INFO] - Training Epoch: 1/2, step 812/1149 completed (loss: 1.1688035726547241, acc: 0.75)
[2025-02-17 10:44:53,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:53,607][root][INFO] - Training Epoch: 1/2, step 813/1149 completed (loss: 0.5512760281562805, acc: 0.8461538553237915)
[2025-02-17 10:44:53,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:53,989][root][INFO] - Training Epoch: 1/2, step 814/1149 completed (loss: 1.2958192825317383, acc: 0.7083333134651184)
[2025-02-17 10:44:54,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:54,352][root][INFO] - Training Epoch: 1/2, step 815/1149 completed (loss: 1.1829559803009033, acc: 0.5833333134651184)
[2025-02-17 10:44:54,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:54,774][root][INFO] - Training Epoch: 1/2, step 816/1149 completed (loss: 0.591754674911499, acc: 0.7272727489471436)
[2025-02-17 10:44:54,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:55,170][root][INFO] - Training Epoch: 1/2, step 817/1149 completed (loss: 1.3768242597579956, acc: 0.625)
[2025-02-17 10:44:55,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:55,529][root][INFO] - Training Epoch: 1/2, step 818/1149 completed (loss: 1.3922903537750244, acc: 0.6428571343421936)
[2025-02-17 10:44:55,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:55,920][root][INFO] - Training Epoch: 1/2, step 819/1149 completed (loss: 0.7060012817382812, acc: 0.8333333134651184)
[2025-02-17 10:44:56,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:56,296][root][INFO] - Training Epoch: 1/2, step 820/1149 completed (loss: 2.0939886569976807, acc: 0.5357142686843872)
[2025-02-17 10:44:56,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:56,730][root][INFO] - Training Epoch: 1/2, step 821/1149 completed (loss: 1.2828748226165771, acc: 0.699999988079071)
[2025-02-17 10:44:56,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:57,110][root][INFO] - Training Epoch: 1/2, step 822/1149 completed (loss: 2.2708799839019775, acc: 0.5)
[2025-02-17 10:44:57,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:57,506][root][INFO] - Training Epoch: 1/2, step 823/1149 completed (loss: 0.4926380217075348, acc: 0.8125)
[2025-02-17 10:44:57,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:57,880][root][INFO] - Training Epoch: 1/2, step 824/1149 completed (loss: 1.5791373252868652, acc: 0.5)
[2025-02-17 10:44:58,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:58,227][root][INFO] - Training Epoch: 1/2, step 825/1149 completed (loss: 0.4662625789642334, acc: 1.0)
[2025-02-17 10:44:58,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:58,596][root][INFO] - Training Epoch: 1/2, step 826/1149 completed (loss: 1.342898964881897, acc: 0.7142857313156128)
[2025-02-17 10:44:58,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:58,970][root][INFO] - Training Epoch: 1/2, step 827/1149 completed (loss: 1.033726453781128, acc: 0.5789473652839661)
[2025-02-17 10:44:59,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:59,302][root][INFO] - Training Epoch: 1/2, step 828/1149 completed (loss: 1.3884754180908203, acc: 0.6153846383094788)
[2025-02-17 10:44:59,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:44:59,651][root][INFO] - Training Epoch: 1/2, step 829/1149 completed (loss: 0.9649889469146729, acc: 0.8888888955116272)
[2025-02-17 10:44:59,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:00,031][root][INFO] - Training Epoch: 1/2, step 830/1149 completed (loss: 2.4494454860687256, acc: 0.42307692766189575)
[2025-02-17 10:45:00,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:00,375][root][INFO] - Training Epoch: 1/2, step 831/1149 completed (loss: 1.961184024810791, acc: 0.6399999856948853)
[2025-02-17 10:45:00,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:00,811][root][INFO] - Training Epoch: 1/2, step 832/1149 completed (loss: 1.4710664749145508, acc: 0.7222222089767456)
[2025-02-17 10:45:01,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:01,209][root][INFO] - Training Epoch: 1/2, step 833/1149 completed (loss: 1.8104840517044067, acc: 0.5)
[2025-02-17 10:45:01,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:01,555][root][INFO] - Training Epoch: 1/2, step 834/1149 completed (loss: 1.616105556488037, acc: 0.5454545617103577)
[2025-02-17 10:45:01,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:01,916][root][INFO] - Training Epoch: 1/2, step 835/1149 completed (loss: 1.7316226959228516, acc: 0.5714285969734192)
[2025-02-17 10:45:02,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:02,304][root][INFO] - Training Epoch: 1/2, step 836/1149 completed (loss: 2.0145647525787354, acc: 0.5161290168762207)
[2025-02-17 10:45:02,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:02,677][root][INFO] - Training Epoch: 1/2, step 837/1149 completed (loss: 1.0570719242095947, acc: 0.6666666865348816)
[2025-02-17 10:45:02,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:03,045][root][INFO] - Training Epoch: 1/2, step 838/1149 completed (loss: 1.3911370038986206, acc: 0.5)
[2025-02-17 10:45:03,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:03,389][root][INFO] - Training Epoch: 1/2, step 839/1149 completed (loss: 3.123692750930786, acc: 0.3333333432674408)
[2025-02-17 10:45:03,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:03,754][root][INFO] - Training Epoch: 1/2, step 840/1149 completed (loss: 0.5352490544319153, acc: 0.8999999761581421)
[2025-02-17 10:45:03,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:04,116][root][INFO] - Training Epoch: 1/2, step 841/1149 completed (loss: 0.5427072644233704, acc: 0.8181818127632141)
[2025-02-17 10:45:04,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:04,501][root][INFO] - Training Epoch: 1/2, step 842/1149 completed (loss: 1.5887445211410522, acc: 0.4375)
[2025-02-17 10:45:04,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:04,836][root][INFO] - Training Epoch: 1/2, step 843/1149 completed (loss: 0.7740587592124939, acc: 0.7857142686843872)
[2025-02-17 10:45:04,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:05,180][root][INFO] - Training Epoch: 1/2, step 844/1149 completed (loss: 0.6549968123435974, acc: 0.8333333134651184)
[2025-02-17 10:45:05,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:05,531][root][INFO] - Training Epoch: 1/2, step 845/1149 completed (loss: 1.8584730625152588, acc: 0.4000000059604645)
[2025-02-17 10:45:05,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:05,887][root][INFO] - Training Epoch: 1/2, step 846/1149 completed (loss: 1.7569752931594849, acc: 0.6000000238418579)
[2025-02-17 10:45:06,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:06,274][root][INFO] - Training Epoch: 1/2, step 847/1149 completed (loss: 1.7288084030151367, acc: 0.4285714328289032)
[2025-02-17 10:45:06,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:06,634][root][INFO] - Training Epoch: 1/2, step 848/1149 completed (loss: 1.1732970476150513, acc: 0.9090909361839294)
[2025-02-17 10:45:06,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:06,983][root][INFO] - Training Epoch: 1/2, step 849/1149 completed (loss: 1.0914475917816162, acc: 0.875)
[2025-02-17 10:45:07,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:07,331][root][INFO] - Training Epoch: 1/2, step 850/1149 completed (loss: 1.300450325012207, acc: 0.6315789222717285)
[2025-02-17 10:45:07,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:07,722][root][INFO] - Training Epoch: 1/2, step 851/1149 completed (loss: 0.9623362421989441, acc: 0.8571428656578064)
[2025-02-17 10:45:07,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:08,160][root][INFO] - Training Epoch: 1/2, step 852/1149 completed (loss: 0.5306679010391235, acc: 0.9166666865348816)
[2025-02-17 10:45:08,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:08,574][root][INFO] - Training Epoch: 1/2, step 853/1149 completed (loss: 0.42521393299102783, acc: 0.9090909361839294)
[2025-02-17 10:45:08,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:08,985][root][INFO] - Training Epoch: 1/2, step 854/1149 completed (loss: 1.16105318069458, acc: 0.7692307829856873)
[2025-02-17 10:45:09,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:09,319][root][INFO] - Training Epoch: 1/2, step 855/1149 completed (loss: 0.6289177536964417, acc: 0.7272727489471436)
[2025-02-17 10:45:09,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:09,682][root][INFO] - Training Epoch: 1/2, step 856/1149 completed (loss: 1.1348670721054077, acc: 0.75)
[2025-02-17 10:45:09,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:10,039][root][INFO] - Training Epoch: 1/2, step 857/1149 completed (loss: 1.008775234222412, acc: 0.6666666865348816)
[2025-02-17 10:45:10,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:10,400][root][INFO] - Training Epoch: 1/2, step 858/1149 completed (loss: 0.1943000704050064, acc: 0.9090909361839294)
[2025-02-17 10:45:10,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:10,752][root][INFO] - Training Epoch: 1/2, step 859/1149 completed (loss: 2.1820082664489746, acc: 0.6666666865348816)
[2025-02-17 10:45:10,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:11,110][root][INFO] - Training Epoch: 1/2, step 860/1149 completed (loss: 0.7116069793701172, acc: 0.6153846383094788)
[2025-02-17 10:45:11,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:12,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:12,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:13,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:13,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:14,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:14,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:15,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:15,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:16,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:16,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:17,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:17,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:18,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:18,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:19,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:19,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:20,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:20,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:21,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:21,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:22,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:22,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:23,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:23,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:24,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:24,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:25,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:25,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:26,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:26,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:27,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:27,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:28,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:28,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:29,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:29,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:29,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:30,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:30,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:31,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:31,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:31,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:32,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:32,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:33,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:33,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:34,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:35,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:35,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:35,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:36,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:36,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:37,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:37,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:38,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:38,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:39,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:39,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:40,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:40,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:41,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:41,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:42,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:42,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:43,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:43,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:44,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:44,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:45,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:45,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:46,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:46,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:47,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:48,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:48,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:48,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:49,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:49,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:50,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:50,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:51,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:51,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:52,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:52,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:53,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:53,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:54,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:54,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:55,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:55,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:56,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:56,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:57,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:57,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:58,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:58,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:59,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:45:59,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:00,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:00,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:00,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:01,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:01,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:02,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:02,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:03,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:03,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:04,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:04,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:05,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:05,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:06,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:06,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:07,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:07,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:08,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:08,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:09,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:09,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:09,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:10,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:11,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:11,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:12,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:12,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:13,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:13,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:13,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:14,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:14,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:15,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:15,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:16,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:17,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:17,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:18,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:18,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:19,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:19,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:20,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:20,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:21,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:21,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:21,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:22,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:22,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:23,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:23,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:24,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:24,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:24,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:25,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:25,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:26,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:26,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:27,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:27,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:27,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:28,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:28,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:29,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:29,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:30,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:30,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:31,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:31,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:31,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:32,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:32,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:33,568][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(8.3779, device='cuda:0') eval_epoch_loss=tensor(2.1256, device='cuda:0') eval_epoch_acc=tensor(0.4927, device='cuda:0')
[2025-02-17 10:46:33,570][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 10:46:33,570][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 10:46:37,911][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_1_step_861_loss_2.1255948543548584/model.pt
[2025-02-17 10:46:37,932][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 10:46:37,933][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.1255948543548584
[2025-02-17 10:46:37,934][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.4926810562610626
[2025-02-17 10:46:38,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:38,390][root][INFO] - Training Epoch: 1/2, step 861/1149 completed (loss: 0.514950692653656, acc: 0.7142857313156128)
[2025-02-17 10:46:38,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:38,760][root][INFO] - Training Epoch: 1/2, step 862/1149 completed (loss: 0.17596435546875, acc: 1.0)
[2025-02-17 10:46:38,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:39,131][root][INFO] - Training Epoch: 1/2, step 863/1149 completed (loss: 0.5567746758460999, acc: 0.9333333373069763)
[2025-02-17 10:46:39,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:39,483][root][INFO] - Training Epoch: 1/2, step 864/1149 completed (loss: 0.7280449867248535, acc: 0.75)
[2025-02-17 10:46:39,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:39,828][root][INFO] - Training Epoch: 1/2, step 865/1149 completed (loss: 1.7319228649139404, acc: 0.4545454680919647)
[2025-02-17 10:46:40,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:40,287][root][INFO] - Training Epoch: 1/2, step 866/1149 completed (loss: 2.390864133834839, acc: 0.47826087474823)
[2025-02-17 10:46:40,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:40,692][root][INFO] - Training Epoch: 1/2, step 867/1149 completed (loss: 0.1612553894519806, acc: 1.0)
[2025-02-17 10:46:40,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:41,071][root][INFO] - Training Epoch: 1/2, step 868/1149 completed (loss: 0.2764114439487457, acc: 0.9230769276618958)
[2025-02-17 10:46:41,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:41,446][root][INFO] - Training Epoch: 1/2, step 869/1149 completed (loss: 0.37251541018486023, acc: 0.75)
[2025-02-17 10:46:41,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:41,809][root][INFO] - Training Epoch: 1/2, step 870/1149 completed (loss: 1.4438462257385254, acc: 0.6111111044883728)
[2025-02-17 10:46:42,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:42,250][root][INFO] - Training Epoch: 1/2, step 871/1149 completed (loss: 0.9406636357307434, acc: 0.800000011920929)
[2025-02-17 10:46:42,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:42,668][root][INFO] - Training Epoch: 1/2, step 872/1149 completed (loss: 1.3131288290023804, acc: 0.6666666865348816)
[2025-02-17 10:46:42,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:43,030][root][INFO] - Training Epoch: 1/2, step 873/1149 completed (loss: 1.1415510177612305, acc: 0.6666666865348816)
[2025-02-17 10:46:43,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:43,406][root][INFO] - Training Epoch: 1/2, step 874/1149 completed (loss: 0.838166356086731, acc: 0.8125)
[2025-02-17 10:46:43,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:43,835][root][INFO] - Training Epoch: 1/2, step 875/1149 completed (loss: 1.629258394241333, acc: 0.5714285969734192)
[2025-02-17 10:46:43,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:44,226][root][INFO] - Training Epoch: 1/2, step 876/1149 completed (loss: 0.3217318058013916, acc: 0.9166666865348816)
[2025-02-17 10:46:44,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:44,603][root][INFO] - Training Epoch: 1/2, step 877/1149 completed (loss: 0.4702451527118683, acc: 0.8888888955116272)
[2025-02-17 10:46:44,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:44,942][root][INFO] - Training Epoch: 1/2, step 878/1149 completed (loss: 1.8451712131500244, acc: 0.5714285969734192)
[2025-02-17 10:46:45,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:45,322][root][INFO] - Training Epoch: 1/2, step 879/1149 completed (loss: 2.380431890487671, acc: 0.4000000059604645)
[2025-02-17 10:46:45,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:45,864][root][INFO] - Training Epoch: 1/2, step 880/1149 completed (loss: 2.4535133838653564, acc: 0.3333333432674408)
[2025-02-17 10:46:46,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:46,293][root][INFO] - Training Epoch: 1/2, step 881/1149 completed (loss: 2.1115787029266357, acc: 0.6111111044883728)
[2025-02-17 10:46:46,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:47,104][root][INFO] - Training Epoch: 1/2, step 882/1149 completed (loss: 2.8050529956817627, acc: 0.2921348214149475)
[2025-02-17 10:46:47,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:47,481][root][INFO] - Training Epoch: 1/2, step 883/1149 completed (loss: 2.598832130432129, acc: 0.4166666567325592)
[2025-02-17 10:46:47,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:47,895][root][INFO] - Training Epoch: 1/2, step 884/1149 completed (loss: 2.479905366897583, acc: 0.3636363744735718)
[2025-02-17 10:46:48,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:48,396][root][INFO] - Training Epoch: 1/2, step 885/1149 completed (loss: 2.335221290588379, acc: 0.37681159377098083)
[2025-02-17 10:46:48,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:49,062][root][INFO] - Training Epoch: 1/2, step 886/1149 completed (loss: 2.4221696853637695, acc: 0.37956205010414124)
[2025-02-17 10:46:49,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:49,537][root][INFO] - Training Epoch: 1/2, step 887/1149 completed (loss: 2.5264363288879395, acc: 0.359375)
[2025-02-17 10:46:49,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:49,993][root][INFO] - Training Epoch: 1/2, step 888/1149 completed (loss: 1.8930931091308594, acc: 0.5555555820465088)
[2025-02-17 10:46:50,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:50,401][root][INFO] - Training Epoch: 1/2, step 889/1149 completed (loss: 2.4842491149902344, acc: 0.4000000059604645)
[2025-02-17 10:46:50,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:50,857][root][INFO] - Training Epoch: 1/2, step 890/1149 completed (loss: 1.7685707807540894, acc: 0.5454545617103577)
[2025-02-17 10:46:51,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:51,206][root][INFO] - Training Epoch: 1/2, step 891/1149 completed (loss: 0.41000959277153015, acc: 1.0)
[2025-02-17 10:46:51,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:51,547][root][INFO] - Training Epoch: 1/2, step 892/1149 completed (loss: 0.7138256430625916, acc: 0.8461538553237915)
[2025-02-17 10:46:51,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:51,882][root][INFO] - Training Epoch: 1/2, step 893/1149 completed (loss: 1.9648023843765259, acc: 0.6000000238418579)
[2025-02-17 10:46:52,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:52,234][root][INFO] - Training Epoch: 1/2, step 894/1149 completed (loss: 0.21522648632526398, acc: 1.0)
[2025-02-17 10:46:52,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:52,593][root][INFO] - Training Epoch: 1/2, step 895/1149 completed (loss: 1.8364477157592773, acc: 0.4285714328289032)
[2025-02-17 10:46:52,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:52,956][root][INFO] - Training Epoch: 1/2, step 896/1149 completed (loss: 0.9007580876350403, acc: 0.7058823704719543)
[2025-02-17 10:46:53,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:53,322][root][INFO] - Training Epoch: 1/2, step 897/1149 completed (loss: 0.4356919229030609, acc: 0.9090909361839294)
[2025-02-17 10:46:53,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:53,695][root][INFO] - Training Epoch: 1/2, step 898/1149 completed (loss: 2.160794734954834, acc: 0.44117647409439087)
[2025-02-17 10:46:53,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:54,095][root][INFO] - Training Epoch: 1/2, step 899/1149 completed (loss: 2.389213800430298, acc: 0.42424243688583374)
[2025-02-17 10:46:54,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:54,485][root][INFO] - Training Epoch: 1/2, step 900/1149 completed (loss: 1.8512969017028809, acc: 0.5555555820465088)
[2025-02-17 10:46:54,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:54,846][root][INFO] - Training Epoch: 1/2, step 901/1149 completed (loss: 1.5395514965057373, acc: 0.5833333134651184)
[2025-02-17 10:46:55,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:55,236][root][INFO] - Training Epoch: 1/2, step 902/1149 completed (loss: 2.102713108062744, acc: 0.37037035822868347)
[2025-02-17 10:46:55,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:55,647][root][INFO] - Training Epoch: 1/2, step 903/1149 completed (loss: 1.8062591552734375, acc: 0.4736842215061188)
[2025-02-17 10:46:55,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:56,073][root][INFO] - Training Epoch: 1/2, step 904/1149 completed (loss: 1.606920599937439, acc: 0.5588235259056091)
[2025-02-17 10:46:56,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:56,450][root][INFO] - Training Epoch: 1/2, step 905/1149 completed (loss: 1.9034419059753418, acc: 0.4545454680919647)
[2025-02-17 10:46:56,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:56,821][root][INFO] - Training Epoch: 1/2, step 906/1149 completed (loss: 2.4666550159454346, acc: 0.3488371968269348)
[2025-02-17 10:46:56,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:57,187][root][INFO] - Training Epoch: 1/2, step 907/1149 completed (loss: 1.0955183506011963, acc: 0.6666666865348816)
[2025-02-17 10:46:57,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:57,568][root][INFO] - Training Epoch: 1/2, step 908/1149 completed (loss: 1.724867820739746, acc: 0.5641025900840759)
[2025-02-17 10:46:57,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:57,961][root][INFO] - Training Epoch: 1/2, step 909/1149 completed (loss: 1.4924070835113525, acc: 0.6000000238418579)
[2025-02-17 10:46:58,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:58,350][root][INFO] - Training Epoch: 1/2, step 910/1149 completed (loss: 0.7121841311454773, acc: 0.8333333134651184)
[2025-02-17 10:46:58,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:58,740][root][INFO] - Training Epoch: 1/2, step 911/1149 completed (loss: 1.6855559349060059, acc: 0.6190476417541504)
[2025-02-17 10:46:58,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:59,145][root][INFO] - Training Epoch: 1/2, step 912/1149 completed (loss: 1.6268572807312012, acc: 0.4583333432674408)
[2025-02-17 10:46:59,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:59,566][root][INFO] - Training Epoch: 1/2, step 913/1149 completed (loss: 2.2866930961608887, acc: 0.39726027846336365)
[2025-02-17 10:46:59,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:46:59,977][root][INFO] - Training Epoch: 1/2, step 914/1149 completed (loss: 2.026395559310913, acc: 0.4390243887901306)
[2025-02-17 10:47:00,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:00,345][root][INFO] - Training Epoch: 1/2, step 915/1149 completed (loss: 1.802643060684204, acc: 0.5862069129943848)
[2025-02-17 10:47:00,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:00,802][root][INFO] - Training Epoch: 1/2, step 916/1149 completed (loss: 2.2906718254089355, acc: 0.31578946113586426)
[2025-02-17 10:47:01,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:01,276][root][INFO] - Training Epoch: 1/2, step 917/1149 completed (loss: 2.0822293758392334, acc: 0.4375)
[2025-02-17 10:47:01,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:01,768][root][INFO] - Training Epoch: 1/2, step 918/1149 completed (loss: 2.2414329051971436, acc: 0.3913043439388275)
[2025-02-17 10:47:01,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:02,162][root][INFO] - Training Epoch: 1/2, step 919/1149 completed (loss: 1.877778172492981, acc: 0.4545454680919647)
[2025-02-17 10:47:02,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:02,532][root][INFO] - Training Epoch: 1/2, step 920/1149 completed (loss: 1.4154857397079468, acc: 0.6551724076271057)
[2025-02-17 10:47:02,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:02,906][root][INFO] - Training Epoch: 1/2, step 921/1149 completed (loss: 2.0480573177337646, acc: 0.4038461446762085)
[2025-02-17 10:47:03,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:03,261][root][INFO] - Training Epoch: 1/2, step 922/1149 completed (loss: 1.611171841621399, acc: 0.4444444477558136)
[2025-02-17 10:47:03,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:03,654][root][INFO] - Training Epoch: 1/2, step 923/1149 completed (loss: 0.22771082818508148, acc: 0.8888888955116272)
[2025-02-17 10:47:03,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:04,071][root][INFO] - Training Epoch: 1/2, step 924/1149 completed (loss: 1.8127927780151367, acc: 0.6842105388641357)
[2025-02-17 10:47:04,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:04,433][root][INFO] - Training Epoch: 1/2, step 925/1149 completed (loss: 0.8913828134536743, acc: 0.692307710647583)
[2025-02-17 10:47:04,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:04,780][root][INFO] - Training Epoch: 1/2, step 926/1149 completed (loss: 0.8276407718658447, acc: 0.5833333134651184)
[2025-02-17 10:47:04,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:05,146][root][INFO] - Training Epoch: 1/2, step 927/1149 completed (loss: 1.4155136346817017, acc: 0.5714285969734192)
[2025-02-17 10:47:05,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:05,533][root][INFO] - Training Epoch: 1/2, step 928/1149 completed (loss: 1.3629533052444458, acc: 0.6060606241226196)
[2025-02-17 10:47:05,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:05,878][root][INFO] - Training Epoch: 1/2, step 929/1149 completed (loss: 2.2144768238067627, acc: 0.38461539149284363)
[2025-02-17 10:47:06,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:06,247][root][INFO] - Training Epoch: 1/2, step 930/1149 completed (loss: 1.6471142768859863, acc: 0.5581395626068115)
[2025-02-17 10:47:06,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:06,595][root][INFO] - Training Epoch: 1/2, step 931/1149 completed (loss: 1.9727355241775513, acc: 0.4390243887901306)
[2025-02-17 10:47:06,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:06,953][root][INFO] - Training Epoch: 1/2, step 932/1149 completed (loss: 1.7919977903366089, acc: 0.6346153616905212)
[2025-02-17 10:47:07,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:07,292][root][INFO] - Training Epoch: 1/2, step 933/1149 completed (loss: 2.2732326984405518, acc: 0.4516128897666931)
[2025-02-17 10:47:07,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:07,703][root][INFO] - Training Epoch: 1/2, step 934/1149 completed (loss: 1.7704763412475586, acc: 0.5060241222381592)
[2025-02-17 10:47:07,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:08,074][root][INFO] - Training Epoch: 1/2, step 935/1149 completed (loss: 1.7650285959243774, acc: 0.5357142686843872)
[2025-02-17 10:47:08,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:08,462][root][INFO] - Training Epoch: 1/2, step 936/1149 completed (loss: 1.939556360244751, acc: 0.46666666865348816)
[2025-02-17 10:47:08,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:08,839][root][INFO] - Training Epoch: 1/2, step 937/1149 completed (loss: 1.6684247255325317, acc: 0.604651153087616)
[2025-02-17 10:47:09,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:09,231][root][INFO] - Training Epoch: 1/2, step 938/1149 completed (loss: 1.7071071863174438, acc: 0.5465116500854492)
[2025-02-17 10:47:09,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:09,576][root][INFO] - Training Epoch: 1/2, step 939/1149 completed (loss: 1.3179901838302612, acc: 0.6486486196517944)
[2025-02-17 10:47:09,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:09,912][root][INFO] - Training Epoch: 1/2, step 940/1149 completed (loss: 0.9597422480583191, acc: 0.75)
[2025-02-17 10:47:10,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:10,265][root][INFO] - Training Epoch: 1/2, step 941/1149 completed (loss: 2.315560817718506, acc: 0.25)
[2025-02-17 10:47:10,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:10,647][root][INFO] - Training Epoch: 1/2, step 942/1149 completed (loss: 2.080479383468628, acc: 0.4000000059604645)
[2025-02-17 10:47:10,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:10,999][root][INFO] - Training Epoch: 1/2, step 943/1149 completed (loss: 2.040975570678711, acc: 0.4615384638309479)
[2025-02-17 10:47:11,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:11,366][root][INFO] - Training Epoch: 1/2, step 944/1149 completed (loss: 2.435864210128784, acc: 0.4545454680919647)
[2025-02-17 10:47:11,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:11,871][root][INFO] - Training Epoch: 1/2, step 945/1149 completed (loss: 2.0916733741760254, acc: 0.4395604431629181)
[2025-02-17 10:47:12,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:12,267][root][INFO] - Training Epoch: 1/2, step 946/1149 completed (loss: 2.424060821533203, acc: 0.3372093141078949)
[2025-02-17 10:47:12,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:12,698][root][INFO] - Training Epoch: 1/2, step 947/1149 completed (loss: 2.29115891456604, acc: 0.41258740425109863)
[2025-02-17 10:47:12,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:13,144][root][INFO] - Training Epoch: 1/2, step 948/1149 completed (loss: 1.7334362268447876, acc: 0.5249999761581421)
[2025-02-17 10:47:13,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:13,560][root][INFO] - Training Epoch: 1/2, step 949/1149 completed (loss: 1.996135950088501, acc: 0.4464285671710968)
[2025-02-17 10:47:13,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:13,996][root][INFO] - Training Epoch: 1/2, step 950/1149 completed (loss: 1.563207983970642, acc: 0.59375)
[2025-02-17 10:47:14,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:14,406][root][INFO] - Training Epoch: 1/2, step 951/1149 completed (loss: 2.2961373329162598, acc: 0.447761207818985)
[2025-02-17 10:47:14,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:14,802][root][INFO] - Training Epoch: 1/2, step 952/1149 completed (loss: 1.590765118598938, acc: 0.54666668176651)
[2025-02-17 10:47:15,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:15,244][root][INFO] - Training Epoch: 1/2, step 953/1149 completed (loss: 2.3890767097473145, acc: 0.453125)
[2025-02-17 10:47:15,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:15,688][root][INFO] - Training Epoch: 1/2, step 954/1149 completed (loss: 2.2878541946411133, acc: 0.4677419364452362)
[2025-02-17 10:47:15,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:16,117][root][INFO] - Training Epoch: 1/2, step 955/1149 completed (loss: 1.8094730377197266, acc: 0.4399999976158142)
[2025-02-17 10:47:16,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:16,533][root][INFO] - Training Epoch: 1/2, step 956/1149 completed (loss: 0.6407350301742554, acc: 0.625)
[2025-02-17 10:47:16,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:16,979][root][INFO] - Training Epoch: 1/2, step 957/1149 completed (loss: 1.880132794380188, acc: 0.5714285969734192)
[2025-02-17 10:47:17,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:17,390][root][INFO] - Training Epoch: 1/2, step 958/1149 completed (loss: 1.3644437789916992, acc: 0.5714285969734192)
[2025-02-17 10:47:17,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:17,736][root][INFO] - Training Epoch: 1/2, step 959/1149 completed (loss: 1.366617202758789, acc: 0.6315789222717285)
[2025-02-17 10:47:17,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:18,133][root][INFO] - Training Epoch: 1/2, step 960/1149 completed (loss: 1.6480755805969238, acc: 0.6153846383094788)
[2025-02-17 10:47:18,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:18,574][root][INFO] - Training Epoch: 1/2, step 961/1149 completed (loss: 1.1536554098129272, acc: 0.7857142686843872)
[2025-02-17 10:47:18,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:18,939][root][INFO] - Training Epoch: 1/2, step 962/1149 completed (loss: 2.2994234561920166, acc: 0.4444444477558136)
[2025-02-17 10:47:19,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:19,288][root][INFO] - Training Epoch: 1/2, step 963/1149 completed (loss: 1.419835090637207, acc: 0.5454545617103577)
[2025-02-17 10:47:19,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:19,674][root][INFO] - Training Epoch: 1/2, step 964/1149 completed (loss: 1.755547046661377, acc: 0.3333333432674408)
[2025-02-17 10:47:19,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:20,101][root][INFO] - Training Epoch: 1/2, step 965/1149 completed (loss: 0.3646881878376007, acc: 0.9090909361839294)
[2025-02-17 10:47:20,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:20,533][root][INFO] - Training Epoch: 1/2, step 966/1149 completed (loss: 0.8747093081474304, acc: 0.7272727489471436)
[2025-02-17 10:47:20,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:21,002][root][INFO] - Training Epoch: 1/2, step 967/1149 completed (loss: 1.7555168867111206, acc: 0.5744680762290955)
[2025-02-17 10:47:21,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:21,408][root][INFO] - Training Epoch: 1/2, step 968/1149 completed (loss: 1.5581934452056885, acc: 0.6315789222717285)
[2025-02-17 10:47:21,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:21,769][root][INFO] - Training Epoch: 1/2, step 969/1149 completed (loss: 0.8070312142372131, acc: 0.75)
[2025-02-17 10:47:21,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:22,142][root][INFO] - Training Epoch: 1/2, step 970/1149 completed (loss: 0.48481667041778564, acc: 0.875)
[2025-02-17 10:47:22,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:22,534][root][INFO] - Training Epoch: 1/2, step 971/1149 completed (loss: 1.4366461038589478, acc: 0.5454545617103577)
[2025-02-17 10:47:22,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:22,881][root][INFO] - Training Epoch: 1/2, step 972/1149 completed (loss: 1.583037257194519, acc: 0.46666666865348816)
[2025-02-17 10:47:23,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:23,233][root][INFO] - Training Epoch: 1/2, step 973/1149 completed (loss: 3.3208744525909424, acc: 0.25)
[2025-02-17 10:47:23,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:23,596][root][INFO] - Training Epoch: 1/2, step 974/1149 completed (loss: 1.6550219058990479, acc: 0.6000000238418579)
[2025-02-17 10:47:23,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:23,974][root][INFO] - Training Epoch: 1/2, step 975/1149 completed (loss: 1.9540549516677856, acc: 0.5454545617103577)
[2025-02-17 10:47:24,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:24,404][root][INFO] - Training Epoch: 1/2, step 976/1149 completed (loss: 1.7911086082458496, acc: 0.3333333432674408)
[2025-02-17 10:47:24,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:24,809][root][INFO] - Training Epoch: 1/2, step 977/1149 completed (loss: 1.339986801147461, acc: 0.4615384638309479)
[2025-02-17 10:47:25,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:25,251][root][INFO] - Training Epoch: 1/2, step 978/1149 completed (loss: 2.564605712890625, acc: 0.3461538553237915)
[2025-02-17 10:47:25,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:25,662][root][INFO] - Training Epoch: 1/2, step 979/1149 completed (loss: 1.0800026655197144, acc: 0.692307710647583)
[2025-02-17 10:47:25,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:26,032][root][INFO] - Training Epoch: 1/2, step 980/1149 completed (loss: 1.263840675354004, acc: 0.5789473652839661)
[2025-02-17 10:47:26,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:26,379][root][INFO] - Training Epoch: 1/2, step 981/1149 completed (loss: 0.40186384320259094, acc: 0.9090909361839294)
[2025-02-17 10:47:26,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:26,748][root][INFO] - Training Epoch: 1/2, step 982/1149 completed (loss: 0.9833312630653381, acc: 0.7857142686843872)
[2025-02-17 10:47:26,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:27,127][root][INFO] - Training Epoch: 1/2, step 983/1149 completed (loss: 1.858866572380066, acc: 0.6000000238418579)
[2025-02-17 10:47:27,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:27,529][root][INFO] - Training Epoch: 1/2, step 984/1149 completed (loss: 2.32412052154541, acc: 0.3684210479259491)
[2025-02-17 10:47:27,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:27,875][root][INFO] - Training Epoch: 1/2, step 985/1149 completed (loss: 0.9220534563064575, acc: 0.7692307829856873)
[2025-02-17 10:47:28,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:28,254][root][INFO] - Training Epoch: 1/2, step 986/1149 completed (loss: 1.1428003311157227, acc: 0.625)
[2025-02-17 10:47:28,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:28,563][root][INFO] - Training Epoch: 1/2, step 987/1149 completed (loss: 1.6204354763031006, acc: 0.5384615659713745)
[2025-02-17 10:47:28,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:28,932][root][INFO] - Training Epoch: 1/2, step 988/1149 completed (loss: 0.759028971195221, acc: 0.8181818127632141)
[2025-02-17 10:47:29,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:29,278][root][INFO] - Training Epoch: 1/2, step 989/1149 completed (loss: 1.1250460147857666, acc: 0.625)
[2025-02-17 10:47:29,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:29,612][root][INFO] - Training Epoch: 1/2, step 990/1149 completed (loss: 3.0854108333587646, acc: 0.27272728085517883)
[2025-02-17 10:47:29,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:29,962][root][INFO] - Training Epoch: 1/2, step 991/1149 completed (loss: 2.7801949977874756, acc: 0.5)
[2025-02-17 10:47:30,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:30,336][root][INFO] - Training Epoch: 1/2, step 992/1149 completed (loss: 1.8677515983581543, acc: 0.3913043439388275)
[2025-02-17 10:47:30,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:30,765][root][INFO] - Training Epoch: 1/2, step 993/1149 completed (loss: 1.9691047668457031, acc: 0.4157303273677826)
[2025-02-17 10:47:31,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:31,233][root][INFO] - Training Epoch: 1/2, step 994/1149 completed (loss: 1.8318556547164917, acc: 0.5094339847564697)
[2025-02-17 10:47:31,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:31,592][root][INFO] - Training Epoch: 1/2, step 995/1149 completed (loss: 1.5402027368545532, acc: 0.5)
[2025-02-17 10:47:31,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:31,975][root][INFO] - Training Epoch: 1/2, step 996/1149 completed (loss: 1.9246673583984375, acc: 0.48148149251937866)
[2025-02-17 10:47:32,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:32,352][root][INFO] - Training Epoch: 1/2, step 997/1149 completed (loss: 2.3499157428741455, acc: 0.37142857909202576)
[2025-02-17 10:47:32,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:32,742][root][INFO] - Training Epoch: 1/2, step 998/1149 completed (loss: 2.370445489883423, acc: 0.3478260934352875)
[2025-02-17 10:47:32,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:33,122][root][INFO] - Training Epoch: 1/2, step 999/1149 completed (loss: 2.0781867504119873, acc: 0.4000000059604645)
[2025-02-17 10:47:33,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:33,516][root][INFO] - Training Epoch: 1/2, step 1000/1149 completed (loss: 2.1269149780273438, acc: 0.4363636374473572)
[2025-02-17 10:47:33,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:33,907][root][INFO] - Training Epoch: 1/2, step 1001/1149 completed (loss: 1.7459120750427246, acc: 0.4864864945411682)
[2025-02-17 10:47:34,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:34,274][root][INFO] - Training Epoch: 1/2, step 1002/1149 completed (loss: 0.6876005530357361, acc: 0.7857142686843872)
[2025-02-17 10:47:34,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:34,610][root][INFO] - Training Epoch: 1/2, step 1003/1149 completed (loss: 0.5879577994346619, acc: 0.8181818127632141)
[2025-02-17 10:47:34,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:34,968][root][INFO] - Training Epoch: 1/2, step 1004/1149 completed (loss: 0.9895899891853333, acc: 0.692307710647583)
[2025-02-17 10:47:35,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:35,330][root][INFO] - Training Epoch: 1/2, step 1005/1149 completed (loss: 1.5865148305892944, acc: 0.5384615659713745)
[2025-02-17 10:47:35,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:35,706][root][INFO] - Training Epoch: 1/2, step 1006/1149 completed (loss: 1.1518703699111938, acc: 0.5833333134651184)
[2025-02-17 10:47:35,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:36,067][root][INFO] - Training Epoch: 1/2, step 1007/1149 completed (loss: 0.6852821111679077, acc: 0.800000011920929)
[2025-02-17 10:47:36,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:36,404][root][INFO] - Training Epoch: 1/2, step 1008/1149 completed (loss: 1.1284024715423584, acc: 0.6000000238418579)
[2025-02-17 10:47:36,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:36,765][root][INFO] - Training Epoch: 1/2, step 1009/1149 completed (loss: 1.4182851314544678, acc: 0.7058823704719543)
[2025-02-17 10:47:36,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:37,147][root][INFO] - Training Epoch: 1/2, step 1010/1149 completed (loss: 1.695862054824829, acc: 0.6129032373428345)
[2025-02-17 10:47:37,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:37,513][root][INFO] - Training Epoch: 1/2, step 1011/1149 completed (loss: 1.1029188632965088, acc: 0.6818181872367859)
[2025-02-17 10:47:37,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:37,874][root][INFO] - Training Epoch: 1/2, step 1012/1149 completed (loss: 0.28130969405174255, acc: 0.9285714030265808)
[2025-02-17 10:47:38,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:38,280][root][INFO] - Training Epoch: 1/2, step 1013/1149 completed (loss: 2.0138800144195557, acc: 0.4000000059604645)
[2025-02-17 10:47:38,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:38,947][root][INFO] - Training Epoch: 1/2, step 1014/1149 completed (loss: 2.124350070953369, acc: 0.4029850661754608)
[2025-02-17 10:47:39,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:39,423][root][INFO] - Training Epoch: 1/2, step 1015/1149 completed (loss: 1.9817601442337036, acc: 0.4318181872367859)
[2025-02-17 10:47:39,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:39,961][root][INFO] - Training Epoch: 1/2, step 1016/1149 completed (loss: 1.7786438465118408, acc: 0.5714285969734192)
[2025-02-17 10:47:40,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:40,343][root][INFO] - Training Epoch: 1/2, step 1017/1149 completed (loss: 1.105697512626648, acc: 0.7727272510528564)
[2025-02-17 10:47:40,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:40,717][root][INFO] - Training Epoch: 1/2, step 1018/1149 completed (loss: 0.06691034883260727, acc: 1.0)
[2025-02-17 10:47:40,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:41,135][root][INFO] - Training Epoch: 1/2, step 1019/1149 completed (loss: 0.7985100746154785, acc: 0.75)
[2025-02-17 10:47:41,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:41,558][root][INFO] - Training Epoch: 1/2, step 1020/1149 completed (loss: 0.5640626549720764, acc: 0.8125)
[2025-02-17 10:47:41,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:41,965][root][INFO] - Training Epoch: 1/2, step 1021/1149 completed (loss: 0.5686350464820862, acc: 0.7857142686843872)
[2025-02-17 10:47:42,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:42,325][root][INFO] - Training Epoch: 1/2, step 1022/1149 completed (loss: 1.1080596446990967, acc: 0.699999988079071)
[2025-02-17 10:47:42,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:42,766][root][INFO] - Training Epoch: 1/2, step 1023/1149 completed (loss: 1.1258249282836914, acc: 0.8125)
[2025-02-17 10:47:42,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:43,117][root][INFO] - Training Epoch: 1/2, step 1024/1149 completed (loss: 0.7795303463935852, acc: 0.800000011920929)
[2025-02-17 10:47:45,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:46,593][root][INFO] - Training Epoch: 1/2, step 1025/1149 completed (loss: 2.3503079414367676, acc: 0.40799999237060547)
[2025-02-17 10:47:46,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:47,011][root][INFO] - Training Epoch: 1/2, step 1026/1149 completed (loss: 1.8009761571884155, acc: 0.6041666865348816)
[2025-02-17 10:47:47,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:47,866][root][INFO] - Training Epoch: 1/2, step 1027/1149 completed (loss: 2.01202654838562, acc: 0.5256410241127014)
[2025-02-17 10:47:48,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:48,242][root][INFO] - Training Epoch: 1/2, step 1028/1149 completed (loss: 2.3178601264953613, acc: 0.4000000059604645)
[2025-02-17 10:47:48,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:48,747][root][INFO] - Training Epoch: 1/2, step 1029/1149 completed (loss: 0.60982346534729, acc: 0.8333333134651184)
[2025-02-17 10:47:48,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:49,159][root][INFO] - Training Epoch: 1/2, step 1030/1149 completed (loss: 1.3015086650848389, acc: 0.6000000238418579)
[2025-02-17 10:47:49,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:49,582][root][INFO] - Training Epoch: 1/2, step 1031/1149 completed (loss: 0.8607300519943237, acc: 0.6800000071525574)
[2025-02-17 10:47:49,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:50,323][root][INFO] - Training Epoch: 1/2, step 1032/1149 completed (loss: 1.6430171728134155, acc: 0.5423728823661804)
[2025-02-17 10:47:50,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:50,705][root][INFO] - Training Epoch: 1/2, step 1033/1149 completed (loss: 1.1657353639602661, acc: 0.7692307829856873)
[2025-02-17 10:47:50,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:51,105][root][INFO] - Training Epoch: 1/2, step 1034/1149 completed (loss: 0.30818870663642883, acc: 0.9166666865348816)
[2025-02-17 10:47:51,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:51,474][root][INFO] - Training Epoch: 1/2, step 1035/1149 completed (loss: 0.08286058157682419, acc: 1.0)
[2025-02-17 10:47:51,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:51,854][root][INFO] - Training Epoch: 1/2, step 1036/1149 completed (loss: 1.9967846870422363, acc: 0.47058823704719543)
[2025-02-17 10:47:52,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:52,234][root][INFO] - Training Epoch: 1/2, step 1037/1149 completed (loss: 1.4539772272109985, acc: 0.5714285969734192)
[2025-02-17 10:47:52,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:52,578][root][INFO] - Training Epoch: 1/2, step 1038/1149 completed (loss: 2.4412569999694824, acc: 0.5)
[2025-02-17 10:47:52,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:52,959][root][INFO] - Training Epoch: 1/2, step 1039/1149 completed (loss: 1.1545403003692627, acc: 0.800000011920929)
[2025-02-17 10:47:53,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:53,387][root][INFO] - Training Epoch: 1/2, step 1040/1149 completed (loss: 0.9079736471176147, acc: 0.8461538553237915)
[2025-02-17 10:47:53,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:53,792][root][INFO] - Training Epoch: 1/2, step 1041/1149 completed (loss: 1.8162622451782227, acc: 0.6428571343421936)
[2025-02-17 10:47:53,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:54,151][root][INFO] - Training Epoch: 1/2, step 1042/1149 completed (loss: 0.6083519458770752, acc: 0.800000011920929)
[2025-02-17 10:47:54,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:55,163][root][INFO] - Training Epoch: 1/2, step 1043/1149 completed (loss: 2.329983711242676, acc: 0.3539822995662689)
[2025-02-17 10:47:55,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:55,577][root][INFO] - Training Epoch: 1/2, step 1044/1149 completed (loss: 1.6742810010910034, acc: 0.4923076927661896)
[2025-02-17 10:47:55,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:55,977][root][INFO] - Training Epoch: 1/2, step 1045/1149 completed (loss: 1.4409765005111694, acc: 0.6521739363670349)
[2025-02-17 10:47:56,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:56,385][root][INFO] - Training Epoch: 1/2, step 1046/1149 completed (loss: 1.724543809890747, acc: 0.5641025900840759)
[2025-02-17 10:47:56,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:56,796][root][INFO] - Training Epoch: 1/2, step 1047/1149 completed (loss: 1.6093919277191162, acc: 0.4406779706478119)
[2025-02-17 10:47:57,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:57,362][root][INFO] - Training Epoch: 1/2, step 1048/1149 completed (loss: 1.659961223602295, acc: 0.5520833134651184)
[2025-02-17 10:47:57,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:57,772][root][INFO] - Training Epoch: 1/2, step 1049/1149 completed (loss: 1.8052890300750732, acc: 0.49038460850715637)
[2025-02-17 10:47:57,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:58,149][root][INFO] - Training Epoch: 1/2, step 1050/1149 completed (loss: 1.0428173542022705, acc: 0.65625)
[2025-02-17 10:47:58,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:58,546][root][INFO] - Training Epoch: 1/2, step 1051/1149 completed (loss: 1.16868257522583, acc: 0.6363636255264282)
[2025-02-17 10:47:58,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:58,988][root][INFO] - Training Epoch: 1/2, step 1052/1149 completed (loss: 1.453715205192566, acc: 0.5454545617103577)
[2025-02-17 10:47:59,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:59,418][root][INFO] - Training Epoch: 1/2, step 1053/1149 completed (loss: 0.6563374996185303, acc: 0.875)
[2025-02-17 10:47:59,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:47:59,809][root][INFO] - Training Epoch: 1/2, step 1054/1149 completed (loss: 1.4074742794036865, acc: 0.375)
[2025-02-17 10:48:00,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:00,237][root][INFO] - Training Epoch: 1/2, step 1055/1149 completed (loss: 1.3566025495529175, acc: 0.4615384638309479)
[2025-02-17 10:48:00,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:00,669][root][INFO] - Training Epoch: 1/2, step 1056/1149 completed (loss: 2.282088041305542, acc: 0.3333333432674408)
[2025-02-17 10:48:00,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:01,063][root][INFO] - Training Epoch: 1/2, step 1057/1149 completed (loss: 2.4611945152282715, acc: 0.3478260934352875)
[2025-02-17 10:48:01,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:01,431][root][INFO] - Training Epoch: 1/2, step 1058/1149 completed (loss: 1.7442604303359985, acc: 0.52173912525177)
[2025-02-17 10:48:01,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:01,809][root][INFO] - Training Epoch: 1/2, step 1059/1149 completed (loss: 0.7021134495735168, acc: 0.7692307829856873)
[2025-02-17 10:48:02,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:02,216][root][INFO] - Training Epoch: 1/2, step 1060/1149 completed (loss: 2.362151861190796, acc: 0.4399999976158142)
[2025-02-17 10:48:02,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:02,568][root][INFO] - Training Epoch: 1/2, step 1061/1149 completed (loss: 2.553443670272827, acc: 0.2777777910232544)
[2025-02-17 10:48:02,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:03,014][root][INFO] - Training Epoch: 1/2, step 1062/1149 completed (loss: 1.644667387008667, acc: 0.5909090638160706)
[2025-02-17 10:48:03,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:03,437][root][INFO] - Training Epoch: 1/2, step 1063/1149 completed (loss: 2.216848373413086, acc: 0.4545454680919647)
[2025-02-17 10:48:03,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:03,818][root][INFO] - Training Epoch: 1/2, step 1064/1149 completed (loss: 2.3237693309783936, acc: 0.31578946113586426)
[2025-02-17 10:48:03,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:04,222][root][INFO] - Training Epoch: 1/2, step 1065/1149 completed (loss: 2.1411144733428955, acc: 0.44117647409439087)
[2025-02-17 10:48:04,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:04,607][root][INFO] - Training Epoch: 1/2, step 1066/1149 completed (loss: 0.8867552280426025, acc: 0.7333333492279053)
[2025-02-17 10:48:04,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:04,994][root][INFO] - Training Epoch: 1/2, step 1067/1149 completed (loss: 1.6541727781295776, acc: 0.6551724076271057)
[2025-02-17 10:48:05,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:05,421][root][INFO] - Training Epoch: 1/2, step 1068/1149 completed (loss: 0.5944021344184875, acc: 0.875)
[2025-02-17 10:48:05,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:05,772][root][INFO] - Training Epoch: 1/2, step 1069/1149 completed (loss: 1.6505842208862305, acc: 0.529411792755127)
[2025-02-17 10:48:05,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:06,197][root][INFO] - Training Epoch: 1/2, step 1070/1149 completed (loss: 1.253670334815979, acc: 0.75)
[2025-02-17 10:48:06,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:06,582][root][INFO] - Training Epoch: 1/2, step 1071/1149 completed (loss: 1.1005748510360718, acc: 0.625)
[2025-02-17 10:48:06,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:06,991][root][INFO] - Training Epoch: 1/2, step 1072/1149 completed (loss: 0.8034399151802063, acc: 0.7857142686843872)
[2025-02-17 10:48:07,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:07,325][root][INFO] - Training Epoch: 1/2, step 1073/1149 completed (loss: 1.3604575395584106, acc: 0.625)
[2025-02-17 10:48:07,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:07,746][root][INFO] - Training Epoch: 1/2, step 1074/1149 completed (loss: 1.8157824277877808, acc: 0.41860464215278625)
[2025-02-17 10:48:07,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:08,106][root][INFO] - Training Epoch: 1/2, step 1075/1149 completed (loss: 0.879246711730957, acc: 0.7272727489471436)
[2025-02-17 10:48:08,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:08,452][root][INFO] - Training Epoch: 1/2, step 1076/1149 completed (loss: 1.0001566410064697, acc: 0.7333333492279053)
[2025-02-17 10:48:08,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:08,817][root][INFO] - Training Epoch: 1/2, step 1077/1149 completed (loss: 1.5787949562072754, acc: 0.5306122303009033)
[2025-02-17 10:48:08,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:09,180][root][INFO] - Training Epoch: 1/2, step 1078/1149 completed (loss: 0.8045905232429504, acc: 0.7857142686843872)
[2025-02-17 10:48:09,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:09,603][root][INFO] - Training Epoch: 1/2, step 1079/1149 completed (loss: 1.4385507106781006, acc: 0.6111111044883728)
[2025-02-17 10:48:09,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:09,958][root][INFO] - Training Epoch: 1/2, step 1080/1149 completed (loss: 1.7360451221466064, acc: 0.46666666865348816)
[2025-02-17 10:48:10,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:10,304][root][INFO] - Training Epoch: 1/2, step 1081/1149 completed (loss: 0.8644747138023376, acc: 0.7777777910232544)
[2025-02-17 10:48:10,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:10,662][root][INFO] - Training Epoch: 1/2, step 1082/1149 completed (loss: 1.0896819829940796, acc: 0.7142857313156128)
[2025-02-17 10:48:10,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:11,015][root][INFO] - Training Epoch: 1/2, step 1083/1149 completed (loss: 0.757972776889801, acc: 0.7777777910232544)
[2025-02-17 10:48:11,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:11,347][root][INFO] - Training Epoch: 1/2, step 1084/1149 completed (loss: 0.7401801943778992, acc: 0.7777777910232544)
[2025-02-17 10:48:11,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:11,722][root][INFO] - Training Epoch: 1/2, step 1085/1149 completed (loss: 1.1938966512680054, acc: 0.6153846383094788)
[2025-02-17 10:48:11,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:12,060][root][INFO] - Training Epoch: 1/2, step 1086/1149 completed (loss: 0.4599067270755768, acc: 0.8999999761581421)
[2025-02-17 10:48:12,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:12,396][root][INFO] - Training Epoch: 1/2, step 1087/1149 completed (loss: 0.4863205552101135, acc: 0.9230769276618958)
[2025-02-17 10:48:12,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:12,737][root][INFO] - Training Epoch: 1/2, step 1088/1149 completed (loss: 1.224593162536621, acc: 0.6111111044883728)
[2025-02-17 10:48:12,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:13,068][root][INFO] - Training Epoch: 1/2, step 1089/1149 completed (loss: 2.4020068645477295, acc: 0.3333333432674408)
[2025-02-17 10:48:13,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:13,404][root][INFO] - Training Epoch: 1/2, step 1090/1149 completed (loss: 0.1289987713098526, acc: 1.0)
[2025-02-17 10:48:13,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:13,743][root][INFO] - Training Epoch: 1/2, step 1091/1149 completed (loss: 1.021829605102539, acc: 0.5862069129943848)
[2025-02-17 10:48:13,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:14,103][root][INFO] - Training Epoch: 1/2, step 1092/1149 completed (loss: 0.9457241296768188, acc: 0.7599999904632568)
[2025-02-17 10:48:14,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:14,437][root][INFO] - Training Epoch: 1/2, step 1093/1149 completed (loss: 0.0635199099779129, acc: 1.0)
[2025-02-17 10:48:14,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:14,800][root][INFO] - Training Epoch: 1/2, step 1094/1149 completed (loss: 1.1827605962753296, acc: 0.7307692170143127)
[2025-02-17 10:48:14,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:15,172][root][INFO] - Training Epoch: 1/2, step 1095/1149 completed (loss: 1.114533543586731, acc: 0.75)
[2025-02-17 10:48:15,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:15,514][root][INFO] - Training Epoch: 1/2, step 1096/1149 completed (loss: 0.9339017271995544, acc: 0.7894737124443054)
[2025-02-17 10:48:15,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:15,847][root][INFO] - Training Epoch: 1/2, step 1097/1149 completed (loss: 0.900708019733429, acc: 0.75)
[2025-02-17 10:48:16,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:16,215][root][INFO] - Training Epoch: 1/2, step 1098/1149 completed (loss: 0.28655728697776794, acc: 0.9166666865348816)
[2025-02-17 10:48:16,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:16,559][root][INFO] - Training Epoch: 1/2, step 1099/1149 completed (loss: 1.0547858476638794, acc: 0.692307710647583)
[2025-02-17 10:48:16,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:16,908][root][INFO] - Training Epoch: 1/2, step 1100/1149 completed (loss: 0.30751800537109375, acc: 0.9166666865348816)
[2025-02-17 10:48:17,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:17,330][root][INFO] - Training Epoch: 1/2, step 1101/1149 completed (loss: 1.0574109554290771, acc: 0.761904776096344)
[2025-02-17 10:48:17,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:17,694][root][INFO] - Training Epoch: 1/2, step 1102/1149 completed (loss: 0.08341863751411438, acc: 1.0)
[2025-02-17 10:48:17,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:18,053][root][INFO] - Training Epoch: 1/2, step 1103/1149 completed (loss: 1.0034370422363281, acc: 0.6071428656578064)
[2025-02-17 10:48:18,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:18,407][root][INFO] - Training Epoch: 1/2, step 1104/1149 completed (loss: 0.673987627029419, acc: 0.774193525314331)
[2025-02-17 10:48:18,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:18,750][root][INFO] - Training Epoch: 1/2, step 1105/1149 completed (loss: 0.972691535949707, acc: 0.7692307829856873)
[2025-02-17 10:48:18,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:19,120][root][INFO] - Training Epoch: 1/2, step 1106/1149 completed (loss: 1.5488218069076538, acc: 0.6666666865348816)
[2025-02-17 10:48:19,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:19,530][root][INFO] - Training Epoch: 1/2, step 1107/1149 completed (loss: 1.866821527481079, acc: 0.45783132314682007)
[2025-02-17 10:48:19,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:19,942][root][INFO] - Training Epoch: 1/2, step 1108/1149 completed (loss: 0.5254520773887634, acc: 0.8367347121238708)
[2025-02-17 10:48:20,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:20,330][root][INFO] - Training Epoch: 1/2, step 1109/1149 completed (loss: 1.9071221351623535, acc: 0.5104166865348816)
[2025-02-17 10:48:20,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:20,811][root][INFO] - Training Epoch: 1/2, step 1110/1149 completed (loss: 1.7351046800613403, acc: 0.5942028760910034)
[2025-02-17 10:48:20,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:21,193][root][INFO] - Training Epoch: 1/2, step 1111/1149 completed (loss: 2.2975122928619385, acc: 0.43661972880363464)
[2025-02-17 10:48:21,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:21,601][root][INFO] - Training Epoch: 1/2, step 1112/1149 completed (loss: 1.6067006587982178, acc: 0.6197183132171631)
[2025-02-17 10:48:21,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:22,010][root][INFO] - Training Epoch: 1/2, step 1113/1149 completed (loss: 2.029418468475342, acc: 0.4749999940395355)
[2025-02-17 10:48:22,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:22,393][root][INFO] - Training Epoch: 1/2, step 1114/1149 completed (loss: 1.4763128757476807, acc: 0.6307692527770996)
[2025-02-17 10:48:22,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:22,763][root][INFO] - Training Epoch: 1/2, step 1115/1149 completed (loss: 1.1099944114685059, acc: 0.692307710647583)
[2025-02-17 10:48:22,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:23,085][root][INFO] - Training Epoch: 1/2, step 1116/1149 completed (loss: 0.9042016267776489, acc: 0.7777777910232544)
[2025-02-17 10:48:23,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:23,423][root][INFO] - Training Epoch: 1/2, step 1117/1149 completed (loss: 0.42215701937675476, acc: 0.8125)
[2025-02-17 10:48:23,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:23,771][root][INFO] - Training Epoch: 1/2, step 1118/1149 completed (loss: 0.5145007967948914, acc: 0.7692307829856873)
[2025-02-17 10:48:23,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:24,162][root][INFO] - Training Epoch: 1/2, step 1119/1149 completed (loss: 1.8622183799743652, acc: 0.5384615659713745)
[2025-02-17 10:48:24,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:24,527][root][INFO] - Training Epoch: 1/2, step 1120/1149 completed (loss: 0.5771239995956421, acc: 0.75)
[2025-02-17 10:48:24,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:24,869][root][INFO] - Training Epoch: 1/2, step 1121/1149 completed (loss: 0.24829162657260895, acc: 1.0)
[2025-02-17 10:48:25,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:25,236][root][INFO] - Training Epoch: 1/2, step 1122/1149 completed (loss: 0.5065956711769104, acc: 0.800000011920929)
[2025-02-17 10:48:25,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:25,636][root][INFO] - Training Epoch: 1/2, step 1123/1149 completed (loss: 1.391964077949524, acc: 0.5862069129943848)
[2025-02-17 10:48:25,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:26,015][root][INFO] - Training Epoch: 1/2, step 1124/1149 completed (loss: 1.6101192235946655, acc: 0.5555555820465088)
[2025-02-17 10:48:26,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:26,372][root][INFO] - Training Epoch: 1/2, step 1125/1149 completed (loss: 1.034387230873108, acc: 0.7407407164573669)
[2025-02-17 10:48:26,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:26,824][root][INFO] - Training Epoch: 1/2, step 1126/1149 completed (loss: 1.653247594833374, acc: 0.5918367505073547)
[2025-02-17 10:48:27,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:27,259][root][INFO] - Training Epoch: 1/2, step 1127/1149 completed (loss: 1.1828707456588745, acc: 0.75)
[2025-02-17 10:48:27,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:27,646][root][INFO] - Training Epoch: 1/2, step 1128/1149 completed (loss: 1.172132134437561, acc: 0.761904776096344)
[2025-02-17 10:48:27,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:28,066][root][INFO] - Training Epoch: 1/2, step 1129/1149 completed (loss: 1.333206295967102, acc: 0.6296296119689941)
[2025-02-17 10:48:28,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:28,491][root][INFO] - Training Epoch: 1/2, step 1130/1149 completed (loss: 1.2593631744384766, acc: 0.6785714030265808)
[2025-02-17 10:48:28,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:28,870][root][INFO] - Training Epoch: 1/2, step 1131/1149 completed (loss: 0.8977038860321045, acc: 0.7666666507720947)
[2025-02-17 10:48:29,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:29,247][root][INFO] - Training Epoch: 1/2, step 1132/1149 completed (loss: 1.454977035522461, acc: 0.5820895433425903)
[2025-02-17 10:48:29,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:29,636][root][INFO] - Training Epoch: 1/2, step 1133/1149 completed (loss: 0.8119766712188721, acc: 0.8235294222831726)
[2025-02-17 10:48:29,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:29,993][root][INFO] - Training Epoch: 1/2, step 1134/1149 completed (loss: 0.46836888790130615, acc: 0.8947368264198303)
[2025-02-17 10:48:30,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:30,347][root][INFO] - Training Epoch: 1/2, step 1135/1149 completed (loss: 0.7085198760032654, acc: 0.8421052694320679)
[2025-02-17 10:48:30,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:30,767][root][INFO] - Training Epoch: 1/2, step 1136/1149 completed (loss: 0.4639328420162201, acc: 0.8666666746139526)
[2025-02-17 10:48:30,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:31,187][root][INFO] - Training Epoch: 1/2, step 1137/1149 completed (loss: 0.4982766807079315, acc: 0.8333333134651184)
[2025-02-17 10:48:31,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:31,568][root][INFO] - Training Epoch: 1/2, step 1138/1149 completed (loss: 0.9135662317276001, acc: 0.719298243522644)
[2025-02-17 10:48:31,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:32,022][root][INFO] - Training Epoch: 1/2, step 1139/1149 completed (loss: 1.433835506439209, acc: 0.5846154093742371)
[2025-02-17 10:48:32,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:32,365][root][INFO] - Training Epoch: 1/2, step 1140/1149 completed (loss: 0.366283655166626, acc: 0.8387096524238586)
[2025-02-17 10:48:32,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:32,707][root][INFO] - Training Epoch: 1/2, step 1141/1149 completed (loss: 0.9757223725318909, acc: 0.774193525314331)
[2025-02-17 10:48:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:33,062][root][INFO] - Training Epoch: 1/2, step 1142/1149 completed (loss: 1.7856967449188232, acc: 0.581818163394928)
[2025-02-17 10:48:33,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:33,441][root][INFO] - Training Epoch: 1/2, step 1143/1149 completed (loss: 1.171501636505127, acc: 0.7096773982048035)
[2025-02-17 10:48:33,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:33,832][root][INFO] - Training Epoch: 1/2, step 1144/1149 completed (loss: 1.314192771911621, acc: 0.6875)
[2025-02-17 10:48:34,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:34,229][root][INFO] - Training Epoch: 1/2, step 1145/1149 completed (loss: 1.804908275604248, acc: 0.49242424964904785)
[2025-02-17 10:48:34,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:34,599][root][INFO] - Training Epoch: 1/2, step 1146/1149 completed (loss: 1.2672861814498901, acc: 0.6600000262260437)
[2025-02-17 10:48:34,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:35,002][root][INFO] - Training Epoch: 1/2, step 1147/1149 completed (loss: 1.7526614665985107, acc: 0.5688073635101318)
[2025-02-17 10:48:35,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:36,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:36,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:37,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:37,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:38,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:38,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:38,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:39,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:39,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:40,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:41,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:41,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:41,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:42,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:43,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:43,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:44,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:44,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:45,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:45,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:46,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:46,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:47,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:47,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:48,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:48,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:49,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:49,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:50,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:50,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:51,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:51,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:52,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:52,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:53,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:53,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:54,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:54,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:55,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:55,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:56,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:56,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:57,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:57,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:57,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:58,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:58,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:59,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:48:59,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:00,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:00,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:01,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:01,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:02,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:02,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:03,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:03,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:04,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:04,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:05,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:05,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:06,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:06,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:07,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:07,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:07,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:08,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:09,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:09,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:09,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:10,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:10,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:11,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:11,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:12,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:12,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:13,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:13,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:14,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:14,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:15,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:15,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:16,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:16,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:17,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:17,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:17,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:18,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:18,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:19,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:19,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:20,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:20,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:21,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:21,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:22,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:23,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:23,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:24,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:24,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:25,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:25,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:26,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:26,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:26,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:27,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:27,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:28,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:28,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:28,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:29,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:30,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:30,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:31,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:32,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:32,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:33,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:33,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:33,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:34,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:34,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:35,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:35,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:36,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:37,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:37,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:38,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:38,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:38,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:39,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:40,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:40,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:41,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:41,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:41,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:42,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:42,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:43,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:43,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:44,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:44,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:45,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:45,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:46,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:46,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:46,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:47,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:47,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:48,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:48,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:49,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:49,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:50,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:50,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:50,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:51,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:51,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:52,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:52,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:52,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:53,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:53,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:54,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:54,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:55,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:55,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:56,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:56,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:49:57,381][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.0677, device='cuda:0') eval_epoch_loss=tensor(1.1209, device='cuda:0') eval_epoch_acc=tensor(0.6903, device='cuda:0')
[2025-02-17 10:49:57,382][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 10:49:57,383][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 10:50:00,999][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_1_step_1148_loss_1.1209418773651123/model.pt
[2025-02-17 10:50:01,010][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 10:50:01,011][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.1209418773651123
[2025-02-17 10:50:01,011][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.690331220626831
[2025-02-17 10:50:01,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:01,409][root][INFO] - Training Epoch: 1/2, step 1148/1149 completed (loss: 0.6153355240821838, acc: 0.7924528121948242)
[2025-02-17 10:50:01,882][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=10.8930, train_epoch_loss=2.3881, epoch time 835.6644297242165s
[2025-02-17 10:50:01,882][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2025-02-17 10:50:01,883][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2025-02-17 10:50:01,883][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2025-02-17 10:50:01,883][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-17 10:50:01,883][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-02-17 10:50:02,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:02,721][root][INFO] - Training Epoch: 2/2, step 0/1149 completed (loss: 0.7100950479507446, acc: 0.875)
[2025-02-17 10:50:02,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:03,140][root][INFO] - Training Epoch: 2/2, step 1/1149 completed (loss: 1.4271806478500366, acc: 0.6842105388641357)
[2025-02-17 10:50:03,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:03,529][root][INFO] - Training Epoch: 2/2, step 2/1149 completed (loss: 1.1917537450790405, acc: 0.7857142686843872)
[2025-02-17 10:50:03,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:03,952][root][INFO] - Training Epoch: 2/2, step 3/1149 completed (loss: 0.9297258853912354, acc: 0.5454545617103577)
[2025-02-17 10:50:04,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:04,317][root][INFO] - Training Epoch: 2/2, step 4/1149 completed (loss: 1.129290223121643, acc: 0.7142857313156128)
[2025-02-17 10:50:04,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:04,686][root][INFO] - Training Epoch: 2/2, step 5/1149 completed (loss: 2.0924460887908936, acc: 0.47826087474823)
[2025-02-17 10:50:04,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:05,032][root][INFO] - Training Epoch: 2/2, step 6/1149 completed (loss: 2.2573232650756836, acc: 0.4545454680919647)
[2025-02-17 10:50:05,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:05,475][root][INFO] - Training Epoch: 2/2, step 7/1149 completed (loss: 1.4599746465682983, acc: 0.5555555820465088)
[2025-02-17 10:50:05,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:05,818][root][INFO] - Training Epoch: 2/2, step 8/1149 completed (loss: 1.0965379476547241, acc: 0.692307710647583)
[2025-02-17 10:50:05,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:06,225][root][INFO] - Training Epoch: 2/2, step 9/1149 completed (loss: 1.1419352293014526, acc: 0.6666666865348816)
[2025-02-17 10:50:06,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:06,607][root][INFO] - Training Epoch: 2/2, step 10/1149 completed (loss: 0.6574242115020752, acc: 0.8461538553237915)
[2025-02-17 10:50:06,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:07,047][root][INFO] - Training Epoch: 2/2, step 11/1149 completed (loss: 1.6645472049713135, acc: 0.6666666865348816)
[2025-02-17 10:50:07,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:07,441][root][INFO] - Training Epoch: 2/2, step 12/1149 completed (loss: 1.215822696685791, acc: 0.692307710647583)
[2025-02-17 10:50:07,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:07,803][root][INFO] - Training Epoch: 2/2, step 13/1149 completed (loss: 1.6419395208358765, acc: 0.5652173757553101)
[2025-02-17 10:50:07,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:08,208][root][INFO] - Training Epoch: 2/2, step 14/1149 completed (loss: 1.5818839073181152, acc: 0.75)
[2025-02-17 10:50:08,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:08,611][root][INFO] - Training Epoch: 2/2, step 15/1149 completed (loss: 0.7895950078964233, acc: 0.7777777910232544)
[2025-02-17 10:50:08,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:08,992][root][INFO] - Training Epoch: 2/2, step 16/1149 completed (loss: 0.3772106170654297, acc: 0.8181818127632141)
[2025-02-17 10:50:09,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:09,350][root][INFO] - Training Epoch: 2/2, step 17/1149 completed (loss: 0.07207756489515305, acc: 1.0)
[2025-02-17 10:50:09,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:09,732][root][INFO] - Training Epoch: 2/2, step 18/1149 completed (loss: 0.3878098130226135, acc: 0.8461538553237915)
[2025-02-17 10:50:09,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:10,141][root][INFO] - Training Epoch: 2/2, step 19/1149 completed (loss: 0.46774202585220337, acc: 0.8461538553237915)
[2025-02-17 10:50:10,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:10,593][root][INFO] - Training Epoch: 2/2, step 20/1149 completed (loss: 0.9849972128868103, acc: 0.8181818127632141)
[2025-02-17 10:50:10,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:10,952][root][INFO] - Training Epoch: 2/2, step 21/1149 completed (loss: 0.2456415593624115, acc: 0.9375)
[2025-02-17 10:50:11,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:11,366][root][INFO] - Training Epoch: 2/2, step 22/1149 completed (loss: 0.7814302444458008, acc: 0.6666666865348816)
[2025-02-17 10:50:11,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:11,738][root][INFO] - Training Epoch: 2/2, step 23/1149 completed (loss: 1.4396522045135498, acc: 0.5925925970077515)
[2025-02-17 10:50:11,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:12,165][root][INFO] - Training Epoch: 2/2, step 24/1149 completed (loss: 0.8374430537223816, acc: 0.8095238208770752)
[2025-02-17 10:50:12,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:12,567][root][INFO] - Training Epoch: 2/2, step 25/1149 completed (loss: 0.7855024337768555, acc: 0.75)
[2025-02-17 10:50:12,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:12,954][root][INFO] - Training Epoch: 2/2, step 26/1149 completed (loss: 0.384951651096344, acc: 0.8999999761581421)
[2025-02-17 10:50:13,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:13,316][root][INFO] - Training Epoch: 2/2, step 27/1149 completed (loss: 0.7961181402206421, acc: 0.75)
[2025-02-17 10:50:13,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:13,664][root][INFO] - Training Epoch: 2/2, step 28/1149 completed (loss: 0.8839811086654663, acc: 0.774193525314331)
[2025-02-17 10:50:13,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:14,070][root][INFO] - Training Epoch: 2/2, step 29/1149 completed (loss: 0.846898078918457, acc: 0.699999988079071)
[2025-02-17 10:50:14,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:14,483][root][INFO] - Training Epoch: 2/2, step 30/1149 completed (loss: 1.263498067855835, acc: 0.7272727489471436)
[2025-02-17 10:50:14,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:14,856][root][INFO] - Training Epoch: 2/2, step 31/1149 completed (loss: 0.8898868560791016, acc: 0.8148148059844971)
[2025-02-17 10:50:14,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:15,269][root][INFO] - Training Epoch: 2/2, step 32/1149 completed (loss: 0.9172680377960205, acc: 0.8888888955116272)
[2025-02-17 10:50:15,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:15,663][root][INFO] - Training Epoch: 2/2, step 33/1149 completed (loss: 0.7723053693771362, acc: 0.800000011920929)
[2025-02-17 10:50:15,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:16,065][root][INFO] - Training Epoch: 2/2, step 34/1149 completed (loss: 1.163893461227417, acc: 0.692307710647583)
[2025-02-17 10:50:16,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:16,419][root][INFO] - Training Epoch: 2/2, step 35/1149 completed (loss: 0.9307997226715088, acc: 0.7272727489471436)
[2025-02-17 10:50:16,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:16,817][root][INFO] - Training Epoch: 2/2, step 36/1149 completed (loss: 2.1646170616149902, acc: 0.5555555820465088)
[2025-02-17 10:50:17,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:17,219][root][INFO] - Training Epoch: 2/2, step 37/1149 completed (loss: 1.5210527181625366, acc: 0.6666666865348816)
[2025-02-17 10:50:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:17,596][root][INFO] - Training Epoch: 2/2, step 38/1149 completed (loss: 1.8004883527755737, acc: 0.5)
[2025-02-17 10:50:17,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:17,999][root][INFO] - Training Epoch: 2/2, step 39/1149 completed (loss: 0.5297378897666931, acc: 0.7272727489471436)
[2025-02-17 10:50:18,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:18,365][root][INFO] - Training Epoch: 2/2, step 40/1149 completed (loss: 1.186381220817566, acc: 0.5833333134651184)
[2025-02-17 10:50:18,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:18,774][root][INFO] - Training Epoch: 2/2, step 41/1149 completed (loss: 1.4124314785003662, acc: 0.5714285969734192)
[2025-02-17 10:50:18,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:19,182][root][INFO] - Training Epoch: 2/2, step 42/1149 completed (loss: 0.44369345903396606, acc: 0.8461538553237915)
[2025-02-17 10:50:19,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:19,600][root][INFO] - Training Epoch: 2/2, step 43/1149 completed (loss: 1.597434639930725, acc: 0.5625)
[2025-02-17 10:50:19,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:19,940][root][INFO] - Training Epoch: 2/2, step 44/1149 completed (loss: 1.522741436958313, acc: 0.5)
[2025-02-17 10:50:20,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:20,341][root][INFO] - Training Epoch: 2/2, step 45/1149 completed (loss: 0.7172521948814392, acc: 0.7692307829856873)
[2025-02-17 10:50:20,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:20,707][root][INFO] - Training Epoch: 2/2, step 46/1149 completed (loss: 0.20145854353904724, acc: 0.9090909361839294)
[2025-02-17 10:50:20,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:21,066][root][INFO] - Training Epoch: 2/2, step 47/1149 completed (loss: 1.9177510738372803, acc: 0.699999988079071)
[2025-02-17 10:50:21,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:21,435][root][INFO] - Training Epoch: 2/2, step 48/1149 completed (loss: 1.4919512271881104, acc: 0.800000011920929)
[2025-02-17 10:50:21,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:21,784][root][INFO] - Training Epoch: 2/2, step 49/1149 completed (loss: 0.9337126016616821, acc: 0.7272727489471436)
[2025-02-17 10:50:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:22,159][root][INFO] - Training Epoch: 2/2, step 50/1149 completed (loss: 2.105876922607422, acc: 0.4545454680919647)
[2025-02-17 10:50:22,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:22,551][root][INFO] - Training Epoch: 2/2, step 51/1149 completed (loss: 1.3093669414520264, acc: 0.6000000238418579)
[2025-02-17 10:50:22,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:22,930][root][INFO] - Training Epoch: 2/2, step 52/1149 completed (loss: 1.402424931526184, acc: 0.6216216087341309)
[2025-02-17 10:50:23,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:23,290][root][INFO] - Training Epoch: 2/2, step 53/1149 completed (loss: 1.968749761581421, acc: 0.4722222089767456)
[2025-02-17 10:50:23,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:23,651][root][INFO] - Training Epoch: 2/2, step 54/1149 completed (loss: 1.20146906375885, acc: 0.6428571343421936)
[2025-02-17 10:50:24,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:24,566][root][INFO] - Training Epoch: 2/2, step 55/1149 completed (loss: 2.4131276607513428, acc: 0.3838862478733063)
[2025-02-17 10:50:24,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:24,981][root][INFO] - Training Epoch: 2/2, step 56/1149 completed (loss: 1.4662139415740967, acc: 0.6499999761581421)
[2025-02-17 10:50:25,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:25,410][root][INFO] - Training Epoch: 2/2, step 57/1149 completed (loss: 1.4129897356033325, acc: 0.5652173757553101)
[2025-02-17 10:50:25,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:25,852][root][INFO] - Training Epoch: 2/2, step 58/1149 completed (loss: 1.4383808374404907, acc: 0.5957446694374084)
[2025-02-17 10:50:26,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:26,298][root][INFO] - Training Epoch: 2/2, step 59/1149 completed (loss: 1.4818836450576782, acc: 0.6111111044883728)
[2025-02-17 10:50:26,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:26,684][root][INFO] - Training Epoch: 2/2, step 60/1149 completed (loss: 1.881855845451355, acc: 0.4637681245803833)
[2025-02-17 10:50:26,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:27,024][root][INFO] - Training Epoch: 2/2, step 61/1149 completed (loss: 0.6392793655395508, acc: 0.75)
[2025-02-17 10:50:27,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:27,376][root][INFO] - Training Epoch: 2/2, step 62/1149 completed (loss: 1.8206236362457275, acc: 0.4444444477558136)
[2025-02-17 10:50:27,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:27,728][root][INFO] - Training Epoch: 2/2, step 63/1149 completed (loss: 0.7052515149116516, acc: 0.699999988079071)
[2025-02-17 10:50:27,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:28,075][root][INFO] - Training Epoch: 2/2, step 64/1149 completed (loss: 0.5397680997848511, acc: 0.8333333134651184)
[2025-02-17 10:50:28,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:28,427][root][INFO] - Training Epoch: 2/2, step 65/1149 completed (loss: 1.3447842597961426, acc: 0.6666666865348816)
[2025-02-17 10:50:28,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:28,773][root][INFO] - Training Epoch: 2/2, step 66/1149 completed (loss: 0.9062812328338623, acc: 0.8181818127632141)
[2025-02-17 10:50:28,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:29,118][root][INFO] - Training Epoch: 2/2, step 67/1149 completed (loss: 0.12231210619211197, acc: 1.0)
[2025-02-17 10:50:29,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:29,474][root][INFO] - Training Epoch: 2/2, step 68/1149 completed (loss: 1.4182735681533813, acc: 0.6206896305084229)
[2025-02-17 10:50:29,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:29,826][root][INFO] - Training Epoch: 2/2, step 69/1149 completed (loss: 1.1573948860168457, acc: 0.53125)
[2025-02-17 10:50:29,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:30,172][root][INFO] - Training Epoch: 2/2, step 70/1149 completed (loss: 0.9192956686019897, acc: 0.7692307829856873)
[2025-02-17 10:50:30,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:30,513][root][INFO] - Training Epoch: 2/2, step 71/1149 completed (loss: 1.2429367303848267, acc: 0.5428571701049805)
[2025-02-17 10:50:30,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:30,873][root][INFO] - Training Epoch: 2/2, step 72/1149 completed (loss: 1.1998014450073242, acc: 0.7234042286872864)
[2025-02-17 10:50:31,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:31,225][root][INFO] - Training Epoch: 2/2, step 73/1149 completed (loss: 0.5771646499633789, acc: 0.9375)
[2025-02-17 10:50:31,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:31,571][root][INFO] - Training Epoch: 2/2, step 74/1149 completed (loss: 1.0945770740509033, acc: 0.7714285850524902)
[2025-02-17 10:50:31,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:31,914][root][INFO] - Training Epoch: 2/2, step 75/1149 completed (loss: 1.6936402320861816, acc: 0.625)
[2025-02-17 10:50:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:32,283][root][INFO] - Training Epoch: 2/2, step 76/1149 completed (loss: 0.9949839115142822, acc: 0.6666666865348816)
[2025-02-17 10:50:32,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:32,640][root][INFO] - Training Epoch: 2/2, step 77/1149 completed (loss: 0.7229077219963074, acc: 0.7948718070983887)
[2025-02-17 10:50:32,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:32,980][root][INFO] - Training Epoch: 2/2, step 78/1149 completed (loss: 0.16521625220775604, acc: 1.0)
[2025-02-17 10:50:33,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:33,328][root][INFO] - Training Epoch: 2/2, step 79/1149 completed (loss: 1.0585840940475464, acc: 0.692307710647583)
[2025-02-17 10:50:33,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:33,677][root][INFO] - Training Epoch: 2/2, step 80/1149 completed (loss: 1.1754390001296997, acc: 0.6666666865348816)
[2025-02-17 10:50:33,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:34,017][root][INFO] - Training Epoch: 2/2, step 81/1149 completed (loss: 1.3534939289093018, acc: 0.5)
[2025-02-17 10:50:34,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:34,379][root][INFO] - Training Epoch: 2/2, step 82/1149 completed (loss: 1.2553926706314087, acc: 0.6470588445663452)
[2025-02-17 10:50:34,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:34,757][root][INFO] - Training Epoch: 2/2, step 83/1149 completed (loss: 2.002598762512207, acc: 0.4736842215061188)
[2025-02-17 10:50:34,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:35,122][root][INFO] - Training Epoch: 2/2, step 84/1149 completed (loss: 1.6518752574920654, acc: 0.5641025900840759)
[2025-02-17 10:50:35,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:35,480][root][INFO] - Training Epoch: 2/2, step 85/1149 completed (loss: 2.208045482635498, acc: 0.4615384638309479)
[2025-02-17 10:50:35,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:35,835][root][INFO] - Training Epoch: 2/2, step 86/1149 completed (loss: 1.5310639142990112, acc: 0.5531914830207825)
[2025-02-17 10:50:36,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:36,246][root][INFO] - Training Epoch: 2/2, step 87/1149 completed (loss: 1.8419127464294434, acc: 0.5384615659713745)
[2025-02-17 10:50:36,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:36,611][root][INFO] - Training Epoch: 2/2, step 88/1149 completed (loss: 1.1867374181747437, acc: 0.5833333134651184)
[2025-02-17 10:50:36,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:37,022][root][INFO] - Training Epoch: 2/2, step 89/1149 completed (loss: 1.271790623664856, acc: 0.6938775777816772)
[2025-02-17 10:50:37,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:37,407][root][INFO] - Training Epoch: 2/2, step 90/1149 completed (loss: 1.4540058374404907, acc: 0.5874999761581421)
[2025-02-17 10:50:37,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:37,807][root][INFO] - Training Epoch: 2/2, step 91/1149 completed (loss: 1.4718263149261475, acc: 0.6428571343421936)
[2025-02-17 10:50:37,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:38,148][root][INFO] - Training Epoch: 2/2, step 92/1149 completed (loss: 0.84504234790802, acc: 0.7333333492279053)
[2025-02-17 10:50:38,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:38,528][root][INFO] - Training Epoch: 2/2, step 93/1149 completed (loss: 0.1810649037361145, acc: 1.0)
[2025-02-17 10:50:38,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:38,876][root][INFO] - Training Epoch: 2/2, step 94/1149 completed (loss: 0.8632538914680481, acc: 0.7692307829856873)
[2025-02-17 10:50:39,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:39,230][root][INFO] - Training Epoch: 2/2, step 95/1149 completed (loss: 0.5187464952468872, acc: 0.7857142686843872)
[2025-02-17 10:50:39,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:39,619][root][INFO] - Training Epoch: 2/2, step 96/1149 completed (loss: 0.14390572905540466, acc: 1.0)
[2025-02-17 10:50:39,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:39,962][root][INFO] - Training Epoch: 2/2, step 97/1149 completed (loss: 0.9705564975738525, acc: 0.7647058963775635)
[2025-02-17 10:50:40,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:40,312][root][INFO] - Training Epoch: 2/2, step 98/1149 completed (loss: 0.2328074723482132, acc: 0.9230769276618958)
[2025-02-17 10:50:40,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:40,663][root][INFO] - Training Epoch: 2/2, step 99/1149 completed (loss: 0.7261138558387756, acc: 0.739130437374115)
[2025-02-17 10:50:40,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:41,011][root][INFO] - Training Epoch: 2/2, step 100/1149 completed (loss: 1.1511045694351196, acc: 0.6363636255264282)
[2025-02-17 10:50:41,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:41,371][root][INFO] - Training Epoch: 2/2, step 101/1149 completed (loss: 0.9890334010124207, acc: 0.6857143044471741)
[2025-02-17 10:50:41,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:41,715][root][INFO] - Training Epoch: 2/2, step 102/1149 completed (loss: 1.1063735485076904, acc: 0.6666666865348816)
[2025-02-17 10:50:41,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:42,067][root][INFO] - Training Epoch: 2/2, step 103/1149 completed (loss: 1.2236685752868652, acc: 0.7111111283302307)
[2025-02-17 10:50:42,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:42,410][root][INFO] - Training Epoch: 2/2, step 104/1149 completed (loss: 1.472320556640625, acc: 0.550000011920929)
[2025-02-17 10:50:42,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:42,761][root][INFO] - Training Epoch: 2/2, step 105/1149 completed (loss: 1.835801124572754, acc: 0.47058823704719543)
[2025-02-17 10:50:42,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:43,132][root][INFO] - Training Epoch: 2/2, step 106/1149 completed (loss: 1.9247456789016724, acc: 0.4406779706478119)
[2025-02-17 10:50:43,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:43,572][root][INFO] - Training Epoch: 2/2, step 107/1149 completed (loss: 2.242706775665283, acc: 0.4615384638309479)
[2025-02-17 10:50:43,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:43,919][root][INFO] - Training Epoch: 2/2, step 108/1149 completed (loss: 1.3823314905166626, acc: 0.6000000238418579)
[2025-02-17 10:50:44,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:44,263][root][INFO] - Training Epoch: 2/2, step 109/1149 completed (loss: 0.7808317542076111, acc: 0.8333333134651184)
[2025-02-17 10:50:44,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:44,610][root][INFO] - Training Epoch: 2/2, step 110/1149 completed (loss: 0.22633062303066254, acc: 0.8888888955116272)
[2025-02-17 10:50:44,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:44,957][root][INFO] - Training Epoch: 2/2, step 111/1149 completed (loss: 0.2677774131298065, acc: 0.8823529481887817)
[2025-02-17 10:50:45,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:45,298][root][INFO] - Training Epoch: 2/2, step 112/1149 completed (loss: 0.11274343729019165, acc: 1.0)
[2025-02-17 10:50:46,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:47,397][root][INFO] - Training Epoch: 2/2, step 113/1149 completed (loss: 1.9403375387191772, acc: 0.47349822521209717)
[2025-02-17 10:50:47,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:48,232][root][INFO] - Training Epoch: 2/2, step 114/1149 completed (loss: 2.2012953758239746, acc: 0.41064637899398804)
[2025-02-17 10:50:48,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:49,309][root][INFO] - Training Epoch: 2/2, step 115/1149 completed (loss: 2.224132776260376, acc: 0.3877550959587097)
[2025-02-17 10:50:49,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:49,897][root][INFO] - Training Epoch: 2/2, step 116/1149 completed (loss: 1.5466309785842896, acc: 0.6262626051902771)
[2025-02-17 10:50:50,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:50,357][root][INFO] - Training Epoch: 2/2, step 117/1149 completed (loss: 1.6333339214324951, acc: 0.5324675440788269)
[2025-02-17 10:50:50,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:50,924][root][INFO] - Training Epoch: 2/2, step 118/1149 completed (loss: 1.4586546421051025, acc: 0.6000000238418579)
[2025-02-17 10:50:51,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:51,311][root][INFO] - Training Epoch: 2/2, step 119/1149 completed (loss: 1.6808032989501953, acc: 0.5573770403862)
[2025-02-17 10:50:51,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:51,659][root][INFO] - Training Epoch: 2/2, step 120/1149 completed (loss: 1.295628309249878, acc: 0.6666666865348816)
[2025-02-17 10:50:51,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:52,204][root][INFO] - Training Epoch: 2/2, step 121/1149 completed (loss: 1.5112218856811523, acc: 0.5520833134651184)
[2025-02-17 10:50:52,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:52,627][root][INFO] - Training Epoch: 2/2, step 122/1149 completed (loss: 1.6854486465454102, acc: 0.5694444179534912)
[2025-02-17 10:50:52,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:52,975][root][INFO] - Training Epoch: 2/2, step 123/1149 completed (loss: 0.18537960946559906, acc: 0.875)
[2025-02-17 10:50:53,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:53,327][root][INFO] - Training Epoch: 2/2, step 124/1149 completed (loss: 1.6344915628433228, acc: 0.5263158082962036)
[2025-02-17 10:50:53,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:53,674][root][INFO] - Training Epoch: 2/2, step 125/1149 completed (loss: 0.5204761624336243, acc: 0.8666666746139526)
[2025-02-17 10:50:53,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:54,036][root][INFO] - Training Epoch: 2/2, step 126/1149 completed (loss: 0.4387945532798767, acc: 0.8571428656578064)
[2025-02-17 10:50:54,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:54,400][root][INFO] - Training Epoch: 2/2, step 127/1149 completed (loss: 0.7730280160903931, acc: 0.8181818127632141)
[2025-02-17 10:50:54,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:54,760][root][INFO] - Training Epoch: 2/2, step 128/1149 completed (loss: 0.634557843208313, acc: 0.8235294222831726)
[2025-02-17 10:50:54,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:55,129][root][INFO] - Training Epoch: 2/2, step 129/1149 completed (loss: 0.7708185315132141, acc: 0.7659574747085571)
[2025-02-17 10:50:55,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:55,450][root][INFO] - Training Epoch: 2/2, step 130/1149 completed (loss: 0.46114659309387207, acc: 0.8421052694320679)
[2025-02-17 10:50:55,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:55,806][root][INFO] - Training Epoch: 2/2, step 131/1149 completed (loss: 0.21723699569702148, acc: 1.0)
[2025-02-17 10:50:55,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:56,169][root][INFO] - Training Epoch: 2/2, step 132/1149 completed (loss: 1.594286561012268, acc: 0.5862069129943848)
[2025-02-17 10:50:56,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:56,521][root][INFO] - Training Epoch: 2/2, step 133/1149 completed (loss: 1.4569343328475952, acc: 0.6296296119689941)
[2025-02-17 10:50:56,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:56,878][root][INFO] - Training Epoch: 2/2, step 134/1149 completed (loss: 1.3544045686721802, acc: 0.6206896305084229)
[2025-02-17 10:50:57,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:57,246][root][INFO] - Training Epoch: 2/2, step 135/1149 completed (loss: 0.687880277633667, acc: 0.8709677457809448)
[2025-02-17 10:50:57,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:57,603][root][INFO] - Training Epoch: 2/2, step 136/1149 completed (loss: 0.051868319511413574, acc: 1.0)
[2025-02-17 10:50:57,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:57,955][root][INFO] - Training Epoch: 2/2, step 137/1149 completed (loss: 0.20534023642539978, acc: 0.9230769276618958)
[2025-02-17 10:50:58,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:58,303][root][INFO] - Training Epoch: 2/2, step 138/1149 completed (loss: 1.3565266132354736, acc: 0.5652173757553101)
[2025-02-17 10:50:58,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:58,656][root][INFO] - Training Epoch: 2/2, step 139/1149 completed (loss: 0.7316361665725708, acc: 0.7692307829856873)
[2025-02-17 10:50:58,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:59,005][root][INFO] - Training Epoch: 2/2, step 140/1149 completed (loss: 1.2412207126617432, acc: 0.6000000238418579)
[2025-02-17 10:50:59,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:59,351][root][INFO] - Training Epoch: 2/2, step 141/1149 completed (loss: 1.6147043704986572, acc: 0.6666666865348816)
[2025-02-17 10:50:59,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:50:59,704][root][INFO] - Training Epoch: 2/2, step 142/1149 completed (loss: 1.6700003147125244, acc: 0.529411792755127)
[2025-02-17 10:50:59,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:00,064][root][INFO] - Training Epoch: 2/2, step 143/1149 completed (loss: 1.78128182888031, acc: 0.529411792755127)
[2025-02-17 10:51:00,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:00,406][root][INFO] - Training Epoch: 2/2, step 144/1149 completed (loss: 1.7015622854232788, acc: 0.5243902206420898)
[2025-02-17 10:51:00,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:00,759][root][INFO] - Training Epoch: 2/2, step 145/1149 completed (loss: 1.577492117881775, acc: 0.5681818127632141)
[2025-02-17 10:51:00,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:01,119][root][INFO] - Training Epoch: 2/2, step 146/1149 completed (loss: 1.7954649925231934, acc: 0.5274725556373596)
[2025-02-17 10:51:01,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:01,475][root][INFO] - Training Epoch: 2/2, step 147/1149 completed (loss: 1.9281400442123413, acc: 0.4711538553237915)
[2025-02-17 10:51:01,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:01,845][root][INFO] - Training Epoch: 2/2, step 148/1149 completed (loss: 1.7377476692199707, acc: 0.5)
[2025-02-17 10:51:01,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:02,184][root][INFO] - Training Epoch: 2/2, step 149/1149 completed (loss: 1.5248054265975952, acc: 0.5263158082962036)
[2025-02-17 10:51:02,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:02,535][root][INFO] - Training Epoch: 2/2, step 150/1149 completed (loss: 1.6182589530944824, acc: 0.5974025726318359)
[2025-02-17 10:51:02,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:02,893][root][INFO] - Training Epoch: 2/2, step 151/1149 completed (loss: 1.8965630531311035, acc: 0.38596490025520325)
[2025-02-17 10:51:03,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:03,265][root][INFO] - Training Epoch: 2/2, step 152/1149 completed (loss: 1.800877332687378, acc: 0.515625)
[2025-02-17 10:51:03,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:03,652][root][INFO] - Training Epoch: 2/2, step 153/1149 completed (loss: 1.9267216920852661, acc: 0.48630136251449585)
[2025-02-17 10:51:03,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:04,002][root][INFO] - Training Epoch: 2/2, step 154/1149 completed (loss: 0.45116138458251953, acc: 0.8888888955116272)
[2025-02-17 10:51:04,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:04,350][root][INFO] - Training Epoch: 2/2, step 155/1149 completed (loss: 0.13925008475780487, acc: 1.0)
[2025-02-17 10:51:04,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:04,695][root][INFO] - Training Epoch: 2/2, step 156/1149 completed (loss: 0.6022146344184875, acc: 0.7857142686843872)
[2025-02-17 10:51:04,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:05,049][root][INFO] - Training Epoch: 2/2, step 157/1149 completed (loss: 0.11726902425289154, acc: 1.0)
[2025-02-17 10:51:05,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:05,390][root][INFO] - Training Epoch: 2/2, step 158/1149 completed (loss: 0.5497424006462097, acc: 0.7692307829856873)
[2025-02-17 10:51:05,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:05,739][root][INFO] - Training Epoch: 2/2, step 159/1149 completed (loss: 0.7533135414123535, acc: 0.800000011920929)
[2025-02-17 10:51:05,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:06,100][root][INFO] - Training Epoch: 2/2, step 160/1149 completed (loss: 0.6698701977729797, acc: 0.8666666746139526)
[2025-02-17 10:51:06,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:06,485][root][INFO] - Training Epoch: 2/2, step 161/1149 completed (loss: 0.16193093359470367, acc: 1.0)
[2025-02-17 10:51:06,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:06,834][root][INFO] - Training Epoch: 2/2, step 162/1149 completed (loss: 1.713301420211792, acc: 0.6666666865348816)
[2025-02-17 10:51:06,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:07,184][root][INFO] - Training Epoch: 2/2, step 163/1149 completed (loss: 1.0372920036315918, acc: 0.7027027010917664)
[2025-02-17 10:51:07,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:07,526][root][INFO] - Training Epoch: 2/2, step 164/1149 completed (loss: 1.6911888122558594, acc: 0.6666666865348816)
[2025-02-17 10:51:07,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:07,873][root][INFO] - Training Epoch: 2/2, step 165/1149 completed (loss: 1.1302354335784912, acc: 0.6071428656578064)
[2025-02-17 10:51:08,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:08,259][root][INFO] - Training Epoch: 2/2, step 166/1149 completed (loss: 0.9822953939437866, acc: 0.75)
[2025-02-17 10:51:08,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:08,598][root][INFO] - Training Epoch: 2/2, step 167/1149 completed (loss: 0.5639154314994812, acc: 0.8333333134651184)
[2025-02-17 10:51:08,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:08,942][root][INFO] - Training Epoch: 2/2, step 168/1149 completed (loss: 0.6225124597549438, acc: 0.8666666746139526)
[2025-02-17 10:51:09,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:09,286][root][INFO] - Training Epoch: 2/2, step 169/1149 completed (loss: 1.2179323434829712, acc: 0.6410256624221802)
[2025-02-17 10:51:09,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:09,645][root][INFO] - Training Epoch: 2/2, step 170/1149 completed (loss: 1.220727801322937, acc: 0.6470588445663452)
[2025-02-17 10:51:09,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:10,008][root][INFO] - Training Epoch: 2/2, step 171/1149 completed (loss: 0.38519808650016785, acc: 0.9375)
[2025-02-17 10:51:10,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:10,408][root][INFO] - Training Epoch: 2/2, step 172/1149 completed (loss: 0.5722150206565857, acc: 0.8461538553237915)
[2025-02-17 10:51:10,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:10,765][root][INFO] - Training Epoch: 2/2, step 173/1149 completed (loss: 0.858864426612854, acc: 0.800000011920929)
[2025-02-17 10:51:10,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:11,177][root][INFO] - Training Epoch: 2/2, step 174/1149 completed (loss: 1.988079309463501, acc: 0.5)
[2025-02-17 10:51:11,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:11,641][root][INFO] - Training Epoch: 2/2, step 175/1149 completed (loss: 1.537253975868225, acc: 0.550000011920929)
[2025-02-17 10:51:11,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:12,095][root][INFO] - Training Epoch: 2/2, step 176/1149 completed (loss: 1.819191336631775, acc: 0.523809552192688)
[2025-02-17 10:51:12,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:12,465][root][INFO] - Training Epoch: 2/2, step 177/1149 completed (loss: 1.3026436567306519, acc: 0.699999988079071)
[2025-02-17 10:51:13,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:13,472][root][INFO] - Training Epoch: 2/2, step 178/1149 completed (loss: 1.4199784994125366, acc: 0.5733333230018616)
[2025-02-17 10:51:13,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:13,912][root][INFO] - Training Epoch: 2/2, step 179/1149 completed (loss: 1.5954298973083496, acc: 0.6259542107582092)
[2025-02-17 10:51:14,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:14,684][root][INFO] - Training Epoch: 2/2, step 180/1149 completed (loss: 1.6704565286636353, acc: 0.5660377144813538)
[2025-02-17 10:51:14,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:15,200][root][INFO] - Training Epoch: 2/2, step 181/1149 completed (loss: 1.7079042196273804, acc: 0.5)
[2025-02-17 10:51:15,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:15,955][root][INFO] - Training Epoch: 2/2, step 182/1149 completed (loss: 1.44380784034729, acc: 0.6734693646430969)
[2025-02-17 10:51:16,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:16,379][root][INFO] - Training Epoch: 2/2, step 183/1149 completed (loss: 1.3076454401016235, acc: 0.6352941393852234)
[2025-02-17 10:51:16,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:17,082][root][INFO] - Training Epoch: 2/2, step 184/1149 completed (loss: 1.4312636852264404, acc: 0.5844155550003052)
[2025-02-17 10:51:17,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:17,437][root][INFO] - Training Epoch: 2/2, step 185/1149 completed (loss: 0.49230730533599854, acc: 0.9444444179534912)
[2025-02-17 10:51:17,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:18,381][root][INFO] - Training Epoch: 2/2, step 186/1149 completed (loss: 2.1679954528808594, acc: 0.40229883790016174)
[2025-02-17 10:51:18,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:18,807][root][INFO] - Training Epoch: 2/2, step 187/1149 completed (loss: 1.5795371532440186, acc: 0.4285714328289032)
[2025-02-17 10:51:18,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:19,186][root][INFO] - Training Epoch: 2/2, step 188/1149 completed (loss: 2.6665287017822266, acc: 0.20000000298023224)
[2025-02-17 10:51:19,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:19,611][root][INFO] - Training Epoch: 2/2, step 189/1149 completed (loss: 1.5345498323440552, acc: 0.5192307829856873)
[2025-02-17 10:51:19,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:20,017][root][INFO] - Training Epoch: 2/2, step 190/1149 completed (loss: 1.367780089378357, acc: 0.6153846383094788)
[2025-02-17 10:51:20,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:20,441][root][INFO] - Training Epoch: 2/2, step 191/1149 completed (loss: 1.7140387296676636, acc: 0.5333333611488342)
[2025-02-17 10:51:20,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:20,830][root][INFO] - Training Epoch: 2/2, step 192/1149 completed (loss: 1.9635179042816162, acc: 0.3888888955116272)
[2025-02-17 10:51:20,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:21,191][root][INFO] - Training Epoch: 2/2, step 193/1149 completed (loss: 2.007662296295166, acc: 0.45783132314682007)
[2025-02-17 10:51:21,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:21,574][root][INFO] - Training Epoch: 2/2, step 194/1149 completed (loss: 2.1261227130889893, acc: 0.42592594027519226)
[2025-02-17 10:51:21,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:22,039][root][INFO] - Training Epoch: 2/2, step 195/1149 completed (loss: 1.5094304084777832, acc: 0.6000000238418579)
[2025-02-17 10:51:22,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:22,440][root][INFO] - Training Epoch: 2/2, step 196/1149 completed (loss: 1.8415440320968628, acc: 0.4576271176338196)
[2025-02-17 10:51:22,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:22,801][root][INFO] - Training Epoch: 2/2, step 197/1149 completed (loss: 1.8061392307281494, acc: 0.43589743971824646)
[2025-02-17 10:51:23,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:23,235][root][INFO] - Training Epoch: 2/2, step 198/1149 completed (loss: 1.9786978960037231, acc: 0.5)
[2025-02-17 10:51:23,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:23,652][root][INFO] - Training Epoch: 2/2, step 199/1149 completed (loss: 2.2984511852264404, acc: 0.29729729890823364)
[2025-02-17 10:51:23,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:24,069][root][INFO] - Training Epoch: 2/2, step 200/1149 completed (loss: 0.8476648926734924, acc: 0.6666666865348816)
[2025-02-17 10:51:24,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:24,412][root][INFO] - Training Epoch: 2/2, step 201/1149 completed (loss: 1.5559512376785278, acc: 0.5)
[2025-02-17 10:51:24,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:24,773][root][INFO] - Training Epoch: 2/2, step 202/1149 completed (loss: 0.31310492753982544, acc: 0.8999999761581421)
[2025-02-17 10:51:24,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:25,137][root][INFO] - Training Epoch: 2/2, step 203/1149 completed (loss: 0.42622697353363037, acc: 0.9166666865348816)
[2025-02-17 10:51:25,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:25,539][root][INFO] - Training Epoch: 2/2, step 204/1149 completed (loss: 0.3344550132751465, acc: 0.9166666865348816)
[2025-02-17 10:51:25,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:25,917][root][INFO] - Training Epoch: 2/2, step 205/1149 completed (loss: 0.43561914563179016, acc: 0.9090909361839294)
[2025-02-17 10:51:26,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:26,336][root][INFO] - Training Epoch: 2/2, step 206/1149 completed (loss: 0.37958824634552, acc: 0.8823529481887817)
[2025-02-17 10:51:26,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:26,742][root][INFO] - Training Epoch: 2/2, step 207/1149 completed (loss: 0.6250496506690979, acc: 0.8888888955116272)
[2025-02-17 10:51:26,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:27,139][root][INFO] - Training Epoch: 2/2, step 208/1149 completed (loss: 1.2262893915176392, acc: 0.692307710647583)
[2025-02-17 10:51:27,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:27,531][root][INFO] - Training Epoch: 2/2, step 209/1149 completed (loss: 0.8079020380973816, acc: 0.7894737124443054)
[2025-02-17 10:51:27,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:27,921][root][INFO] - Training Epoch: 2/2, step 210/1149 completed (loss: 0.22215160727500916, acc: 0.9333333373069763)
[2025-02-17 10:51:28,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:28,333][root][INFO] - Training Epoch: 2/2, step 211/1149 completed (loss: 1.1102169752120972, acc: 0.6428571343421936)
[2025-02-17 10:51:28,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:28,679][root][INFO] - Training Epoch: 2/2, step 212/1149 completed (loss: 1.7309740781784058, acc: 0.6666666865348816)
[2025-02-17 10:51:28,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:29,038][root][INFO] - Training Epoch: 2/2, step 213/1149 completed (loss: 0.7280176877975464, acc: 0.8461538553237915)
[2025-02-17 10:51:29,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:29,447][root][INFO] - Training Epoch: 2/2, step 214/1149 completed (loss: 0.06158383563160896, acc: 1.0)
[2025-02-17 10:51:29,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:29,795][root][INFO] - Training Epoch: 2/2, step 215/1149 completed (loss: 0.22928929328918457, acc: 0.8888888955116272)
[2025-02-17 10:51:29,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:30,147][root][INFO] - Training Epoch: 2/2, step 216/1149 completed (loss: 0.04624026268720627, acc: 1.0)
[2025-02-17 10:51:30,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:30,547][root][INFO] - Training Epoch: 2/2, step 217/1149 completed (loss: 0.1970697045326233, acc: 0.9166666865348816)
[2025-02-17 10:51:30,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:30,973][root][INFO] - Training Epoch: 2/2, step 218/1149 completed (loss: 0.6589167714118958, acc: 0.8235294222831726)
[2025-02-17 10:51:31,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:31,426][root][INFO] - Training Epoch: 2/2, step 219/1149 completed (loss: 1.1237484216690063, acc: 0.7200000286102295)
[2025-02-17 10:51:31,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:31,809][root][INFO] - Training Epoch: 2/2, step 220/1149 completed (loss: 0.8127618432044983, acc: 0.7058823704719543)
[2025-02-17 10:51:32,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:32,257][root][INFO] - Training Epoch: 2/2, step 221/1149 completed (loss: 0.7660112380981445, acc: 0.75)
[2025-02-17 10:51:32,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:32,687][root][INFO] - Training Epoch: 2/2, step 222/1149 completed (loss: 1.3146313428878784, acc: 0.6388888955116272)
[2025-02-17 10:51:32,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:33,115][root][INFO] - Training Epoch: 2/2, step 223/1149 completed (loss: 0.8317021131515503, acc: 0.8095238208770752)
[2025-02-17 10:51:33,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:33,574][root][INFO] - Training Epoch: 2/2, step 224/1149 completed (loss: 1.2806156873703003, acc: 0.6896551847457886)
[2025-02-17 10:51:33,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:33,999][root][INFO] - Training Epoch: 2/2, step 225/1149 completed (loss: 1.4366979598999023, acc: 0.6071428656578064)
[2025-02-17 10:51:34,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:34,425][root][INFO] - Training Epoch: 2/2, step 226/1149 completed (loss: 1.1001121997833252, acc: 0.7142857313156128)
[2025-02-17 10:51:34,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:34,782][root][INFO] - Training Epoch: 2/2, step 227/1149 completed (loss: 0.753311276435852, acc: 0.7777777910232544)
[2025-02-17 10:51:34,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:35,196][root][INFO] - Training Epoch: 2/2, step 228/1149 completed (loss: 0.8500925302505493, acc: 0.6875)
[2025-02-17 10:51:35,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:35,595][root][INFO] - Training Epoch: 2/2, step 229/1149 completed (loss: 0.27683377265930176, acc: 0.9411764740943909)
[2025-02-17 10:51:35,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:35,966][root][INFO] - Training Epoch: 2/2, step 230/1149 completed (loss: 0.7300102710723877, acc: 0.7692307829856873)
[2025-02-17 10:51:36,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:36,382][root][INFO] - Training Epoch: 2/2, step 231/1149 completed (loss: 0.04056158661842346, acc: 1.0)
[2025-02-17 10:51:36,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:36,795][root][INFO] - Training Epoch: 2/2, step 232/1149 completed (loss: 0.8313834071159363, acc: 0.6666666865348816)
[2025-02-17 10:51:36,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:37,172][root][INFO] - Training Epoch: 2/2, step 233/1149 completed (loss: 1.3773409128189087, acc: 0.6470588445663452)
[2025-02-17 10:51:37,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:37,547][root][INFO] - Training Epoch: 2/2, step 234/1149 completed (loss: 1.9253621101379395, acc: 0.5423728823661804)
[2025-02-17 10:51:37,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:37,936][root][INFO] - Training Epoch: 2/2, step 235/1149 completed (loss: 0.9012095928192139, acc: 0.78125)
[2025-02-17 10:51:38,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:38,374][root][INFO] - Training Epoch: 2/2, step 236/1149 completed (loss: 0.24411384761333466, acc: 0.9166666865348816)
[2025-02-17 10:51:38,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:38,812][root][INFO] - Training Epoch: 2/2, step 237/1149 completed (loss: 1.2234810590744019, acc: 0.6399999856948853)
[2025-02-17 10:51:39,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:39,566][root][INFO] - Training Epoch: 2/2, step 238/1149 completed (loss: 1.6407307386398315, acc: 0.5538461804389954)
[2025-02-17 10:51:39,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:39,938][root][INFO] - Training Epoch: 2/2, step 239/1149 completed (loss: 0.8352547287940979, acc: 0.7352941036224365)
[2025-02-17 10:51:40,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:40,355][root][INFO] - Training Epoch: 2/2, step 240/1149 completed (loss: 0.790314257144928, acc: 0.8125)
[2025-02-17 10:51:40,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:40,766][root][INFO] - Training Epoch: 2/2, step 241/1149 completed (loss: 0.8824872970581055, acc: 0.7441860437393188)
[2025-02-17 10:51:40,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:41,208][root][INFO] - Training Epoch: 2/2, step 242/1149 completed (loss: 1.2099740505218506, acc: 0.7441860437393188)
[2025-02-17 10:51:41,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:41,553][root][INFO] - Training Epoch: 2/2, step 243/1149 completed (loss: 0.5585334300994873, acc: 0.8888888955116272)
[2025-02-17 10:51:41,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:41,922][root][INFO] - Training Epoch: 2/2, step 244/1149 completed (loss: 0.27821746468544006, acc: 0.9090909361839294)
[2025-02-17 10:51:42,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:42,330][root][INFO] - Training Epoch: 2/2, step 245/1149 completed (loss: 0.8636273741722107, acc: 0.7692307829856873)
[2025-02-17 10:51:42,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:42,681][root][INFO] - Training Epoch: 2/2, step 246/1149 completed (loss: 0.4811089038848877, acc: 0.75)
[2025-02-17 10:51:42,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:43,025][root][INFO] - Training Epoch: 2/2, step 247/1149 completed (loss: 1.4687656164169312, acc: 0.5454545617103577)
[2025-02-17 10:51:43,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:43,382][root][INFO] - Training Epoch: 2/2, step 248/1149 completed (loss: 1.8477632999420166, acc: 0.5277777910232544)
[2025-02-17 10:51:43,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:43,833][root][INFO] - Training Epoch: 2/2, step 249/1149 completed (loss: 1.9899556636810303, acc: 0.450549453496933)
[2025-02-17 10:51:43,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:44,206][root][INFO] - Training Epoch: 2/2, step 250/1149 completed (loss: 2.321798801422119, acc: 0.4375)
[2025-02-17 10:51:44,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:44,637][root][INFO] - Training Epoch: 2/2, step 251/1149 completed (loss: 1.6619888544082642, acc: 0.546875)
[2025-02-17 10:51:44,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:45,027][root][INFO] - Training Epoch: 2/2, step 252/1149 completed (loss: 1.6381021738052368, acc: 0.5679012537002563)
[2025-02-17 10:51:45,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:45,388][root][INFO] - Training Epoch: 2/2, step 253/1149 completed (loss: 2.179809093475342, acc: 0.43589743971824646)
[2025-02-17 10:51:45,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:45,753][root][INFO] - Training Epoch: 2/2, step 254/1149 completed (loss: 2.114576578140259, acc: 0.38738739490509033)
[2025-02-17 10:51:45,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:46,113][root][INFO] - Training Epoch: 2/2, step 255/1149 completed (loss: 1.4625929594039917, acc: 0.5614035129547119)
[2025-02-17 10:51:46,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:46,530][root][INFO] - Training Epoch: 2/2, step 256/1149 completed (loss: 1.5479439496994019, acc: 0.5714285969734192)
[2025-02-17 10:51:46,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:46,903][root][INFO] - Training Epoch: 2/2, step 257/1149 completed (loss: 1.3623077869415283, acc: 0.6016949415206909)
[2025-02-17 10:51:47,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:47,308][root][INFO] - Training Epoch: 2/2, step 258/1149 completed (loss: 1.6937141418457031, acc: 0.5625)
[2025-02-17 10:51:47,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:47,658][root][INFO] - Training Epoch: 2/2, step 259/1149 completed (loss: 0.7381415367126465, acc: 0.625)
[2025-02-17 10:51:47,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:48,065][root][INFO] - Training Epoch: 2/2, step 260/1149 completed (loss: 1.3626987934112549, acc: 0.7142857313156128)
[2025-02-17 10:51:48,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:48,476][root][INFO] - Training Epoch: 2/2, step 261/1149 completed (loss: 0.18234442174434662, acc: 0.9166666865348816)
[2025-02-17 10:51:48,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:48,818][root][INFO] - Training Epoch: 2/2, step 262/1149 completed (loss: 0.9358924627304077, acc: 0.699999988079071)
[2025-02-17 10:51:48,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:49,167][root][INFO] - Training Epoch: 2/2, step 263/1149 completed (loss: 0.647689163684845, acc: 0.692307710647583)
[2025-02-17 10:51:49,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:49,567][root][INFO] - Training Epoch: 2/2, step 264/1149 completed (loss: 1.252581000328064, acc: 0.5)
[2025-02-17 10:51:49,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:49,936][root][INFO] - Training Epoch: 2/2, step 265/1149 completed (loss: 1.3007423877716064, acc: 0.5)
[2025-02-17 10:51:50,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:50,344][root][INFO] - Training Epoch: 2/2, step 266/1149 completed (loss: 1.0351169109344482, acc: 0.7272727489471436)
[2025-02-17 10:51:50,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:50,767][root][INFO] - Training Epoch: 2/2, step 267/1149 completed (loss: 1.7647935152053833, acc: 0.5833333134651184)
[2025-02-17 10:51:50,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:51,140][root][INFO] - Training Epoch: 2/2, step 268/1149 completed (loss: 0.6521530151367188, acc: 0.9230769276618958)
[2025-02-17 10:51:51,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:51,570][root][INFO] - Training Epoch: 2/2, step 269/1149 completed (loss: 1.1714516878128052, acc: 0.6818181872367859)
[2025-02-17 10:51:51,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:51,933][root][INFO] - Training Epoch: 2/2, step 270/1149 completed (loss: 1.0035226345062256, acc: 0.7333333492279053)
[2025-02-17 10:51:52,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:52,340][root][INFO] - Training Epoch: 2/2, step 271/1149 completed (loss: 1.1296749114990234, acc: 0.5454545617103577)
[2025-02-17 10:51:52,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:52,757][root][INFO] - Training Epoch: 2/2, step 272/1149 completed (loss: 0.6957575082778931, acc: 0.800000011920929)
[2025-02-17 10:51:52,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:53,136][root][INFO] - Training Epoch: 2/2, step 273/1149 completed (loss: 1.6998077630996704, acc: 0.46875)
[2025-02-17 10:51:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:53,514][root][INFO] - Training Epoch: 2/2, step 274/1149 completed (loss: 1.523598074913025, acc: 0.4285714328289032)
[2025-02-17 10:51:53,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:53,874][root][INFO] - Training Epoch: 2/2, step 275/1149 completed (loss: 1.3809986114501953, acc: 0.75)
[2025-02-17 10:51:54,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:54,232][root][INFO] - Training Epoch: 2/2, step 276/1149 completed (loss: 1.7412831783294678, acc: 0.5384615659713745)
[2025-02-17 10:51:54,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:54,558][root][INFO] - Training Epoch: 2/2, step 277/1149 completed (loss: 1.246551275253296, acc: 0.800000011920929)
[2025-02-17 10:51:54,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:54,910][root][INFO] - Training Epoch: 2/2, step 278/1149 completed (loss: 1.6077157258987427, acc: 0.5454545617103577)
[2025-02-17 10:51:55,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:55,237][root][INFO] - Training Epoch: 2/2, step 279/1149 completed (loss: 2.2926621437072754, acc: 0.6000000238418579)
[2025-02-17 10:51:55,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:55,591][root][INFO] - Training Epoch: 2/2, step 280/1149 completed (loss: 1.6323059797286987, acc: 0.5)
[2025-02-17 10:51:55,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:55,950][root][INFO] - Training Epoch: 2/2, step 281/1149 completed (loss: 1.7511235475540161, acc: 0.5)
[2025-02-17 10:51:56,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:56,291][root][INFO] - Training Epoch: 2/2, step 282/1149 completed (loss: 1.5562989711761475, acc: 0.6666666865348816)
[2025-02-17 10:51:56,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:56,665][root][INFO] - Training Epoch: 2/2, step 283/1149 completed (loss: 1.5553309917449951, acc: 0.6153846383094788)
[2025-02-17 10:51:56,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:57,058][root][INFO] - Training Epoch: 2/2, step 284/1149 completed (loss: 1.575217604637146, acc: 0.6470588445663452)
[2025-02-17 10:51:57,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:57,489][root][INFO] - Training Epoch: 2/2, step 285/1149 completed (loss: 1.188478708267212, acc: 0.6499999761581421)
[2025-02-17 10:51:58,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:58,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:59,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:51:59,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:00,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:00,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:01,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:01,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:02,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:03,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:03,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:04,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:04,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:05,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:05,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:06,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:06,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:06,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:07,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:08,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:08,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:09,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:09,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:10,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:10,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:11,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:12,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:12,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:13,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:13,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:14,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:14,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:14,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:15,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:15,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:16,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:16,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:17,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:17,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:18,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:18,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:19,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:19,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:20,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:20,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:21,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:21,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:22,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:22,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:23,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:23,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:24,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:24,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:25,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:25,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:26,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:26,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:27,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:27,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:28,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:28,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:28,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:29,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:29,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:30,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:30,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:31,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:31,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:32,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:32,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:33,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:33,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:34,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:34,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:35,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:35,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:36,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:36,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:37,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:37,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:38,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:38,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:39,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:39,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:40,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:41,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:41,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:41,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:42,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:42,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:43,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:43,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:44,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:44,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:45,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:45,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:46,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:46,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:47,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:47,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:48,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:48,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:48,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:49,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:49,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:50,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:50,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:51,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:51,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:52,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:52,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:53,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:53,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:54,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:54,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:55,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:55,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:56,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:56,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:57,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:58,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:58,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:59,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:52:59,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:00,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:00,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:01,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:01,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:02,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:03,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:03,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:04,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:04,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:05,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:06,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:06,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:06,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:07,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:08,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:08,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:08,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:09,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:09,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:10,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:10,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:11,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:11,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:12,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:12,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:13,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:13,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:13,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:14,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:15,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:15,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:16,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:16,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:17,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:17,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:18,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:18,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:19,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:19,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:20,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:20,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:20,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:21,459][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.9993, device='cuda:0') eval_epoch_loss=tensor(1.0984, device='cuda:0') eval_epoch_acc=tensor(0.7077, device='cuda:0')
[2025-02-17 10:53:21,460][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 10:53:21,460][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 10:53:24,679][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_2_step_286_loss_1.0983686447143555/model.pt
[2025-02-17 10:53:24,686][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 10:53:24,686][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.0983686447143555
[2025-02-17 10:53:24,687][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.7077093720436096
[2025-02-17 10:53:24,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:25,187][root][INFO] - Training Epoch: 2/2, step 286/1149 completed (loss: 1.4393349885940552, acc: 0.5595238208770752)
[2025-02-17 10:53:25,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:25,573][root][INFO] - Training Epoch: 2/2, step 287/1149 completed (loss: 0.9966732263565063, acc: 0.6333333253860474)
[2025-02-17 10:53:25,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:26,014][root][INFO] - Training Epoch: 2/2, step 288/1149 completed (loss: 0.9511542320251465, acc: 0.7105262875556946)
[2025-02-17 10:53:26,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:26,451][root][INFO] - Training Epoch: 2/2, step 289/1149 completed (loss: 1.4573726654052734, acc: 0.5729166865348816)
[2025-02-17 10:53:26,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:26,830][root][INFO] - Training Epoch: 2/2, step 290/1149 completed (loss: 1.1576426029205322, acc: 0.5)
[2025-02-17 10:53:26,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:27,187][root][INFO] - Training Epoch: 2/2, step 291/1149 completed (loss: 1.3905178308486938, acc: 0.5595238208770752)
[2025-02-17 10:53:27,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:27,651][root][INFO] - Training Epoch: 2/2, step 292/1149 completed (loss: 1.7888025045394897, acc: 0.5142857432365417)
[2025-02-17 10:53:27,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:28,063][root][INFO] - Training Epoch: 2/2, step 293/1149 completed (loss: 1.1685243844985962, acc: 0.75)
[2025-02-17 10:53:28,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:28,442][root][INFO] - Training Epoch: 2/2, step 294/1149 completed (loss: 1.433014154434204, acc: 0.6290322542190552)
[2025-02-17 10:53:28,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:28,841][root][INFO] - Training Epoch: 2/2, step 295/1149 completed (loss: 0.5437431931495667, acc: 0.75)
[2025-02-17 10:53:28,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:29,184][root][INFO] - Training Epoch: 2/2, step 296/1149 completed (loss: 1.460360050201416, acc: 0.6666666865348816)
[2025-02-17 10:53:29,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:29,526][root][INFO] - Training Epoch: 2/2, step 297/1149 completed (loss: 1.3791484832763672, acc: 0.692307710647583)
[2025-02-17 10:53:29,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:29,907][root][INFO] - Training Epoch: 2/2, step 298/1149 completed (loss: 0.9467898607254028, acc: 0.800000011920929)
[2025-02-17 10:53:30,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:30,261][root][INFO] - Training Epoch: 2/2, step 299/1149 completed (loss: 1.5413366556167603, acc: 0.4615384638309479)
[2025-02-17 10:53:30,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:30,655][root][INFO] - Training Epoch: 2/2, step 300/1149 completed (loss: 2.2493510246276855, acc: 0.3333333432674408)
[2025-02-17 10:53:30,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:31,019][root][INFO] - Training Epoch: 2/2, step 301/1149 completed (loss: 0.938651978969574, acc: 0.6428571343421936)
[2025-02-17 10:53:31,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:31,427][root][INFO] - Training Epoch: 2/2, step 302/1149 completed (loss: 1.6148117780685425, acc: 0.625)
[2025-02-17 10:53:31,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:31,821][root][INFO] - Training Epoch: 2/2, step 303/1149 completed (loss: 1.4274710416793823, acc: 0.5909090638160706)
[2025-02-17 10:53:32,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:32,215][root][INFO] - Training Epoch: 2/2, step 304/1149 completed (loss: 1.302869439125061, acc: 0.6808510422706604)
[2025-02-17 10:53:32,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:32,597][root][INFO] - Training Epoch: 2/2, step 305/1149 completed (loss: 1.0609732866287231, acc: 0.75)
[2025-02-17 10:53:32,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:32,963][root][INFO] - Training Epoch: 2/2, step 306/1149 completed (loss: 1.9363075494766235, acc: 0.4736842215061188)
[2025-02-17 10:53:33,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:33,343][root][INFO] - Training Epoch: 2/2, step 307/1149 completed (loss: 1.495708703994751, acc: 0.7368420958518982)
[2025-02-17 10:53:33,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:33,753][root][INFO] - Training Epoch: 2/2, step 308/1149 completed (loss: 0.7924609184265137, acc: 0.8235294222831726)
[2025-02-17 10:53:33,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:34,146][root][INFO] - Training Epoch: 2/2, step 309/1149 completed (loss: 1.5346667766571045, acc: 0.6666666865348816)
[2025-02-17 10:53:34,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:34,464][root][INFO] - Training Epoch: 2/2, step 310/1149 completed (loss: 1.7477953433990479, acc: 0.692307710647583)
[2025-02-17 10:53:34,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:34,798][root][INFO] - Training Epoch: 2/2, step 311/1149 completed (loss: 0.6176081299781799, acc: 0.800000011920929)
[2025-02-17 10:53:34,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:35,147][root][INFO] - Training Epoch: 2/2, step 312/1149 completed (loss: 0.8059489130973816, acc: 0.7333333492279053)
[2025-02-17 10:53:35,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:35,504][root][INFO] - Training Epoch: 2/2, step 313/1149 completed (loss: 0.8879994750022888, acc: 0.875)
[2025-02-17 10:53:35,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:35,876][root][INFO] - Training Epoch: 2/2, step 314/1149 completed (loss: 0.8970792293548584, acc: 0.625)
[2025-02-17 10:53:36,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:36,267][root][INFO] - Training Epoch: 2/2, step 315/1149 completed (loss: 2.7774136066436768, acc: 0.1818181872367859)
[2025-02-17 10:53:36,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:36,732][root][INFO] - Training Epoch: 2/2, step 316/1149 completed (loss: 1.1260628700256348, acc: 0.7200000286102295)
[2025-02-17 10:53:37,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:38,199][root][INFO] - Training Epoch: 2/2, step 317/1149 completed (loss: 1.913282871246338, acc: 0.4285714328289032)
[2025-02-17 10:53:38,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:38,622][root][INFO] - Training Epoch: 2/2, step 318/1149 completed (loss: 2.2761387825012207, acc: 0.48275861144065857)
[2025-02-17 10:53:38,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:38,989][root][INFO] - Training Epoch: 2/2, step 319/1149 completed (loss: 1.4787623882293701, acc: 0.6399999856948853)
[2025-02-17 10:53:39,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:39,412][root][INFO] - Training Epoch: 2/2, step 320/1149 completed (loss: 1.7579034566879272, acc: 0.4727272689342499)
[2025-02-17 10:53:39,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:39,817][root][INFO] - Training Epoch: 2/2, step 321/1149 completed (loss: 1.615954875946045, acc: 0.5161290168762207)
[2025-02-17 10:53:40,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:40,259][root][INFO] - Training Epoch: 2/2, step 322/1149 completed (loss: 1.929436206817627, acc: 0.4516128897666931)
[2025-02-17 10:53:40,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:40,854][root][INFO] - Training Epoch: 2/2, step 323/1149 completed (loss: 1.4309004545211792, acc: 0.5740740895271301)
[2025-02-17 10:53:41,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:41,460][root][INFO] - Training Epoch: 2/2, step 324/1149 completed (loss: 1.7845996618270874, acc: 0.5)
[2025-02-17 10:53:41,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:41,866][root][INFO] - Training Epoch: 2/2, step 325/1149 completed (loss: 1.5700796842575073, acc: 0.6296296119689941)
[2025-02-17 10:53:42,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:42,242][root][INFO] - Training Epoch: 2/2, step 326/1149 completed (loss: 0.7797156572341919, acc: 0.6896551847457886)
[2025-02-17 10:53:42,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:42,613][root][INFO] - Training Epoch: 2/2, step 327/1149 completed (loss: 1.4859700202941895, acc: 0.6666666865348816)
[2025-02-17 10:53:42,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:43,001][root][INFO] - Training Epoch: 2/2, step 328/1149 completed (loss: 1.8471972942352295, acc: 0.5454545617103577)
[2025-02-17 10:53:43,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:43,338][root][INFO] - Training Epoch: 2/2, step 329/1149 completed (loss: 0.0799153745174408, acc: 1.0)
[2025-02-17 10:53:43,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:43,710][root][INFO] - Training Epoch: 2/2, step 330/1149 completed (loss: 1.5382916927337646, acc: 0.5)
[2025-02-17 10:53:43,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:44,076][root][INFO] - Training Epoch: 2/2, step 331/1149 completed (loss: 0.6067509651184082, acc: 0.8461538553237915)
[2025-02-17 10:53:44,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:44,427][root][INFO] - Training Epoch: 2/2, step 332/1149 completed (loss: 1.4363901615142822, acc: 0.5454545617103577)
[2025-02-17 10:53:44,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:44,836][root][INFO] - Training Epoch: 2/2, step 333/1149 completed (loss: 0.4751327931880951, acc: 0.8157894611358643)
[2025-02-17 10:53:44,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:45,217][root][INFO] - Training Epoch: 2/2, step 334/1149 completed (loss: 0.3785417377948761, acc: 0.8999999761581421)
[2025-02-17 10:53:45,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:45,585][root][INFO] - Training Epoch: 2/2, step 335/1149 completed (loss: 1.5202523469924927, acc: 0.550000011920929)
[2025-02-17 10:53:45,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:46,009][root][INFO] - Training Epoch: 2/2, step 336/1149 completed (loss: 0.8369560837745667, acc: 0.75)
[2025-02-17 10:53:46,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:46,376][root][INFO] - Training Epoch: 2/2, step 337/1149 completed (loss: 1.1468770503997803, acc: 0.7250000238418579)
[2025-02-17 10:53:46,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:46,755][root][INFO] - Training Epoch: 2/2, step 338/1149 completed (loss: 1.7815093994140625, acc: 0.5753424763679504)
[2025-02-17 10:53:46,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:47,176][root][INFO] - Training Epoch: 2/2, step 339/1149 completed (loss: 0.4326442778110504, acc: 0.8620689511299133)
[2025-02-17 10:53:47,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:48,149][root][INFO] - Training Epoch: 2/2, step 340/1149 completed (loss: 1.8098763227462769, acc: 0.5196850299835205)
[2025-02-17 10:53:48,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:48,554][root][INFO] - Training Epoch: 2/2, step 341/1149 completed (loss: 0.7554116249084473, acc: 0.8421052694320679)
[2025-02-17 10:53:48,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:48,909][root][INFO] - Training Epoch: 2/2, step 342/1149 completed (loss: 0.5848447680473328, acc: 0.875)
[2025-02-17 10:53:49,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:49,252][root][INFO] - Training Epoch: 2/2, step 343/1149 completed (loss: 0.3020591735839844, acc: 0.75)
[2025-02-17 10:53:49,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:49,638][root][INFO] - Training Epoch: 2/2, step 344/1149 completed (loss: 0.5974805355072021, acc: 0.875)
[2025-02-17 10:53:49,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:50,049][root][INFO] - Training Epoch: 2/2, step 345/1149 completed (loss: 1.3103657960891724, acc: 0.5454545617103577)
[2025-02-17 10:53:50,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:50,404][root][INFO] - Training Epoch: 2/2, step 346/1149 completed (loss: 1.3477877378463745, acc: 0.6153846383094788)
[2025-02-17 10:53:50,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:50,712][root][INFO] - Training Epoch: 2/2, step 347/1149 completed (loss: 0.8320381045341492, acc: 0.7333333492279053)
[2025-02-17 10:53:50,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:51,078][root][INFO] - Training Epoch: 2/2, step 348/1149 completed (loss: 1.2559552192687988, acc: 0.6756756901741028)
[2025-02-17 10:53:51,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:51,606][root][INFO] - Training Epoch: 2/2, step 349/1149 completed (loss: 1.8180614709854126, acc: 0.5789473652839661)
[2025-02-17 10:53:51,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:52,033][root][INFO] - Training Epoch: 2/2, step 350/1149 completed (loss: 1.0507570505142212, acc: 0.7941176295280457)
[2025-02-17 10:53:52,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:52,462][root][INFO] - Training Epoch: 2/2, step 351/1149 completed (loss: 1.6117732524871826, acc: 0.5714285969734192)
[2025-02-17 10:53:52,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:52,833][root][INFO] - Training Epoch: 2/2, step 352/1149 completed (loss: 1.0718941688537598, acc: 0.6617646813392639)
[2025-02-17 10:53:53,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:53,261][root][INFO] - Training Epoch: 2/2, step 353/1149 completed (loss: 0.916265606880188, acc: 0.800000011920929)
[2025-02-17 10:53:53,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:54,149][root][INFO] - Training Epoch: 2/2, step 354/1149 completed (loss: 2.0211455821990967, acc: 0.40963855385780334)
[2025-02-17 10:53:54,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:54,600][root][INFO] - Training Epoch: 2/2, step 355/1149 completed (loss: 1.548807144165039, acc: 0.5625)
[2025-02-17 10:53:54,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:55,254][root][INFO] - Training Epoch: 2/2, step 356/1149 completed (loss: 1.4802716970443726, acc: 0.5647059082984924)
[2025-02-17 10:53:55,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:55,623][root][INFO] - Training Epoch: 2/2, step 357/1149 completed (loss: 1.376387596130371, acc: 0.5799999833106995)
[2025-02-17 10:53:55,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:55,970][root][INFO] - Training Epoch: 2/2, step 358/1149 completed (loss: 1.149688959121704, acc: 0.6785714030265808)
[2025-02-17 10:53:56,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:56,314][root][INFO] - Training Epoch: 2/2, step 359/1149 completed (loss: 0.7588146328926086, acc: 0.8181818127632141)
[2025-02-17 10:53:56,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:56,671][root][INFO] - Training Epoch: 2/2, step 360/1149 completed (loss: 0.046482380479574203, acc: 1.0)
[2025-02-17 10:53:56,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:57,062][root][INFO] - Training Epoch: 2/2, step 361/1149 completed (loss: 0.4724334180355072, acc: 0.9230769276618958)
[2025-02-17 10:53:57,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:57,372][root][INFO] - Training Epoch: 2/2, step 362/1149 completed (loss: 0.9240427613258362, acc: 0.8571428656578064)
[2025-02-17 10:53:57,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:57,730][root][INFO] - Training Epoch: 2/2, step 363/1149 completed (loss: 0.4529915153980255, acc: 0.8181818127632141)
[2025-02-17 10:53:57,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:58,073][root][INFO] - Training Epoch: 2/2, step 364/1149 completed (loss: 0.5220197439193726, acc: 0.7857142686843872)
[2025-02-17 10:53:58,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:58,463][root][INFO] - Training Epoch: 2/2, step 365/1149 completed (loss: 0.1571168452501297, acc: 1.0)
[2025-02-17 10:53:58,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:58,797][root][INFO] - Training Epoch: 2/2, step 366/1149 completed (loss: 0.2606697976589203, acc: 0.8999999761581421)
[2025-02-17 10:53:58,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:59,151][root][INFO] - Training Epoch: 2/2, step 367/1149 completed (loss: 1.0743556022644043, acc: 0.7777777910232544)
[2025-02-17 10:53:59,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:59,568][root][INFO] - Training Epoch: 2/2, step 368/1149 completed (loss: 1.1383881568908691, acc: 0.6906474828720093)
[2025-02-17 10:53:59,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:53:59,970][root][INFO] - Training Epoch: 2/2, step 369/1149 completed (loss: 1.478044033050537, acc: 0.5989583134651184)
[2025-02-17 10:54:00,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:00,334][root][INFO] - Training Epoch: 2/2, step 370/1149 completed (loss: 1.2364355325698853, acc: 0.6063829660415649)
[2025-02-17 10:54:00,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:00,687][root][INFO] - Training Epoch: 2/2, step 371/1149 completed (loss: 1.4469200372695923, acc: 0.6226415038108826)
[2025-02-17 10:54:00,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:01,161][root][INFO] - Training Epoch: 2/2, step 372/1149 completed (loss: 1.4319766759872437, acc: 0.6168830990791321)
[2025-02-17 10:54:01,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:01,541][root][INFO] - Training Epoch: 2/2, step 373/1149 completed (loss: 1.276530385017395, acc: 0.6385542154312134)
[2025-02-17 10:54:01,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:02,029][root][INFO] - Training Epoch: 2/2, step 374/1149 completed (loss: 1.2170277833938599, acc: 0.6553030014038086)
[2025-02-17 10:54:02,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:02,521][root][INFO] - Training Epoch: 2/2, step 375/1149 completed (loss: 1.3836075067520142, acc: 0.6468401551246643)
[2025-02-17 10:54:02,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:02,885][root][INFO] - Training Epoch: 2/2, step 376/1149 completed (loss: 1.0307681560516357, acc: 0.699999988079071)
[2025-02-17 10:54:03,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:03,281][root][INFO] - Training Epoch: 2/2, step 377/1149 completed (loss: 1.299147605895996, acc: 0.6291390657424927)
[2025-02-17 10:54:03,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:03,625][root][INFO] - Training Epoch: 2/2, step 378/1149 completed (loss: 1.7634347677230835, acc: 0.46666666865348816)
[2025-02-17 10:54:03,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:03,968][root][INFO] - Training Epoch: 2/2, step 379/1149 completed (loss: 0.2941705584526062, acc: 0.8999999761581421)
[2025-02-17 10:54:04,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:04,341][root][INFO] - Training Epoch: 2/2, step 380/1149 completed (loss: 1.8297168016433716, acc: 0.5333333611488342)
[2025-02-17 10:54:04,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:04,899][root][INFO] - Training Epoch: 2/2, step 381/1149 completed (loss: 1.675709843635559, acc: 0.5357142686843872)
[2025-02-17 10:54:05,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:05,672][root][INFO] - Training Epoch: 2/2, step 382/1149 completed (loss: 1.6426459550857544, acc: 0.534246563911438)
[2025-02-17 10:54:05,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:06,174][root][INFO] - Training Epoch: 2/2, step 383/1149 completed (loss: 1.7191336154937744, acc: 0.5283018946647644)
[2025-02-17 10:54:06,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:07,068][root][INFO] - Training Epoch: 2/2, step 384/1149 completed (loss: 1.410496711730957, acc: 0.5670102834701538)
[2025-02-17 10:54:07,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:07,765][root][INFO] - Training Epoch: 2/2, step 385/1149 completed (loss: 1.7186064720153809, acc: 0.4571428596973419)
[2025-02-17 10:54:08,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:08,501][root][INFO] - Training Epoch: 2/2, step 386/1149 completed (loss: 1.1929810047149658, acc: 0.640625)
[2025-02-17 10:54:08,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:08,855][root][INFO] - Training Epoch: 2/2, step 387/1149 completed (loss: 1.2646424770355225, acc: 0.6190476417541504)
[2025-02-17 10:54:09,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:09,636][root][INFO] - Training Epoch: 2/2, step 388/1149 completed (loss: 1.842685580253601, acc: 0.42424243688583374)
[2025-02-17 10:54:10,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:10,674][root][INFO] - Training Epoch: 2/2, step 389/1149 completed (loss: 1.242750883102417, acc: 0.682539701461792)
[2025-02-17 10:54:11,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:11,622][root][INFO] - Training Epoch: 2/2, step 390/1149 completed (loss: 1.0256869792938232, acc: 0.686274528503418)
[2025-02-17 10:54:11,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:11,961][root][INFO] - Training Epoch: 2/2, step 391/1149 completed (loss: 0.13210032880306244, acc: 0.9090909361839294)
[2025-02-17 10:54:12,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:12,297][root][INFO] - Training Epoch: 2/2, step 392/1149 completed (loss: 0.7418810725212097, acc: 0.7857142686843872)
[2025-02-17 10:54:12,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:12,628][root][INFO] - Training Epoch: 2/2, step 393/1149 completed (loss: 0.28537237644195557, acc: 0.8571428656578064)
[2025-02-17 10:54:12,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:12,966][root][INFO] - Training Epoch: 2/2, step 394/1149 completed (loss: 1.53313410282135, acc: 0.5625)
[2025-02-17 10:54:13,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:13,307][root][INFO] - Training Epoch: 2/2, step 395/1149 completed (loss: 1.1266368627548218, acc: 0.6666666865348816)
[2025-02-17 10:54:13,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:13,654][root][INFO] - Training Epoch: 2/2, step 396/1149 completed (loss: 1.3447822332382202, acc: 0.6399999856948853)
[2025-02-17 10:54:13,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:14,005][root][INFO] - Training Epoch: 2/2, step 397/1149 completed (loss: 1.2495079040527344, acc: 0.604651153087616)
[2025-02-17 10:54:14,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:14,358][root][INFO] - Training Epoch: 2/2, step 398/1149 completed (loss: 1.392646074295044, acc: 0.6483516693115234)
[2025-02-17 10:54:14,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:14,712][root][INFO] - Training Epoch: 2/2, step 399/1149 completed (loss: 1.308408260345459, acc: 0.6222222447395325)
[2025-02-17 10:54:14,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:15,054][root][INFO] - Training Epoch: 2/2, step 400/1149 completed (loss: 1.2494759559631348, acc: 0.7115384340286255)
[2025-02-17 10:54:15,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:15,404][root][INFO] - Training Epoch: 2/2, step 401/1149 completed (loss: 1.3918445110321045, acc: 0.6818181872367859)
[2025-02-17 10:54:15,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:15,752][root][INFO] - Training Epoch: 2/2, step 402/1149 completed (loss: 1.4795262813568115, acc: 0.6170212626457214)
[2025-02-17 10:54:15,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:16,098][root][INFO] - Training Epoch: 2/2, step 403/1149 completed (loss: 1.3016555309295654, acc: 0.6551724076271057)
[2025-02-17 10:54:16,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:16,467][root][INFO] - Training Epoch: 2/2, step 404/1149 completed (loss: 1.4614084959030151, acc: 0.6231883764266968)
[2025-02-17 10:54:16,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:16,810][root][INFO] - Training Epoch: 2/2, step 405/1149 completed (loss: 1.0044262409210205, acc: 0.7058823704719543)
[2025-02-17 10:54:16,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:17,176][root][INFO] - Training Epoch: 2/2, step 406/1149 completed (loss: 1.526991844177246, acc: 0.6200000047683716)
[2025-02-17 10:54:17,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:17,525][root][INFO] - Training Epoch: 2/2, step 407/1149 completed (loss: 1.2645306587219238, acc: 0.6153846383094788)
[2025-02-17 10:54:17,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:17,902][root][INFO] - Training Epoch: 2/2, step 408/1149 completed (loss: 0.8520626425743103, acc: 0.7027027010917664)
[2025-02-17 10:54:18,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:18,308][root][INFO] - Training Epoch: 2/2, step 409/1149 completed (loss: 0.9139614701271057, acc: 0.7777777910232544)
[2025-02-17 10:54:18,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:18,720][root][INFO] - Training Epoch: 2/2, step 410/1149 completed (loss: 0.8152507543563843, acc: 0.7971014380455017)
[2025-02-17 10:54:18,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:19,153][root][INFO] - Training Epoch: 2/2, step 411/1149 completed (loss: 1.0985063314437866, acc: 0.701298713684082)
[2025-02-17 10:54:19,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:19,599][root][INFO] - Training Epoch: 2/2, step 412/1149 completed (loss: 1.0460668802261353, acc: 0.7342657446861267)
[2025-02-17 10:54:19,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:20,053][root][INFO] - Training Epoch: 2/2, step 413/1149 completed (loss: 1.0469887256622314, acc: 0.7567567825317383)
[2025-02-17 10:54:20,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:20,494][root][INFO] - Training Epoch: 2/2, step 414/1149 completed (loss: 0.8652945160865784, acc: 0.8020833134651184)
[2025-02-17 10:54:20,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:20,889][root][INFO] - Training Epoch: 2/2, step 415/1149 completed (loss: 0.9015727043151855, acc: 0.7426470518112183)
[2025-02-17 10:54:21,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:21,302][root][INFO] - Training Epoch: 2/2, step 416/1149 completed (loss: 0.7851213812828064, acc: 0.8246753215789795)
[2025-02-17 10:54:21,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:21,798][root][INFO] - Training Epoch: 2/2, step 417/1149 completed (loss: 0.8428114056587219, acc: 0.7459016442298889)
[2025-02-17 10:54:21,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:22,207][root][INFO] - Training Epoch: 2/2, step 418/1149 completed (loss: 0.8015972375869751, acc: 0.769911527633667)
[2025-02-17 10:54:22,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:22,582][root][INFO] - Training Epoch: 2/2, step 419/1149 completed (loss: 1.2161688804626465, acc: 0.7222222089767456)
[2025-02-17 10:54:22,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:22,965][root][INFO] - Training Epoch: 2/2, step 420/1149 completed (loss: 1.1317570209503174, acc: 0.6875)
[2025-02-17 10:54:23,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:23,345][root][INFO] - Training Epoch: 2/2, step 421/1149 completed (loss: 1.47432279586792, acc: 0.8333333134651184)
[2025-02-17 10:54:23,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:23,725][root][INFO] - Training Epoch: 2/2, step 422/1149 completed (loss: 0.800974428653717, acc: 0.6428571343421936)
[2025-02-17 10:54:23,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:24,091][root][INFO] - Training Epoch: 2/2, step 423/1149 completed (loss: 0.42912885546684265, acc: 0.8888888955116272)
[2025-02-17 10:54:24,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:24,513][root][INFO] - Training Epoch: 2/2, step 424/1149 completed (loss: 0.6810118556022644, acc: 0.75)
[2025-02-17 10:54:24,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:24,881][root][INFO] - Training Epoch: 2/2, step 425/1149 completed (loss: 0.6374661922454834, acc: 0.8333333134651184)
[2025-02-17 10:54:25,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:25,256][root][INFO] - Training Epoch: 2/2, step 426/1149 completed (loss: 0.5686278939247131, acc: 0.7272727489471436)
[2025-02-17 10:54:25,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:25,606][root][INFO] - Training Epoch: 2/2, step 427/1149 completed (loss: 0.5912721157073975, acc: 0.8611111044883728)
[2025-02-17 10:54:25,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:26,272][root][INFO] - Training Epoch: 2/2, step 428/1149 completed (loss: 0.7699503302574158, acc: 0.8275862336158752)
[2025-02-17 10:54:26,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:26,626][root][INFO] - Training Epoch: 2/2, step 429/1149 completed (loss: 0.22988247871398926, acc: 0.930232584476471)
[2025-02-17 10:54:26,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:26,984][root][INFO] - Training Epoch: 2/2, step 430/1149 completed (loss: 0.6225839257240295, acc: 0.7560975551605225)
[2025-02-17 10:54:27,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:27,334][root][INFO] - Training Epoch: 2/2, step 431/1149 completed (loss: 0.37961456179618835, acc: 0.9090909361839294)
[2025-02-17 10:54:27,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:27,687][root][INFO] - Training Epoch: 2/2, step 432/1149 completed (loss: 0.6101786494255066, acc: 0.8095238208770752)
[2025-02-17 10:54:27,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:28,054][root][INFO] - Training Epoch: 2/2, step 433/1149 completed (loss: 0.31592991948127747, acc: 0.9318181872367859)
[2025-02-17 10:54:28,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:28,595][root][INFO] - Training Epoch: 2/2, step 434/1149 completed (loss: 0.6319554448127747, acc: 0.7903226017951965)
[2025-02-17 10:54:28,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:29,041][root][INFO] - Training Epoch: 2/2, step 435/1149 completed (loss: 0.4724876880645752, acc: 0.8571428656578064)
[2025-02-17 10:54:29,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:29,448][root][INFO] - Training Epoch: 2/2, step 436/1149 completed (loss: 0.7411875128746033, acc: 0.8035714030265808)
[2025-02-17 10:54:29,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:29,796][root][INFO] - Training Epoch: 2/2, step 437/1149 completed (loss: 0.20894916355609894, acc: 0.9411764740943909)
[2025-02-17 10:54:29,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:30,142][root][INFO] - Training Epoch: 2/2, step 438/1149 completed (loss: 0.6306232810020447, acc: 0.8636363744735718)
[2025-02-17 10:54:30,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:30,518][root][INFO] - Training Epoch: 2/2, step 439/1149 completed (loss: 0.06809902936220169, acc: 1.0)
[2025-02-17 10:54:30,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:30,858][root][INFO] - Training Epoch: 2/2, step 440/1149 completed (loss: 0.2345346361398697, acc: 0.9230769276618958)
[2025-02-17 10:54:30,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:31,193][root][INFO] - Training Epoch: 2/2, step 441/1149 completed (loss: 0.6654080748558044, acc: 0.7857142686843872)
[2025-02-17 10:54:31,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:31,532][root][INFO] - Training Epoch: 2/2, step 442/1149 completed (loss: 0.44095686078071594, acc: 0.9090909361839294)
[2025-02-17 10:54:31,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:31,915][root][INFO] - Training Epoch: 2/2, step 443/1149 completed (loss: 0.5558393001556396, acc: 0.7857142686843872)
[2025-02-17 10:54:32,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:32,255][root][INFO] - Training Epoch: 2/2, step 444/1149 completed (loss: 0.7946768403053284, acc: 0.8181818127632141)
[2025-02-17 10:54:32,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:32,602][root][INFO] - Training Epoch: 2/2, step 445/1149 completed (loss: 1.3565424680709839, acc: 0.5853658318519592)
[2025-02-17 10:54:32,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:33,331][root][INFO] - Training Epoch: 2/2, step 446/1149 completed (loss: 0.975548267364502, acc: 0.6969696879386902)
[2025-02-17 10:54:33,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:33,828][root][INFO] - Training Epoch: 2/2, step 447/1149 completed (loss: 0.6072333455085754, acc: 0.8235294222831726)
[2025-02-17 10:54:34,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:34,298][root][INFO] - Training Epoch: 2/2, step 448/1149 completed (loss: 1.3250359296798706, acc: 0.6774193644523621)
[2025-02-17 10:54:34,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:34,822][root][INFO] - Training Epoch: 2/2, step 449/1149 completed (loss: 0.8000161051750183, acc: 0.7831325531005859)
[2025-02-17 10:54:35,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:35,268][root][INFO] - Training Epoch: 2/2, step 450/1149 completed (loss: 1.300179123878479, acc: 0.6746987700462341)
[2025-02-17 10:54:35,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:35,605][root][INFO] - Training Epoch: 2/2, step 451/1149 completed (loss: 0.3700389564037323, acc: 0.9090909361839294)
[2025-02-17 10:54:35,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:35,954][root][INFO] - Training Epoch: 2/2, step 452/1149 completed (loss: 0.5687950849533081, acc: 0.7599999904632568)
[2025-02-17 10:54:36,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:36,338][root][INFO] - Training Epoch: 2/2, step 453/1149 completed (loss: 1.4015045166015625, acc: 0.6428571343421936)
[2025-02-17 10:54:36,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:36,676][root][INFO] - Training Epoch: 2/2, step 454/1149 completed (loss: 1.1206599473953247, acc: 0.6666666865348816)
[2025-02-17 10:54:36,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:37,054][root][INFO] - Training Epoch: 2/2, step 455/1149 completed (loss: 0.6968478560447693, acc: 0.8333333134651184)
[2025-02-17 10:54:37,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:37,471][root][INFO] - Training Epoch: 2/2, step 456/1149 completed (loss: 0.780786395072937, acc: 0.8125)
[2025-02-17 10:54:37,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:37,881][root][INFO] - Training Epoch: 2/2, step 457/1149 completed (loss: 0.7592153549194336, acc: 0.7272727489471436)
[2025-02-17 10:54:38,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:38,252][root][INFO] - Training Epoch: 2/2, step 458/1149 completed (loss: 1.4461307525634766, acc: 0.6875)
[2025-02-17 10:54:38,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:38,665][root][INFO] - Training Epoch: 2/2, step 459/1149 completed (loss: 1.5234414339065552, acc: 0.6428571343421936)
[2025-02-17 10:54:38,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:39,060][root][INFO] - Training Epoch: 2/2, step 460/1149 completed (loss: 1.7989052534103394, acc: 0.5416666865348816)
[2025-02-17 10:54:39,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:39,462][root][INFO] - Training Epoch: 2/2, step 461/1149 completed (loss: 2.4356627464294434, acc: 0.3661971688270569)
[2025-02-17 10:54:39,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:39,885][root][INFO] - Training Epoch: 2/2, step 462/1149 completed (loss: 1.2760257720947266, acc: 0.6530612111091614)
[2025-02-17 10:54:40,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:40,231][root][INFO] - Training Epoch: 2/2, step 463/1149 completed (loss: 1.8724498748779297, acc: 0.46341463923454285)
[2025-02-17 10:54:40,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:40,594][root][INFO] - Training Epoch: 2/2, step 464/1149 completed (loss: 1.8608964681625366, acc: 0.5421686768531799)
[2025-02-17 10:54:40,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:41,042][root][INFO] - Training Epoch: 2/2, step 465/1149 completed (loss: 1.3152865171432495, acc: 0.6288659572601318)
[2025-02-17 10:54:41,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:41,500][root][INFO] - Training Epoch: 2/2, step 466/1149 completed (loss: 1.7546495199203491, acc: 0.5483871102333069)
[2025-02-17 10:54:41,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:41,976][root][INFO] - Training Epoch: 2/2, step 467/1149 completed (loss: 1.9264825582504272, acc: 0.527999997138977)
[2025-02-17 10:54:42,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:42,355][root][INFO] - Training Epoch: 2/2, step 468/1149 completed (loss: 1.8001757860183716, acc: 0.4615384638309479)
[2025-02-17 10:54:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:42,818][root][INFO] - Training Epoch: 2/2, step 469/1149 completed (loss: 1.533983588218689, acc: 0.5769230723381042)
[2025-02-17 10:54:42,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:43,160][root][INFO] - Training Epoch: 2/2, step 470/1149 completed (loss: 0.13164189457893372, acc: 1.0)
[2025-02-17 10:54:43,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:43,542][root][INFO] - Training Epoch: 2/2, step 471/1149 completed (loss: 1.2314754724502563, acc: 0.7272727489471436)
[2025-02-17 10:54:43,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:43,910][root][INFO] - Training Epoch: 2/2, step 472/1149 completed (loss: 0.9102883338928223, acc: 0.5833333134651184)
[2025-02-17 10:54:44,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:44,260][root][INFO] - Training Epoch: 2/2, step 473/1149 completed (loss: 0.9519777297973633, acc: 0.75)
[2025-02-17 10:54:44,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:44,597][root][INFO] - Training Epoch: 2/2, step 474/1149 completed (loss: 1.8230922222137451, acc: 0.6000000238418579)
[2025-02-17 10:54:44,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:44,932][root][INFO] - Training Epoch: 2/2, step 475/1149 completed (loss: 2.0387284755706787, acc: 0.5)
[2025-02-17 10:54:45,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:45,281][root][INFO] - Training Epoch: 2/2, step 476/1149 completed (loss: 0.7059655785560608, acc: 0.7142857313156128)
[2025-02-17 10:54:45,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:45,629][root][INFO] - Training Epoch: 2/2, step 477/1149 completed (loss: 1.1199272871017456, acc: 0.6499999761581421)
[2025-02-17 10:54:45,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:46,007][root][INFO] - Training Epoch: 2/2, step 478/1149 completed (loss: 1.0931710004806519, acc: 0.692307710647583)
[2025-02-17 10:54:46,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:46,426][root][INFO] - Training Epoch: 2/2, step 479/1149 completed (loss: 0.7992007732391357, acc: 0.6818181872367859)
[2025-02-17 10:54:46,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:46,876][root][INFO] - Training Epoch: 2/2, step 480/1149 completed (loss: 1.1071183681488037, acc: 0.7142857313156128)
[2025-02-17 10:54:47,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:47,293][root][INFO] - Training Epoch: 2/2, step 481/1149 completed (loss: 1.6148818731307983, acc: 0.6521739363670349)
[2025-02-17 10:54:47,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:47,734][root][INFO] - Training Epoch: 2/2, step 482/1149 completed (loss: 1.181656002998352, acc: 0.5652173757553101)
[2025-02-17 10:54:47,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:48,091][root][INFO] - Training Epoch: 2/2, step 483/1149 completed (loss: 1.6796499490737915, acc: 0.5714285969734192)
[2025-02-17 10:54:48,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:48,565][root][INFO] - Training Epoch: 2/2, step 484/1149 completed (loss: 1.5665087699890137, acc: 0.4838709533214569)
[2025-02-17 10:54:48,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:49,166][root][INFO] - Training Epoch: 2/2, step 485/1149 completed (loss: 1.9417396783828735, acc: 0.4516128897666931)
[2025-02-17 10:54:49,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:49,565][root][INFO] - Training Epoch: 2/2, step 486/1149 completed (loss: 1.4592350721359253, acc: 0.6000000238418579)
[2025-02-17 10:54:49,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:50,131][root][INFO] - Training Epoch: 2/2, step 487/1149 completed (loss: 1.3002448081970215, acc: 0.6551724076271057)
[2025-02-17 10:54:50,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:50,542][root][INFO] - Training Epoch: 2/2, step 488/1149 completed (loss: 0.512462317943573, acc: 0.875)
[2025-02-17 10:54:50,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:50,893][root][INFO] - Training Epoch: 2/2, step 489/1149 completed (loss: 0.1277669370174408, acc: 1.0)
[2025-02-17 10:54:51,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:51,304][root][INFO] - Training Epoch: 2/2, step 490/1149 completed (loss: 0.3213346302509308, acc: 0.9285714030265808)
[2025-02-17 10:54:51,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:51,617][root][INFO] - Training Epoch: 2/2, step 491/1149 completed (loss: 0.6134435534477234, acc: 0.75)
[2025-02-17 10:54:51,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:51,975][root][INFO] - Training Epoch: 2/2, step 492/1149 completed (loss: 1.513443112373352, acc: 0.7333333492279053)
[2025-02-17 10:54:52,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:52,363][root][INFO] - Training Epoch: 2/2, step 493/1149 completed (loss: 0.24301554262638092, acc: 0.9375)
[2025-02-17 10:54:52,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:52,756][root][INFO] - Training Epoch: 2/2, step 494/1149 completed (loss: 0.15073087811470032, acc: 1.0)
[2025-02-17 10:54:52,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:53,093][root][INFO] - Training Epoch: 2/2, step 495/1149 completed (loss: 1.3036689758300781, acc: 0.6000000238418579)
[2025-02-17 10:54:53,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:53,467][root][INFO] - Training Epoch: 2/2, step 496/1149 completed (loss: 0.907471776008606, acc: 0.807692289352417)
[2025-02-17 10:54:53,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:53,822][root][INFO] - Training Epoch: 2/2, step 497/1149 completed (loss: 0.8792371153831482, acc: 0.7272727489471436)
[2025-02-17 10:54:53,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:54,206][root][INFO] - Training Epoch: 2/2, step 498/1149 completed (loss: 0.9387223124504089, acc: 0.6315789222717285)
[2025-02-17 10:54:54,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:54,557][root][INFO] - Training Epoch: 2/2, step 499/1149 completed (loss: 0.6617764234542847, acc: 0.6111111044883728)
[2025-02-17 10:54:54,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:54,978][root][INFO] - Training Epoch: 2/2, step 500/1149 completed (loss: 0.2296612560749054, acc: 0.8999999761581421)
[2025-02-17 10:54:55,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:55,277][root][INFO] - Training Epoch: 2/2, step 501/1149 completed (loss: 0.4927605986595154, acc: 0.8888888955116272)
[2025-02-17 10:54:55,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:55,638][root][INFO] - Training Epoch: 2/2, step 502/1149 completed (loss: 0.8320769667625427, acc: 0.6666666865348816)
[2025-02-17 10:54:55,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:56,034][root][INFO] - Training Epoch: 2/2, step 503/1149 completed (loss: 0.5918048620223999, acc: 0.8695651888847351)
[2025-02-17 10:54:56,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:56,401][root][INFO] - Training Epoch: 2/2, step 504/1149 completed (loss: 0.3732962906360626, acc: 0.8518518805503845)
[2025-02-17 10:54:56,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:56,796][root][INFO] - Training Epoch: 2/2, step 505/1149 completed (loss: 0.2923068702220917, acc: 0.9285714030265808)
[2025-02-17 10:54:57,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:57,226][root][INFO] - Training Epoch: 2/2, step 506/1149 completed (loss: 0.27234208583831787, acc: 0.8888888955116272)
[2025-02-17 10:54:57,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:57,577][root][INFO] - Training Epoch: 2/2, step 507/1149 completed (loss: 0.4724199175834656, acc: 0.875)
[2025-02-17 10:54:57,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:57,943][root][INFO] - Training Epoch: 2/2, step 508/1149 completed (loss: 0.09292629361152649, acc: 1.0)
[2025-02-17 10:54:58,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:58,334][root][INFO] - Training Epoch: 2/2, step 509/1149 completed (loss: 0.05440803989768028, acc: 1.0)
[2025-02-17 10:54:58,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:58,752][root][INFO] - Training Epoch: 2/2, step 510/1149 completed (loss: 0.6856411099433899, acc: 0.7222222089767456)
[2025-02-17 10:54:58,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:59,167][root][INFO] - Training Epoch: 2/2, step 511/1149 completed (loss: 0.5353531837463379, acc: 0.7692307829856873)
[2025-02-17 10:54:59,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:59,518][root][INFO] - Training Epoch: 2/2, step 512/1149 completed (loss: 0.4593009948730469, acc: 0.8181818127632141)
[2025-02-17 10:54:59,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:54:59,878][root][INFO] - Training Epoch: 2/2, step 513/1149 completed (loss: 0.5625810623168945, acc: 0.9142857193946838)
[2025-02-17 10:55:00,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:00,224][root][INFO] - Training Epoch: 2/2, step 514/1149 completed (loss: 0.6782972812652588, acc: 0.7647058963775635)
[2025-02-17 10:55:00,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:00,565][root][INFO] - Training Epoch: 2/2, step 515/1149 completed (loss: 0.44410911202430725, acc: 0.8333333134651184)
[2025-02-17 10:55:00,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:00,914][root][INFO] - Training Epoch: 2/2, step 516/1149 completed (loss: 0.2373204082250595, acc: 0.9090909361839294)
[2025-02-17 10:55:01,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:01,266][root][INFO] - Training Epoch: 2/2, step 517/1149 completed (loss: 0.6648076176643372, acc: 0.8372092843055725)
[2025-02-17 10:55:01,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:01,820][root][INFO] - Training Epoch: 2/2, step 518/1149 completed (loss: 0.8982911109924316, acc: 0.7567567825317383)
[2025-02-17 10:55:01,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:02,159][root][INFO] - Training Epoch: 2/2, step 519/1149 completed (loss: 1.2060147523880005, acc: 0.75)
[2025-02-17 10:55:02,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:02,497][root][INFO] - Training Epoch: 2/2, step 520/1149 completed (loss: 0.4587400257587433, acc: 0.931034505367279)
[2025-02-17 10:55:02,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:03,076][root][INFO] - Training Epoch: 2/2, step 521/1149 completed (loss: 1.0728764533996582, acc: 0.6483516693115234)
[2025-02-17 10:55:03,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:03,433][root][INFO] - Training Epoch: 2/2, step 522/1149 completed (loss: 0.26250478625297546, acc: 0.9259259104728699)
[2025-02-17 10:55:03,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:03,763][root][INFO] - Training Epoch: 2/2, step 523/1149 completed (loss: 0.6582397222518921, acc: 0.7777777910232544)
[2025-02-17 10:55:03,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:04,105][root][INFO] - Training Epoch: 2/2, step 524/1149 completed (loss: 1.8690483570098877, acc: 0.47058823704719543)
[2025-02-17 10:55:04,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:04,444][root][INFO] - Training Epoch: 2/2, step 525/1149 completed (loss: 0.23408471047878265, acc: 0.8571428656578064)
[2025-02-17 10:55:04,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:04,780][root][INFO] - Training Epoch: 2/2, step 526/1149 completed (loss: 2.0164170265197754, acc: 0.5625)
[2025-02-17 10:55:04,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:05,200][root][INFO] - Training Epoch: 2/2, step 527/1149 completed (loss: 2.0403101444244385, acc: 0.49152541160583496)
[2025-02-17 10:55:05,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:05,565][root][INFO] - Training Epoch: 2/2, step 528/1149 completed (loss: 1.885926604270935, acc: 0.4444444477558136)
[2025-02-17 10:55:05,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:05,913][root][INFO] - Training Epoch: 2/2, step 529/1149 completed (loss: 0.10365783423185349, acc: 1.0)
[2025-02-17 10:55:06,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:06,696][root][INFO] - Training Epoch: 2/2, step 530/1149 completed (loss: 2.0711488723754883, acc: 0.38461539149284363)
[2025-02-17 10:55:06,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:07,053][root][INFO] - Training Epoch: 2/2, step 531/1149 completed (loss: 0.9819107055664062, acc: 0.6666666865348816)
[2025-02-17 10:55:07,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:07,405][root][INFO] - Training Epoch: 2/2, step 532/1149 completed (loss: 1.564464807510376, acc: 0.5333333611488342)
[2025-02-17 10:55:07,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:07,760][root][INFO] - Training Epoch: 2/2, step 533/1149 completed (loss: 1.6940734386444092, acc: 0.5454545617103577)
[2025-02-17 10:55:07,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:08,142][root][INFO] - Training Epoch: 2/2, step 534/1149 completed (loss: 1.6385678052902222, acc: 0.5686274766921997)
[2025-02-17 10:55:08,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:08,483][root][INFO] - Training Epoch: 2/2, step 535/1149 completed (loss: 0.6239986419677734, acc: 0.8260869383811951)
[2025-02-17 10:55:08,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:08,934][root][INFO] - Training Epoch: 2/2, step 536/1149 completed (loss: 1.4669419527053833, acc: 0.5952380895614624)
[2025-02-17 10:55:09,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:09,279][root][INFO] - Training Epoch: 2/2, step 537/1149 completed (loss: 0.08518184721469879, acc: 1.0)
[2025-02-17 10:55:09,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:09,612][root][INFO] - Training Epoch: 2/2, step 538/1149 completed (loss: 0.13360148668289185, acc: 1.0)
[2025-02-17 10:55:09,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:09,940][root][INFO] - Training Epoch: 2/2, step 539/1149 completed (loss: 0.3611927628517151, acc: 0.8888888955116272)
[2025-02-17 10:55:10,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:10,280][root][INFO] - Training Epoch: 2/2, step 540/1149 completed (loss: 0.21099179983139038, acc: 0.9090909361839294)
[2025-02-17 10:55:10,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:10,619][root][INFO] - Training Epoch: 2/2, step 541/1149 completed (loss: 0.7683752179145813, acc: 0.7272727489471436)
[2025-02-17 10:55:10,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:10,959][root][INFO] - Training Epoch: 2/2, step 542/1149 completed (loss: 0.7942603230476379, acc: 0.7368420958518982)
[2025-02-17 10:55:11,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:11,300][root][INFO] - Training Epoch: 2/2, step 543/1149 completed (loss: 0.04591932147741318, acc: 1.0)
[2025-02-17 10:55:11,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:11,646][root][INFO] - Training Epoch: 2/2, step 544/1149 completed (loss: 0.22295883297920227, acc: 0.9444444179534912)
[2025-02-17 10:55:11,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:11,980][root][INFO] - Training Epoch: 2/2, step 545/1149 completed (loss: 0.6888012290000916, acc: 0.8333333134651184)
[2025-02-17 10:55:12,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:12,318][root][INFO] - Training Epoch: 2/2, step 546/1149 completed (loss: 0.4047018885612488, acc: 0.9333333373069763)
[2025-02-17 10:55:12,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:12,700][root][INFO] - Training Epoch: 2/2, step 547/1149 completed (loss: 1.3641701936721802, acc: 0.6000000238418579)
[2025-02-17 10:55:12,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:13,051][root][INFO] - Training Epoch: 2/2, step 548/1149 completed (loss: 0.5514973402023315, acc: 0.8095238208770752)
[2025-02-17 10:55:13,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:13,438][root][INFO] - Training Epoch: 2/2, step 549/1149 completed (loss: 0.4408832788467407, acc: 0.8181818127632141)
[2025-02-17 10:55:13,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:13,801][root][INFO] - Training Epoch: 2/2, step 550/1149 completed (loss: 1.0532963275909424, acc: 0.8571428656578064)
[2025-02-17 10:55:13,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:14,164][root][INFO] - Training Epoch: 2/2, step 551/1149 completed (loss: 0.16023635864257812, acc: 0.9375)
[2025-02-17 10:55:14,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:14,501][root][INFO] - Training Epoch: 2/2, step 552/1149 completed (loss: 0.677453339099884, acc: 0.8823529481887817)
[2025-02-17 10:55:14,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:14,850][root][INFO] - Training Epoch: 2/2, step 553/1149 completed (loss: 0.14633744955062866, acc: 0.9166666865348816)
[2025-02-17 10:55:14,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:15,204][root][INFO] - Training Epoch: 2/2, step 554/1149 completed (loss: 0.6891672611236572, acc: 0.8181818127632141)
[2025-02-17 10:55:15,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:15,567][root][INFO] - Training Epoch: 2/2, step 555/1149 completed (loss: 0.47636857628822327, acc: 0.8571428656578064)
[2025-02-17 10:55:15,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:15,985][root][INFO] - Training Epoch: 2/2, step 556/1149 completed (loss: 0.8116810917854309, acc: 0.7368420958518982)
[2025-02-17 10:55:16,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:16,394][root][INFO] - Training Epoch: 2/2, step 557/1149 completed (loss: 1.951774001121521, acc: 0.4642857015132904)
[2025-02-17 10:55:16,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:16,826][root][INFO] - Training Epoch: 2/2, step 558/1149 completed (loss: 0.8485968112945557, acc: 0.75)
[2025-02-17 10:55:17,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:17,220][root][INFO] - Training Epoch: 2/2, step 559/1149 completed (loss: 0.6617868542671204, acc: 0.8125)
[2025-02-17 10:55:17,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:17,571][root][INFO] - Training Epoch: 2/2, step 560/1149 completed (loss: 0.3325512409210205, acc: 0.9200000166893005)
[2025-02-17 10:55:17,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:17,932][root][INFO] - Training Epoch: 2/2, step 561/1149 completed (loss: 0.531019389629364, acc: 0.7894737124443054)
[2025-02-17 10:55:18,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:18,339][root][INFO] - Training Epoch: 2/2, step 562/1149 completed (loss: 0.32537591457366943, acc: 0.9473684430122375)
[2025-02-17 10:55:18,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:18,787][root][INFO] - Training Epoch: 2/2, step 563/1149 completed (loss: 1.6216384172439575, acc: 0.609375)
[2025-02-17 10:55:18,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:19,178][root][INFO] - Training Epoch: 2/2, step 564/1149 completed (loss: 1.6600637435913086, acc: 0.5740740895271301)
[2025-02-17 10:55:19,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:19,543][root][INFO] - Training Epoch: 2/2, step 565/1149 completed (loss: 1.569924235343933, acc: 0.5740740895271301)
[2025-02-17 10:55:19,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:19,944][root][INFO] - Training Epoch: 2/2, step 566/1149 completed (loss: 0.7219336032867432, acc: 0.8421052694320679)
[2025-02-17 10:55:20,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:20,359][root][INFO] - Training Epoch: 2/2, step 567/1149 completed (loss: 1.2085719108581543, acc: 0.5789473652839661)
[2025-02-17 10:55:20,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:20,766][root][INFO] - Training Epoch: 2/2, step 568/1149 completed (loss: 1.6440372467041016, acc: 0.692307710647583)
[2025-02-17 10:55:20,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:21,174][root][INFO] - Training Epoch: 2/2, step 569/1149 completed (loss: 0.46219903230667114, acc: 0.8095238208770752)
[2025-02-17 10:55:21,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:21,568][root][INFO] - Training Epoch: 2/2, step 570/1149 completed (loss: 0.7572084069252014, acc: 0.7142857313156128)
[2025-02-17 10:55:21,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:21,944][root][INFO] - Training Epoch: 2/2, step 571/1149 completed (loss: 0.8160169124603271, acc: 0.8461538553237915)
[2025-02-17 10:55:22,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:22,286][root][INFO] - Training Epoch: 2/2, step 572/1149 completed (loss: 0.9299203753471375, acc: 0.7049180269241333)
[2025-02-17 10:55:23,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:23,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:23,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:24,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:24,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:25,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:25,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:26,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:26,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:27,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:27,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:27,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:28,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:29,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:29,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:29,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:30,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:30,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:31,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:31,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:32,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:32,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:33,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:33,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:34,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:34,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:35,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:35,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:36,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:36,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:37,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:37,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:38,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:38,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:39,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:39,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:40,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:40,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:41,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:42,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:42,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:43,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:43,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:44,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:44,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:45,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:45,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:46,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:46,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:47,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:47,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:48,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:48,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:49,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:49,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:50,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:50,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:51,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:51,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:52,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:52,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:52,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:53,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:53,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:54,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:55,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:55,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:56,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:56,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:57,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:57,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:58,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:58,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:59,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:59,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:55:59,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:00,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:00,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:01,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:01,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:02,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:03,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:03,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:03,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:04,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:04,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:05,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:05,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:06,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:06,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:07,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:07,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:08,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:08,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:09,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:09,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:10,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:10,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:11,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:11,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:12,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:12,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:13,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:13,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:14,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:14,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:15,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:15,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:16,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:16,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:16,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:17,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:17,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:18,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:18,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:19,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:19,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:20,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:20,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:21,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:21,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:22,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:23,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:23,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:24,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:24,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:24,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:25,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:25,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:26,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:26,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:27,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:27,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:28,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:28,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:29,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:30,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:30,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:31,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:31,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:32,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:32,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:32,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:33,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:33,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:34,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:34,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:35,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:35,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:36,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:36,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:37,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:37,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:38,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:38,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:39,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:39,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:40,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:40,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:41,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:41,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:42,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:42,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:43,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:43,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:44,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:44,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:45,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:45,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:46,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:47,031][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4635, device='cuda:0') eval_epoch_loss=tensor(0.9016, device='cuda:0') eval_epoch_acc=tensor(0.7618, device='cuda:0')
[2025-02-17 10:56:47,032][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 10:56:47,032][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 10:56:50,314][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_2_step_573_loss_0.9015860557556152/model.pt
[2025-02-17 10:56:50,326][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 10:56:50,328][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.9015860557556152
[2025-02-17 10:56:50,328][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.7617558240890503
[2025-02-17 10:56:50,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:50,745][root][INFO] - Training Epoch: 2/2, step 573/1149 completed (loss: 0.8449496626853943, acc: 0.7761194109916687)
[2025-02-17 10:56:50,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:51,145][root][INFO] - Training Epoch: 2/2, step 574/1149 completed (loss: 1.1165167093276978, acc: 0.7105262875556946)
[2025-02-17 10:56:51,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:51,531][root][INFO] - Training Epoch: 2/2, step 575/1149 completed (loss: 1.4039138555526733, acc: 0.5714285969734192)
[2025-02-17 10:56:51,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:51,869][root][INFO] - Training Epoch: 2/2, step 576/1149 completed (loss: 1.0267068147659302, acc: 0.7254902124404907)
[2025-02-17 10:56:52,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:52,238][root][INFO] - Training Epoch: 2/2, step 577/1149 completed (loss: 0.6905606985092163, acc: 0.824999988079071)
[2025-02-17 10:56:52,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:52,621][root][INFO] - Training Epoch: 2/2, step 578/1149 completed (loss: 1.1286848783493042, acc: 0.7123287916183472)
[2025-02-17 10:56:52,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:52,969][root][INFO] - Training Epoch: 2/2, step 579/1149 completed (loss: 1.0762133598327637, acc: 0.7159090638160706)
[2025-02-17 10:56:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:53,352][root][INFO] - Training Epoch: 2/2, step 580/1149 completed (loss: 1.3339500427246094, acc: 0.6615384817123413)
[2025-02-17 10:56:53,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:53,700][root][INFO] - Training Epoch: 2/2, step 581/1149 completed (loss: 1.0050472021102905, acc: 0.703125)
[2025-02-17 10:56:53,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:54,039][root][INFO] - Training Epoch: 2/2, step 582/1149 completed (loss: 0.43151411414146423, acc: 0.7777777910232544)
[2025-02-17 10:56:54,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:54,374][root][INFO] - Training Epoch: 2/2, step 583/1149 completed (loss: 0.3073502779006958, acc: 0.9230769276618958)
[2025-02-17 10:56:54,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:54,717][root][INFO] - Training Epoch: 2/2, step 584/1149 completed (loss: 1.0923340320587158, acc: 0.6296296119689941)
[2025-02-17 10:56:54,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:55,057][root][INFO] - Training Epoch: 2/2, step 585/1149 completed (loss: 1.7345296144485474, acc: 0.5333333611488342)
[2025-02-17 10:56:55,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:55,396][root][INFO] - Training Epoch: 2/2, step 586/1149 completed (loss: 0.4984298348426819, acc: 0.800000011920929)
[2025-02-17 10:56:55,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:55,758][root][INFO] - Training Epoch: 2/2, step 587/1149 completed (loss: 0.6423229575157166, acc: 0.8541666865348816)
[2025-02-17 10:56:55,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:56,236][root][INFO] - Training Epoch: 2/2, step 588/1149 completed (loss: 0.917931854724884, acc: 0.7906976938247681)
[2025-02-17 10:56:56,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:56,613][root][INFO] - Training Epoch: 2/2, step 589/1149 completed (loss: 0.4545535147190094, acc: 0.75)
[2025-02-17 10:56:56,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:56,995][root][INFO] - Training Epoch: 2/2, step 590/1149 completed (loss: 0.850528359413147, acc: 0.7250000238418579)
[2025-02-17 10:56:57,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:57,521][root][INFO] - Training Epoch: 2/2, step 591/1149 completed (loss: 1.293341040611267, acc: 0.6688311696052551)
[2025-02-17 10:56:57,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:57,894][root][INFO] - Training Epoch: 2/2, step 592/1149 completed (loss: 0.7589343786239624, acc: 0.800000011920929)
[2025-02-17 10:56:58,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:58,230][root][INFO] - Training Epoch: 2/2, step 593/1149 completed (loss: 1.298940658569336, acc: 0.6976743936538696)
[2025-02-17 10:56:58,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:58,588][root][INFO] - Training Epoch: 2/2, step 594/1149 completed (loss: 0.5749265551567078, acc: 0.8235294222831726)
[2025-02-17 10:56:58,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:58,933][root][INFO] - Training Epoch: 2/2, step 595/1149 completed (loss: 0.9348031878471375, acc: 0.699999988079071)
[2025-02-17 10:56:59,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:59,330][root][INFO] - Training Epoch: 2/2, step 596/1149 completed (loss: 1.0904498100280762, acc: 0.7407407164573669)
[2025-02-17 10:56:59,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:56:59,745][root][INFO] - Training Epoch: 2/2, step 597/1149 completed (loss: 1.342491865158081, acc: 0.6363636255264282)
[2025-02-17 10:56:59,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:00,096][root][INFO] - Training Epoch: 2/2, step 598/1149 completed (loss: 0.9948402643203735, acc: 0.7407407164573669)
[2025-02-17 10:57:00,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:00,450][root][INFO] - Training Epoch: 2/2, step 599/1149 completed (loss: 0.5842421054840088, acc: 0.8275862336158752)
[2025-02-17 10:57:00,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:00,854][root][INFO] - Training Epoch: 2/2, step 600/1149 completed (loss: 0.3606381416320801, acc: 0.8999999761581421)
[2025-02-17 10:57:01,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:01,263][root][INFO] - Training Epoch: 2/2, step 601/1149 completed (loss: 0.26330333948135376, acc: 1.0)
[2025-02-17 10:57:01,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:01,668][root][INFO] - Training Epoch: 2/2, step 602/1149 completed (loss: 0.5670192241668701, acc: 0.800000011920929)
[2025-02-17 10:57:01,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:02,041][root][INFO] - Training Epoch: 2/2, step 603/1149 completed (loss: 0.8356884717941284, acc: 0.7878788113594055)
[2025-02-17 10:57:02,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:02,446][root][INFO] - Training Epoch: 2/2, step 604/1149 completed (loss: 0.5310690999031067, acc: 0.75)
[2025-02-17 10:57:02,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:02,804][root][INFO] - Training Epoch: 2/2, step 605/1149 completed (loss: 0.12014434486627579, acc: 0.9512194991111755)
[2025-02-17 10:57:02,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:03,148][root][INFO] - Training Epoch: 2/2, step 606/1149 completed (loss: 0.12846194207668304, acc: 1.0)
[2025-02-17 10:57:03,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:03,490][root][INFO] - Training Epoch: 2/2, step 607/1149 completed (loss: 0.8677416443824768, acc: 0.7647058963775635)
[2025-02-17 10:57:03,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:03,834][root][INFO] - Training Epoch: 2/2, step 608/1149 completed (loss: 0.5036341547966003, acc: 0.800000011920929)
[2025-02-17 10:57:03,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:04,180][root][INFO] - Training Epoch: 2/2, step 609/1149 completed (loss: 1.2587006092071533, acc: 0.7058823704719543)
[2025-02-17 10:57:04,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:04,540][root][INFO] - Training Epoch: 2/2, step 610/1149 completed (loss: 0.4969250559806824, acc: 0.8684210777282715)
[2025-02-17 10:57:04,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:04,882][root][INFO] - Training Epoch: 2/2, step 611/1149 completed (loss: 1.0078399181365967, acc: 0.782608687877655)
[2025-02-17 10:57:05,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:05,278][root][INFO] - Training Epoch: 2/2, step 612/1149 completed (loss: 0.2611972391605377, acc: 0.9444444179534912)
[2025-02-17 10:57:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:05,625][root][INFO] - Training Epoch: 2/2, step 613/1149 completed (loss: 0.17898304760456085, acc: 0.9166666865348816)
[2025-02-17 10:57:05,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:05,977][root][INFO] - Training Epoch: 2/2, step 614/1149 completed (loss: 0.04454787075519562, acc: 1.0)
[2025-02-17 10:57:06,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:06,368][root][INFO] - Training Epoch: 2/2, step 615/1149 completed (loss: 0.20894303917884827, acc: 0.9090909361839294)
[2025-02-17 10:57:06,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:06,733][root][INFO] - Training Epoch: 2/2, step 616/1149 completed (loss: 1.0607759952545166, acc: 0.6944444179534912)
[2025-02-17 10:57:06,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:07,115][root][INFO] - Training Epoch: 2/2, step 617/1149 completed (loss: 0.4069707691669464, acc: 0.8787878751754761)
[2025-02-17 10:57:07,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:07,578][root][INFO] - Training Epoch: 2/2, step 618/1149 completed (loss: 1.011350393295288, acc: 0.7307692170143127)
[2025-02-17 10:57:07,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:07,999][root][INFO] - Training Epoch: 2/2, step 619/1149 completed (loss: 0.2848876118659973, acc: 0.8999999761581421)
[2025-02-17 10:57:08,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:08,382][root][INFO] - Training Epoch: 2/2, step 620/1149 completed (loss: 0.7686357498168945, acc: 0.7804877758026123)
[2025-02-17 10:57:08,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:08,814][root][INFO] - Training Epoch: 2/2, step 621/1149 completed (loss: 0.545887291431427, acc: 0.9047619104385376)
[2025-02-17 10:57:08,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:09,183][root][INFO] - Training Epoch: 2/2, step 622/1149 completed (loss: 0.9308620691299438, acc: 0.7272727489471436)
[2025-02-17 10:57:09,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:09,552][root][INFO] - Training Epoch: 2/2, step 623/1149 completed (loss: 1.05522882938385, acc: 0.6521739363670349)
[2025-02-17 10:57:09,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:09,975][root][INFO] - Training Epoch: 2/2, step 624/1149 completed (loss: 0.7495682835578918, acc: 0.8059701323509216)
[2025-02-17 10:57:10,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:10,386][root][INFO] - Training Epoch: 2/2, step 625/1149 completed (loss: 0.41929543018341064, acc: 0.8709677457809448)
[2025-02-17 10:57:10,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:10,734][root][INFO] - Training Epoch: 2/2, step 626/1149 completed (loss: 0.035318125039339066, acc: 1.0)
[2025-02-17 10:57:10,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:11,074][root][INFO] - Training Epoch: 2/2, step 627/1149 completed (loss: 0.16682419180870056, acc: 0.9375)
[2025-02-17 10:57:11,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:11,408][root][INFO] - Training Epoch: 2/2, step 628/1149 completed (loss: 0.123235784471035, acc: 0.9230769276618958)
[2025-02-17 10:57:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:11,748][root][INFO] - Training Epoch: 2/2, step 629/1149 completed (loss: 1.232305884361267, acc: 0.7272727489471436)
[2025-02-17 10:57:11,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:12,080][root][INFO] - Training Epoch: 2/2, step 630/1149 completed (loss: 0.06739849597215652, acc: 1.0)
[2025-02-17 10:57:12,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:12,496][root][INFO] - Training Epoch: 2/2, step 631/1149 completed (loss: 0.942093551158905, acc: 0.7777777910232544)
[2025-02-17 10:57:12,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:12,861][root][INFO] - Training Epoch: 2/2, step 632/1149 completed (loss: 0.15823549032211304, acc: 0.9333333373069763)
[2025-02-17 10:57:13,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:13,204][root][INFO] - Training Epoch: 2/2, step 633/1149 completed (loss: 0.04588207229971886, acc: 1.0)
[2025-02-17 10:57:13,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:13,613][root][INFO] - Training Epoch: 2/2, step 634/1149 completed (loss: 0.8152630925178528, acc: 0.7674418687820435)
[2025-02-17 10:57:13,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:13,961][root][INFO] - Training Epoch: 2/2, step 635/1149 completed (loss: 0.8626105785369873, acc: 0.7916666865348816)
[2025-02-17 10:57:14,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:14,299][root][INFO] - Training Epoch: 2/2, step 636/1149 completed (loss: 0.04619069769978523, acc: 1.0)
[2025-02-17 10:57:14,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:14,640][root][INFO] - Training Epoch: 2/2, step 637/1149 completed (loss: 0.5655209422111511, acc: 0.8433734774589539)
[2025-02-17 10:57:14,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:14,975][root][INFO] - Training Epoch: 2/2, step 638/1149 completed (loss: 0.7807504534721375, acc: 0.75)
[2025-02-17 10:57:15,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:15,310][root][INFO] - Training Epoch: 2/2, step 639/1149 completed (loss: 0.4173552095890045, acc: 0.8571428656578064)
[2025-02-17 10:57:15,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:15,655][root][INFO] - Training Epoch: 2/2, step 640/1149 completed (loss: 0.5684120655059814, acc: 0.8571428656578064)
[2025-02-17 10:57:15,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:15,989][root][INFO] - Training Epoch: 2/2, step 641/1149 completed (loss: 0.45600515604019165, acc: 0.800000011920929)
[2025-02-17 10:57:16,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:16,327][root][INFO] - Training Epoch: 2/2, step 642/1149 completed (loss: 0.19514861702919006, acc: 0.9090909361839294)
[2025-02-17 10:57:16,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:16,659][root][INFO] - Training Epoch: 2/2, step 643/1149 completed (loss: 0.020568275824189186, acc: 1.0)
[2025-02-17 10:57:16,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:16,998][root][INFO] - Training Epoch: 2/2, step 644/1149 completed (loss: 1.6493483781814575, acc: 0.3333333432674408)
[2025-02-17 10:57:17,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:17,342][root][INFO] - Training Epoch: 2/2, step 645/1149 completed (loss: 1.6281670331954956, acc: 0.5555555820465088)
[2025-02-17 10:57:17,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:17,680][root][INFO] - Training Epoch: 2/2, step 646/1149 completed (loss: 2.6295695304870605, acc: 0.2083333283662796)
[2025-02-17 10:57:17,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:18,027][root][INFO] - Training Epoch: 2/2, step 647/1149 completed (loss: 2.623988151550293, acc: 0.3636363744735718)
[2025-02-17 10:57:18,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:18,386][root][INFO] - Training Epoch: 2/2, step 648/1149 completed (loss: 1.9052796363830566, acc: 0.5)
[2025-02-17 10:57:18,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:18,734][root][INFO] - Training Epoch: 2/2, step 649/1149 completed (loss: 1.7087385654449463, acc: 0.46666666865348816)
[2025-02-17 10:57:18,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:19,069][root][INFO] - Training Epoch: 2/2, step 650/1149 completed (loss: 2.2165422439575195, acc: 0.4333333373069763)
[2025-02-17 10:57:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:19,407][root][INFO] - Training Epoch: 2/2, step 651/1149 completed (loss: 2.8041510581970215, acc: 0.3636363744735718)
[2025-02-17 10:57:19,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:19,830][root][INFO] - Training Epoch: 2/2, step 652/1149 completed (loss: 1.4808309078216553, acc: 0.6153846383094788)
[2025-02-17 10:57:19,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:20,175][root][INFO] - Training Epoch: 2/2, step 653/1149 completed (loss: 1.8970417976379395, acc: 0.5600000023841858)
[2025-02-17 10:57:20,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:20,516][root][INFO] - Training Epoch: 2/2, step 654/1149 completed (loss: 1.4320100545883179, acc: 0.7142857313156128)
[2025-02-17 10:57:20,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:20,866][root][INFO] - Training Epoch: 2/2, step 655/1149 completed (loss: 0.7272083759307861, acc: 0.75)
[2025-02-17 10:57:21,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:21,202][root][INFO] - Training Epoch: 2/2, step 656/1149 completed (loss: 0.46707862615585327, acc: 0.8571428656578064)
[2025-02-17 10:57:21,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:21,554][root][INFO] - Training Epoch: 2/2, step 657/1149 completed (loss: 0.11433694511651993, acc: 1.0)
[2025-02-17 10:57:21,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:21,890][root][INFO] - Training Epoch: 2/2, step 658/1149 completed (loss: 1.3983222246170044, acc: 0.5833333134651184)
[2025-02-17 10:57:22,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:22,233][root][INFO] - Training Epoch: 2/2, step 659/1149 completed (loss: 0.31023845076560974, acc: 1.0)
[2025-02-17 10:57:22,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:22,582][root][INFO] - Training Epoch: 2/2, step 660/1149 completed (loss: 0.05192556977272034, acc: 1.0)
[2025-02-17 10:57:22,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:22,950][root][INFO] - Training Epoch: 2/2, step 661/1149 completed (loss: 0.2919715344905853, acc: 1.0)
[2025-02-17 10:57:23,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:23,298][root][INFO] - Training Epoch: 2/2, step 662/1149 completed (loss: 0.872864842414856, acc: 0.7575757503509521)
[2025-02-17 10:57:23,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:23,645][root][INFO] - Training Epoch: 2/2, step 663/1149 completed (loss: 0.6447501182556152, acc: 0.8275862336158752)
[2025-02-17 10:57:23,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:23,986][root][INFO] - Training Epoch: 2/2, step 664/1149 completed (loss: 0.45635494589805603, acc: 0.9166666865348816)
[2025-02-17 10:57:24,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:24,361][root][INFO] - Training Epoch: 2/2, step 665/1149 completed (loss: 0.43669772148132324, acc: 0.8888888955116272)
[2025-02-17 10:57:24,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:24,708][root][INFO] - Training Epoch: 2/2, step 666/1149 completed (loss: 1.143644094467163, acc: 0.7777777910232544)
[2025-02-17 10:57:24,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:25,064][root][INFO] - Training Epoch: 2/2, step 667/1149 completed (loss: 0.6296815872192383, acc: 0.8571428656578064)
[2025-02-17 10:57:25,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:25,454][root][INFO] - Training Epoch: 2/2, step 668/1149 completed (loss: 0.3174782395362854, acc: 1.0)
[2025-02-17 10:57:25,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:25,813][root][INFO] - Training Epoch: 2/2, step 669/1149 completed (loss: 0.23661144077777863, acc: 0.9333333373069763)
[2025-02-17 10:57:25,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:26,172][root][INFO] - Training Epoch: 2/2, step 670/1149 completed (loss: 1.2020014524459839, acc: 0.5833333134651184)
[2025-02-17 10:57:26,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:26,546][root][INFO] - Training Epoch: 2/2, step 671/1149 completed (loss: 0.41293150186538696, acc: 0.8571428656578064)
[2025-02-17 10:57:26,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:26,890][root][INFO] - Training Epoch: 2/2, step 672/1149 completed (loss: 0.6005449891090393, acc: 0.8181818127632141)
[2025-02-17 10:57:27,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:27,246][root][INFO] - Training Epoch: 2/2, step 673/1149 completed (loss: 1.6054216623306274, acc: 0.6153846383094788)
[2025-02-17 10:57:27,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:27,617][root][INFO] - Training Epoch: 2/2, step 674/1149 completed (loss: 1.895032286643982, acc: 0.5322580933570862)
[2025-02-17 10:57:27,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:27,957][root][INFO] - Training Epoch: 2/2, step 675/1149 completed (loss: 1.6280947923660278, acc: 0.6000000238418579)
[2025-02-17 10:57:28,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:28,313][root][INFO] - Training Epoch: 2/2, step 676/1149 completed (loss: 1.5720432996749878, acc: 0.5714285969734192)
[2025-02-17 10:57:28,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:28,671][root][INFO] - Training Epoch: 2/2, step 677/1149 completed (loss: 1.862151026725769, acc: 0.5333333611488342)
[2025-02-17 10:57:28,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:28,990][root][INFO] - Training Epoch: 2/2, step 678/1149 completed (loss: 2.0363526344299316, acc: 0.5538461804389954)
[2025-02-17 10:57:29,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:29,336][root][INFO] - Training Epoch: 2/2, step 679/1149 completed (loss: 0.7016471028327942, acc: 0.7777777910232544)
[2025-02-17 10:57:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:29,707][root][INFO] - Training Epoch: 2/2, step 680/1149 completed (loss: 0.210784450173378, acc: 0.8888888955116272)
[2025-02-17 10:57:29,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:30,056][root][INFO] - Training Epoch: 2/2, step 681/1149 completed (loss: 0.4199322760105133, acc: 0.8571428656578064)
[2025-02-17 10:57:30,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:30,408][root][INFO] - Training Epoch: 2/2, step 682/1149 completed (loss: 1.7508729696273804, acc: 0.6000000238418579)
[2025-02-17 10:57:30,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:30,747][root][INFO] - Training Epoch: 2/2, step 683/1149 completed (loss: 0.3561326563358307, acc: 0.8947368264198303)
[2025-02-17 10:57:30,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:31,095][root][INFO] - Training Epoch: 2/2, step 684/1149 completed (loss: 1.0142195224761963, acc: 0.8181818127632141)
[2025-02-17 10:57:31,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:31,445][root][INFO] - Training Epoch: 2/2, step 685/1149 completed (loss: 1.6275858879089355, acc: 0.6200000047683716)
[2025-02-17 10:57:31,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:31,783][root][INFO] - Training Epoch: 2/2, step 686/1149 completed (loss: 0.9072437286376953, acc: 0.7222222089767456)
[2025-02-17 10:57:31,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:32,117][root][INFO] - Training Epoch: 2/2, step 687/1149 completed (loss: 0.09501457214355469, acc: 0.9411764740943909)
[2025-02-17 10:57:32,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:32,455][root][INFO] - Training Epoch: 2/2, step 688/1149 completed (loss: 0.683804988861084, acc: 0.7804877758026123)
[2025-02-17 10:57:32,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:32,785][root][INFO] - Training Epoch: 2/2, step 689/1149 completed (loss: 0.42254218459129333, acc: 0.8947368264198303)
[2025-02-17 10:57:32,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:33,125][root][INFO] - Training Epoch: 2/2, step 690/1149 completed (loss: 0.9349980354309082, acc: 0.7777777910232544)
[2025-02-17 10:57:33,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:33,460][root][INFO] - Training Epoch: 2/2, step 691/1149 completed (loss: 0.5723074674606323, acc: 0.875)
[2025-02-17 10:57:33,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:33,798][root][INFO] - Training Epoch: 2/2, step 692/1149 completed (loss: 1.0974335670471191, acc: 0.6875)
[2025-02-17 10:57:33,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:34,154][root][INFO] - Training Epoch: 2/2, step 693/1149 completed (loss: 1.5343717336654663, acc: 0.6666666865348816)
[2025-02-17 10:57:34,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:34,533][root][INFO] - Training Epoch: 2/2, step 694/1149 completed (loss: 0.6786264777183533, acc: 0.9166666865348816)
[2025-02-17 10:57:34,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:34,883][root][INFO] - Training Epoch: 2/2, step 695/1149 completed (loss: 0.5638001561164856, acc: 0.75)
[2025-02-17 10:57:35,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:35,217][root][INFO] - Training Epoch: 2/2, step 696/1149 completed (loss: 0.1863536238670349, acc: 1.0)
[2025-02-17 10:57:35,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:35,554][root][INFO] - Training Epoch: 2/2, step 697/1149 completed (loss: 0.45482414960861206, acc: 0.8571428656578064)
[2025-02-17 10:57:35,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:35,939][root][INFO] - Training Epoch: 2/2, step 698/1149 completed (loss: 1.3373759984970093, acc: 0.6666666865348816)
[2025-02-17 10:57:36,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:36,281][root][INFO] - Training Epoch: 2/2, step 699/1149 completed (loss: 0.10978590697050095, acc: 1.0)
[2025-02-17 10:57:36,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:36,627][root][INFO] - Training Epoch: 2/2, step 700/1149 completed (loss: 1.9583144187927246, acc: 0.5199999809265137)
[2025-02-17 10:57:36,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:36,974][root][INFO] - Training Epoch: 2/2, step 701/1149 completed (loss: 0.7100448608398438, acc: 0.6666666865348816)
[2025-02-17 10:57:37,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:37,319][root][INFO] - Training Epoch: 2/2, step 702/1149 completed (loss: 0.7768934369087219, acc: 0.800000011920929)
[2025-02-17 10:57:37,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:37,672][root][INFO] - Training Epoch: 2/2, step 703/1149 completed (loss: 0.9970389604568481, acc: 0.7777777910232544)
[2025-02-17 10:57:37,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:38,026][root][INFO] - Training Epoch: 2/2, step 704/1149 completed (loss: 0.996745765209198, acc: 0.75)
[2025-02-17 10:57:38,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:38,404][root][INFO] - Training Epoch: 2/2, step 705/1149 completed (loss: 1.755276083946228, acc: 0.5151515007019043)
[2025-02-17 10:57:38,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:38,826][root][INFO] - Training Epoch: 2/2, step 706/1149 completed (loss: 0.2914758026599884, acc: 0.8666666746139526)
[2025-02-17 10:57:38,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:39,192][root][INFO] - Training Epoch: 2/2, step 707/1149 completed (loss: 0.24063365161418915, acc: 0.875)
[2025-02-17 10:57:39,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:39,548][root][INFO] - Training Epoch: 2/2, step 708/1149 completed (loss: 1.4971022605895996, acc: 0.6666666865348816)
[2025-02-17 10:57:39,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:39,893][root][INFO] - Training Epoch: 2/2, step 709/1149 completed (loss: 1.424062967300415, acc: 0.5714285969734192)
[2025-02-17 10:57:40,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:40,247][root][INFO] - Training Epoch: 2/2, step 710/1149 completed (loss: 1.4115314483642578, acc: 0.6176470518112183)
[2025-02-17 10:57:40,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:40,616][root][INFO] - Training Epoch: 2/2, step 711/1149 completed (loss: 1.5593693256378174, acc: 0.5964912176132202)
[2025-02-17 10:57:40,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:41,094][root][INFO] - Training Epoch: 2/2, step 712/1149 completed (loss: 1.08516263961792, acc: 0.7105262875556946)
[2025-02-17 10:57:41,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:41,449][root][INFO] - Training Epoch: 2/2, step 713/1149 completed (loss: 1.0992496013641357, acc: 0.8205128312110901)
[2025-02-17 10:57:41,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:41,793][root][INFO] - Training Epoch: 2/2, step 714/1149 completed (loss: 1.1294599771499634, acc: 0.6326530575752258)
[2025-02-17 10:57:41,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:42,158][root][INFO] - Training Epoch: 2/2, step 715/1149 completed (loss: 1.286868929862976, acc: 0.6279069781303406)
[2025-02-17 10:57:42,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:42,493][root][INFO] - Training Epoch: 2/2, step 716/1149 completed (loss: 1.2539503574371338, acc: 0.6190476417541504)
[2025-02-17 10:57:42,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:42,854][root][INFO] - Training Epoch: 2/2, step 717/1149 completed (loss: 1.0114352703094482, acc: 0.7142857313156128)
[2025-02-17 10:57:42,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:43,205][root][INFO] - Training Epoch: 2/2, step 718/1149 completed (loss: 0.20524735748767853, acc: 0.9090909361839294)
[2025-02-17 10:57:43,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:43,544][root][INFO] - Training Epoch: 2/2, step 719/1149 completed (loss: 0.025612618774175644, acc: 1.0)
[2025-02-17 10:57:43,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:43,883][root][INFO] - Training Epoch: 2/2, step 720/1149 completed (loss: 0.3490605056285858, acc: 0.9285714030265808)
[2025-02-17 10:57:44,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:44,225][root][INFO] - Training Epoch: 2/2, step 721/1149 completed (loss: 0.820544958114624, acc: 0.8333333134651184)
[2025-02-17 10:57:44,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:44,569][root][INFO] - Training Epoch: 2/2, step 722/1149 completed (loss: 1.079896330833435, acc: 0.6818181872367859)
[2025-02-17 10:57:44,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:44,990][root][INFO] - Training Epoch: 2/2, step 723/1149 completed (loss: 1.1293562650680542, acc: 0.6842105388641357)
[2025-02-17 10:57:45,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:45,366][root][INFO] - Training Epoch: 2/2, step 724/1149 completed (loss: 1.8702812194824219, acc: 0.5)
[2025-02-17 10:57:45,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:45,721][root][INFO] - Training Epoch: 2/2, step 725/1149 completed (loss: 0.3542148172855377, acc: 0.8799999952316284)
[2025-02-17 10:57:45,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:46,058][root][INFO] - Training Epoch: 2/2, step 726/1149 completed (loss: 0.3890399634838104, acc: 0.8999999761581421)
[2025-02-17 10:57:46,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:46,406][root][INFO] - Training Epoch: 2/2, step 727/1149 completed (loss: 0.9512246251106262, acc: 0.7272727489471436)
[2025-02-17 10:57:46,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:46,785][root][INFO] - Training Epoch: 2/2, step 728/1149 completed (loss: 0.3537328839302063, acc: 0.8999999761581421)
[2025-02-17 10:57:46,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:47,130][root][INFO] - Training Epoch: 2/2, step 729/1149 completed (loss: 0.8683696985244751, acc: 0.7419354915618896)
[2025-02-17 10:57:47,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:47,469][root][INFO] - Training Epoch: 2/2, step 730/1149 completed (loss: 0.4790375828742981, acc: 0.8636363744735718)
[2025-02-17 10:57:47,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:47,801][root][INFO] - Training Epoch: 2/2, step 731/1149 completed (loss: 0.4013528525829315, acc: 0.9090909361839294)
[2025-02-17 10:57:47,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:48,132][root][INFO] - Training Epoch: 2/2, step 732/1149 completed (loss: 0.06614191830158234, acc: 1.0)
[2025-02-17 10:57:48,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:48,471][root][INFO] - Training Epoch: 2/2, step 733/1149 completed (loss: 0.27745524048805237, acc: 0.9090909361839294)
[2025-02-17 10:57:48,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:48,808][root][INFO] - Training Epoch: 2/2, step 734/1149 completed (loss: 0.16197824478149414, acc: 0.9230769276618958)
[2025-02-17 10:57:48,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:49,163][root][INFO] - Training Epoch: 2/2, step 735/1149 completed (loss: 0.06772011518478394, acc: 1.0)
[2025-02-17 10:57:49,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:49,556][root][INFO] - Training Epoch: 2/2, step 736/1149 completed (loss: 0.08694946765899658, acc: 1.0)
[2025-02-17 10:57:49,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:49,896][root][INFO] - Training Epoch: 2/2, step 737/1149 completed (loss: 0.45727282762527466, acc: 0.7857142686843872)
[2025-02-17 10:57:50,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:50,237][root][INFO] - Training Epoch: 2/2, step 738/1149 completed (loss: 0.36700937151908875, acc: 0.9090909361839294)
[2025-02-17 10:57:50,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:50,578][root][INFO] - Training Epoch: 2/2, step 739/1149 completed (loss: 1.1486896276474, acc: 0.6190476417541504)
[2025-02-17 10:57:50,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:51,161][root][INFO] - Training Epoch: 2/2, step 740/1149 completed (loss: 1.3678356409072876, acc: 0.6379310488700867)
[2025-02-17 10:57:51,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:51,629][root][INFO] - Training Epoch: 2/2, step 741/1149 completed (loss: 0.5928307175636292, acc: 0.8571428656578064)
[2025-02-17 10:57:52,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:52,414][root][INFO] - Training Epoch: 2/2, step 742/1149 completed (loss: 0.7755934596061707, acc: 0.800000011920929)
[2025-02-17 10:57:52,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:52,775][root][INFO] - Training Epoch: 2/2, step 743/1149 completed (loss: 0.8468679189682007, acc: 0.8333333134651184)
[2025-02-17 10:57:52,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:53,212][root][INFO] - Training Epoch: 2/2, step 744/1149 completed (loss: 0.7333731651306152, acc: 0.795918345451355)
[2025-02-17 10:57:53,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:53,615][root][INFO] - Training Epoch: 2/2, step 745/1149 completed (loss: 0.5404371619224548, acc: 0.8292682766914368)
[2025-02-17 10:57:53,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:53,965][root][INFO] - Training Epoch: 2/2, step 746/1149 completed (loss: 0.4804524779319763, acc: 0.9230769276618958)
[2025-02-17 10:57:54,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:54,304][root][INFO] - Training Epoch: 2/2, step 747/1149 completed (loss: 0.37406301498413086, acc: 0.8823529481887817)
[2025-02-17 10:57:54,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:54,670][root][INFO] - Training Epoch: 2/2, step 748/1149 completed (loss: 0.6207126975059509, acc: 0.8148148059844971)
[2025-02-17 10:57:54,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:54,971][root][INFO] - Training Epoch: 2/2, step 749/1149 completed (loss: 0.017934923991560936, acc: 1.0)
[2025-02-17 10:57:55,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:55,331][root][INFO] - Training Epoch: 2/2, step 750/1149 completed (loss: 0.06635347008705139, acc: 1.0)
[2025-02-17 10:57:55,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:55,709][root][INFO] - Training Epoch: 2/2, step 751/1149 completed (loss: 0.027455192059278488, acc: 1.0)
[2025-02-17 10:57:55,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:56,081][root][INFO] - Training Epoch: 2/2, step 752/1149 completed (loss: 0.0236049834638834, acc: 1.0)
[2025-02-17 10:57:56,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:56,415][root][INFO] - Training Epoch: 2/2, step 753/1149 completed (loss: 0.3024637997150421, acc: 0.9090909361839294)
[2025-02-17 10:57:56,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:56,764][root][INFO] - Training Epoch: 2/2, step 754/1149 completed (loss: 1.0415160655975342, acc: 0.800000011920929)
[2025-02-17 10:57:56,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:57,160][root][INFO] - Training Epoch: 2/2, step 755/1149 completed (loss: 0.869598388671875, acc: 0.7575757503509521)
[2025-02-17 10:57:57,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:57,555][root][INFO] - Training Epoch: 2/2, step 756/1149 completed (loss: 0.26206251978874207, acc: 0.9444444179534912)
[2025-02-17 10:57:57,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:57,958][root][INFO] - Training Epoch: 2/2, step 757/1149 completed (loss: 0.24459834396839142, acc: 0.9756097793579102)
[2025-02-17 10:57:58,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:58,386][root][INFO] - Training Epoch: 2/2, step 758/1149 completed (loss: 0.9670544862747192, acc: 0.7093023061752319)
[2025-02-17 10:57:58,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:58,935][root][INFO] - Training Epoch: 2/2, step 759/1149 completed (loss: 0.3620859682559967, acc: 0.8765432238578796)
[2025-02-17 10:57:59,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:59,400][root][INFO] - Training Epoch: 2/2, step 760/1149 completed (loss: 0.3417870104312897, acc: 0.84375)
[2025-02-17 10:57:59,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:57:59,812][root][INFO] - Training Epoch: 2/2, step 761/1149 completed (loss: 0.7564299702644348, acc: 0.8115941882133484)
[2025-02-17 10:57:59,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:00,169][root][INFO] - Training Epoch: 2/2, step 762/1149 completed (loss: 1.275707483291626, acc: 0.7222222089767456)
[2025-02-17 10:58:00,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:01,166][root][INFO] - Training Epoch: 2/2, step 763/1149 completed (loss: 0.5996562838554382, acc: 0.8270676732063293)
[2025-02-17 10:58:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:01,737][root][INFO] - Training Epoch: 2/2, step 764/1149 completed (loss: 0.2475762963294983, acc: 0.9367088675498962)
[2025-02-17 10:58:01,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:02,157][root][INFO] - Training Epoch: 2/2, step 765/1149 completed (loss: 0.613979160785675, acc: 0.875)
[2025-02-17 10:58:02,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:02,551][root][INFO] - Training Epoch: 2/2, step 766/1149 completed (loss: 0.07695145159959793, acc: 1.0)
[2025-02-17 10:58:02,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:02,902][root][INFO] - Training Epoch: 2/2, step 767/1149 completed (loss: 0.9297415614128113, acc: 0.7058823704719543)
[2025-02-17 10:58:03,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:03,268][root][INFO] - Training Epoch: 2/2, step 768/1149 completed (loss: 0.31930917501449585, acc: 0.875)
[2025-02-17 10:58:03,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:03,665][root][INFO] - Training Epoch: 2/2, step 769/1149 completed (loss: 0.4038972854614258, acc: 0.9166666865348816)
[2025-02-17 10:58:03,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:04,028][root][INFO] - Training Epoch: 2/2, step 770/1149 completed (loss: 0.3237106204032898, acc: 0.9333333373069763)
[2025-02-17 10:58:04,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:04,421][root][INFO] - Training Epoch: 2/2, step 771/1149 completed (loss: 0.35133805871009827, acc: 0.8823529481887817)
[2025-02-17 10:58:04,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:04,807][root][INFO] - Training Epoch: 2/2, step 772/1149 completed (loss: 0.110018290579319, acc: 1.0)
[2025-02-17 10:58:04,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:05,154][root][INFO] - Training Epoch: 2/2, step 773/1149 completed (loss: 0.19589506089687347, acc: 0.9583333134651184)
[2025-02-17 10:58:05,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:05,536][root][INFO] - Training Epoch: 2/2, step 774/1149 completed (loss: 0.0811675563454628, acc: 1.0)
[2025-02-17 10:58:05,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:05,918][root][INFO] - Training Epoch: 2/2, step 775/1149 completed (loss: 0.21090632677078247, acc: 0.9230769276618958)
[2025-02-17 10:58:06,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:06,310][root][INFO] - Training Epoch: 2/2, step 776/1149 completed (loss: 0.12011681497097015, acc: 1.0)
[2025-02-17 10:58:06,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:06,694][root][INFO] - Training Epoch: 2/2, step 777/1149 completed (loss: 0.17161202430725098, acc: 0.9166666865348816)
[2025-02-17 10:58:06,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:07,030][root][INFO] - Training Epoch: 2/2, step 778/1149 completed (loss: 0.3225174844264984, acc: 0.9166666865348816)
[2025-02-17 10:58:07,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:07,367][root][INFO] - Training Epoch: 2/2, step 779/1149 completed (loss: 0.06417564302682877, acc: 1.0)
[2025-02-17 10:58:07,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:07,720][root][INFO] - Training Epoch: 2/2, step 780/1149 completed (loss: 0.00786078255623579, acc: 1.0)
[2025-02-17 10:58:07,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:08,080][root][INFO] - Training Epoch: 2/2, step 781/1149 completed (loss: 1.705636978149414, acc: 0.5555555820465088)
[2025-02-17 10:58:08,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:08,442][root][INFO] - Training Epoch: 2/2, step 782/1149 completed (loss: 1.7316703796386719, acc: 0.692307710647583)
[2025-02-17 10:58:08,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:08,791][root][INFO] - Training Epoch: 2/2, step 783/1149 completed (loss: 1.6929484605789185, acc: 0.5714285969734192)
[2025-02-17 10:58:08,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:09,187][root][INFO] - Training Epoch: 2/2, step 784/1149 completed (loss: 1.9787484407424927, acc: 0.5510203838348389)
[2025-02-17 10:58:09,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:09,549][root][INFO] - Training Epoch: 2/2, step 785/1149 completed (loss: 1.2464146614074707, acc: 0.7592592835426331)
[2025-02-17 10:58:09,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:09,959][root][INFO] - Training Epoch: 2/2, step 786/1149 completed (loss: 1.2140153646469116, acc: 0.707317054271698)
[2025-02-17 10:58:10,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:10,484][root][INFO] - Training Epoch: 2/2, step 787/1149 completed (loss: 1.4535611867904663, acc: 0.6315789222717285)
[2025-02-17 10:58:10,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:10,882][root][INFO] - Training Epoch: 2/2, step 788/1149 completed (loss: 1.711664080619812, acc: 0.5925925970077515)
[2025-02-17 10:58:11,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:11,237][root][INFO] - Training Epoch: 2/2, step 789/1149 completed (loss: 1.4958722591400146, acc: 0.6086956262588501)
[2025-02-17 10:58:11,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:11,632][root][INFO] - Training Epoch: 2/2, step 790/1149 completed (loss: 1.3157780170440674, acc: 0.6804123520851135)
[2025-02-17 10:58:11,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:12,085][root][INFO] - Training Epoch: 2/2, step 791/1149 completed (loss: 0.8696639537811279, acc: 0.7872340679168701)
[2025-02-17 10:58:12,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:12,471][root][INFO] - Training Epoch: 2/2, step 792/1149 completed (loss: 0.33929744362831116, acc: 0.9583333134651184)
[2025-02-17 10:58:12,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:12,874][root][INFO] - Training Epoch: 2/2, step 793/1149 completed (loss: 1.7222322225570679, acc: 0.5789473652839661)
[2025-02-17 10:58:13,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:13,228][root][INFO] - Training Epoch: 2/2, step 794/1149 completed (loss: 0.02764763869345188, acc: 1.0)
[2025-02-17 10:58:13,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:13,581][root][INFO] - Training Epoch: 2/2, step 795/1149 completed (loss: 0.543103814125061, acc: 0.800000011920929)
[2025-02-17 10:58:13,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:13,939][root][INFO] - Training Epoch: 2/2, step 796/1149 completed (loss: 0.43655475974082947, acc: 0.875)
[2025-02-17 10:58:14,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:14,320][root][INFO] - Training Epoch: 2/2, step 797/1149 completed (loss: 0.7761769890785217, acc: 0.8148148059844971)
[2025-02-17 10:58:14,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:14,680][root][INFO] - Training Epoch: 2/2, step 798/1149 completed (loss: 0.09113819152116776, acc: 1.0)
[2025-02-17 10:58:14,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:15,050][root][INFO] - Training Epoch: 2/2, step 799/1149 completed (loss: 0.9127366542816162, acc: 0.8666666746139526)
[2025-02-17 10:58:15,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:15,660][root][INFO] - Training Epoch: 2/2, step 800/1149 completed (loss: 1.0286425352096558, acc: 0.6730769276618958)
[2025-02-17 10:58:15,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:16,045][root][INFO] - Training Epoch: 2/2, step 801/1149 completed (loss: 0.4826824367046356, acc: 0.875)
[2025-02-17 10:58:16,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:16,418][root][INFO] - Training Epoch: 2/2, step 802/1149 completed (loss: 0.7719160318374634, acc: 0.7749999761581421)
[2025-02-17 10:58:16,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:16,807][root][INFO] - Training Epoch: 2/2, step 803/1149 completed (loss: 0.8672358989715576, acc: 0.7714285850524902)
[2025-02-17 10:58:16,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:17,184][root][INFO] - Training Epoch: 2/2, step 804/1149 completed (loss: 1.4192967414855957, acc: 0.6818181872367859)
[2025-02-17 10:58:17,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:17,560][root][INFO] - Training Epoch: 2/2, step 805/1149 completed (loss: 0.47915181517601013, acc: 0.9090909361839294)
[2025-02-17 10:58:17,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:17,918][root][INFO] - Training Epoch: 2/2, step 806/1149 completed (loss: 0.6628182530403137, acc: 0.8636363744735718)
[2025-02-17 10:58:18,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:18,271][root][INFO] - Training Epoch: 2/2, step 807/1149 completed (loss: 0.42877933382987976, acc: 0.9090909361839294)
[2025-02-17 10:58:18,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:18,631][root][INFO] - Training Epoch: 2/2, step 808/1149 completed (loss: 0.24562610685825348, acc: 0.9230769276618958)
[2025-02-17 10:58:18,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:19,004][root][INFO] - Training Epoch: 2/2, step 809/1149 completed (loss: 0.3841618299484253, acc: 0.8333333134651184)
[2025-02-17 10:58:19,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:19,371][root][INFO] - Training Epoch: 2/2, step 810/1149 completed (loss: 0.301621675491333, acc: 0.8571428656578064)
[2025-02-17 10:58:19,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:19,738][root][INFO] - Training Epoch: 2/2, step 811/1149 completed (loss: 0.7133408784866333, acc: 0.8461538553237915)
[2025-02-17 10:58:19,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:20,176][root][INFO] - Training Epoch: 2/2, step 812/1149 completed (loss: 0.6922154426574707, acc: 0.8333333134651184)
[2025-02-17 10:58:20,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:20,560][root][INFO] - Training Epoch: 2/2, step 813/1149 completed (loss: 0.025280801579356194, acc: 1.0)
[2025-02-17 10:58:20,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:20,913][root][INFO] - Training Epoch: 2/2, step 814/1149 completed (loss: 0.22487837076187134, acc: 0.875)
[2025-02-17 10:58:21,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:21,288][root][INFO] - Training Epoch: 2/2, step 815/1149 completed (loss: 0.22841037809848785, acc: 0.9166666865348816)
[2025-02-17 10:58:21,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:21,629][root][INFO] - Training Epoch: 2/2, step 816/1149 completed (loss: 0.18111218512058258, acc: 0.9090909361839294)
[2025-02-17 10:58:21,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:21,996][root][INFO] - Training Epoch: 2/2, step 817/1149 completed (loss: 0.5039388537406921, acc: 0.875)
[2025-02-17 10:58:22,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:22,377][root][INFO] - Training Epoch: 2/2, step 818/1149 completed (loss: 0.21975722908973694, acc: 0.9285714030265808)
[2025-02-17 10:58:22,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:22,762][root][INFO] - Training Epoch: 2/2, step 819/1149 completed (loss: 0.09459327906370163, acc: 1.0)
[2025-02-17 10:58:22,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:23,145][root][INFO] - Training Epoch: 2/2, step 820/1149 completed (loss: 0.42267513275146484, acc: 0.8928571343421936)
[2025-02-17 10:58:23,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:23,495][root][INFO] - Training Epoch: 2/2, step 821/1149 completed (loss: 0.17603616416454315, acc: 0.9666666388511658)
[2025-02-17 10:58:23,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:23,824][root][INFO] - Training Epoch: 2/2, step 822/1149 completed (loss: 0.43033114075660706, acc: 0.9166666865348816)
[2025-02-17 10:58:23,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:24,172][root][INFO] - Training Epoch: 2/2, step 823/1149 completed (loss: 0.05945783853530884, acc: 1.0)
[2025-02-17 10:58:24,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:24,517][root][INFO] - Training Epoch: 2/2, step 824/1149 completed (loss: 1.092925786972046, acc: 0.75)
[2025-02-17 10:58:24,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:24,857][root][INFO] - Training Epoch: 2/2, step 825/1149 completed (loss: 0.13047780096530914, acc: 1.0)
[2025-02-17 10:58:24,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:25,192][root][INFO] - Training Epoch: 2/2, step 826/1149 completed (loss: 0.751067042350769, acc: 0.8571428656578064)
[2025-02-17 10:58:25,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:25,528][root][INFO] - Training Epoch: 2/2, step 827/1149 completed (loss: 0.3279787003993988, acc: 0.9473684430122375)
[2025-02-17 10:58:25,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:25,869][root][INFO] - Training Epoch: 2/2, step 828/1149 completed (loss: 0.30535268783569336, acc: 0.9230769276618958)
[2025-02-17 10:58:26,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:26,207][root][INFO] - Training Epoch: 2/2, step 829/1149 completed (loss: 0.07846971601247787, acc: 1.0)
[2025-02-17 10:58:26,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:26,573][root][INFO] - Training Epoch: 2/2, step 830/1149 completed (loss: 1.1481859683990479, acc: 0.692307710647583)
[2025-02-17 10:58:26,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:26,932][root][INFO] - Training Epoch: 2/2, step 831/1149 completed (loss: 0.38598597049713135, acc: 0.9200000166893005)
[2025-02-17 10:58:27,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:27,289][root][INFO] - Training Epoch: 2/2, step 832/1149 completed (loss: 0.2052939534187317, acc: 0.8888888955116272)
[2025-02-17 10:58:27,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:27,643][root][INFO] - Training Epoch: 2/2, step 833/1149 completed (loss: 0.6289077401161194, acc: 0.875)
[2025-02-17 10:58:27,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:27,989][root][INFO] - Training Epoch: 2/2, step 834/1149 completed (loss: 0.3534544110298157, acc: 0.9090909361839294)
[2025-02-17 10:58:28,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:28,346][root][INFO] - Training Epoch: 2/2, step 835/1149 completed (loss: 0.4931116998195648, acc: 0.8571428656578064)
[2025-02-17 10:58:28,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:28,752][root][INFO] - Training Epoch: 2/2, step 836/1149 completed (loss: 0.35119491815567017, acc: 0.8709677457809448)
[2025-02-17 10:58:28,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:29,105][root][INFO] - Training Epoch: 2/2, step 837/1149 completed (loss: 0.19682902097702026, acc: 0.8888888955116272)
[2025-02-17 10:58:29,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:29,434][root][INFO] - Training Epoch: 2/2, step 838/1149 completed (loss: 0.16231825947761536, acc: 1.0)
[2025-02-17 10:58:29,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:29,775][root][INFO] - Training Epoch: 2/2, step 839/1149 completed (loss: 0.9022992253303528, acc: 0.8333333134651184)
[2025-02-17 10:58:29,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:30,118][root][INFO] - Training Epoch: 2/2, step 840/1149 completed (loss: 0.1639798879623413, acc: 0.8999999761581421)
[2025-02-17 10:58:30,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:30,490][root][INFO] - Training Epoch: 2/2, step 841/1149 completed (loss: 0.04980139434337616, acc: 1.0)
[2025-02-17 10:58:30,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:30,832][root][INFO] - Training Epoch: 2/2, step 842/1149 completed (loss: 1.1156582832336426, acc: 0.75)
[2025-02-17 10:58:30,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:31,189][root][INFO] - Training Epoch: 2/2, step 843/1149 completed (loss: 0.359115868806839, acc: 0.8571428656578064)
[2025-02-17 10:58:31,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:31,542][root][INFO] - Training Epoch: 2/2, step 844/1149 completed (loss: 0.7498673796653748, acc: 0.75)
[2025-02-17 10:58:31,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:31,924][root][INFO] - Training Epoch: 2/2, step 845/1149 completed (loss: 1.5361526012420654, acc: 0.5)
[2025-02-17 10:58:32,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:32,310][root][INFO] - Training Epoch: 2/2, step 846/1149 completed (loss: 1.3799197673797607, acc: 0.6666666865348816)
[2025-02-17 10:58:32,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:32,706][root][INFO] - Training Epoch: 2/2, step 847/1149 completed (loss: 0.4401978552341461, acc: 0.8571428656578064)
[2025-02-17 10:58:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:33,043][root][INFO] - Training Epoch: 2/2, step 848/1149 completed (loss: 0.6283158659934998, acc: 0.8181818127632141)
[2025-02-17 10:58:33,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:33,435][root][INFO] - Training Epoch: 2/2, step 849/1149 completed (loss: 0.6885915994644165, acc: 0.8125)
[2025-02-17 10:58:33,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:33,844][root][INFO] - Training Epoch: 2/2, step 850/1149 completed (loss: 0.5717722773551941, acc: 0.8421052694320679)
[2025-02-17 10:58:33,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:34,222][root][INFO] - Training Epoch: 2/2, step 851/1149 completed (loss: 0.3670327067375183, acc: 0.8571428656578064)
[2025-02-17 10:58:34,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:34,597][root][INFO] - Training Epoch: 2/2, step 852/1149 completed (loss: 0.2543007731437683, acc: 0.9166666865348816)
[2025-02-17 10:58:34,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:34,951][root][INFO] - Training Epoch: 2/2, step 853/1149 completed (loss: 0.3056350648403168, acc: 0.9090909361839294)
[2025-02-17 10:58:35,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:35,305][root][INFO] - Training Epoch: 2/2, step 854/1149 completed (loss: 0.37129008769989014, acc: 0.8461538553237915)
[2025-02-17 10:58:35,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:35,647][root][INFO] - Training Epoch: 2/2, step 855/1149 completed (loss: 0.7165300846099854, acc: 0.8181818127632141)
[2025-02-17 10:58:35,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:35,986][root][INFO] - Training Epoch: 2/2, step 856/1149 completed (loss: 0.14051537215709686, acc: 0.9166666865348816)
[2025-02-17 10:58:36,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:36,332][root][INFO] - Training Epoch: 2/2, step 857/1149 completed (loss: 0.4920717179775238, acc: 0.800000011920929)
[2025-02-17 10:58:36,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:36,678][root][INFO] - Training Epoch: 2/2, step 858/1149 completed (loss: 0.09747038036584854, acc: 1.0)
[2025-02-17 10:58:36,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:37,093][root][INFO] - Training Epoch: 2/2, step 859/1149 completed (loss: 1.0101832151412964, acc: 0.75)
[2025-02-17 10:58:38,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:38,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:39,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:39,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:40,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:40,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:41,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:41,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:42,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:42,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:43,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:43,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:44,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:44,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:45,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:45,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:46,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:46,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:47,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:47,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:48,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:48,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:49,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:49,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:50,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:50,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:51,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:51,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:52,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:52,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:53,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:53,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:54,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:54,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:54,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:55,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:55,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:56,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:56,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:57,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:57,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:58,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:58,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:58,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:59,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:58:59,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:00,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:00,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:01,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:01,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:02,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:03,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:03,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:03,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:04,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:04,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:05,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:05,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:06,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:06,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:07,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:07,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:08,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:08,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:09,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:09,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:10,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:10,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:10,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:11,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:11,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:12,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:13,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:13,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:13,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:14,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:14,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:15,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:16,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:16,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:17,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:17,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:18,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:18,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:19,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:19,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:19,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:20,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:21,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:21,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:22,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:22,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:22,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:23,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:23,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:24,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:24,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:25,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:25,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:26,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:26,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:26,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:27,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:27,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:28,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:28,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:28,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:29,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:30,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:30,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:31,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:31,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:31,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:32,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:33,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:33,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:34,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:34,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:35,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:35,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:36,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:36,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:37,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:37,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:38,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:38,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:39,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:39,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:39,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:40,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:40,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:41,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:41,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:42,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:42,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:43,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:43,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:44,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:44,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:45,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:45,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:46,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:46,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:47,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:47,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:48,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:48,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:49,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:49,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:50,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:50,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:51,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:51,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:52,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:52,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:52,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:53,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:53,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:54,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:54,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:55,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:55,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:56,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:56,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:56,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:57,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:57,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:58,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 10:59:59,049][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.0617, device='cuda:0') eval_epoch_loss=tensor(1.1190, device='cuda:0') eval_epoch_acc=tensor(0.7083, device='cuda:0')
[2025-02-17 10:59:59,050][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 10:59:59,051][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 11:00:02,686][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_2_step_860_loss_1.118972659111023/model.pt
[2025-02-17 11:00:02,718][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 11:00:02,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:03,179][root][INFO] - Training Epoch: 2/2, step 860/1149 completed (loss: 0.09078919887542725, acc: 1.0)
[2025-02-17 11:00:03,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:03,526][root][INFO] - Training Epoch: 2/2, step 861/1149 completed (loss: 0.2892412841320038, acc: 0.8571428656578064)
[2025-02-17 11:00:03,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:03,913][root][INFO] - Training Epoch: 2/2, step 862/1149 completed (loss: 0.013387120328843594, acc: 1.0)
[2025-02-17 11:00:04,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:04,266][root][INFO] - Training Epoch: 2/2, step 863/1149 completed (loss: 0.3097133934497833, acc: 0.9333333373069763)
[2025-02-17 11:00:04,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:04,590][root][INFO] - Training Epoch: 2/2, step 864/1149 completed (loss: 0.37440988421440125, acc: 0.9166666865348816)
[2025-02-17 11:00:04,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:04,939][root][INFO] - Training Epoch: 2/2, step 865/1149 completed (loss: 0.12399651855230331, acc: 1.0)
[2025-02-17 11:00:05,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:05,380][root][INFO] - Training Epoch: 2/2, step 866/1149 completed (loss: 0.6948902010917664, acc: 0.782608687877655)
[2025-02-17 11:00:05,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:05,721][root][INFO] - Training Epoch: 2/2, step 867/1149 completed (loss: 0.00551046896725893, acc: 1.0)
[2025-02-17 11:00:05,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:06,107][root][INFO] - Training Epoch: 2/2, step 868/1149 completed (loss: 0.09561903029680252, acc: 0.9230769276618958)
[2025-02-17 11:00:06,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:06,489][root][INFO] - Training Epoch: 2/2, step 869/1149 completed (loss: 0.08350897580385208, acc: 1.0)
[2025-02-17 11:00:06,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:06,842][root][INFO] - Training Epoch: 2/2, step 870/1149 completed (loss: 0.2864360809326172, acc: 0.8888888955116272)
[2025-02-17 11:00:06,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:07,200][root][INFO] - Training Epoch: 2/2, step 871/1149 completed (loss: 0.030953295528888702, acc: 1.0)
[2025-02-17 11:00:07,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:07,571][root][INFO] - Training Epoch: 2/2, step 872/1149 completed (loss: 0.2562308609485626, acc: 0.8666666746139526)
[2025-02-17 11:00:07,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:07,999][root][INFO] - Training Epoch: 2/2, step 873/1149 completed (loss: 0.5230410099029541, acc: 0.8095238208770752)
[2025-02-17 11:00:08,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:08,418][root][INFO] - Training Epoch: 2/2, step 874/1149 completed (loss: 0.48309990763664246, acc: 0.875)
[2025-02-17 11:00:08,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:08,852][root][INFO] - Training Epoch: 2/2, step 875/1149 completed (loss: 1.0843406915664673, acc: 0.7142857313156128)
[2025-02-17 11:00:09,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:09,268][root][INFO] - Training Epoch: 2/2, step 876/1149 completed (loss: 0.4104325473308563, acc: 0.9166666865348816)
[2025-02-17 11:00:09,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:09,656][root][INFO] - Training Epoch: 2/2, step 877/1149 completed (loss: 0.051780808717012405, acc: 1.0)
[2025-02-17 11:00:09,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:10,069][root][INFO] - Training Epoch: 2/2, step 878/1149 completed (loss: 0.9816338419914246, acc: 0.8571428656578064)
[2025-02-17 11:00:10,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:10,475][root][INFO] - Training Epoch: 2/2, step 879/1149 completed (loss: 0.824070155620575, acc: 0.8399999737739563)
[2025-02-17 11:00:10,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:10,951][root][INFO] - Training Epoch: 2/2, step 880/1149 completed (loss: 1.37296724319458, acc: 0.6458333134651184)
[2025-02-17 11:00:11,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:11,300][root][INFO] - Training Epoch: 2/2, step 881/1149 completed (loss: 0.5535247921943665, acc: 0.6666666865348816)
[2025-02-17 11:00:11,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:11,939][root][INFO] - Training Epoch: 2/2, step 882/1149 completed (loss: 1.2016595602035522, acc: 0.6292135119438171)
[2025-02-17 11:00:12,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:12,301][root][INFO] - Training Epoch: 2/2, step 883/1149 completed (loss: 1.1430788040161133, acc: 0.6944444179534912)
[2025-02-17 11:00:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:12,753][root][INFO] - Training Epoch: 2/2, step 884/1149 completed (loss: 1.3483397960662842, acc: 0.6727272868156433)
[2025-02-17 11:00:12,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:13,185][root][INFO] - Training Epoch: 2/2, step 885/1149 completed (loss: 1.3145235776901245, acc: 0.6811594367027283)
[2025-02-17 11:00:13,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:13,808][root][INFO] - Training Epoch: 2/2, step 886/1149 completed (loss: 1.0497933626174927, acc: 0.7080292105674744)
[2025-02-17 11:00:13,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:14,254][root][INFO] - Training Epoch: 2/2, step 887/1149 completed (loss: 0.9234501719474792, acc: 0.71875)
[2025-02-17 11:00:14,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:14,597][root][INFO] - Training Epoch: 2/2, step 888/1149 completed (loss: 0.538309633731842, acc: 0.7777777910232544)
[2025-02-17 11:00:14,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:14,945][root][INFO] - Training Epoch: 2/2, step 889/1149 completed (loss: 0.7188546657562256, acc: 0.7428571581840515)
[2025-02-17 11:00:15,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:15,392][root][INFO] - Training Epoch: 2/2, step 890/1149 completed (loss: 0.5124863386154175, acc: 0.7575757503509521)
[2025-02-17 11:00:15,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:15,743][root][INFO] - Training Epoch: 2/2, step 891/1149 completed (loss: 0.16441427171230316, acc: 0.9090909361839294)
[2025-02-17 11:00:15,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:16,180][root][INFO] - Training Epoch: 2/2, step 892/1149 completed (loss: 0.02664477378129959, acc: 1.0)
[2025-02-17 11:00:16,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:16,576][root][INFO] - Training Epoch: 2/2, step 893/1149 completed (loss: 1.5253117084503174, acc: 0.6000000238418579)
[2025-02-17 11:00:16,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:16,962][root][INFO] - Training Epoch: 2/2, step 894/1149 completed (loss: 0.08735685795545578, acc: 1.0)
[2025-02-17 11:00:17,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:17,302][root][INFO] - Training Epoch: 2/2, step 895/1149 completed (loss: 0.5728186368942261, acc: 0.8571428656578064)
[2025-02-17 11:00:17,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:17,669][root][INFO] - Training Epoch: 2/2, step 896/1149 completed (loss: 0.3781922459602356, acc: 0.9411764740943909)
[2025-02-17 11:00:17,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:18,016][root][INFO] - Training Epoch: 2/2, step 897/1149 completed (loss: 0.19952405989170074, acc: 0.9090909361839294)
[2025-02-17 11:00:18,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:18,405][root][INFO] - Training Epoch: 2/2, step 898/1149 completed (loss: 0.280128538608551, acc: 0.9411764740943909)
[2025-02-17 11:00:18,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:18,758][root][INFO] - Training Epoch: 2/2, step 899/1149 completed (loss: 0.6840760111808777, acc: 0.8484848737716675)
[2025-02-17 11:00:18,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:19,147][root][INFO] - Training Epoch: 2/2, step 900/1149 completed (loss: 0.17125707864761353, acc: 0.9722222089767456)
[2025-02-17 11:00:19,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:19,508][root][INFO] - Training Epoch: 2/2, step 901/1149 completed (loss: 0.264987587928772, acc: 0.9166666865348816)
[2025-02-17 11:00:19,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:19,918][root][INFO] - Training Epoch: 2/2, step 902/1149 completed (loss: 0.22634175419807434, acc: 0.9629629850387573)
[2025-02-17 11:00:20,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:20,335][root][INFO] - Training Epoch: 2/2, step 903/1149 completed (loss: 0.31595978140830994, acc: 0.8684210777282715)
[2025-02-17 11:00:20,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:20,713][root][INFO] - Training Epoch: 2/2, step 904/1149 completed (loss: 1.2205512523651123, acc: 0.7058823704719543)
[2025-02-17 11:00:20,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:21,056][root][INFO] - Training Epoch: 2/2, step 905/1149 completed (loss: 0.2007884383201599, acc: 0.9772727489471436)
[2025-02-17 11:00:21,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:21,403][root][INFO] - Training Epoch: 2/2, step 906/1149 completed (loss: 1.2326271533966064, acc: 0.6511628031730652)
[2025-02-17 11:00:21,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:21,731][root][INFO] - Training Epoch: 2/2, step 907/1149 completed (loss: 0.21592094004154205, acc: 0.9090909361839294)
[2025-02-17 11:00:21,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:22,101][root][INFO] - Training Epoch: 2/2, step 908/1149 completed (loss: 0.5220801830291748, acc: 0.8461538553237915)
[2025-02-17 11:00:22,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:22,504][root][INFO] - Training Epoch: 2/2, step 909/1149 completed (loss: 0.5047680139541626, acc: 0.800000011920929)
[2025-02-17 11:00:22,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:22,878][root][INFO] - Training Epoch: 2/2, step 910/1149 completed (loss: 0.15550673007965088, acc: 1.0)
[2025-02-17 11:00:23,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:23,226][root][INFO] - Training Epoch: 2/2, step 911/1149 completed (loss: 0.7549903392791748, acc: 0.761904776096344)
[2025-02-17 11:00:23,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:23,621][root][INFO] - Training Epoch: 2/2, step 912/1149 completed (loss: 0.6785821914672852, acc: 0.875)
[2025-02-17 11:00:23,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:24,000][root][INFO] - Training Epoch: 2/2, step 913/1149 completed (loss: 1.3518708944320679, acc: 0.6438356041908264)
[2025-02-17 11:00:24,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:24,416][root][INFO] - Training Epoch: 2/2, step 914/1149 completed (loss: 0.7966914176940918, acc: 0.7560975551605225)
[2025-02-17 11:00:24,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:24,833][root][INFO] - Training Epoch: 2/2, step 915/1149 completed (loss: 0.46407854557037354, acc: 0.8620689511299133)
[2025-02-17 11:00:24,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:25,195][root][INFO] - Training Epoch: 2/2, step 916/1149 completed (loss: 1.096527338027954, acc: 0.7236841917037964)
[2025-02-17 11:00:25,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:25,596][root][INFO] - Training Epoch: 2/2, step 917/1149 completed (loss: 1.0996845960617065, acc: 0.6666666865348816)
[2025-02-17 11:00:25,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:26,016][root][INFO] - Training Epoch: 2/2, step 918/1149 completed (loss: 0.712246835231781, acc: 0.695652186870575)
[2025-02-17 11:00:26,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:26,388][root][INFO] - Training Epoch: 2/2, step 919/1149 completed (loss: 0.7649932503700256, acc: 0.7878788113594055)
[2025-02-17 11:00:26,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:26,803][root][INFO] - Training Epoch: 2/2, step 920/1149 completed (loss: 0.637453019618988, acc: 0.8965517282485962)
[2025-02-17 11:00:26,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:27,174][root][INFO] - Training Epoch: 2/2, step 921/1149 completed (loss: 0.9517450928688049, acc: 0.7307692170143127)
[2025-02-17 11:00:27,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:27,561][root][INFO] - Training Epoch: 2/2, step 922/1149 completed (loss: 0.8641631007194519, acc: 0.8518518805503845)
[2025-02-17 11:00:27,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:27,947][root][INFO] - Training Epoch: 2/2, step 923/1149 completed (loss: 0.16512642800807953, acc: 1.0)
[2025-02-17 11:00:28,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:28,299][root][INFO] - Training Epoch: 2/2, step 924/1149 completed (loss: 0.37837323546409607, acc: 0.8947368264198303)
[2025-02-17 11:00:28,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:28,612][root][INFO] - Training Epoch: 2/2, step 925/1149 completed (loss: 0.10813181102275848, acc: 0.9230769276618958)
[2025-02-17 11:00:28,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:29,000][root][INFO] - Training Epoch: 2/2, step 926/1149 completed (loss: 0.04019230604171753, acc: 1.0)
[2025-02-17 11:00:29,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:29,352][root][INFO] - Training Epoch: 2/2, step 927/1149 completed (loss: 1.1521178483963013, acc: 0.6428571343421936)
[2025-02-17 11:00:29,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:29,770][root][INFO] - Training Epoch: 2/2, step 928/1149 completed (loss: 0.5077694654464722, acc: 0.8484848737716675)
[2025-02-17 11:00:29,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:30,167][root][INFO] - Training Epoch: 2/2, step 929/1149 completed (loss: 1.765139102935791, acc: 0.5384615659713745)
[2025-02-17 11:00:30,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:30,563][root][INFO] - Training Epoch: 2/2, step 930/1149 completed (loss: 0.5749278664588928, acc: 0.8604651093482971)
[2025-02-17 11:00:30,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:30,997][root][INFO] - Training Epoch: 2/2, step 931/1149 completed (loss: 0.8020875453948975, acc: 0.8048780560493469)
[2025-02-17 11:00:31,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:31,377][root][INFO] - Training Epoch: 2/2, step 932/1149 completed (loss: 0.7881913781166077, acc: 0.75)
[2025-02-17 11:00:31,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:31,797][root][INFO] - Training Epoch: 2/2, step 933/1149 completed (loss: 1.3744659423828125, acc: 0.6774193644523621)
[2025-02-17 11:00:31,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:32,219][root][INFO] - Training Epoch: 2/2, step 934/1149 completed (loss: 0.8767266273498535, acc: 0.7469879388809204)
[2025-02-17 11:00:32,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:32,627][root][INFO] - Training Epoch: 2/2, step 935/1149 completed (loss: 0.4014968276023865, acc: 0.8928571343421936)
[2025-02-17 11:00:32,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:33,072][root][INFO] - Training Epoch: 2/2, step 936/1149 completed (loss: 1.068342685699463, acc: 0.7333333492279053)
[2025-02-17 11:00:33,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:33,443][root][INFO] - Training Epoch: 2/2, step 937/1149 completed (loss: 1.1560039520263672, acc: 0.7209302186965942)
[2025-02-17 11:00:33,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:33,855][root][INFO] - Training Epoch: 2/2, step 938/1149 completed (loss: 1.2006916999816895, acc: 0.7093023061752319)
[2025-02-17 11:00:34,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:34,226][root][INFO] - Training Epoch: 2/2, step 939/1149 completed (loss: 0.6774315237998962, acc: 0.837837815284729)
[2025-02-17 11:00:34,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:34,610][root][INFO] - Training Epoch: 2/2, step 940/1149 completed (loss: 0.25143706798553467, acc: 0.875)
[2025-02-17 11:00:34,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:35,037][root][INFO] - Training Epoch: 2/2, step 941/1149 completed (loss: 0.7721289992332458, acc: 0.875)
[2025-02-17 11:00:35,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:35,470][root][INFO] - Training Epoch: 2/2, step 942/1149 completed (loss: 1.3011205196380615, acc: 0.6000000238418579)
[2025-02-17 11:00:35,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:35,832][root][INFO] - Training Epoch: 2/2, step 943/1149 completed (loss: 0.17833736538887024, acc: 0.9230769276618958)
[2025-02-17 11:00:35,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:36,170][root][INFO] - Training Epoch: 2/2, step 944/1149 completed (loss: 0.8605502843856812, acc: 0.8181818127632141)
[2025-02-17 11:00:36,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:36,554][root][INFO] - Training Epoch: 2/2, step 945/1149 completed (loss: 1.2898354530334473, acc: 0.6703296899795532)
[2025-02-17 11:00:36,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:36,907][root][INFO] - Training Epoch: 2/2, step 946/1149 completed (loss: 1.4071238040924072, acc: 0.5581395626068115)
[2025-02-17 11:00:37,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:37,280][root][INFO] - Training Epoch: 2/2, step 947/1149 completed (loss: 1.2613816261291504, acc: 0.6293706297874451)
[2025-02-17 11:00:37,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:37,620][root][INFO] - Training Epoch: 2/2, step 948/1149 completed (loss: 0.9927444458007812, acc: 0.7250000238418579)
[2025-02-17 11:00:37,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:37,981][root][INFO] - Training Epoch: 2/2, step 949/1149 completed (loss: 0.8947668671607971, acc: 0.75)
[2025-02-17 11:00:38,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:38,341][root][INFO] - Training Epoch: 2/2, step 950/1149 completed (loss: 0.6398599743843079, acc: 0.8333333134651184)
[2025-02-17 11:00:38,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:38,710][root][INFO] - Training Epoch: 2/2, step 951/1149 completed (loss: 1.041335940361023, acc: 0.7313432693481445)
[2025-02-17 11:00:38,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:39,088][root][INFO] - Training Epoch: 2/2, step 952/1149 completed (loss: 0.5001770257949829, acc: 0.8799999952316284)
[2025-02-17 11:00:39,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:39,429][root][INFO] - Training Epoch: 2/2, step 953/1149 completed (loss: 1.1145097017288208, acc: 0.625)
[2025-02-17 11:00:39,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:39,799][root][INFO] - Training Epoch: 2/2, step 954/1149 completed (loss: 1.369112253189087, acc: 0.6129032373428345)
[2025-02-17 11:00:39,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:40,162][root][INFO] - Training Epoch: 2/2, step 955/1149 completed (loss: 0.7889658212661743, acc: 0.7866666913032532)
[2025-02-17 11:00:40,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:40,499][root][INFO] - Training Epoch: 2/2, step 956/1149 completed (loss: 0.4044990539550781, acc: 0.75)
[2025-02-17 11:00:40,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:40,841][root][INFO] - Training Epoch: 2/2, step 957/1149 completed (loss: 1.2781105041503906, acc: 0.6071428656578064)
[2025-02-17 11:00:40,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:41,210][root][INFO] - Training Epoch: 2/2, step 958/1149 completed (loss: 0.46041014790534973, acc: 0.7857142686843872)
[2025-02-17 11:00:41,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:41,559][root][INFO] - Training Epoch: 2/2, step 959/1149 completed (loss: 0.8029487133026123, acc: 0.7368420958518982)
[2025-02-17 11:00:41,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:41,892][root][INFO] - Training Epoch: 2/2, step 960/1149 completed (loss: 0.6405327320098877, acc: 0.8461538553237915)
[2025-02-17 11:00:42,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:42,230][root][INFO] - Training Epoch: 2/2, step 961/1149 completed (loss: 0.6956397891044617, acc: 0.7142857313156128)
[2025-02-17 11:00:42,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:42,567][root][INFO] - Training Epoch: 2/2, step 962/1149 completed (loss: 0.41853082180023193, acc: 0.8888888955116272)
[2025-02-17 11:00:42,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:42,906][root][INFO] - Training Epoch: 2/2, step 963/1149 completed (loss: 0.7608767151832581, acc: 0.6363636255264282)
[2025-02-17 11:00:43,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:43,247][root][INFO] - Training Epoch: 2/2, step 964/1149 completed (loss: 1.1773533821105957, acc: 0.5555555820465088)
[2025-02-17 11:00:43,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:43,593][root][INFO] - Training Epoch: 2/2, step 965/1149 completed (loss: 0.24035774171352386, acc: 0.9090909361839294)
[2025-02-17 11:00:43,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:43,934][root][INFO] - Training Epoch: 2/2, step 966/1149 completed (loss: 0.4713993966579437, acc: 0.8181818127632141)
[2025-02-17 11:00:44,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:44,327][root][INFO] - Training Epoch: 2/2, step 967/1149 completed (loss: 1.4218493700027466, acc: 0.5744680762290955)
[2025-02-17 11:00:44,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:44,671][root][INFO] - Training Epoch: 2/2, step 968/1149 completed (loss: 0.43312764167785645, acc: 0.8947368264198303)
[2025-02-17 11:00:44,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:45,005][root][INFO] - Training Epoch: 2/2, step 969/1149 completed (loss: 0.4042067229747772, acc: 0.8333333134651184)
[2025-02-17 11:00:45,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:45,347][root][INFO] - Training Epoch: 2/2, step 970/1149 completed (loss: 0.11907654255628586, acc: 0.875)
[2025-02-17 11:00:45,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:45,685][root][INFO] - Training Epoch: 2/2, step 971/1149 completed (loss: 0.8116950392723083, acc: 0.8181818127632141)
[2025-02-17 11:00:45,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:46,018][root][INFO] - Training Epoch: 2/2, step 972/1149 completed (loss: 1.398364543914795, acc: 0.6666666865348816)
[2025-02-17 11:00:46,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:46,353][root][INFO] - Training Epoch: 2/2, step 973/1149 completed (loss: 1.9053255319595337, acc: 0.4166666567325592)
[2025-02-17 11:00:46,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:46,692][root][INFO] - Training Epoch: 2/2, step 974/1149 completed (loss: 0.7623733282089233, acc: 0.800000011920929)
[2025-02-17 11:00:46,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:47,038][root][INFO] - Training Epoch: 2/2, step 975/1149 completed (loss: 1.0951594114303589, acc: 0.5454545617103577)
[2025-02-17 11:00:47,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:47,372][root][INFO] - Training Epoch: 2/2, step 976/1149 completed (loss: 0.7396022081375122, acc: 0.7777777910232544)
[2025-02-17 11:00:47,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:47,712][root][INFO] - Training Epoch: 2/2, step 977/1149 completed (loss: 0.799021303653717, acc: 0.692307710647583)
[2025-02-17 11:00:47,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:48,073][root][INFO] - Training Epoch: 2/2, step 978/1149 completed (loss: 1.5414351224899292, acc: 0.5769230723381042)
[2025-02-17 11:00:48,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:48,413][root][INFO] - Training Epoch: 2/2, step 979/1149 completed (loss: 0.1849081814289093, acc: 0.9230769276618958)
[2025-02-17 11:00:48,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:48,754][root][INFO] - Training Epoch: 2/2, step 980/1149 completed (loss: 0.43488430976867676, acc: 0.8947368264198303)
[2025-02-17 11:00:48,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:49,090][root][INFO] - Training Epoch: 2/2, step 981/1149 completed (loss: 0.336687833070755, acc: 0.9090909361839294)
[2025-02-17 11:00:49,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:49,446][root][INFO] - Training Epoch: 2/2, step 982/1149 completed (loss: 0.33613571524620056, acc: 0.9285714030265808)
[2025-02-17 11:00:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:49,779][root][INFO] - Training Epoch: 2/2, step 983/1149 completed (loss: 1.2074764966964722, acc: 0.7333333492279053)
[2025-02-17 11:00:49,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:50,124][root][INFO] - Training Epoch: 2/2, step 984/1149 completed (loss: 0.9832072257995605, acc: 0.7105262875556946)
[2025-02-17 11:00:50,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:50,502][root][INFO] - Training Epoch: 2/2, step 985/1149 completed (loss: 0.38207516074180603, acc: 0.9230769276618958)
[2025-02-17 11:00:50,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:50,840][root][INFO] - Training Epoch: 2/2, step 986/1149 completed (loss: 0.45201894640922546, acc: 0.875)
[2025-02-17 11:00:50,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:51,170][root][INFO] - Training Epoch: 2/2, step 987/1149 completed (loss: 1.101677656173706, acc: 0.5384615659713745)
[2025-02-17 11:00:51,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:51,509][root][INFO] - Training Epoch: 2/2, step 988/1149 completed (loss: 0.7905954718589783, acc: 0.8181818127632141)
[2025-02-17 11:00:51,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:51,842][root][INFO] - Training Epoch: 2/2, step 989/1149 completed (loss: 0.5854002237319946, acc: 0.875)
[2025-02-17 11:00:51,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:52,193][root][INFO] - Training Epoch: 2/2, step 990/1149 completed (loss: 1.2373253107070923, acc: 0.7272727489471436)
[2025-02-17 11:00:52,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:52,535][root][INFO] - Training Epoch: 2/2, step 991/1149 completed (loss: 1.8281066417694092, acc: 0.5)
[2025-02-17 11:00:52,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:52,881][root][INFO] - Training Epoch: 2/2, step 992/1149 completed (loss: 0.8206997513771057, acc: 0.739130437374115)
[2025-02-17 11:00:53,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:53,250][root][INFO] - Training Epoch: 2/2, step 993/1149 completed (loss: 1.1942622661590576, acc: 0.6516854166984558)
[2025-02-17 11:00:53,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:53,625][root][INFO] - Training Epoch: 2/2, step 994/1149 completed (loss: 0.7350716590881348, acc: 0.7924528121948242)
[2025-02-17 11:00:53,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:53,974][root][INFO] - Training Epoch: 2/2, step 995/1149 completed (loss: 0.6499844193458557, acc: 0.8611111044883728)
[2025-02-17 11:00:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:54,329][root][INFO] - Training Epoch: 2/2, step 996/1149 completed (loss: 1.1122095584869385, acc: 0.6481481194496155)
[2025-02-17 11:00:54,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:54,761][root][INFO] - Training Epoch: 2/2, step 997/1149 completed (loss: 1.1628881692886353, acc: 0.6857143044471741)
[2025-02-17 11:00:54,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:55,136][root][INFO] - Training Epoch: 2/2, step 998/1149 completed (loss: 1.679466724395752, acc: 0.47826087474823)
[2025-02-17 11:00:55,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:55,498][root][INFO] - Training Epoch: 2/2, step 999/1149 completed (loss: 1.7935887575149536, acc: 0.5052631497383118)
[2025-02-17 11:00:55,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:55,863][root][INFO] - Training Epoch: 2/2, step 1000/1149 completed (loss: 1.3572548627853394, acc: 0.6909090876579285)
[2025-02-17 11:00:56,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:56,205][root][INFO] - Training Epoch: 2/2, step 1001/1149 completed (loss: 1.0744080543518066, acc: 0.6486486196517944)
[2025-02-17 11:00:56,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:56,536][root][INFO] - Training Epoch: 2/2, step 1002/1149 completed (loss: 0.4279480576515198, acc: 0.8571428656578064)
[2025-02-17 11:00:56,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:56,876][root][INFO] - Training Epoch: 2/2, step 1003/1149 completed (loss: 0.05266612768173218, acc: 1.0)
[2025-02-17 11:00:57,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:57,206][root][INFO] - Training Epoch: 2/2, step 1004/1149 completed (loss: 0.03629019483923912, acc: 1.0)
[2025-02-17 11:00:57,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:57,545][root][INFO] - Training Epoch: 2/2, step 1005/1149 completed (loss: 0.5672619938850403, acc: 0.8461538553237915)
[2025-02-17 11:00:57,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:57,891][root][INFO] - Training Epoch: 2/2, step 1006/1149 completed (loss: 0.643866240978241, acc: 0.75)
[2025-02-17 11:00:58,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:58,230][root][INFO] - Training Epoch: 2/2, step 1007/1149 completed (loss: 0.3176334500312805, acc: 0.9333333373069763)
[2025-02-17 11:00:58,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:58,624][root][INFO] - Training Epoch: 2/2, step 1008/1149 completed (loss: 0.146813303232193, acc: 1.0)
[2025-02-17 11:00:58,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:59,051][root][INFO] - Training Epoch: 2/2, step 1009/1149 completed (loss: 1.0368481874465942, acc: 0.7058823704719543)
[2025-02-17 11:00:59,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:59,438][root][INFO] - Training Epoch: 2/2, step 1010/1149 completed (loss: 0.9579637050628662, acc: 0.7096773982048035)
[2025-02-17 11:00:59,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:00:59,847][root][INFO] - Training Epoch: 2/2, step 1011/1149 completed (loss: 0.35947975516319275, acc: 0.9545454382896423)
[2025-02-17 11:01:00,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:00,220][root][INFO] - Training Epoch: 2/2, step 1012/1149 completed (loss: 0.06086376681923866, acc: 1.0)
[2025-02-17 11:01:00,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:00,637][root][INFO] - Training Epoch: 2/2, step 1013/1149 completed (loss: 1.4606666564941406, acc: 0.5333333611488342)
[2025-02-17 11:01:00,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:01,228][root][INFO] - Training Epoch: 2/2, step 1014/1149 completed (loss: 1.713782548904419, acc: 0.5671641826629639)
[2025-02-17 11:01:01,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:01,598][root][INFO] - Training Epoch: 2/2, step 1015/1149 completed (loss: 1.0970686674118042, acc: 0.7272727489471436)
[2025-02-17 11:01:01,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:02,084][root][INFO] - Training Epoch: 2/2, step 1016/1149 completed (loss: 1.0393939018249512, acc: 0.7755101919174194)
[2025-02-17 11:01:02,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:02,527][root][INFO] - Training Epoch: 2/2, step 1017/1149 completed (loss: 1.0593498945236206, acc: 0.7272727489471436)
[2025-02-17 11:01:02,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:02,915][root][INFO] - Training Epoch: 2/2, step 1018/1149 completed (loss: 0.011241917498409748, acc: 1.0)
[2025-02-17 11:01:03,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:03,318][root][INFO] - Training Epoch: 2/2, step 1019/1149 completed (loss: 0.46439129114151, acc: 0.875)
[2025-02-17 11:01:03,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:03,710][root][INFO] - Training Epoch: 2/2, step 1020/1149 completed (loss: 0.22046561539173126, acc: 0.9375)
[2025-02-17 11:01:03,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:04,138][root][INFO] - Training Epoch: 2/2, step 1021/1149 completed (loss: 0.08488104492425919, acc: 1.0)
[2025-02-17 11:01:04,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:04,505][root][INFO] - Training Epoch: 2/2, step 1022/1149 completed (loss: 0.19351869821548462, acc: 0.8999999761581421)
[2025-02-17 11:01:04,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:04,820][root][INFO] - Training Epoch: 2/2, step 1023/1149 completed (loss: 0.6964071393013, acc: 0.8125)
[2025-02-17 11:01:04,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:05,162][root][INFO] - Training Epoch: 2/2, step 1024/1149 completed (loss: 0.7028936743736267, acc: 0.800000011920929)
[2025-02-17 11:01:06,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:07,110][root][INFO] - Training Epoch: 2/2, step 1025/1149 completed (loss: 1.7220020294189453, acc: 0.5680000185966492)
[2025-02-17 11:01:07,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:07,539][root][INFO] - Training Epoch: 2/2, step 1026/1149 completed (loss: 0.6261172294616699, acc: 0.8333333134651184)
[2025-02-17 11:01:07,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:08,288][root][INFO] - Training Epoch: 2/2, step 1027/1149 completed (loss: 0.7498465776443481, acc: 0.7692307829856873)
[2025-02-17 11:01:08,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:08,735][root][INFO] - Training Epoch: 2/2, step 1028/1149 completed (loss: 1.73826003074646, acc: 0.6000000238418579)
[2025-02-17 11:01:08,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:09,117][root][INFO] - Training Epoch: 2/2, step 1029/1149 completed (loss: 0.2708647549152374, acc: 0.8888888955116272)
[2025-02-17 11:01:09,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:09,541][root][INFO] - Training Epoch: 2/2, step 1030/1149 completed (loss: 0.3923688530921936, acc: 0.8571428656578064)
[2025-02-17 11:01:09,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:09,934][root][INFO] - Training Epoch: 2/2, step 1031/1149 completed (loss: 0.31580811738967896, acc: 0.9200000166893005)
[2025-02-17 11:01:10,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:10,600][root][INFO] - Training Epoch: 2/2, step 1032/1149 completed (loss: 0.838448166847229, acc: 0.7796609997749329)
[2025-02-17 11:01:10,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:10,962][root][INFO] - Training Epoch: 2/2, step 1033/1149 completed (loss: 0.7518987059593201, acc: 0.8461538553237915)
[2025-02-17 11:01:11,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:11,389][root][INFO] - Training Epoch: 2/2, step 1034/1149 completed (loss: 0.024439333006739616, acc: 1.0)
[2025-02-17 11:01:11,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:11,786][root][INFO] - Training Epoch: 2/2, step 1035/1149 completed (loss: 0.026837220415472984, acc: 1.0)
[2025-02-17 11:01:11,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:12,194][root][INFO] - Training Epoch: 2/2, step 1036/1149 completed (loss: 0.687752366065979, acc: 0.8823529481887817)
[2025-02-17 11:01:12,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:12,604][root][INFO] - Training Epoch: 2/2, step 1037/1149 completed (loss: 0.30680614709854126, acc: 0.9285714030265808)
[2025-02-17 11:01:12,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:13,003][root][INFO] - Training Epoch: 2/2, step 1038/1149 completed (loss: 1.2001712322235107, acc: 0.800000011920929)
[2025-02-17 11:01:13,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:13,348][root][INFO] - Training Epoch: 2/2, step 1039/1149 completed (loss: 0.0917985737323761, acc: 1.0)
[2025-02-17 11:01:13,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:13,676][root][INFO] - Training Epoch: 2/2, step 1040/1149 completed (loss: 0.3868434727191925, acc: 0.7692307829856873)
[2025-02-17 11:01:13,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:14,016][root][INFO] - Training Epoch: 2/2, step 1041/1149 completed (loss: 1.2181676626205444, acc: 0.7857142686843872)
[2025-02-17 11:01:14,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:14,368][root][INFO] - Training Epoch: 2/2, step 1042/1149 completed (loss: 0.382347971200943, acc: 0.800000011920929)
[2025-02-17 11:01:14,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:15,266][root][INFO] - Training Epoch: 2/2, step 1043/1149 completed (loss: 1.1002931594848633, acc: 0.6769911646842957)
[2025-02-17 11:01:15,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:15,651][root][INFO] - Training Epoch: 2/2, step 1044/1149 completed (loss: 0.8594081997871399, acc: 0.800000011920929)
[2025-02-17 11:01:15,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:16,007][root][INFO] - Training Epoch: 2/2, step 1045/1149 completed (loss: 0.48110371828079224, acc: 0.8695651888847351)
[2025-02-17 11:01:16,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:16,375][root][INFO] - Training Epoch: 2/2, step 1046/1149 completed (loss: 0.8794723153114319, acc: 0.7051281929016113)
[2025-02-17 11:01:16,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:16,748][root][INFO] - Training Epoch: 2/2, step 1047/1149 completed (loss: 0.6249867677688599, acc: 0.7796609997749329)
[2025-02-17 11:01:16,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:17,283][root][INFO] - Training Epoch: 2/2, step 1048/1149 completed (loss: 1.079666018486023, acc: 0.6875)
[2025-02-17 11:01:17,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:17,665][root][INFO] - Training Epoch: 2/2, step 1049/1149 completed (loss: 0.8330158591270447, acc: 0.7692307829856873)
[2025-02-17 11:01:17,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:18,002][root][INFO] - Training Epoch: 2/2, step 1050/1149 completed (loss: 0.1039123386144638, acc: 1.0)
[2025-02-17 11:01:18,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:18,337][root][INFO] - Training Epoch: 2/2, step 1051/1149 completed (loss: 0.4063597023487091, acc: 0.9090909361839294)
[2025-02-17 11:01:18,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:18,679][root][INFO] - Training Epoch: 2/2, step 1052/1149 completed (loss: 0.409071147441864, acc: 0.8863636255264282)
[2025-02-17 11:01:18,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:19,013][root][INFO] - Training Epoch: 2/2, step 1053/1149 completed (loss: 0.8598060011863708, acc: 0.625)
[2025-02-17 11:01:19,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:19,349][root][INFO] - Training Epoch: 2/2, step 1054/1149 completed (loss: 0.3082638084888458, acc: 1.0)
[2025-02-17 11:01:19,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:19,703][root][INFO] - Training Epoch: 2/2, step 1055/1149 completed (loss: 0.5450831651687622, acc: 0.8461538553237915)
[2025-02-17 11:01:19,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:20,038][root][INFO] - Training Epoch: 2/2, step 1056/1149 completed (loss: 2.00201678276062, acc: 0.4000000059604645)
[2025-02-17 11:01:20,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:20,417][root][INFO] - Training Epoch: 2/2, step 1057/1149 completed (loss: 2.3086588382720947, acc: 0.43478259444236755)
[2025-02-17 11:01:20,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:20,764][root][INFO] - Training Epoch: 2/2, step 1058/1149 completed (loss: 0.7750139832496643, acc: 0.717391312122345)
[2025-02-17 11:01:20,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:21,129][root][INFO] - Training Epoch: 2/2, step 1059/1149 completed (loss: 0.07511074841022491, acc: 1.0)
[2025-02-17 11:01:21,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:21,473][root][INFO] - Training Epoch: 2/2, step 1060/1149 completed (loss: 1.6021742820739746, acc: 0.6399999856948853)
[2025-02-17 11:01:21,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:21,814][root][INFO] - Training Epoch: 2/2, step 1061/1149 completed (loss: 1.3441755771636963, acc: 0.6666666865348816)
[2025-02-17 11:01:21,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:22,192][root][INFO] - Training Epoch: 2/2, step 1062/1149 completed (loss: 1.494317650794983, acc: 0.6818181872367859)
[2025-02-17 11:01:22,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:22,532][root][INFO] - Training Epoch: 2/2, step 1063/1149 completed (loss: 1.4984068870544434, acc: 0.6363636255264282)
[2025-02-17 11:01:22,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:22,882][root][INFO] - Training Epoch: 2/2, step 1064/1149 completed (loss: 1.737504482269287, acc: 0.5789473652839661)
[2025-02-17 11:01:23,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:23,235][root][INFO] - Training Epoch: 2/2, step 1065/1149 completed (loss: 1.8563494682312012, acc: 0.47058823704719543)
[2025-02-17 11:01:23,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:23,630][root][INFO] - Training Epoch: 2/2, step 1066/1149 completed (loss: 0.28852614760398865, acc: 1.0)
[2025-02-17 11:01:23,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:24,059][root][INFO] - Training Epoch: 2/2, step 1067/1149 completed (loss: 1.5459754467010498, acc: 0.7241379022598267)
[2025-02-17 11:01:24,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:24,421][root][INFO] - Training Epoch: 2/2, step 1068/1149 completed (loss: 0.0656430721282959, acc: 1.0)
[2025-02-17 11:01:24,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:24,829][root][INFO] - Training Epoch: 2/2, step 1069/1149 completed (loss: 1.2734379768371582, acc: 0.6470588445663452)
[2025-02-17 11:01:24,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:25,156][root][INFO] - Training Epoch: 2/2, step 1070/1149 completed (loss: 0.9568397402763367, acc: 0.8333333134651184)
[2025-02-17 11:01:25,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:25,559][root][INFO] - Training Epoch: 2/2, step 1071/1149 completed (loss: 0.37635043263435364, acc: 0.875)
[2025-02-17 11:01:25,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:25,958][root][INFO] - Training Epoch: 2/2, step 1072/1149 completed (loss: 0.4072437882423401, acc: 0.9285714030265808)
[2025-02-17 11:01:26,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:26,291][root][INFO] - Training Epoch: 2/2, step 1073/1149 completed (loss: 0.38147854804992676, acc: 0.875)
[2025-02-17 11:01:26,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:26,694][root][INFO] - Training Epoch: 2/2, step 1074/1149 completed (loss: 1.0603351593017578, acc: 0.6976743936538696)
[2025-02-17 11:01:26,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:27,047][root][INFO] - Training Epoch: 2/2, step 1075/1149 completed (loss: 0.4944598376750946, acc: 0.7272727489471436)
[2025-02-17 11:01:27,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:27,464][root][INFO] - Training Epoch: 2/2, step 1076/1149 completed (loss: 0.4376690685749054, acc: 0.8666666746139526)
[2025-02-17 11:01:27,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:27,837][root][INFO] - Training Epoch: 2/2, step 1077/1149 completed (loss: 0.8551155924797058, acc: 0.7755101919174194)
[2025-02-17 11:01:27,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:28,194][root][INFO] - Training Epoch: 2/2, step 1078/1149 completed (loss: 0.4932311475276947, acc: 0.8571428656578064)
[2025-02-17 11:01:28,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:28,616][root][INFO] - Training Epoch: 2/2, step 1079/1149 completed (loss: 1.0994361639022827, acc: 0.7222222089767456)
[2025-02-17 11:01:28,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:29,008][root][INFO] - Training Epoch: 2/2, step 1080/1149 completed (loss: 1.0745370388031006, acc: 0.7333333492279053)
[2025-02-17 11:01:29,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:29,435][root][INFO] - Training Epoch: 2/2, step 1081/1149 completed (loss: 0.3655190169811249, acc: 0.8333333134651184)
[2025-02-17 11:01:29,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:29,804][root][INFO] - Training Epoch: 2/2, step 1082/1149 completed (loss: 0.4215579032897949, acc: 0.8571428656578064)
[2025-02-17 11:01:29,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:30,181][root][INFO] - Training Epoch: 2/2, step 1083/1149 completed (loss: 0.5610073804855347, acc: 0.8888888955116272)
[2025-02-17 11:01:30,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:30,529][root][INFO] - Training Epoch: 2/2, step 1084/1149 completed (loss: 0.09653078019618988, acc: 1.0)
[2025-02-17 11:01:30,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:30,889][root][INFO] - Training Epoch: 2/2, step 1085/1149 completed (loss: 0.2784631848335266, acc: 0.9230769276618958)
[2025-02-17 11:01:31,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:31,238][root][INFO] - Training Epoch: 2/2, step 1086/1149 completed (loss: 0.01949778012931347, acc: 1.0)
[2025-02-17 11:01:31,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:31,610][root][INFO] - Training Epoch: 2/2, step 1087/1149 completed (loss: 0.1432417333126068, acc: 0.9230769276618958)
[2025-02-17 11:01:31,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:32,017][root][INFO] - Training Epoch: 2/2, step 1088/1149 completed (loss: 0.350075364112854, acc: 0.9444444179534912)
[2025-02-17 11:01:32,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:32,421][root][INFO] - Training Epoch: 2/2, step 1089/1149 completed (loss: 0.8047618269920349, acc: 0.75)
[2025-02-17 11:01:32,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:32,784][root][INFO] - Training Epoch: 2/2, step 1090/1149 completed (loss: 0.011809371411800385, acc: 1.0)
[2025-02-17 11:01:32,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:33,183][root][INFO] - Training Epoch: 2/2, step 1091/1149 completed (loss: 0.27752846479415894, acc: 0.9655172228813171)
[2025-02-17 11:01:33,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:33,530][root][INFO] - Training Epoch: 2/2, step 1092/1149 completed (loss: 0.29366177320480347, acc: 0.8399999737739563)
[2025-02-17 11:01:33,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:33,878][root][INFO] - Training Epoch: 2/2, step 1093/1149 completed (loss: 0.003587317420169711, acc: 1.0)
[2025-02-17 11:01:34,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:34,220][root][INFO] - Training Epoch: 2/2, step 1094/1149 completed (loss: 0.3848276138305664, acc: 0.9230769276618958)
[2025-02-17 11:01:34,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:34,564][root][INFO] - Training Epoch: 2/2, step 1095/1149 completed (loss: 0.06661073118448257, acc: 1.0)
[2025-02-17 11:01:34,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:34,927][root][INFO] - Training Epoch: 2/2, step 1096/1149 completed (loss: 0.31843146681785583, acc: 0.9473684430122375)
[2025-02-17 11:01:35,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:35,322][root][INFO] - Training Epoch: 2/2, step 1097/1149 completed (loss: 0.4755561351776123, acc: 0.75)
[2025-02-17 11:01:35,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:35,700][root][INFO] - Training Epoch: 2/2, step 1098/1149 completed (loss: 0.2302723079919815, acc: 0.9166666865348816)
[2025-02-17 11:01:35,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:36,090][root][INFO] - Training Epoch: 2/2, step 1099/1149 completed (loss: 0.35315531492233276, acc: 0.8461538553237915)
[2025-02-17 11:01:36,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:36,442][root][INFO] - Training Epoch: 2/2, step 1100/1149 completed (loss: 0.08798399567604065, acc: 1.0)
[2025-02-17 11:01:36,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:36,790][root][INFO] - Training Epoch: 2/2, step 1101/1149 completed (loss: 0.9076035022735596, acc: 0.761904776096344)
[2025-02-17 11:01:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:37,193][root][INFO] - Training Epoch: 2/2, step 1102/1149 completed (loss: 0.015469550155103207, acc: 1.0)
[2025-02-17 11:01:37,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:37,577][root][INFO] - Training Epoch: 2/2, step 1103/1149 completed (loss: 0.23699533939361572, acc: 0.8928571343421936)
[2025-02-17 11:01:37,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:37,949][root][INFO] - Training Epoch: 2/2, step 1104/1149 completed (loss: 0.16387492418289185, acc: 0.9354838728904724)
[2025-02-17 11:01:38,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:38,303][root][INFO] - Training Epoch: 2/2, step 1105/1149 completed (loss: 0.4524528384208679, acc: 0.8461538553237915)
[2025-02-17 11:01:38,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:38,662][root][INFO] - Training Epoch: 2/2, step 1106/1149 completed (loss: 0.43507230281829834, acc: 0.9259259104728699)
[2025-02-17 11:01:38,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:39,038][root][INFO] - Training Epoch: 2/2, step 1107/1149 completed (loss: 1.0580053329467773, acc: 0.7349397540092468)
[2025-02-17 11:01:39,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:39,427][root][INFO] - Training Epoch: 2/2, step 1108/1149 completed (loss: 0.2219116985797882, acc: 0.9387755393981934)
[2025-02-17 11:01:39,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:39,828][root][INFO] - Training Epoch: 2/2, step 1109/1149 completed (loss: 0.9152183532714844, acc: 0.7604166865348816)
[2025-02-17 11:01:40,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:40,238][root][INFO] - Training Epoch: 2/2, step 1110/1149 completed (loss: 0.925990879535675, acc: 0.782608687877655)
[2025-02-17 11:01:40,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:40,646][root][INFO] - Training Epoch: 2/2, step 1111/1149 completed (loss: 0.8704603314399719, acc: 0.7464788556098938)
[2025-02-17 11:01:40,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:41,027][root][INFO] - Training Epoch: 2/2, step 1112/1149 completed (loss: 0.45781487226486206, acc: 0.8450704216957092)
[2025-02-17 11:01:41,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:41,440][root][INFO] - Training Epoch: 2/2, step 1113/1149 completed (loss: 0.9765764474868774, acc: 0.7250000238418579)
[2025-02-17 11:01:41,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:41,808][root][INFO] - Training Epoch: 2/2, step 1114/1149 completed (loss: 0.42300426959991455, acc: 0.8307692408561707)
[2025-02-17 11:01:41,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:42,209][root][INFO] - Training Epoch: 2/2, step 1115/1149 completed (loss: 0.2418152242898941, acc: 0.9038461446762085)
[2025-02-17 11:01:42,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:42,600][root][INFO] - Training Epoch: 2/2, step 1116/1149 completed (loss: 0.4603077173233032, acc: 0.8888888955116272)
[2025-02-17 11:01:42,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:42,967][root][INFO] - Training Epoch: 2/2, step 1117/1149 completed (loss: 0.08765456825494766, acc: 1.0)
[2025-02-17 11:01:43,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:43,338][root][INFO] - Training Epoch: 2/2, step 1118/1149 completed (loss: 0.09478256106376648, acc: 1.0)
[2025-02-17 11:01:43,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:43,760][root][INFO] - Training Epoch: 2/2, step 1119/1149 completed (loss: 0.630499005317688, acc: 0.6153846383094788)
[2025-02-17 11:01:43,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:44,133][root][INFO] - Training Epoch: 2/2, step 1120/1149 completed (loss: 0.19330847263336182, acc: 0.9166666865348816)
[2025-02-17 11:01:44,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:44,480][root][INFO] - Training Epoch: 2/2, step 1121/1149 completed (loss: 0.03577440604567528, acc: 1.0)
[2025-02-17 11:01:44,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:44,889][root][INFO] - Training Epoch: 2/2, step 1122/1149 completed (loss: 0.07635018229484558, acc: 1.0)
[2025-02-17 11:01:45,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:45,351][root][INFO] - Training Epoch: 2/2, step 1123/1149 completed (loss: 0.6378827095031738, acc: 0.7931034564971924)
[2025-02-17 11:01:45,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:45,787][root][INFO] - Training Epoch: 2/2, step 1124/1149 completed (loss: 0.8879139423370361, acc: 0.7301587462425232)
[2025-02-17 11:01:45,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:46,167][root][INFO] - Training Epoch: 2/2, step 1125/1149 completed (loss: 0.8060004115104675, acc: 0.7407407164573669)
[2025-02-17 11:01:46,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:46,550][root][INFO] - Training Epoch: 2/2, step 1126/1149 completed (loss: 0.8086611032485962, acc: 0.7551020383834839)
[2025-02-17 11:01:46,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:46,894][root][INFO] - Training Epoch: 2/2, step 1127/1149 completed (loss: 0.4824342131614685, acc: 0.7857142686843872)
[2025-02-17 11:01:47,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:47,294][root][INFO] - Training Epoch: 2/2, step 1128/1149 completed (loss: 0.633783757686615, acc: 0.8095238208770752)
[2025-02-17 11:01:47,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:47,699][root][INFO] - Training Epoch: 2/2, step 1129/1149 completed (loss: 0.8869925737380981, acc: 0.6296296119689941)
[2025-02-17 11:01:47,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:48,076][root][INFO] - Training Epoch: 2/2, step 1130/1149 completed (loss: 0.6817960143089294, acc: 0.7857142686843872)
[2025-02-17 11:01:48,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:48,428][root][INFO] - Training Epoch: 2/2, step 1131/1149 completed (loss: 0.24896255135536194, acc: 0.8999999761581421)
[2025-02-17 11:01:48,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:48,862][root][INFO] - Training Epoch: 2/2, step 1132/1149 completed (loss: 0.5991035103797913, acc: 0.89552241563797)
[2025-02-17 11:01:49,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:49,215][root][INFO] - Training Epoch: 2/2, step 1133/1149 completed (loss: 0.20735101401805878, acc: 0.9411764740943909)
[2025-02-17 11:01:49,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:49,566][root][INFO] - Training Epoch: 2/2, step 1134/1149 completed (loss: 0.0989280492067337, acc: 1.0)
[2025-02-17 11:01:49,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:49,920][root][INFO] - Training Epoch: 2/2, step 1135/1149 completed (loss: 0.4641196131706238, acc: 0.8947368264198303)
[2025-02-17 11:01:50,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:50,341][root][INFO] - Training Epoch: 2/2, step 1136/1149 completed (loss: 0.16247883439064026, acc: 0.8666666746139526)
[2025-02-17 11:01:50,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:50,697][root][INFO] - Training Epoch: 2/2, step 1137/1149 completed (loss: 0.1165432557463646, acc: 1.0)
[2025-02-17 11:01:50,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:51,146][root][INFO] - Training Epoch: 2/2, step 1138/1149 completed (loss: 0.48988404870033264, acc: 0.8771929740905762)
[2025-02-17 11:01:51,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:51,598][root][INFO] - Training Epoch: 2/2, step 1139/1149 completed (loss: 0.7212005257606506, acc: 0.7692307829856873)
[2025-02-17 11:01:51,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:51,991][root][INFO] - Training Epoch: 2/2, step 1140/1149 completed (loss: 0.09979646652936935, acc: 0.9677419066429138)
[2025-02-17 11:01:52,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:52,348][root][INFO] - Training Epoch: 2/2, step 1141/1149 completed (loss: 0.3950877785682678, acc: 0.8387096524238586)
[2025-02-17 11:01:52,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:52,751][root][INFO] - Training Epoch: 2/2, step 1142/1149 completed (loss: 0.9698024988174438, acc: 0.7636363506317139)
[2025-02-17 11:01:52,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:53,214][root][INFO] - Training Epoch: 2/2, step 1143/1149 completed (loss: 0.43294695019721985, acc: 0.8870967626571655)
[2025-02-17 11:01:53,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:53,650][root][INFO] - Training Epoch: 2/2, step 1144/1149 completed (loss: 0.7091338634490967, acc: 0.78125)
[2025-02-17 11:01:53,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:54,043][root][INFO] - Training Epoch: 2/2, step 1145/1149 completed (loss: 1.08828866481781, acc: 0.689393937587738)
[2025-02-17 11:01:54,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:54,411][root][INFO] - Training Epoch: 2/2, step 1146/1149 completed (loss: 0.7176963686943054, acc: 0.7799999713897705)
[2025-02-17 11:01:55,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:55,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:56,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:56,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:57,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:57,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:58,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:58,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:58,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:01:59,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:00,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:00,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:00,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:01,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:01,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:02,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:03,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:03,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:03,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:04,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:04,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:05,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:05,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:06,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:07,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:07,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:07,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:08,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:08,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:09,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:09,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:10,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:11,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:11,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:11,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:12,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:12,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:13,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:13,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:14,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:14,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:14,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:15,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:15,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:16,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:16,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:17,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:17,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:17,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:18,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:18,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:19,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:19,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:20,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:21,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:21,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:22,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:22,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:23,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:23,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:24,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:24,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:24,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:25,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:25,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:26,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:26,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:27,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:27,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:28,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:28,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:29,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:29,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:30,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:30,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:31,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:31,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:32,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:32,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:33,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:33,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:34,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:34,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:35,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:35,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:36,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:36,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:37,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:37,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:38,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:38,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:39,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:39,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:40,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:40,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:41,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:41,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:42,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:42,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:43,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:43,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:44,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:44,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:45,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:46,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:46,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:47,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:47,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:47,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:48,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:48,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:50,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:50,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:50,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:51,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:51,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:52,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:52,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:53,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:53,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:54,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:54,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:55,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:55,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:56,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:56,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:56,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:57,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:57,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:58,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:58,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:59,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:02:59,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:00,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:01,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:01,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:02,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:02,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:03,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:03,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:03,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:04,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:04,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:05,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:05,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:05,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:06,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:06,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:07,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:07,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:08,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:08,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:09,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:09,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:10,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:10,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:11,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:11,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:12,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:12,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:12,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:13,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:13,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:14,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:14,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:15,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:15,986][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2743, device='cuda:0') eval_epoch_loss=tensor(0.8217, device='cuda:0') eval_epoch_acc=tensor(0.7737, device='cuda:0')
[2025-02-17 11:03:15,988][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-17 11:03:15,988][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-17 11:03:20,464][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2/asr_epoch_2_step_1147_loss_0.8216672539710999/model.pt
[2025-02-17 11:03:20,474][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_seed_42_unfreeze_encoder2 directory
[2025-02-17 11:03:20,474][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.8216672539710999
[2025-02-17 11:03:20,475][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.7736963033676147
[2025-02-17 11:03:20,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:20,875][root][INFO] - Training Epoch: 2/2, step 1147/1149 completed (loss: 1.0111738443374634, acc: 0.7339449524879456)
[2025-02-17 11:03:21,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-17 11:03:21,230][root][INFO] - Training Epoch: 2/2, step 1148/1149 completed (loss: 0.17448580265045166, acc: 0.9245283007621765)
[2025-02-17 11:03:21,604][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=2.4561, train_epoch_loss=0.8986, epoch time 799.7203972153366s
[2025-02-17 11:03:21,605][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 11 GB
[2025-02-17 11:03:21,605][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2025-02-17 11:03:21,605][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 11 GB
[2025-02-17 11:03:21,605][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-17 11:03:21,605][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-17 11:03:21,608][root][INFO] - Key: avg_train_prep, Value: 6.674558639526367
[2025-02-17 11:03:21,610][root][INFO] - Key: avg_train_loss, Value: 1.643349289894104
[2025-02-17 11:03:21,610][root][INFO] - Key: avg_train_acc, Value: 0.5959835052490234
[2025-02-17 11:03:21,610][root][INFO] - Key: avg_eval_prep, Value: 5.738663196563721
[2025-02-17 11:03:21,610][root][INFO] - Key: avg_eval_loss, Value: 1.5077635049819946
[2025-02-17 11:03:21,610][root][INFO] - Key: avg_eval_acc, Value: 0.6069151759147644
[2025-02-17 11:03:21,610][root][INFO] - Key: avg_epoch_time, Value: 817.6924134697765
[2025-02-17 11:03:21,610][root][INFO] - Key: avg_checkpoint_time, Value: 3.7753592641092837
