[2025-02-12 21:56:43,179][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-12 21:56:43,180][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-12 21:56:43,180][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-02-12 21:56:43,180][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-12_21-56-42.txt', 'log_interval': 5}
[2025-02-12 21:57:11,472][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-12 21:57:16,594][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 21:57:16,597][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-12 21:57:16,600][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 21:57:16,601][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-12 21:57:26,911][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 21:57:26,919][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-12 21:57:26,919][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-12 21:57:27,036][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 21:57:27,038][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-12 21:57:27,129][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-12 21:57:27,130][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-12 21:57:27,130][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_0.4641912281513214/model.pt
[2025-02-12 21:57:27,395][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-12 21:57:27,400][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-12 21:57:28,912][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-12 21:57:34,027][root][INFO] - --> Training Set Length = 2298
[2025-02-12 21:57:34,049][root][INFO] - --> Validation Set Length = 341
[2025-02-12 21:57:34,050][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-12 21:57:34,050][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-12 21:57:36,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:37,736][root][INFO] - Training Epoch: 1/2, step 0/574 completed (loss: 1.2123773097991943, acc: 0.5925925970077515)
[2025-02-12 21:57:37,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:38,150][root][INFO] - Training Epoch: 1/2, step 1/574 completed (loss: 1.0402058362960815, acc: 0.7200000286102295)
[2025-02-12 21:57:38,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:38,659][root][INFO] - Training Epoch: 1/2, step 2/574 completed (loss: 1.9819635152816772, acc: 0.5945945978164673)
[2025-02-12 21:57:38,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:39,005][root][INFO] - Training Epoch: 1/2, step 3/574 completed (loss: 2.007331132888794, acc: 0.6315789222717285)
[2025-02-12 21:57:39,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:39,359][root][INFO] - Training Epoch: 1/2, step 4/574 completed (loss: 2.044408082962036, acc: 0.5135135054588318)
[2025-02-12 21:57:39,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:39,728][root][INFO] - Training Epoch: 1/2, step 5/574 completed (loss: 0.9733689427375793, acc: 0.6428571343421936)
[2025-02-12 21:57:39,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:40,159][root][INFO] - Training Epoch: 1/2, step 6/574 completed (loss: 2.235187530517578, acc: 0.5918367505073547)
[2025-02-12 21:57:40,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:40,484][root][INFO] - Training Epoch: 1/2, step 7/574 completed (loss: 1.080947756767273, acc: 0.7666666507720947)
[2025-02-12 21:57:40,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:40,933][root][INFO] - Training Epoch: 1/2, step 8/574 completed (loss: 0.5479683876037598, acc: 0.8636363744735718)
[2025-02-12 21:57:41,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:41,315][root][INFO] - Training Epoch: 1/2, step 9/574 completed (loss: 0.3224884569644928, acc: 0.8461538553237915)
[2025-02-12 21:57:41,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:41,714][root][INFO] - Training Epoch: 1/2, step 10/574 completed (loss: 0.5700385570526123, acc: 0.8518518805503845)
[2025-02-12 21:57:41,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:42,138][root][INFO] - Training Epoch: 1/2, step 11/574 completed (loss: 2.7608680725097656, acc: 0.5641025900840759)
[2025-02-12 21:57:42,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:42,505][root][INFO] - Training Epoch: 1/2, step 12/574 completed (loss: 1.9869766235351562, acc: 0.7272727489471436)
[2025-02-12 21:57:42,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:42,833][root][INFO] - Training Epoch: 1/2, step 13/574 completed (loss: 1.67548406124115, acc: 0.6304348111152649)
[2025-02-12 21:57:42,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:43,208][root][INFO] - Training Epoch: 1/2, step 14/574 completed (loss: 1.803454875946045, acc: 0.7647058963775635)
[2025-02-12 21:57:43,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:43,618][root][INFO] - Training Epoch: 1/2, step 15/574 completed (loss: 1.240387201309204, acc: 0.7142857313156128)
[2025-02-12 21:57:43,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:44,063][root][INFO] - Training Epoch: 1/2, step 16/574 completed (loss: 0.8095865845680237, acc: 0.7894737124443054)
[2025-02-12 21:57:44,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:44,426][root][INFO] - Training Epoch: 1/2, step 17/574 completed (loss: 0.9106447100639343, acc: 0.8333333134651184)
[2025-02-12 21:57:44,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:44,746][root][INFO] - Training Epoch: 1/2, step 18/574 completed (loss: 1.6903462409973145, acc: 0.6944444179534912)
[2025-02-12 21:57:44,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:45,123][root][INFO] - Training Epoch: 1/2, step 19/574 completed (loss: 0.5465536117553711, acc: 0.8421052694320679)
[2025-02-12 21:57:45,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:45,442][root][INFO] - Training Epoch: 1/2, step 20/574 completed (loss: 0.8957732319831848, acc: 0.8846153616905212)
[2025-02-12 21:57:45,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:45,779][root][INFO] - Training Epoch: 1/2, step 21/574 completed (loss: 1.5926676988601685, acc: 0.7241379022598267)
[2025-02-12 21:57:45,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:46,165][root][INFO] - Training Epoch: 1/2, step 22/574 completed (loss: 1.7029953002929688, acc: 0.5600000023841858)
[2025-02-12 21:57:46,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:46,497][root][INFO] - Training Epoch: 1/2, step 23/574 completed (loss: 1.1780054569244385, acc: 0.8571428656578064)
[2025-02-12 21:57:46,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:46,860][root][INFO] - Training Epoch: 1/2, step 24/574 completed (loss: 0.841812014579773, acc: 0.75)
[2025-02-12 21:57:46,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:47,215][root][INFO] - Training Epoch: 1/2, step 25/574 completed (loss: 1.5828889608383179, acc: 0.6792452931404114)
[2025-02-12 21:57:47,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:47,587][root][INFO] - Training Epoch: 1/2, step 26/574 completed (loss: 2.1746575832366943, acc: 0.5068492889404297)
[2025-02-12 21:57:48,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:48,811][root][INFO] - Training Epoch: 1/2, step 27/574 completed (loss: 1.8525415658950806, acc: 0.5770751237869263)
[2025-02-12 21:57:48,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:49,129][root][INFO] - Training Epoch: 1/2, step 28/574 completed (loss: 1.5487024784088135, acc: 0.7441860437393188)
[2025-02-12 21:57:49,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:49,480][root][INFO] - Training Epoch: 1/2, step 29/574 completed (loss: 1.6990346908569336, acc: 0.7228915691375732)
[2025-02-12 21:57:49,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:49,914][root][INFO] - Training Epoch: 1/2, step 30/574 completed (loss: 1.3771754503250122, acc: 0.7654321193695068)
[2025-02-12 21:57:50,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:50,275][root][INFO] - Training Epoch: 1/2, step 31/574 completed (loss: 1.5923372507095337, acc: 0.6428571343421936)
[2025-02-12 21:57:50,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:50,612][root][INFO] - Training Epoch: 1/2, step 32/574 completed (loss: 1.13718843460083, acc: 0.7407407164573669)
[2025-02-12 21:57:50,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:50,940][root][INFO] - Training Epoch: 1/2, step 33/574 completed (loss: 0.9883363842964172, acc: 0.9130434989929199)
[2025-02-12 21:57:51,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:51,270][root][INFO] - Training Epoch: 1/2, step 34/574 completed (loss: 1.4844530820846558, acc: 0.680672287940979)
[2025-02-12 21:57:51,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:51,597][root][INFO] - Training Epoch: 1/2, step 35/574 completed (loss: 1.1843876838684082, acc: 0.8196721076965332)
[2025-02-12 21:57:51,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:51,924][root][INFO] - Training Epoch: 1/2, step 36/574 completed (loss: 1.9367684125900269, acc: 0.6349206566810608)
[2025-02-12 21:57:52,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:52,249][root][INFO] - Training Epoch: 1/2, step 37/574 completed (loss: 1.758981704711914, acc: 0.7796609997749329)
[2025-02-12 21:57:52,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:52,602][root][INFO] - Training Epoch: 1/2, step 38/574 completed (loss: 1.778971552848816, acc: 0.7241379022598267)
[2025-02-12 21:57:52,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:52,945][root][INFO] - Training Epoch: 1/2, step 39/574 completed (loss: 0.9752068519592285, acc: 0.7142857313156128)
[2025-02-12 21:57:53,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:53,279][root][INFO] - Training Epoch: 1/2, step 40/574 completed (loss: 0.5208830237388611, acc: 0.8461538553237915)
[2025-02-12 21:57:53,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:53,646][root][INFO] - Training Epoch: 1/2, step 41/574 completed (loss: 1.0186275243759155, acc: 0.7567567825317383)
[2025-02-12 21:57:53,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:54,023][root][INFO] - Training Epoch: 1/2, step 42/574 completed (loss: 2.7743992805480957, acc: 0.5538461804389954)
[2025-02-12 21:57:54,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:54,459][root][INFO] - Training Epoch: 1/2, step 43/574 completed (loss: 2.4932823181152344, acc: 0.5252525210380554)
[2025-02-12 21:57:54,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:54,887][root][INFO] - Training Epoch: 1/2, step 44/574 completed (loss: 2.2864832878112793, acc: 0.6494845151901245)
[2025-02-12 21:57:55,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:55,302][root][INFO] - Training Epoch: 1/2, step 45/574 completed (loss: 2.981733798980713, acc: 0.5661764740943909)
[2025-02-12 21:57:55,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:55,642][root][INFO] - Training Epoch: 1/2, step 46/574 completed (loss: 1.1867396831512451, acc: 0.692307710647583)
[2025-02-12 21:57:55,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:55,994][root][INFO] - Training Epoch: 1/2, step 47/574 completed (loss: 0.7638466954231262, acc: 0.8148148059844971)
[2025-02-12 21:57:56,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:56,332][root][INFO] - Training Epoch: 1/2, step 48/574 completed (loss: 0.9092983603477478, acc: 0.75)
[2025-02-12 21:57:56,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:56,639][root][INFO] - Training Epoch: 1/2, step 49/574 completed (loss: 0.7460682392120361, acc: 0.9444444179534912)
[2025-02-12 21:57:56,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:56,980][root][INFO] - Training Epoch: 1/2, step 50/574 completed (loss: 2.7767059803009033, acc: 0.5964912176132202)
[2025-02-12 21:57:57,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:57,349][root][INFO] - Training Epoch: 1/2, step 51/574 completed (loss: 2.2970659732818604, acc: 0.6507936716079712)
[2025-02-12 21:57:57,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:57,676][root][INFO] - Training Epoch: 1/2, step 52/574 completed (loss: 3.69342303276062, acc: 0.5492957830429077)
[2025-02-12 21:57:57,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:58,144][root][INFO] - Training Epoch: 1/2, step 53/574 completed (loss: 3.6603238582611084, acc: 0.46000000834465027)
[2025-02-12 21:57:58,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:58,612][root][INFO] - Training Epoch: 1/2, step 54/574 completed (loss: 3.4149038791656494, acc: 0.5135135054588318)
[2025-02-12 21:57:58,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:58,995][root][INFO] - Training Epoch: 1/2, step 55/574 completed (loss: 0.7382039427757263, acc: 0.7692307829856873)
[2025-02-12 21:58:00,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:01,646][root][INFO] - Training Epoch: 1/2, step 56/574 completed (loss: 2.2497355937957764, acc: 0.49829351902008057)
[2025-02-12 21:58:02,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:02,895][root][INFO] - Training Epoch: 1/2, step 57/574 completed (loss: 2.7494893074035645, acc: 0.4575163424015045)
[2025-02-12 21:58:03,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:03,610][root][INFO] - Training Epoch: 1/2, step 58/574 completed (loss: 3.457415819168091, acc: 0.3920454680919647)
[2025-02-12 21:58:03,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:04,229][root][INFO] - Training Epoch: 1/2, step 59/574 completed (loss: 2.2614872455596924, acc: 0.5441176295280457)
[2025-02-12 21:58:04,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:04,812][root][INFO] - Training Epoch: 1/2, step 60/574 completed (loss: 3.223626136779785, acc: 0.41304346919059753)
[2025-02-12 21:58:05,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:05,250][root][INFO] - Training Epoch: 1/2, step 61/574 completed (loss: 2.5292458534240723, acc: 0.574999988079071)
[2025-02-12 21:58:05,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:05,612][root][INFO] - Training Epoch: 1/2, step 62/574 completed (loss: 1.5675443410873413, acc: 0.6470588445663452)
[2025-02-12 21:58:05,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:05,998][root][INFO] - Training Epoch: 1/2, step 63/574 completed (loss: 3.2193446159362793, acc: 0.4444444477558136)
[2025-02-12 21:58:06,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:06,339][root][INFO] - Training Epoch: 1/2, step 64/574 completed (loss: 1.4208825826644897, acc: 0.796875)
[2025-02-12 21:58:06,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:06,626][root][INFO] - Training Epoch: 1/2, step 65/574 completed (loss: 0.3042449951171875, acc: 0.8965517282485962)
[2025-02-12 21:58:06,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:06,954][root][INFO] - Training Epoch: 1/2, step 66/574 completed (loss: 3.2106730937957764, acc: 0.5178571343421936)
[2025-02-12 21:58:07,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:07,287][root][INFO] - Training Epoch: 1/2, step 67/574 completed (loss: 3.1955277919769287, acc: 0.4000000059604645)
[2025-02-12 21:58:07,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:07,624][root][INFO] - Training Epoch: 1/2, step 68/574 completed (loss: 0.8033586740493774, acc: 0.800000011920929)
[2025-02-12 21:58:07,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:07,942][root][INFO] - Training Epoch: 1/2, step 69/574 completed (loss: 1.654498815536499, acc: 0.6944444179534912)
[2025-02-12 21:58:08,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:08,206][root][INFO] - Training Epoch: 1/2, step 70/574 completed (loss: 3.0442283153533936, acc: 0.5454545617103577)
[2025-02-12 21:58:08,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:08,584][root][INFO] - Training Epoch: 1/2, step 71/574 completed (loss: 2.3495121002197266, acc: 0.5735294222831726)
[2025-02-12 21:58:08,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:08,992][root][INFO] - Training Epoch: 1/2, step 72/574 completed (loss: 1.4581241607666016, acc: 0.6666666865348816)
[2025-02-12 21:58:09,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:09,347][root][INFO] - Training Epoch: 1/2, step 73/574 completed (loss: 2.0577187538146973, acc: 0.5435897707939148)
[2025-02-12 21:58:09,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:09,726][root][INFO] - Training Epoch: 1/2, step 74/574 completed (loss: 3.233293294906616, acc: 0.5)
[2025-02-12 21:58:09,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:10,096][root][INFO] - Training Epoch: 1/2, step 75/574 completed (loss: 2.3597354888916016, acc: 0.5)
[2025-02-12 21:58:10,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:10,504][root][INFO] - Training Epoch: 1/2, step 76/574 completed (loss: 2.5842175483703613, acc: 0.485401451587677)
[2025-02-12 21:58:10,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:10,931][root][INFO] - Training Epoch: 1/2, step 77/574 completed (loss: 0.37037181854248047, acc: 0.9523809552192688)
[2025-02-12 21:58:11,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:11,325][root][INFO] - Training Epoch: 1/2, step 78/574 completed (loss: 0.7823255658149719, acc: 0.7916666865348816)
[2025-02-12 21:58:11,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:11,689][root][INFO] - Training Epoch: 1/2, step 79/574 completed (loss: 0.46254023909568787, acc: 0.8484848737716675)
[2025-02-12 21:58:11,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:12,107][root][INFO] - Training Epoch: 1/2, step 80/574 completed (loss: 1.080047845840454, acc: 0.8461538553237915)
[2025-02-12 21:58:12,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:12,471][root][INFO] - Training Epoch: 1/2, step 81/574 completed (loss: 2.012784242630005, acc: 0.6153846383094788)
[2025-02-12 21:58:12,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:12,886][root][INFO] - Training Epoch: 1/2, step 82/574 completed (loss: 1.8766556978225708, acc: 0.6730769276618958)
[2025-02-12 21:58:13,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:13,226][root][INFO] - Training Epoch: 1/2, step 83/574 completed (loss: 1.2910434007644653, acc: 0.6875)
[2025-02-12 21:58:13,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:13,600][root][INFO] - Training Epoch: 1/2, step 84/574 completed (loss: 2.0254597663879395, acc: 0.6521739363670349)
[2025-02-12 21:58:13,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:13,981][root][INFO] - Training Epoch: 1/2, step 85/574 completed (loss: 2.6548616886138916, acc: 0.5400000214576721)
[2025-02-12 21:58:14,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:14,310][root][INFO] - Training Epoch: 1/2, step 86/574 completed (loss: 1.1134964227676392, acc: 0.695652186870575)
[2025-02-12 21:58:14,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:14,816][root][INFO] - Training Epoch: 1/2, step 87/574 completed (loss: 3.0203843116760254, acc: 0.46000000834465027)
[2025-02-12 21:58:14,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:15,161][root][INFO] - Training Epoch: 1/2, step 88/574 completed (loss: 2.5188121795654297, acc: 0.6019417643547058)
[2025-02-12 21:58:15,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:16,247][root][INFO] - Training Epoch: 1/2, step 89/574 completed (loss: 2.5474514961242676, acc: 0.5728155374526978)
[2025-02-12 21:58:16,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:17,072][root][INFO] - Training Epoch: 1/2, step 90/574 completed (loss: 2.5929300785064697, acc: 0.5645161271095276)
[2025-02-12 21:58:17,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:17,870][root][INFO] - Training Epoch: 1/2, step 91/574 completed (loss: 2.1956162452697754, acc: 0.5517241358757019)
[2025-02-12 21:58:18,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:18,620][root][INFO] - Training Epoch: 1/2, step 92/574 completed (loss: 2.1674816608428955, acc: 0.6421052813529968)
[2025-02-12 21:58:19,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:19,636][root][INFO] - Training Epoch: 1/2, step 93/574 completed (loss: 2.803779125213623, acc: 0.3465346395969391)
[2025-02-12 21:58:19,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:20,049][root][INFO] - Training Epoch: 1/2, step 94/574 completed (loss: 1.9846004247665405, acc: 0.5967742204666138)
[2025-02-12 21:58:20,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:20,450][root][INFO] - Training Epoch: 1/2, step 95/574 completed (loss: 1.9313743114471436, acc: 0.6811594367027283)
[2025-02-12 21:58:20,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:20,887][root][INFO] - Training Epoch: 1/2, step 96/574 completed (loss: 2.903655529022217, acc: 0.5042017102241516)
[2025-02-12 21:58:21,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:21,373][root][INFO] - Training Epoch: 1/2, step 97/574 completed (loss: 2.4056878089904785, acc: 0.5288461446762085)
[2025-02-12 21:58:21,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:21,793][root][INFO] - Training Epoch: 1/2, step 98/574 completed (loss: 2.4805190563201904, acc: 0.540145993232727)
[2025-02-12 21:58:21,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:22,122][root][INFO] - Training Epoch: 1/2, step 99/574 completed (loss: 3.296006202697754, acc: 0.3731343150138855)
[2025-02-12 21:58:22,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:22,454][root][INFO] - Training Epoch: 1/2, step 100/574 completed (loss: 1.5298702716827393, acc: 0.6000000238418579)
[2025-02-12 21:58:22,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:22,775][root][INFO] - Training Epoch: 1/2, step 101/574 completed (loss: 0.7606174349784851, acc: 0.9545454382896423)
[2025-02-12 21:58:22,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:23,138][root][INFO] - Training Epoch: 1/2, step 102/574 completed (loss: 0.24039141833782196, acc: 0.95652174949646)
[2025-02-12 21:58:23,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:23,466][root][INFO] - Training Epoch: 1/2, step 103/574 completed (loss: 0.6113458871841431, acc: 0.7954545617103577)
[2025-02-12 21:58:23,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:23,790][root][INFO] - Training Epoch: 1/2, step 104/574 completed (loss: 1.434637427330017, acc: 0.6896551847457886)
[2025-02-12 21:58:23,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:24,153][root][INFO] - Training Epoch: 1/2, step 105/574 completed (loss: 0.9927895069122314, acc: 0.7906976938247681)
[2025-02-12 21:58:24,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:24,501][root][INFO] - Training Epoch: 1/2, step 106/574 completed (loss: 1.4568448066711426, acc: 0.7200000286102295)
[2025-02-12 21:58:24,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:24,847][root][INFO] - Training Epoch: 1/2, step 107/574 completed (loss: 0.23853784799575806, acc: 0.9411764740943909)
[2025-02-12 21:58:25,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:25,258][root][INFO] - Training Epoch: 1/2, step 108/574 completed (loss: 0.19032065570354462, acc: 0.9615384340286255)
[2025-02-12 21:58:25,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:25,604][root][INFO] - Training Epoch: 1/2, step 109/574 completed (loss: 0.6855543255805969, acc: 0.9285714030265808)
[2025-02-12 21:58:25,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:25,972][root][INFO] - Training Epoch: 1/2, step 110/574 completed (loss: 1.584069848060608, acc: 0.7384615540504456)
[2025-02-12 21:58:26,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:26,481][root][INFO] - Training Epoch: 1/2, step 111/574 completed (loss: 1.8123921155929565, acc: 0.6315789222717285)
[2025-02-12 21:58:26,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:26,936][root][INFO] - Training Epoch: 1/2, step 112/574 completed (loss: 2.127500534057617, acc: 0.6140350699424744)
[2025-02-12 21:58:27,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:27,350][root][INFO] - Training Epoch: 1/2, step 113/574 completed (loss: 1.9607309103012085, acc: 0.6410256624221802)
[2025-02-12 21:58:27,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:27,735][root][INFO] - Training Epoch: 1/2, step 114/574 completed (loss: 1.5019817352294922, acc: 0.6734693646430969)
[2025-02-12 21:58:27,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:28,065][root][INFO] - Training Epoch: 1/2, step 115/574 completed (loss: 0.3831067383289337, acc: 0.9090909361839294)
[2025-02-12 21:58:28,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:28,414][root][INFO] - Training Epoch: 1/2, step 116/574 completed (loss: 1.154505729675293, acc: 0.6984127163887024)
[2025-02-12 21:58:28,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:28,785][root][INFO] - Training Epoch: 1/2, step 117/574 completed (loss: 1.2515751123428345, acc: 0.7479674816131592)
[2025-02-12 21:58:28,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:29,178][root][INFO] - Training Epoch: 1/2, step 118/574 completed (loss: 1.5560117959976196, acc: 0.7580645084381104)
[2025-02-12 21:58:29,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:30,054][root][INFO] - Training Epoch: 1/2, step 119/574 completed (loss: 1.457958698272705, acc: 0.661596953868866)
[2025-02-12 21:58:30,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:30,441][root][INFO] - Training Epoch: 1/2, step 120/574 completed (loss: 1.4706735610961914, acc: 0.746666669845581)
[2025-02-12 21:58:30,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:30,872][root][INFO] - Training Epoch: 1/2, step 121/574 completed (loss: 2.0692861080169678, acc: 0.6730769276618958)
[2025-02-12 21:58:31,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:31,284][root][INFO] - Training Epoch: 1/2, step 122/574 completed (loss: 0.6971147060394287, acc: 0.8333333134651184)
[2025-02-12 21:58:31,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:31,680][root][INFO] - Training Epoch: 1/2, step 123/574 completed (loss: 0.5011228919029236, acc: 0.7894737124443054)
[2025-02-12 21:58:31,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:32,023][root][INFO] - Training Epoch: 1/2, step 124/574 completed (loss: 1.6680556535720825, acc: 0.6257668733596802)
[2025-02-12 21:58:32,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:32,417][root][INFO] - Training Epoch: 1/2, step 125/574 completed (loss: 1.637912631034851, acc: 0.6388888955116272)
[2025-02-12 21:58:32,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:32,779][root][INFO] - Training Epoch: 1/2, step 126/574 completed (loss: 1.6645395755767822, acc: 0.6416666507720947)
[2025-02-12 21:58:32,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:33,243][root][INFO] - Training Epoch: 1/2, step 127/574 completed (loss: 1.1655194759368896, acc: 0.7202380895614624)
[2025-02-12 21:58:33,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:33,628][root][INFO] - Training Epoch: 1/2, step 128/574 completed (loss: 1.2881964445114136, acc: 0.7025641202926636)
[2025-02-12 21:58:33,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:34,086][root][INFO] - Training Epoch: 1/2, step 129/574 completed (loss: 1.5169670581817627, acc: 0.6617646813392639)
[2025-02-12 21:58:34,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:34,456][root][INFO] - Training Epoch: 1/2, step 130/574 completed (loss: 1.96230947971344, acc: 0.5384615659713745)
[2025-02-12 21:58:34,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:34,776][root][INFO] - Training Epoch: 1/2, step 131/574 completed (loss: 1.1008390188217163, acc: 0.8260869383811951)
[2025-02-12 21:58:34,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:35,226][root][INFO] - Training Epoch: 1/2, step 132/574 completed (loss: 1.467386245727539, acc: 0.59375)
[2025-02-12 21:58:35,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:35,591][root][INFO] - Training Epoch: 1/2, step 133/574 completed (loss: 1.883764386177063, acc: 0.47826087474823)
[2025-02-12 21:58:35,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:35,910][root][INFO] - Training Epoch: 1/2, step 134/574 completed (loss: 1.2442923784255981, acc: 0.6571428775787354)
[2025-02-12 21:58:36,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:36,181][root][INFO] - Training Epoch: 1/2, step 135/574 completed (loss: 1.603270411491394, acc: 0.5)
[2025-02-12 21:58:36,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:36,526][root][INFO] - Training Epoch: 1/2, step 136/574 completed (loss: 1.636455774307251, acc: 0.6904761791229248)
[2025-02-12 21:58:36,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:36,943][root][INFO] - Training Epoch: 1/2, step 137/574 completed (loss: 1.6343705654144287, acc: 0.5)
[2025-02-12 21:58:37,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:37,341][root][INFO] - Training Epoch: 1/2, step 138/574 completed (loss: 1.481778860092163, acc: 0.6521739363670349)
[2025-02-12 21:58:37,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:37,698][root][INFO] - Training Epoch: 1/2, step 139/574 completed (loss: 0.8218656182289124, acc: 0.761904776096344)
[2025-02-12 21:58:37,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:38,063][root][INFO] - Training Epoch: 1/2, step 140/574 completed (loss: 1.9753196239471436, acc: 0.6153846383094788)
[2025-02-12 21:58:38,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:38,413][root][INFO] - Training Epoch: 1/2, step 141/574 completed (loss: 1.9750863313674927, acc: 0.4516128897666931)
[2025-02-12 21:58:38,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:38,720][root][INFO] - Training Epoch: 1/2, step 142/574 completed (loss: 1.3458967208862305, acc: 0.6216216087341309)
[2025-02-12 21:58:39,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:40,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:40,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:41,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:41,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:41,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:42,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:42,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:42,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:43,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:43,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:44,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:44,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:45,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:45,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:45,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:46,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:46,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:47,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:47,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:47,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:48,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:48,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:48,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:49,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:49,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:50,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:50,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:50,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:51,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:51,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:51,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:52,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:52,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:52,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:53,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:53,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:54,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:54,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:54,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:55,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:55,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:55,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:56,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:56,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:56,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:57,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:57,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:57,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:58,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:58,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:58,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:59,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:59,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:59,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:00,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:00,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:01,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:01,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:01,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:02,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:02,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:03,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:03,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:03,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:04,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:04,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:05,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:05,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:06,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:06,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:06,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:07,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:07,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:09,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:09,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:10,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:10,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:10,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:11,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:11,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:12,368][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.9451, device='cuda:0') eval_epoch_loss=tensor(1.0802, device='cuda:0') eval_epoch_acc=tensor(0.7609, device='cuda:0')
[2025-02-12 21:59:12,370][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 21:59:12,370][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 21:59:12,698][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.0801525115966797/model.pt
[2025-02-12 21:59:12,704][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 21:59:12,706][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.0801525115966797
[2025-02-12 21:59:12,706][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7609144449234009
[2025-02-12 21:59:13,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:13,289][root][INFO] - Training Epoch: 1/2, step 143/574 completed (loss: 2.08769154548645, acc: 0.5350877046585083)
[2025-02-12 21:59:13,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:13,731][root][INFO] - Training Epoch: 1/2, step 144/574 completed (loss: 1.4927887916564941, acc: 0.6641790866851807)
[2025-02-12 21:59:13,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:14,114][root][INFO] - Training Epoch: 1/2, step 145/574 completed (loss: 1.5016640424728394, acc: 0.6224489808082581)
[2025-02-12 21:59:14,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:14,611][root][INFO] - Training Epoch: 1/2, step 146/574 completed (loss: 1.9498937129974365, acc: 0.542553186416626)
[2025-02-12 21:59:14,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:15,095][root][INFO] - Training Epoch: 1/2, step 147/574 completed (loss: 2.396799087524414, acc: 0.5285714268684387)
[2025-02-12 21:59:15,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:15,542][root][INFO] - Training Epoch: 1/2, step 148/574 completed (loss: 2.1481878757476807, acc: 0.5)
[2025-02-12 21:59:15,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:15,905][root][INFO] - Training Epoch: 1/2, step 149/574 completed (loss: 1.4212249517440796, acc: 0.6521739363670349)
[2025-02-12 21:59:16,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:16,218][root][INFO] - Training Epoch: 1/2, step 150/574 completed (loss: 1.0913982391357422, acc: 0.8275862336158752)
[2025-02-12 21:59:16,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:16,622][root][INFO] - Training Epoch: 1/2, step 151/574 completed (loss: 1.9369066953659058, acc: 0.6086956262588501)
[2025-02-12 21:59:16,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:16,989][root][INFO] - Training Epoch: 1/2, step 152/574 completed (loss: 1.57001531124115, acc: 0.6101694703102112)
[2025-02-12 21:59:17,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:17,374][root][INFO] - Training Epoch: 1/2, step 153/574 completed (loss: 1.957608938217163, acc: 0.5789473652839661)
[2025-02-12 21:59:17,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:17,742][root][INFO] - Training Epoch: 1/2, step 154/574 completed (loss: 2.0750279426574707, acc: 0.5405405163764954)
[2025-02-12 21:59:17,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:18,051][root][INFO] - Training Epoch: 1/2, step 155/574 completed (loss: 1.3979756832122803, acc: 0.7142857313156128)
[2025-02-12 21:59:18,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:18,390][root][INFO] - Training Epoch: 1/2, step 156/574 completed (loss: 0.5811362862586975, acc: 0.8260869383811951)
[2025-02-12 21:59:18,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:18,742][root][INFO] - Training Epoch: 1/2, step 157/574 completed (loss: 2.0574448108673096, acc: 0.4736842215061188)
[2025-02-12 21:59:19,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:20,356][root][INFO] - Training Epoch: 1/2, step 158/574 completed (loss: 2.7132954597473145, acc: 0.5675675868988037)
[2025-02-12 21:59:20,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:20,691][root][INFO] - Training Epoch: 1/2, step 159/574 completed (loss: 1.8372586965560913, acc: 0.5740740895271301)
[2025-02-12 21:59:20,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:21,151][root][INFO] - Training Epoch: 1/2, step 160/574 completed (loss: 2.2550735473632812, acc: 0.5348837375640869)
[2025-02-12 21:59:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:21,802][root][INFO] - Training Epoch: 1/2, step 161/574 completed (loss: 2.3187878131866455, acc: 0.5647059082984924)
[2025-02-12 21:59:22,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:22,406][root][INFO] - Training Epoch: 1/2, step 162/574 completed (loss: 2.1832025051116943, acc: 0.516853928565979)
[2025-02-12 21:59:22,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:22,732][root][INFO] - Training Epoch: 1/2, step 163/574 completed (loss: 1.3932379484176636, acc: 0.75)
[2025-02-12 21:59:22,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:23,082][root][INFO] - Training Epoch: 1/2, step 164/574 completed (loss: 0.6373917460441589, acc: 0.8095238208770752)
[2025-02-12 21:59:23,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:23,419][root][INFO] - Training Epoch: 1/2, step 165/574 completed (loss: 0.7883686423301697, acc: 0.7241379022598267)
[2025-02-12 21:59:23,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:23,800][root][INFO] - Training Epoch: 1/2, step 166/574 completed (loss: 0.5328935384750366, acc: 0.8979591727256775)
[2025-02-12 21:59:23,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:24,151][root][INFO] - Training Epoch: 1/2, step 167/574 completed (loss: 0.5688377618789673, acc: 0.8399999737739563)
[2025-02-12 21:59:24,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:24,612][root][INFO] - Training Epoch: 1/2, step 168/574 completed (loss: 1.570313572883606, acc: 0.7222222089767456)
[2025-02-12 21:59:24,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:24,980][root][INFO] - Training Epoch: 1/2, step 169/574 completed (loss: 1.160468339920044, acc: 0.7156862616539001)
[2025-02-12 21:59:25,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:26,050][root][INFO] - Training Epoch: 1/2, step 170/574 completed (loss: 1.6547750234603882, acc: 0.6506849527359009)
[2025-02-12 21:59:26,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:26,431][root][INFO] - Training Epoch: 1/2, step 171/574 completed (loss: 0.8481748104095459, acc: 0.875)
[2025-02-12 21:59:26,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:26,831][root][INFO] - Training Epoch: 1/2, step 172/574 completed (loss: 0.8869986534118652, acc: 0.8148148059844971)
[2025-02-12 21:59:26,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:27,196][root][INFO] - Training Epoch: 1/2, step 173/574 completed (loss: 1.3841685056686401, acc: 0.5)
[2025-02-12 21:59:27,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:27,767][root][INFO] - Training Epoch: 1/2, step 174/574 completed (loss: 1.419053554534912, acc: 0.7079645991325378)
[2025-02-12 21:59:27,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:28,106][root][INFO] - Training Epoch: 1/2, step 175/574 completed (loss: 1.5245933532714844, acc: 0.6811594367027283)
[2025-02-12 21:59:28,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:28,476][root][INFO] - Training Epoch: 1/2, step 176/574 completed (loss: 0.8815701007843018, acc: 0.8181818127632141)
[2025-02-12 21:59:29,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:29,459][root][INFO] - Training Epoch: 1/2, step 177/574 completed (loss: 1.7131644487380981, acc: 0.6106870174407959)
[2025-02-12 21:59:29,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:30,144][root][INFO] - Training Epoch: 1/2, step 178/574 completed (loss: 1.9916985034942627, acc: 0.5259259343147278)
[2025-02-12 21:59:30,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:30,508][root][INFO] - Training Epoch: 1/2, step 179/574 completed (loss: 1.0946223735809326, acc: 0.7377049326896667)
[2025-02-12 21:59:30,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:30,855][root][INFO] - Training Epoch: 1/2, step 180/574 completed (loss: 0.5659821629524231, acc: 0.7916666865348816)
[2025-02-12 21:59:30,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:31,230][root][INFO] - Training Epoch: 1/2, step 181/574 completed (loss: 0.28665387630462646, acc: 0.8799999952316284)
[2025-02-12 21:59:31,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:31,552][root][INFO] - Training Epoch: 1/2, step 182/574 completed (loss: 0.5430733561515808, acc: 0.8571428656578064)
[2025-02-12 21:59:31,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:31,893][root][INFO] - Training Epoch: 1/2, step 183/574 completed (loss: 0.7806044816970825, acc: 0.8170731663703918)
[2025-02-12 21:59:32,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:32,303][root][INFO] - Training Epoch: 1/2, step 184/574 completed (loss: 1.3013031482696533, acc: 0.7643504738807678)
[2025-02-12 21:59:32,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:32,646][root][INFO] - Training Epoch: 1/2, step 185/574 completed (loss: 0.9869517683982849, acc: 0.8040345907211304)
[2025-02-12 21:59:32,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:33,160][root][INFO] - Training Epoch: 1/2, step 186/574 completed (loss: 0.9212331771850586, acc: 0.8125)
[2025-02-12 21:59:33,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:33,708][root][INFO] - Training Epoch: 1/2, step 187/574 completed (loss: 0.9023360013961792, acc: 0.7842401266098022)
[2025-02-12 21:59:33,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:34,128][root][INFO] - Training Epoch: 1/2, step 188/574 completed (loss: 1.161608099937439, acc: 0.725978672504425)
[2025-02-12 21:59:34,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:34,477][root][INFO] - Training Epoch: 1/2, step 189/574 completed (loss: 1.1666743755340576, acc: 0.7200000286102295)
[2025-02-12 21:59:34,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:35,052][root][INFO] - Training Epoch: 1/2, step 190/574 completed (loss: 1.853481411933899, acc: 0.5465116500854492)
[2025-02-12 21:59:35,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:35,871][root][INFO] - Training Epoch: 1/2, step 191/574 completed (loss: 2.290001392364502, acc: 0.4920634925365448)
[2025-02-12 21:59:36,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:36,866][root][INFO] - Training Epoch: 1/2, step 192/574 completed (loss: 1.937469244003296, acc: 0.5530303120613098)
[2025-02-12 21:59:37,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:37,657][root][INFO] - Training Epoch: 1/2, step 193/574 completed (loss: 1.8420616388320923, acc: 0.529411792755127)
[2025-02-12 21:59:38,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:38,767][root][INFO] - Training Epoch: 1/2, step 194/574 completed (loss: 1.9001545906066895, acc: 0.5432098507881165)
[2025-02-12 21:59:39,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:39,719][root][INFO] - Training Epoch: 1/2, step 195/574 completed (loss: 1.6814155578613281, acc: 0.5806451439857483)
[2025-02-12 21:59:39,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:40,113][root][INFO] - Training Epoch: 1/2, step 196/574 completed (loss: 0.7297446131706238, acc: 0.8571428656578064)
[2025-02-12 21:59:40,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:40,526][root][INFO] - Training Epoch: 1/2, step 197/574 completed (loss: 2.0972254276275635, acc: 0.574999988079071)
[2025-02-12 21:59:40,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:40,887][root][INFO] - Training Epoch: 1/2, step 198/574 completed (loss: 1.6928048133850098, acc: 0.6323529481887817)
[2025-02-12 21:59:41,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:41,256][root][INFO] - Training Epoch: 1/2, step 199/574 completed (loss: 1.5379893779754639, acc: 0.7132353186607361)
[2025-02-12 21:59:41,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:41,614][root][INFO] - Training Epoch: 1/2, step 200/574 completed (loss: 1.1041122674942017, acc: 0.6525423526763916)
[2025-02-12 21:59:41,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:42,018][root][INFO] - Training Epoch: 1/2, step 201/574 completed (loss: 1.6795494556427002, acc: 0.611940324306488)
[2025-02-12 21:59:42,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:42,443][root][INFO] - Training Epoch: 1/2, step 202/574 completed (loss: 1.818236231803894, acc: 0.6116504669189453)
[2025-02-12 21:59:42,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:42,835][root][INFO] - Training Epoch: 1/2, step 203/574 completed (loss: 1.4484997987747192, acc: 0.60317462682724)
[2025-02-12 21:59:42,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:43,214][root][INFO] - Training Epoch: 1/2, step 204/574 completed (loss: 0.5479060411453247, acc: 0.8791208863258362)
[2025-02-12 21:59:43,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:43,574][root][INFO] - Training Epoch: 1/2, step 205/574 completed (loss: 0.9679054021835327, acc: 0.8161435127258301)
[2025-02-12 21:59:43,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:44,015][root][INFO] - Training Epoch: 1/2, step 206/574 completed (loss: 1.0436527729034424, acc: 0.7440944910049438)
[2025-02-12 21:59:44,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:44,344][root][INFO] - Training Epoch: 1/2, step 207/574 completed (loss: 1.0793498754501343, acc: 0.7758620977401733)
[2025-02-12 21:59:44,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:44,783][root][INFO] - Training Epoch: 1/2, step 208/574 completed (loss: 0.8825123310089111, acc: 0.804347813129425)
[2025-02-12 21:59:44,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:45,276][root][INFO] - Training Epoch: 1/2, step 209/574 completed (loss: 1.1458039283752441, acc: 0.7665369510650635)
[2025-02-12 21:59:45,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:45,608][root][INFO] - Training Epoch: 1/2, step 210/574 completed (loss: 0.6506490707397461, acc: 0.8152173757553101)
[2025-02-12 21:59:45,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:45,939][root][INFO] - Training Epoch: 1/2, step 211/574 completed (loss: 0.6138526797294617, acc: 0.739130437374115)
[2025-02-12 21:59:46,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:46,285][root][INFO] - Training Epoch: 1/2, step 212/574 completed (loss: 0.2716505527496338, acc: 0.9642857313156128)
[2025-02-12 21:59:46,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:46,660][root][INFO] - Training Epoch: 1/2, step 213/574 completed (loss: 0.820440411567688, acc: 0.8936170339584351)
[2025-02-12 21:59:47,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:47,401][root][INFO] - Training Epoch: 1/2, step 214/574 completed (loss: 1.072165608406067, acc: 0.7769230604171753)
[2025-02-12 21:59:47,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:47,760][root][INFO] - Training Epoch: 1/2, step 215/574 completed (loss: 0.48250827193260193, acc: 0.8513513803482056)
[2025-02-12 21:59:47,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:48,195][root][INFO] - Training Epoch: 1/2, step 216/574 completed (loss: 1.0370128154754639, acc: 0.8023256063461304)
[2025-02-12 21:59:48,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:48,824][root][INFO] - Training Epoch: 1/2, step 217/574 completed (loss: 1.1111165285110474, acc: 0.792792797088623)
[2025-02-12 21:59:49,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:49,274][root][INFO] - Training Epoch: 1/2, step 218/574 completed (loss: 0.8534373641014099, acc: 0.855555534362793)
[2025-02-12 21:59:49,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:49,591][root][INFO] - Training Epoch: 1/2, step 219/574 completed (loss: 0.7510775327682495, acc: 0.8484848737716675)
[2025-02-12 21:59:49,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:49,866][root][INFO] - Training Epoch: 1/2, step 220/574 completed (loss: 0.511722207069397, acc: 0.8888888955116272)
[2025-02-12 21:59:49,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:50,193][root][INFO] - Training Epoch: 1/2, step 221/574 completed (loss: 0.2784675657749176, acc: 0.9200000166893005)
[2025-02-12 21:59:50,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:50,551][root][INFO] - Training Epoch: 1/2, step 222/574 completed (loss: 1.3358190059661865, acc: 0.6153846383094788)
[2025-02-12 21:59:51,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:51,419][root][INFO] - Training Epoch: 1/2, step 223/574 completed (loss: 0.8816747069358826, acc: 0.8097826242446899)
[2025-02-12 21:59:51,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:51,991][root][INFO] - Training Epoch: 1/2, step 224/574 completed (loss: 1.1601189374923706, acc: 0.6988636255264282)
[2025-02-12 21:59:52,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:52,455][root][INFO] - Training Epoch: 1/2, step 225/574 completed (loss: 1.0729546546936035, acc: 0.6595744490623474)
[2025-02-12 21:59:52,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:52,832][root][INFO] - Training Epoch: 1/2, step 226/574 completed (loss: 1.4511430263519287, acc: 0.6415094137191772)
[2025-02-12 21:59:52,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:53,179][root][INFO] - Training Epoch: 1/2, step 227/574 completed (loss: 1.1318639516830444, acc: 0.6833333373069763)
[2025-02-12 21:59:53,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:53,542][root][INFO] - Training Epoch: 1/2, step 228/574 completed (loss: 0.828577995300293, acc: 0.7674418687820435)
[2025-02-12 21:59:53,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:53,953][root][INFO] - Training Epoch: 1/2, step 229/574 completed (loss: 1.7356823682785034, acc: 0.6000000238418579)
[2025-02-12 21:59:54,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:54,385][root][INFO] - Training Epoch: 1/2, step 230/574 completed (loss: 1.9508459568023682, acc: 0.5368421077728271)
[2025-02-12 21:59:54,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:54,736][root][INFO] - Training Epoch: 1/2, step 231/574 completed (loss: 1.8662577867507935, acc: 0.5777778029441833)
[2025-02-12 21:59:54,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:55,178][root][INFO] - Training Epoch: 1/2, step 232/574 completed (loss: 1.6468968391418457, acc: 0.5611110925674438)
[2025-02-12 21:59:55,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:55,713][root][INFO] - Training Epoch: 1/2, step 233/574 completed (loss: 2.036818265914917, acc: 0.5137614607810974)
[2025-02-12 21:59:55,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:56,276][root][INFO] - Training Epoch: 1/2, step 234/574 completed (loss: 1.7127125263214111, acc: 0.5692307949066162)
[2025-02-12 21:59:56,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:56,644][root][INFO] - Training Epoch: 1/2, step 235/574 completed (loss: 0.8178277015686035, acc: 0.7368420958518982)
[2025-02-12 21:59:56,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:57,018][root][INFO] - Training Epoch: 1/2, step 236/574 completed (loss: 0.7368059754371643, acc: 0.75)
[2025-02-12 21:59:57,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:57,459][root][INFO] - Training Epoch: 1/2, step 237/574 completed (loss: 1.5920604467391968, acc: 0.6363636255264282)
[2025-02-12 21:59:57,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:57,824][root][INFO] - Training Epoch: 1/2, step 238/574 completed (loss: 0.9472301602363586, acc: 0.7407407164573669)
[2025-02-12 21:59:57,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:58,157][root][INFO] - Training Epoch: 1/2, step 239/574 completed (loss: 1.079761266708374, acc: 0.6285714507102966)
[2025-02-12 21:59:58,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:58,485][root][INFO] - Training Epoch: 1/2, step 240/574 completed (loss: 1.562287449836731, acc: 0.6590909361839294)
[2025-02-12 21:59:58,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:58,828][root][INFO] - Training Epoch: 1/2, step 241/574 completed (loss: 1.2027331590652466, acc: 0.7045454382896423)
[2025-02-12 21:59:59,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:59,447][root][INFO] - Training Epoch: 1/2, step 242/574 completed (loss: 1.7108664512634277, acc: 0.5322580933570862)
[2025-02-12 21:59:59,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:59,985][root][INFO] - Training Epoch: 1/2, step 243/574 completed (loss: 1.7148677110671997, acc: 0.5454545617103577)
[2025-02-12 22:00:00,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:00,294][root][INFO] - Training Epoch: 1/2, step 244/574 completed (loss: 0.1694619357585907, acc: 0.9523809552192688)
[2025-02-12 22:00:00,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:00,579][root][INFO] - Training Epoch: 1/2, step 245/574 completed (loss: 0.3190336227416992, acc: 0.9230769276618958)
[2025-02-12 22:00:00,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:00,896][root][INFO] - Training Epoch: 1/2, step 246/574 completed (loss: 0.3336920142173767, acc: 0.9032257795333862)
[2025-02-12 22:00:01,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:01,265][root][INFO] - Training Epoch: 1/2, step 247/574 completed (loss: 0.5549851655960083, acc: 0.8500000238418579)
[2025-02-12 22:00:01,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:01,640][root][INFO] - Training Epoch: 1/2, step 248/574 completed (loss: 1.2472898960113525, acc: 0.7297297120094299)
[2025-02-12 22:00:01,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:01,985][root][INFO] - Training Epoch: 1/2, step 249/574 completed (loss: 0.7335430383682251, acc: 0.8108108043670654)
[2025-02-12 22:00:02,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:02,324][root][INFO] - Training Epoch: 1/2, step 250/574 completed (loss: 1.071156620979309, acc: 0.837837815284729)
[2025-02-12 22:00:02,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:02,676][root][INFO] - Training Epoch: 1/2, step 251/574 completed (loss: 0.8526139259338379, acc: 0.8235294222831726)
[2025-02-12 22:00:02,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:03,054][root][INFO] - Training Epoch: 1/2, step 252/574 completed (loss: 1.012326717376709, acc: 0.7317073345184326)
[2025-02-12 22:00:03,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:03,389][root][INFO] - Training Epoch: 1/2, step 253/574 completed (loss: 0.46642962098121643, acc: 0.8399999737739563)
[2025-02-12 22:00:03,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:03,767][root][INFO] - Training Epoch: 1/2, step 254/574 completed (loss: 0.07313131541013718, acc: 1.0)
[2025-02-12 22:00:03,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:04,137][root][INFO] - Training Epoch: 1/2, step 255/574 completed (loss: 0.44773146510124207, acc: 0.8387096524238586)
[2025-02-12 22:00:04,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:04,466][root][INFO] - Training Epoch: 1/2, step 256/574 completed (loss: 0.6544640064239502, acc: 0.8947368264198303)
[2025-02-12 22:00:04,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:04,803][root][INFO] - Training Epoch: 1/2, step 257/574 completed (loss: 0.434998482465744, acc: 0.9142857193946838)
[2025-02-12 22:00:04,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:05,193][root][INFO] - Training Epoch: 1/2, step 258/574 completed (loss: 0.3735605776309967, acc: 0.9210526347160339)
[2025-02-12 22:00:05,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:05,810][root][INFO] - Training Epoch: 1/2, step 259/574 completed (loss: 0.7126741409301758, acc: 0.7830188870429993)
[2025-02-12 22:00:06,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:06,399][root][INFO] - Training Epoch: 1/2, step 260/574 completed (loss: 0.7603464722633362, acc: 0.8083333373069763)
[2025-02-12 22:00:06,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:06,703][root][INFO] - Training Epoch: 1/2, step 261/574 completed (loss: 0.5604020357131958, acc: 0.8611111044883728)
[2025-02-12 22:00:06,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:07,010][root][INFO] - Training Epoch: 1/2, step 262/574 completed (loss: 1.2012379169464111, acc: 0.7096773982048035)
[2025-02-12 22:00:07,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:07,353][root][INFO] - Training Epoch: 1/2, step 263/574 completed (loss: 1.8371034860610962, acc: 0.5866666436195374)
[2025-02-12 22:00:07,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:07,689][root][INFO] - Training Epoch: 1/2, step 264/574 completed (loss: 1.2045860290527344, acc: 0.7291666865348816)
[2025-02-12 22:00:08,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:08,585][root][INFO] - Training Epoch: 1/2, step 265/574 completed (loss: 1.9490433931350708, acc: 0.5600000023841858)
[2025-02-12 22:00:08,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:08,913][root][INFO] - Training Epoch: 1/2, step 266/574 completed (loss: 1.9769465923309326, acc: 0.516853928565979)
[2025-02-12 22:00:09,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:09,348][root][INFO] - Training Epoch: 1/2, step 267/574 completed (loss: 1.9049484729766846, acc: 0.44594594836235046)
[2025-02-12 22:00:09,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:09,815][root][INFO] - Training Epoch: 1/2, step 268/574 completed (loss: 1.2393410205841064, acc: 0.6896551847457886)
[2025-02-12 22:00:09,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:10,146][root][INFO] - Training Epoch: 1/2, step 269/574 completed (loss: 0.7818021178245544, acc: 0.8181818127632141)
[2025-02-12 22:00:10,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:10,539][root][INFO] - Training Epoch: 1/2, step 270/574 completed (loss: 0.4831980764865875, acc: 0.8181818127632141)
[2025-02-12 22:00:10,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:10,936][root][INFO] - Training Epoch: 1/2, step 271/574 completed (loss: 0.3251945972442627, acc: 0.90625)
[2025-02-12 22:00:11,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:11,341][root][INFO] - Training Epoch: 1/2, step 272/574 completed (loss: 0.21700340509414673, acc: 0.9333333373069763)
[2025-02-12 22:00:11,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:11,760][root][INFO] - Training Epoch: 1/2, step 273/574 completed (loss: 0.6478086709976196, acc: 0.8333333134651184)
[2025-02-12 22:00:11,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:12,216][root][INFO] - Training Epoch: 1/2, step 274/574 completed (loss: 0.32198503613471985, acc: 0.90625)
[2025-02-12 22:00:12,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:12,580][root][INFO] - Training Epoch: 1/2, step 275/574 completed (loss: 0.5962250232696533, acc: 0.8666666746139526)
[2025-02-12 22:00:12,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:12,902][root][INFO] - Training Epoch: 1/2, step 276/574 completed (loss: 0.40108799934387207, acc: 0.8965517282485962)
[2025-02-12 22:00:12,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:13,152][root][INFO] - Training Epoch: 1/2, step 277/574 completed (loss: 0.5949490070343018, acc: 0.8399999737739563)
[2025-02-12 22:00:13,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:13,461][root][INFO] - Training Epoch: 1/2, step 278/574 completed (loss: 0.7463226914405823, acc: 0.8936170339584351)
[2025-02-12 22:00:13,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:13,782][root][INFO] - Training Epoch: 1/2, step 279/574 completed (loss: 0.8352725505828857, acc: 0.8333333134651184)
[2025-02-12 22:00:13,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:14,159][root][INFO] - Training Epoch: 1/2, step 280/574 completed (loss: 0.3814314901828766, acc: 0.8863636255264282)
[2025-02-12 22:00:14,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:14,632][root][INFO] - Training Epoch: 1/2, step 281/574 completed (loss: 1.2035377025604248, acc: 0.6746987700462341)
[2025-02-12 22:00:14,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:15,019][root][INFO] - Training Epoch: 1/2, step 282/574 completed (loss: 1.2901239395141602, acc: 0.6851851940155029)
[2025-02-12 22:00:15,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:15,338][root][INFO] - Training Epoch: 1/2, step 283/574 completed (loss: 0.6307272911071777, acc: 0.8421052694320679)
[2025-02-12 22:00:15,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:15,719][root][INFO] - Training Epoch: 1/2, step 284/574 completed (loss: 1.1516668796539307, acc: 0.6764705777168274)
[2025-02-12 22:00:15,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:16,064][root][INFO] - Training Epoch: 1/2, step 285/574 completed (loss: 0.40807995200157166, acc: 0.875)
[2025-02-12 22:00:16,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:17,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:17,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:18,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:18,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:18,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:19,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:19,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:19,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:20,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:20,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:20,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:21,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:21,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:22,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:22,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:22,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:23,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:23,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:23,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:24,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:24,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:25,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:25,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:25,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:26,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:26,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:26,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:27,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:27,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:27,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:28,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:28,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:28,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:29,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:29,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:29,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:30,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:30,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:30,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:31,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:31,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:31,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:32,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:32,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:32,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:33,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:33,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:33,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:34,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:34,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:34,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:35,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:35,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:35,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:36,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:36,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:37,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:37,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:37,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:38,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:38,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:39,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:39,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:39,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:40,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:40,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:40,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:41,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:41,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:41,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:42,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:42,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:42,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:43,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:43,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:43,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:45,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:45,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:46,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:46,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:47,199][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1102, device='cuda:0') eval_epoch_loss=tensor(0.7468, device='cuda:0') eval_epoch_acc=tensor(0.8076, device='cuda:0')
[2025-02-12 22:00:47,201][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:00:47,201][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:00:47,595][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.7467750906944275/model.pt
[2025-02-12 22:00:47,599][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:00:47,600][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.7467750906944275
[2025-02-12 22:00:47,601][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8076428771018982
[2025-02-12 22:00:47,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:48,024][root][INFO] - Training Epoch: 1/2, step 286/574 completed (loss: 0.9449479579925537, acc: 0.7421875)
[2025-02-12 22:00:48,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:48,344][root][INFO] - Training Epoch: 1/2, step 287/574 completed (loss: 0.9878478050231934, acc: 0.7440000176429749)
[2025-02-12 22:00:48,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:48,735][root][INFO] - Training Epoch: 1/2, step 288/574 completed (loss: 0.6887773871421814, acc: 0.8351648449897766)
[2025-02-12 22:00:48,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:49,109][root][INFO] - Training Epoch: 1/2, step 289/574 completed (loss: 1.2217451333999634, acc: 0.7142857313156128)
[2025-02-12 22:00:49,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:49,480][root][INFO] - Training Epoch: 1/2, step 290/574 completed (loss: 0.8620022535324097, acc: 0.7989690899848938)
[2025-02-12 22:00:49,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:49,886][root][INFO] - Training Epoch: 1/2, step 291/574 completed (loss: 0.2380959838628769, acc: 0.9090909361839294)
[2025-02-12 22:00:49,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:50,293][root][INFO] - Training Epoch: 1/2, step 292/574 completed (loss: 0.750339925289154, acc: 0.761904776096344)
[2025-02-12 22:00:50,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:50,645][root][INFO] - Training Epoch: 1/2, step 293/574 completed (loss: 0.8763107657432556, acc: 0.8275862336158752)
[2025-02-12 22:00:50,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:51,184][root][INFO] - Training Epoch: 1/2, step 294/574 completed (loss: 0.8441298007965088, acc: 0.7454545497894287)
[2025-02-12 22:00:51,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:51,746][root][INFO] - Training Epoch: 1/2, step 295/574 completed (loss: 0.8440207839012146, acc: 0.8041236996650696)
[2025-02-12 22:00:51,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:52,055][root][INFO] - Training Epoch: 1/2, step 296/574 completed (loss: 0.7602466940879822, acc: 0.8103448152542114)
[2025-02-12 22:00:52,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:52,419][root][INFO] - Training Epoch: 1/2, step 297/574 completed (loss: 0.7117629051208496, acc: 0.7777777910232544)
[2025-02-12 22:00:52,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:52,725][root][INFO] - Training Epoch: 1/2, step 298/574 completed (loss: 1.039329171180725, acc: 0.7105262875556946)
[2025-02-12 22:00:52,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:53,029][root][INFO] - Training Epoch: 1/2, step 299/574 completed (loss: 0.14752556383609772, acc: 0.9821428656578064)
[2025-02-12 22:00:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:53,351][root][INFO] - Training Epoch: 1/2, step 300/574 completed (loss: 0.2419891357421875, acc: 0.875)
[2025-02-12 22:00:53,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:53,854][root][INFO] - Training Epoch: 1/2, step 301/574 completed (loss: 0.6242915391921997, acc: 0.8113207817077637)
[2025-02-12 22:00:53,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:54,210][root][INFO] - Training Epoch: 1/2, step 302/574 completed (loss: 0.42530202865600586, acc: 0.9056603908538818)
[2025-02-12 22:00:54,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:54,526][root][INFO] - Training Epoch: 1/2, step 303/574 completed (loss: 0.14365245401859283, acc: 0.970588207244873)
[2025-02-12 22:00:54,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:54,847][root][INFO] - Training Epoch: 1/2, step 304/574 completed (loss: 0.20995578169822693, acc: 0.9375)
[2025-02-12 22:00:54,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:55,204][root][INFO] - Training Epoch: 1/2, step 305/574 completed (loss: 0.888203501701355, acc: 0.7540983557701111)
[2025-02-12 22:00:55,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:55,518][root][INFO] - Training Epoch: 1/2, step 306/574 completed (loss: 0.5124996304512024, acc: 0.9333333373069763)
[2025-02-12 22:00:55,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:55,928][root][INFO] - Training Epoch: 1/2, step 307/574 completed (loss: 0.3449234068393707, acc: 0.8947368264198303)
[2025-02-12 22:00:56,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:56,259][root][INFO] - Training Epoch: 1/2, step 308/574 completed (loss: 0.7191181182861328, acc: 0.7971014380455017)
[2025-02-12 22:00:56,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:56,678][root][INFO] - Training Epoch: 1/2, step 309/574 completed (loss: 0.5229915380477905, acc: 0.8611111044883728)
[2025-02-12 22:00:56,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:57,004][root][INFO] - Training Epoch: 1/2, step 310/574 completed (loss: 0.6473991274833679, acc: 0.8433734774589539)
[2025-02-12 22:00:57,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:57,386][root][INFO] - Training Epoch: 1/2, step 311/574 completed (loss: 0.7557769417762756, acc: 0.7820512652397156)
[2025-02-12 22:00:57,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:57,790][root][INFO] - Training Epoch: 1/2, step 312/574 completed (loss: 0.31616732478141785, acc: 0.9285714030265808)
[2025-02-12 22:00:57,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:58,108][root][INFO] - Training Epoch: 1/2, step 313/574 completed (loss: 0.09525372833013535, acc: 1.0)
[2025-02-12 22:00:58,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:58,472][root][INFO] - Training Epoch: 1/2, step 314/574 completed (loss: 0.12647919356822968, acc: 1.0)
[2025-02-12 22:00:58,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:58,820][root][INFO] - Training Epoch: 1/2, step 315/574 completed (loss: 0.5358619689941406, acc: 0.8387096524238586)
[2025-02-12 22:00:58,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:59,142][root][INFO] - Training Epoch: 1/2, step 316/574 completed (loss: 0.7168335318565369, acc: 0.774193525314331)
[2025-02-12 22:00:59,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:59,516][root][INFO] - Training Epoch: 1/2, step 317/574 completed (loss: 0.6757089495658875, acc: 0.8059701323509216)
[2025-02-12 22:00:59,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:59,844][root][INFO] - Training Epoch: 1/2, step 318/574 completed (loss: 0.43351465463638306, acc: 0.8942307829856873)
[2025-02-12 22:00:59,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:00,217][root][INFO] - Training Epoch: 1/2, step 319/574 completed (loss: 0.4769134819507599, acc: 0.8888888955116272)
[2025-02-12 22:01:00,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:00,540][root][INFO] - Training Epoch: 1/2, step 320/574 completed (loss: 0.2151661068201065, acc: 0.9193548560142517)
[2025-02-12 22:01:00,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:00,871][root][INFO] - Training Epoch: 1/2, step 321/574 completed (loss: 0.3638240396976471, acc: 0.9399999976158142)
[2025-02-12 22:01:00,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:01,181][root][INFO] - Training Epoch: 1/2, step 322/574 completed (loss: 1.1290398836135864, acc: 0.7037037014961243)
[2025-02-12 22:01:01,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:01,465][root][INFO] - Training Epoch: 1/2, step 323/574 completed (loss: 1.3304983377456665, acc: 0.5714285969734192)
[2025-02-12 22:01:01,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:01,810][root][INFO] - Training Epoch: 1/2, step 324/574 completed (loss: 1.992315411567688, acc: 0.5128205418586731)
[2025-02-12 22:01:01,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:02,125][root][INFO] - Training Epoch: 1/2, step 325/574 completed (loss: 2.0087759494781494, acc: 0.5365853905677795)
[2025-02-12 22:01:02,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:02,430][root][INFO] - Training Epoch: 1/2, step 326/574 completed (loss: 1.243739366531372, acc: 0.5789473652839661)
[2025-02-12 22:01:02,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:02,710][root][INFO] - Training Epoch: 1/2, step 327/574 completed (loss: 0.721504271030426, acc: 0.8947368264198303)
[2025-02-12 22:01:02,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:03,015][root][INFO] - Training Epoch: 1/2, step 328/574 completed (loss: 0.17686648666858673, acc: 0.9642857313156128)
[2025-02-12 22:01:03,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:03,325][root][INFO] - Training Epoch: 1/2, step 329/574 completed (loss: 0.38242554664611816, acc: 0.8888888955116272)
[2025-02-12 22:01:03,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:03,683][root][INFO] - Training Epoch: 1/2, step 330/574 completed (loss: 0.18382851779460907, acc: 0.96875)
[2025-02-12 22:01:03,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:04,010][root][INFO] - Training Epoch: 1/2, step 331/574 completed (loss: 0.5057264566421509, acc: 0.8548387289047241)
[2025-02-12 22:01:04,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:04,385][root][INFO] - Training Epoch: 1/2, step 332/574 completed (loss: 0.3747192621231079, acc: 0.8947368264198303)
[2025-02-12 22:01:04,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:04,695][root][INFO] - Training Epoch: 1/2, step 333/574 completed (loss: 0.35606709122657776, acc: 0.875)
[2025-02-12 22:01:04,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:05,024][root][INFO] - Training Epoch: 1/2, step 334/574 completed (loss: 0.409087598323822, acc: 0.8666666746139526)
[2025-02-12 22:01:05,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:05,327][root][INFO] - Training Epoch: 1/2, step 335/574 completed (loss: 0.9987577199935913, acc: 0.6315789222717285)
[2025-02-12 22:01:05,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:05,675][root][INFO] - Training Epoch: 1/2, step 336/574 completed (loss: 1.126467227935791, acc: 0.6399999856948853)
[2025-02-12 22:01:05,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:06,078][root][INFO] - Training Epoch: 1/2, step 337/574 completed (loss: 1.5852762460708618, acc: 0.6206896305084229)
[2025-02-12 22:01:06,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:06,462][root][INFO] - Training Epoch: 1/2, step 338/574 completed (loss: 1.6444710493087769, acc: 0.5106382966041565)
[2025-02-12 22:01:06,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:06,777][root][INFO] - Training Epoch: 1/2, step 339/574 completed (loss: 1.598536491394043, acc: 0.5783132314682007)
[2025-02-12 22:01:06,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:07,079][root][INFO] - Training Epoch: 1/2, step 340/574 completed (loss: 0.6313288807868958, acc: 0.8695651888847351)
[2025-02-12 22:01:07,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:07,357][root][INFO] - Training Epoch: 1/2, step 341/574 completed (loss: 0.8895851373672485, acc: 0.7692307829856873)
[2025-02-12 22:01:07,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:07,733][root][INFO] - Training Epoch: 1/2, step 342/574 completed (loss: 0.940765380859375, acc: 0.7831325531005859)
[2025-02-12 22:01:07,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:08,159][root][INFO] - Training Epoch: 1/2, step 343/574 completed (loss: 0.8096299767494202, acc: 0.7924528121948242)
[2025-02-12 22:01:08,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:08,470][root][INFO] - Training Epoch: 1/2, step 344/574 completed (loss: 0.38342007994651794, acc: 0.8987341523170471)
[2025-02-12 22:01:08,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:08,776][root][INFO] - Training Epoch: 1/2, step 345/574 completed (loss: 0.26852482557296753, acc: 0.9215686321258545)
[2025-02-12 22:01:08,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:09,130][root][INFO] - Training Epoch: 1/2, step 346/574 completed (loss: 0.641817033290863, acc: 0.8656716346740723)
[2025-02-12 22:01:09,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:09,556][root][INFO] - Training Epoch: 1/2, step 347/574 completed (loss: 0.08767368644475937, acc: 0.949999988079071)
[2025-02-12 22:01:09,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:09,939][root][INFO] - Training Epoch: 1/2, step 348/574 completed (loss: 1.1131420135498047, acc: 0.7599999904632568)
[2025-02-12 22:01:10,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:10,353][root][INFO] - Training Epoch: 1/2, step 349/574 completed (loss: 0.9989109039306641, acc: 0.6666666865348816)
[2025-02-12 22:01:10,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:10,698][root][INFO] - Training Epoch: 1/2, step 350/574 completed (loss: 1.2323874235153198, acc: 0.6279069781303406)
[2025-02-12 22:01:10,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:11,032][root][INFO] - Training Epoch: 1/2, step 351/574 completed (loss: 0.6488567590713501, acc: 0.8205128312110901)
[2025-02-12 22:01:11,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:11,445][root][INFO] - Training Epoch: 1/2, step 352/574 completed (loss: 1.1771858930587769, acc: 0.7333333492279053)
[2025-02-12 22:01:11,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:11,778][root][INFO] - Training Epoch: 1/2, step 353/574 completed (loss: 0.13328494131565094, acc: 0.95652174949646)
[2025-02-12 22:01:11,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:12,142][root][INFO] - Training Epoch: 1/2, step 354/574 completed (loss: 1.4159101247787476, acc: 0.7307692170143127)
[2025-02-12 22:01:12,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:12,529][root][INFO] - Training Epoch: 1/2, step 355/574 completed (loss: 1.2430665493011475, acc: 0.6593406796455383)
[2025-02-12 22:01:12,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:13,061][root][INFO] - Training Epoch: 1/2, step 356/574 completed (loss: 1.014370322227478, acc: 0.695652186870575)
[2025-02-12 22:01:13,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:13,419][root][INFO] - Training Epoch: 1/2, step 357/574 completed (loss: 1.1030431985855103, acc: 0.6847826242446899)
[2025-02-12 22:01:13,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:13,783][root][INFO] - Training Epoch: 1/2, step 358/574 completed (loss: 0.9928907155990601, acc: 0.7551020383834839)
[2025-02-12 22:01:13,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:14,126][root][INFO] - Training Epoch: 1/2, step 359/574 completed (loss: 0.04363548383116722, acc: 1.0)
[2025-02-12 22:01:14,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:14,457][root][INFO] - Training Epoch: 1/2, step 360/574 completed (loss: 0.46537846326828003, acc: 0.9615384340286255)
[2025-02-12 22:01:14,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:14,820][root][INFO] - Training Epoch: 1/2, step 361/574 completed (loss: 0.8291962742805481, acc: 0.7560975551605225)
[2025-02-12 22:01:14,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:15,248][root][INFO] - Training Epoch: 1/2, step 362/574 completed (loss: 0.5664269924163818, acc: 0.8666666746139526)
[2025-02-12 22:01:15,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:15,612][root][INFO] - Training Epoch: 1/2, step 363/574 completed (loss: 0.6012520790100098, acc: 0.8684210777282715)
[2025-02-12 22:01:15,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:16,043][root][INFO] - Training Epoch: 1/2, step 364/574 completed (loss: 0.39705896377563477, acc: 0.8536585569381714)
[2025-02-12 22:01:16,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:16,380][root][INFO] - Training Epoch: 1/2, step 365/574 completed (loss: 0.3324816823005676, acc: 0.9090909361839294)
[2025-02-12 22:01:16,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:16,691][root][INFO] - Training Epoch: 1/2, step 366/574 completed (loss: 0.14668315649032593, acc: 0.9583333134651184)
[2025-02-12 22:01:16,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:17,118][root][INFO] - Training Epoch: 1/2, step 367/574 completed (loss: 0.40847164392471313, acc: 0.8695651888847351)
[2025-02-12 22:01:17,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:17,532][root][INFO] - Training Epoch: 1/2, step 368/574 completed (loss: 0.2961435317993164, acc: 0.9285714030265808)
[2025-02-12 22:01:17,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:17,880][root][INFO] - Training Epoch: 1/2, step 369/574 completed (loss: 0.5092595219612122, acc: 0.78125)
[2025-02-12 22:01:18,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:18,498][root][INFO] - Training Epoch: 1/2, step 370/574 completed (loss: 0.7358112335205078, acc: 0.7757575511932373)
[2025-02-12 22:01:19,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:19,358][root][INFO] - Training Epoch: 1/2, step 371/574 completed (loss: 0.5949068069458008, acc: 0.849056601524353)
[2025-02-12 22:01:19,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:19,767][root][INFO] - Training Epoch: 1/2, step 372/574 completed (loss: 0.3865479826927185, acc: 0.8999999761581421)
[2025-02-12 22:01:19,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:20,123][root][INFO] - Training Epoch: 1/2, step 373/574 completed (loss: 0.5351743698120117, acc: 0.9107142686843872)
[2025-02-12 22:01:20,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:20,500][root][INFO] - Training Epoch: 1/2, step 374/574 completed (loss: 0.33163005113601685, acc: 0.9142857193946838)
[2025-02-12 22:01:20,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:20,808][root][INFO] - Training Epoch: 1/2, step 375/574 completed (loss: 0.0149117112159729, acc: 1.0)
[2025-02-12 22:01:20,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:21,140][root][INFO] - Training Epoch: 1/2, step 376/574 completed (loss: 0.21681445837020874, acc: 0.8695651888847351)
[2025-02-12 22:01:21,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:21,460][root][INFO] - Training Epoch: 1/2, step 377/574 completed (loss: 0.3319031894207001, acc: 0.9375)
[2025-02-12 22:01:21,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:21,886][root][INFO] - Training Epoch: 1/2, step 378/574 completed (loss: 0.42814308404922485, acc: 0.9157894849777222)
[2025-02-12 22:01:22,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:22,533][root][INFO] - Training Epoch: 1/2, step 379/574 completed (loss: 0.5299251079559326, acc: 0.8502994179725647)
[2025-02-12 22:01:22,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:23,031][root][INFO] - Training Epoch: 1/2, step 380/574 completed (loss: 0.5957359075546265, acc: 0.8270676732063293)
[2025-02-12 22:01:23,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:24,335][root][INFO] - Training Epoch: 1/2, step 381/574 completed (loss: 0.8460173606872559, acc: 0.7486631274223328)
[2025-02-12 22:01:24,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:24,903][root][INFO] - Training Epoch: 1/2, step 382/574 completed (loss: 0.39624452590942383, acc: 0.8918918967247009)
[2025-02-12 22:01:24,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:25,210][root][INFO] - Training Epoch: 1/2, step 383/574 completed (loss: 0.5803336501121521, acc: 0.8928571343421936)
[2025-02-12 22:01:25,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:25,493][root][INFO] - Training Epoch: 1/2, step 384/574 completed (loss: 0.13756978511810303, acc: 0.9285714030265808)
[2025-02-12 22:01:25,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:25,808][root][INFO] - Training Epoch: 1/2, step 385/574 completed (loss: 0.3500874638557434, acc: 0.90625)
[2025-02-12 22:01:25,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:26,190][root][INFO] - Training Epoch: 1/2, step 386/574 completed (loss: 0.09885946661233902, acc: 0.9444444179534912)
[2025-02-12 22:01:26,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:26,512][root][INFO] - Training Epoch: 1/2, step 387/574 completed (loss: 0.11075720191001892, acc: 0.9736841917037964)
[2025-02-12 22:01:26,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:26,867][root][INFO] - Training Epoch: 1/2, step 388/574 completed (loss: 0.05630826577544212, acc: 1.0)
[2025-02-12 22:01:26,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:27,171][root][INFO] - Training Epoch: 1/2, step 389/574 completed (loss: 0.019695723429322243, acc: 1.0)
[2025-02-12 22:01:27,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:27,470][root][INFO] - Training Epoch: 1/2, step 390/574 completed (loss: 0.7137486934661865, acc: 0.8095238208770752)
[2025-02-12 22:01:27,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:27,787][root][INFO] - Training Epoch: 1/2, step 391/574 completed (loss: 1.3920313119888306, acc: 0.6481481194496155)
[2025-02-12 22:01:27,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:28,121][root][INFO] - Training Epoch: 1/2, step 392/574 completed (loss: 1.224128246307373, acc: 0.6990291476249695)
[2025-02-12 22:01:28,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:28,662][root][INFO] - Training Epoch: 1/2, step 393/574 completed (loss: 1.2895818948745728, acc: 0.6911764740943909)
[2025-02-12 22:01:28,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:29,035][root][INFO] - Training Epoch: 1/2, step 394/574 completed (loss: 1.1536788940429688, acc: 0.7066666483879089)
[2025-02-12 22:01:29,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:29,432][root][INFO] - Training Epoch: 1/2, step 395/574 completed (loss: 1.0866389274597168, acc: 0.7083333134651184)
[2025-02-12 22:01:29,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:29,747][root][INFO] - Training Epoch: 1/2, step 396/574 completed (loss: 1.2088638544082642, acc: 0.6744186282157898)
[2025-02-12 22:01:29,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:30,164][root][INFO] - Training Epoch: 1/2, step 397/574 completed (loss: 0.6142895221710205, acc: 0.8333333134651184)
[2025-02-12 22:01:30,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:30,516][root][INFO] - Training Epoch: 1/2, step 398/574 completed (loss: 0.6535062789916992, acc: 0.8372092843055725)
[2025-02-12 22:01:30,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:30,876][root][INFO] - Training Epoch: 1/2, step 399/574 completed (loss: 0.35584911704063416, acc: 0.8799999952316284)
[2025-02-12 22:01:31,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:31,455][root][INFO] - Training Epoch: 1/2, step 400/574 completed (loss: 0.7198086977005005, acc: 0.8088235259056091)
[2025-02-12 22:01:31,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:31,887][root][INFO] - Training Epoch: 1/2, step 401/574 completed (loss: 0.8326534032821655, acc: 0.7733333110809326)
[2025-02-12 22:01:32,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:32,299][root][INFO] - Training Epoch: 1/2, step 402/574 completed (loss: 0.6663604378700256, acc: 0.8787878751754761)
[2025-02-12 22:01:32,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:32,630][root][INFO] - Training Epoch: 1/2, step 403/574 completed (loss: 0.899806022644043, acc: 0.7575757503509521)
[2025-02-12 22:01:32,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:32,952][root][INFO] - Training Epoch: 1/2, step 404/574 completed (loss: 0.35270053148269653, acc: 0.8709677457809448)
[2025-02-12 22:01:33,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:33,268][root][INFO] - Training Epoch: 1/2, step 405/574 completed (loss: 0.31052806973457336, acc: 0.8888888955116272)
[2025-02-12 22:01:33,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:33,559][root][INFO] - Training Epoch: 1/2, step 406/574 completed (loss: 0.27266842126846313, acc: 0.8799999952316284)
[2025-02-12 22:01:33,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:33,899][root][INFO] - Training Epoch: 1/2, step 407/574 completed (loss: 0.22492755949497223, acc: 0.9166666865348816)
[2025-02-12 22:01:34,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:34,288][root][INFO] - Training Epoch: 1/2, step 408/574 completed (loss: 0.2566187381744385, acc: 0.9629629850387573)
[2025-02-12 22:01:34,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:34,622][root][INFO] - Training Epoch: 1/2, step 409/574 completed (loss: 0.20181706547737122, acc: 0.9615384340286255)
[2025-02-12 22:01:34,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:35,022][root][INFO] - Training Epoch: 1/2, step 410/574 completed (loss: 0.41948896646499634, acc: 0.8965517282485962)
[2025-02-12 22:01:35,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:35,365][root][INFO] - Training Epoch: 1/2, step 411/574 completed (loss: 0.18074524402618408, acc: 0.9285714030265808)
[2025-02-12 22:01:35,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:35,693][root][INFO] - Training Epoch: 1/2, step 412/574 completed (loss: 0.3296070098876953, acc: 0.9333333373069763)
[2025-02-12 22:01:35,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:36,044][root][INFO] - Training Epoch: 1/2, step 413/574 completed (loss: 0.5829571485519409, acc: 0.8484848737716675)
[2025-02-12 22:01:36,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:36,452][root][INFO] - Training Epoch: 1/2, step 414/574 completed (loss: 0.17943765223026276, acc: 0.9545454382896423)
[2025-02-12 22:01:36,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:36,843][root][INFO] - Training Epoch: 1/2, step 415/574 completed (loss: 0.4384784996509552, acc: 0.8823529481887817)
[2025-02-12 22:01:36,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:37,227][root][INFO] - Training Epoch: 1/2, step 416/574 completed (loss: 0.4412342309951782, acc: 0.8461538553237915)
[2025-02-12 22:01:37,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:37,551][root][INFO] - Training Epoch: 1/2, step 417/574 completed (loss: 0.5084366202354431, acc: 0.7777777910232544)
[2025-02-12 22:01:37,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:37,909][root][INFO] - Training Epoch: 1/2, step 418/574 completed (loss: 0.40386277437210083, acc: 0.875)
[2025-02-12 22:01:38,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:38,261][root][INFO] - Training Epoch: 1/2, step 419/574 completed (loss: 0.7084393501281738, acc: 0.8999999761581421)
[2025-02-12 22:01:38,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:38,635][root][INFO] - Training Epoch: 1/2, step 420/574 completed (loss: 0.31121471524238586, acc: 0.8571428656578064)
[2025-02-12 22:01:38,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:38,988][root][INFO] - Training Epoch: 1/2, step 421/574 completed (loss: 0.5559085607528687, acc: 0.7666666507720947)
[2025-02-12 22:01:39,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:39,279][root][INFO] - Training Epoch: 1/2, step 422/574 completed (loss: 0.5192359685897827, acc: 0.875)
[2025-02-12 22:01:39,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:39,595][root][INFO] - Training Epoch: 1/2, step 423/574 completed (loss: 0.8973360061645508, acc: 0.6944444179534912)
[2025-02-12 22:01:39,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:39,960][root][INFO] - Training Epoch: 1/2, step 424/574 completed (loss: 0.7189182043075562, acc: 0.8518518805503845)
[2025-02-12 22:01:40,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:40,302][root][INFO] - Training Epoch: 1/2, step 425/574 completed (loss: 0.32069671154022217, acc: 0.9090909361839294)
[2025-02-12 22:01:40,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:40,648][root][INFO] - Training Epoch: 1/2, step 426/574 completed (loss: 0.14609383046627045, acc: 0.95652174949646)
[2025-02-12 22:01:40,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:40,995][root][INFO] - Training Epoch: 1/2, step 427/574 completed (loss: 0.24390754103660583, acc: 0.9729729890823364)
[2025-02-12 22:01:41,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:41,314][root][INFO] - Training Epoch: 1/2, step 428/574 completed (loss: 0.3657982349395752, acc: 0.9629629850387573)
[2025-02-12 22:01:42,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:42,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:43,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:43,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:43,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:44,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:44,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:45,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:45,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:45,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:46,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:46,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:46,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:47,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:47,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:48,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:48,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:48,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:49,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:49,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:51,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:51,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:51,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:52,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:52,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:52,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:53,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:53,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:53,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:54,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:54,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:54,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:55,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:55,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:57,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:57,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:57,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:58,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:58,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:59,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:59,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:59,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:00,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:00,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:00,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:01,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:01,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:01,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:02,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:02,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:03,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:03,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:03,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:04,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:04,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:05,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:05,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:05,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:06,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:06,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:07,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:07,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:08,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:08,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:08,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:09,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:09,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:09,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:11,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:11,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:11,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:12,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:12,846][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8494, device='cuda:0') eval_epoch_loss=tensor(0.6148, device='cuda:0') eval_epoch_acc=tensor(0.8306, device='cuda:0')
[2025-02-12 22:02:12,847][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:02:12,847][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:02:13,193][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.6148401498794556/model.pt
[2025-02-12 22:02:13,202][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:02:13,204][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6148401498794556
[2025-02-12 22:02:13,205][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8306153416633606
[2025-02-12 22:02:13,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:13,608][root][INFO] - Training Epoch: 1/2, step 429/574 completed (loss: 0.23397420346736908, acc: 0.8695651888847351)
[2025-02-12 22:02:13,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:13,906][root][INFO] - Training Epoch: 1/2, step 430/574 completed (loss: 0.03239666670560837, acc: 1.0)
[2025-02-12 22:02:14,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:14,255][root][INFO] - Training Epoch: 1/2, step 431/574 completed (loss: 0.0246809720993042, acc: 1.0)
[2025-02-12 22:02:14,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:14,588][root][INFO] - Training Epoch: 1/2, step 432/574 completed (loss: 0.6937286257743835, acc: 0.782608687877655)
[2025-02-12 22:02:14,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:14,959][root][INFO] - Training Epoch: 1/2, step 433/574 completed (loss: 0.2967345416545868, acc: 0.8888888955116272)
[2025-02-12 22:02:15,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:15,251][root][INFO] - Training Epoch: 1/2, step 434/574 completed (loss: 0.019309919327497482, acc: 1.0)
[2025-02-12 22:02:15,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:15,601][root][INFO] - Training Epoch: 1/2, step 435/574 completed (loss: 0.0923340767621994, acc: 0.939393937587738)
[2025-02-12 22:02:15,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:15,987][root][INFO] - Training Epoch: 1/2, step 436/574 completed (loss: 0.3907986283302307, acc: 0.8888888955116272)
[2025-02-12 22:02:16,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:16,346][root][INFO] - Training Epoch: 1/2, step 437/574 completed (loss: 0.15792495012283325, acc: 0.9545454382896423)
[2025-02-12 22:02:16,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:16,635][root][INFO] - Training Epoch: 1/2, step 438/574 completed (loss: 0.10650257766246796, acc: 0.9523809552192688)
[2025-02-12 22:02:16,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:16,956][root][INFO] - Training Epoch: 1/2, step 439/574 completed (loss: 0.8275112509727478, acc: 0.8461538553237915)
[2025-02-12 22:02:17,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:17,477][root][INFO] - Training Epoch: 1/2, step 440/574 completed (loss: 0.8130712509155273, acc: 0.8181818127632141)
[2025-02-12 22:02:17,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:18,191][root][INFO] - Training Epoch: 1/2, step 441/574 completed (loss: 1.0663775205612183, acc: 0.6959999799728394)
[2025-02-12 22:02:18,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:18,650][root][INFO] - Training Epoch: 1/2, step 442/574 completed (loss: 0.9360929727554321, acc: 0.7903226017951965)
[2025-02-12 22:02:19,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:19,346][root][INFO] - Training Epoch: 1/2, step 443/574 completed (loss: 0.7041724920272827, acc: 0.8159204125404358)
[2025-02-12 22:02:19,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:19,716][root][INFO] - Training Epoch: 1/2, step 444/574 completed (loss: 0.21374763548374176, acc: 0.9245283007621765)
[2025-02-12 22:02:19,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:20,185][root][INFO] - Training Epoch: 1/2, step 445/574 completed (loss: 0.23635055124759674, acc: 0.9318181872367859)
[2025-02-12 22:02:20,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:20,609][root][INFO] - Training Epoch: 1/2, step 446/574 completed (loss: 0.39919450879096985, acc: 0.9130434989929199)
[2025-02-12 22:02:20,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:20,930][root][INFO] - Training Epoch: 1/2, step 447/574 completed (loss: 0.7246567010879517, acc: 0.807692289352417)
[2025-02-12 22:02:21,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:21,264][root][INFO] - Training Epoch: 1/2, step 448/574 completed (loss: 0.47080299258232117, acc: 0.8571428656578064)
[2025-02-12 22:02:21,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:21,605][root][INFO] - Training Epoch: 1/2, step 449/574 completed (loss: 0.3923572599887848, acc: 0.9104477763175964)
[2025-02-12 22:02:21,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:21,921][root][INFO] - Training Epoch: 1/2, step 450/574 completed (loss: 0.25193819403648376, acc: 0.9305555820465088)
[2025-02-12 22:02:22,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:22,335][root][INFO] - Training Epoch: 1/2, step 451/574 completed (loss: 0.17494754493236542, acc: 0.95652174949646)
[2025-02-12 22:02:22,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:22,685][root][INFO] - Training Epoch: 1/2, step 452/574 completed (loss: 0.3491184115409851, acc: 0.9102563858032227)
[2025-02-12 22:02:22,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:22,968][root][INFO] - Training Epoch: 1/2, step 453/574 completed (loss: 0.5054208636283875, acc: 0.8684210777282715)
[2025-02-12 22:02:23,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:23,295][root][INFO] - Training Epoch: 1/2, step 454/574 completed (loss: 0.48476994037628174, acc: 0.918367326259613)
[2025-02-12 22:02:23,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:23,650][root][INFO] - Training Epoch: 1/2, step 455/574 completed (loss: 0.32768237590789795, acc: 0.9090909361839294)
[2025-02-12 22:02:23,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:23,970][root][INFO] - Training Epoch: 1/2, step 456/574 completed (loss: 0.8203343152999878, acc: 0.7938144207000732)
[2025-02-12 22:02:24,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:24,286][root][INFO] - Training Epoch: 1/2, step 457/574 completed (loss: 0.11099427193403244, acc: 0.9714285731315613)
[2025-02-12 22:02:24,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:24,669][root][INFO] - Training Epoch: 1/2, step 458/574 completed (loss: 0.5836085677146912, acc: 0.8604651093482971)
[2025-02-12 22:02:24,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:25,048][root][INFO] - Training Epoch: 1/2, step 459/574 completed (loss: 0.09067480266094208, acc: 0.9821428656578064)
[2025-02-12 22:02:25,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:25,396][root][INFO] - Training Epoch: 1/2, step 460/574 completed (loss: 0.2928900420665741, acc: 0.9135802388191223)
[2025-02-12 22:02:25,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:25,719][root][INFO] - Training Epoch: 1/2, step 461/574 completed (loss: 0.7250862121582031, acc: 0.8333333134651184)
[2025-02-12 22:02:25,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:26,082][root][INFO] - Training Epoch: 1/2, step 462/574 completed (loss: 0.20162276923656464, acc: 0.90625)
[2025-02-12 22:02:26,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:26,417][root][INFO] - Training Epoch: 1/2, step 463/574 completed (loss: 0.6230663061141968, acc: 0.8846153616905212)
[2025-02-12 22:02:26,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:26,715][root][INFO] - Training Epoch: 1/2, step 464/574 completed (loss: 0.34417465329170227, acc: 0.8913043737411499)
[2025-02-12 22:02:26,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:27,017][root][INFO] - Training Epoch: 1/2, step 465/574 completed (loss: 0.6045010685920715, acc: 0.7857142686843872)
[2025-02-12 22:02:27,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:27,311][root][INFO] - Training Epoch: 1/2, step 466/574 completed (loss: 0.7032986879348755, acc: 0.8554216623306274)
[2025-02-12 22:02:27,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:27,674][root][INFO] - Training Epoch: 1/2, step 467/574 completed (loss: 0.5460251569747925, acc: 0.837837815284729)
[2025-02-12 22:02:27,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:28,030][root][INFO] - Training Epoch: 1/2, step 468/574 completed (loss: 0.7755334377288818, acc: 0.8252426981925964)
[2025-02-12 22:02:28,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:28,371][root][INFO] - Training Epoch: 1/2, step 469/574 completed (loss: 0.6231670379638672, acc: 0.8536585569381714)
[2025-02-12 22:02:28,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:28,703][root][INFO] - Training Epoch: 1/2, step 470/574 completed (loss: 0.6324350237846375, acc: 0.875)
[2025-02-12 22:02:28,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:29,087][root][INFO] - Training Epoch: 1/2, step 471/574 completed (loss: 0.8765748739242554, acc: 0.75)
[2025-02-12 22:02:29,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:29,529][root][INFO] - Training Epoch: 1/2, step 472/574 completed (loss: 1.1416577100753784, acc: 0.6960784196853638)
[2025-02-12 22:02:29,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:29,901][root][INFO] - Training Epoch: 1/2, step 473/574 completed (loss: 0.9121872782707214, acc: 0.7467249035835266)
[2025-02-12 22:02:30,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:30,214][root][INFO] - Training Epoch: 1/2, step 474/574 completed (loss: 0.9763216972351074, acc: 0.71875)
[2025-02-12 22:02:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:30,543][root][INFO] - Training Epoch: 1/2, step 475/574 completed (loss: 0.544738233089447, acc: 0.8343558311462402)
[2025-02-12 22:02:30,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:30,855][root][INFO] - Training Epoch: 1/2, step 476/574 completed (loss: 0.6898936033248901, acc: 0.7913669347763062)
[2025-02-12 22:02:31,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:31,264][root][INFO] - Training Epoch: 1/2, step 477/574 completed (loss: 1.0856657028198242, acc: 0.7085427045822144)
[2025-02-12 22:02:31,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:31,571][root][INFO] - Training Epoch: 1/2, step 478/574 completed (loss: 0.9581884741783142, acc: 0.75)
[2025-02-12 22:02:31,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:31,917][root][INFO] - Training Epoch: 1/2, step 479/574 completed (loss: 1.0624696016311646, acc: 0.7272727489471436)
[2025-02-12 22:02:32,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:32,275][root][INFO] - Training Epoch: 1/2, step 480/574 completed (loss: 1.0824291706085205, acc: 0.7777777910232544)
[2025-02-12 22:02:32,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:32,620][root][INFO] - Training Epoch: 1/2, step 481/574 completed (loss: 0.9570596814155579, acc: 0.75)
[2025-02-12 22:02:32,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:32,956][root][INFO] - Training Epoch: 1/2, step 482/574 completed (loss: 1.1919349431991577, acc: 0.6499999761581421)
[2025-02-12 22:02:33,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:33,368][root][INFO] - Training Epoch: 1/2, step 483/574 completed (loss: 1.132810354232788, acc: 0.7241379022598267)
[2025-02-12 22:02:33,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:33,697][root][INFO] - Training Epoch: 1/2, step 484/574 completed (loss: 0.3342338800430298, acc: 0.9354838728904724)
[2025-02-12 22:02:33,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:34,028][root][INFO] - Training Epoch: 1/2, step 485/574 completed (loss: 0.7845851182937622, acc: 0.7368420958518982)
[2025-02-12 22:02:34,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:34,390][root][INFO] - Training Epoch: 1/2, step 486/574 completed (loss: 1.3882087469100952, acc: 0.5925925970077515)
[2025-02-12 22:02:34,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:34,748][root][INFO] - Training Epoch: 1/2, step 487/574 completed (loss: 0.7367419004440308, acc: 0.8095238208770752)
[2025-02-12 22:02:34,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:35,056][root][INFO] - Training Epoch: 1/2, step 488/574 completed (loss: 0.8589931726455688, acc: 0.8636363744735718)
[2025-02-12 22:02:35,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:35,427][root][INFO] - Training Epoch: 1/2, step 489/574 completed (loss: 1.134720802307129, acc: 0.6769230961799622)
[2025-02-12 22:02:35,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:35,755][root][INFO] - Training Epoch: 1/2, step 490/574 completed (loss: 0.5097893476486206, acc: 0.8666666746139526)
[2025-02-12 22:02:35,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:36,145][root][INFO] - Training Epoch: 1/2, step 491/574 completed (loss: 0.8545566201210022, acc: 0.7586206793785095)
[2025-02-12 22:02:36,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:36,509][root][INFO] - Training Epoch: 1/2, step 492/574 completed (loss: 0.6422857642173767, acc: 0.7843137383460999)
[2025-02-12 22:02:36,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:36,868][root][INFO] - Training Epoch: 1/2, step 493/574 completed (loss: 0.5139840841293335, acc: 0.8620689511299133)
[2025-02-12 22:02:36,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:37,187][root][INFO] - Training Epoch: 1/2, step 494/574 completed (loss: 0.4817551076412201, acc: 0.8421052694320679)
[2025-02-12 22:02:37,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:37,562][root][INFO] - Training Epoch: 1/2, step 495/574 completed (loss: 1.1419587135314941, acc: 0.7894737124443054)
[2025-02-12 22:02:37,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:37,942][root][INFO] - Training Epoch: 1/2, step 496/574 completed (loss: 0.7847212553024292, acc: 0.7410714030265808)
[2025-02-12 22:02:38,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:38,338][root][INFO] - Training Epoch: 1/2, step 497/574 completed (loss: 0.6229108572006226, acc: 0.8202247023582458)
[2025-02-12 22:02:38,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:38,710][root][INFO] - Training Epoch: 1/2, step 498/574 completed (loss: 0.9209917783737183, acc: 0.7191011309623718)
[2025-02-12 22:02:38,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:39,169][root][INFO] - Training Epoch: 1/2, step 499/574 completed (loss: 1.3222332000732422, acc: 0.588652491569519)
[2025-02-12 22:02:39,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:39,557][root][INFO] - Training Epoch: 1/2, step 500/574 completed (loss: 0.8978514671325684, acc: 0.782608687877655)
[2025-02-12 22:02:39,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:39,960][root][INFO] - Training Epoch: 1/2, step 501/574 completed (loss: 0.08962259441614151, acc: 1.0)
[2025-02-12 22:02:40,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:40,331][root][INFO] - Training Epoch: 1/2, step 502/574 completed (loss: 0.24090997874736786, acc: 0.9615384340286255)
[2025-02-12 22:02:40,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:40,683][root][INFO] - Training Epoch: 1/2, step 503/574 completed (loss: 0.23289383947849274, acc: 0.9629629850387573)
[2025-02-12 22:02:40,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:41,069][root][INFO] - Training Epoch: 1/2, step 504/574 completed (loss: 0.4153297245502472, acc: 0.8518518805503845)
[2025-02-12 22:02:41,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:41,449][root][INFO] - Training Epoch: 1/2, step 505/574 completed (loss: 0.6354835629463196, acc: 0.8679245114326477)
[2025-02-12 22:02:41,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:41,790][root][INFO] - Training Epoch: 1/2, step 506/574 completed (loss: 0.8836973905563354, acc: 0.7241379022598267)
[2025-02-12 22:02:42,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:42,447][root][INFO] - Training Epoch: 1/2, step 507/574 completed (loss: 1.2223364114761353, acc: 0.630630612373352)
[2025-02-12 22:02:42,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:42,934][root][INFO] - Training Epoch: 1/2, step 508/574 completed (loss: 0.7583886981010437, acc: 0.8028169274330139)
[2025-02-12 22:02:43,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:43,270][root][INFO] - Training Epoch: 1/2, step 509/574 completed (loss: 0.15341563522815704, acc: 0.949999988079071)
[2025-02-12 22:02:43,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:43,611][root][INFO] - Training Epoch: 1/2, step 510/574 completed (loss: 0.4311927258968353, acc: 0.9333333373069763)
[2025-02-12 22:02:43,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:43,936][root][INFO] - Training Epoch: 1/2, step 511/574 completed (loss: 0.9533984661102295, acc: 0.7307692170143127)
[2025-02-12 22:02:45,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:46,283][root][INFO] - Training Epoch: 1/2, step 512/574 completed (loss: 1.2077388763427734, acc: 0.6571428775787354)
[2025-02-12 22:02:46,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:47,068][root][INFO] - Training Epoch: 1/2, step 513/574 completed (loss: 0.42071035504341125, acc: 0.8730158805847168)
[2025-02-12 22:02:47,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:47,473][root][INFO] - Training Epoch: 1/2, step 514/574 completed (loss: 0.6845826506614685, acc: 0.8214285969734192)
[2025-02-12 22:02:47,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:47,891][root][INFO] - Training Epoch: 1/2, step 515/574 completed (loss: 0.2544923424720764, acc: 0.8999999761581421)
[2025-02-12 22:02:48,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:48,605][root][INFO] - Training Epoch: 1/2, step 516/574 completed (loss: 0.7815446853637695, acc: 0.7777777910232544)
[2025-02-12 22:02:48,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:48,941][root][INFO] - Training Epoch: 1/2, step 517/574 completed (loss: 0.060841117054224014, acc: 1.0)
[2025-02-12 22:02:49,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:49,246][root][INFO] - Training Epoch: 1/2, step 518/574 completed (loss: 0.20649948716163635, acc: 0.9354838728904724)
[2025-02-12 22:02:49,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:49,547][root][INFO] - Training Epoch: 1/2, step 519/574 completed (loss: 0.5166887640953064, acc: 0.8500000238418579)
[2025-02-12 22:02:49,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:49,866][root][INFO] - Training Epoch: 1/2, step 520/574 completed (loss: 0.652916431427002, acc: 0.8518518805503845)
[2025-02-12 22:02:50,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:50,903][root][INFO] - Training Epoch: 1/2, step 521/574 completed (loss: 0.8395906090736389, acc: 0.7372881174087524)
[2025-02-12 22:02:51,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:51,357][root][INFO] - Training Epoch: 1/2, step 522/574 completed (loss: 0.5060369372367859, acc: 0.8656716346740723)
[2025-02-12 22:02:51,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:51,754][root][INFO] - Training Epoch: 1/2, step 523/574 completed (loss: 0.5630235075950623, acc: 0.8248175382614136)
[2025-02-12 22:02:52,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:52,354][root][INFO] - Training Epoch: 1/2, step 524/574 completed (loss: 0.7908744812011719, acc: 0.7699999809265137)
[2025-02-12 22:02:52,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:52,656][root][INFO] - Training Epoch: 1/2, step 525/574 completed (loss: 0.15848636627197266, acc: 0.9259259104728699)
[2025-02-12 22:02:52,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:53,055][root][INFO] - Training Epoch: 1/2, step 526/574 completed (loss: 0.41438788175582886, acc: 0.8846153616905212)
[2025-02-12 22:02:53,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:53,449][root][INFO] - Training Epoch: 1/2, step 527/574 completed (loss: 0.7318476438522339, acc: 0.761904776096344)
[2025-02-12 22:02:53,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:53,803][root][INFO] - Training Epoch: 1/2, step 528/574 completed (loss: 1.8274356126785278, acc: 0.5245901346206665)
[2025-02-12 22:02:53,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:54,202][root][INFO] - Training Epoch: 1/2, step 529/574 completed (loss: 0.47376397252082825, acc: 0.8813559412956238)
[2025-02-12 22:02:54,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:54,586][root][INFO] - Training Epoch: 1/2, step 530/574 completed (loss: 1.2122288942337036, acc: 0.6279069781303406)
[2025-02-12 22:02:54,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:54,918][root][INFO] - Training Epoch: 1/2, step 531/574 completed (loss: 1.109649896621704, acc: 0.7727272510528564)
[2025-02-12 22:02:55,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:55,172][root][INFO] - Training Epoch: 1/2, step 532/574 completed (loss: 1.1964287757873535, acc: 0.698113203048706)
[2025-02-12 22:02:55,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:55,597][root][INFO] - Training Epoch: 1/2, step 533/574 completed (loss: 0.868507444858551, acc: 0.7954545617103577)
[2025-02-12 22:02:55,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:56,011][root][INFO] - Training Epoch: 1/2, step 534/574 completed (loss: 0.9475024342536926, acc: 0.7200000286102295)
[2025-02-12 22:02:56,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:56,376][root][INFO] - Training Epoch: 1/2, step 535/574 completed (loss: 0.6982576251029968, acc: 0.8500000238418579)
[2025-02-12 22:02:56,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:56,738][root][INFO] - Training Epoch: 1/2, step 536/574 completed (loss: 0.3875473737716675, acc: 0.8636363744735718)
[2025-02-12 22:02:56,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:57,159][root][INFO] - Training Epoch: 1/2, step 537/574 completed (loss: 0.7763177156448364, acc: 0.8153846263885498)
[2025-02-12 22:02:57,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:57,495][root][INFO] - Training Epoch: 1/2, step 538/574 completed (loss: 0.4783547818660736, acc: 0.84375)
[2025-02-12 22:02:57,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:57,891][root][INFO] - Training Epoch: 1/2, step 539/574 completed (loss: 0.8550350666046143, acc: 0.71875)
[2025-02-12 22:02:57,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:58,209][root][INFO] - Training Epoch: 1/2, step 540/574 completed (loss: 0.5368919968605042, acc: 0.8484848737716675)
[2025-02-12 22:02:58,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:58,575][root][INFO] - Training Epoch: 1/2, step 541/574 completed (loss: 0.4483359754085541, acc: 0.8125)
[2025-02-12 22:02:58,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:58,979][root][INFO] - Training Epoch: 1/2, step 542/574 completed (loss: 0.09377630054950714, acc: 0.9677419066429138)
[2025-02-12 22:02:59,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:59,319][root][INFO] - Training Epoch: 1/2, step 543/574 completed (loss: 0.14539583027362823, acc: 0.95652174949646)
[2025-02-12 22:02:59,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:59,622][root][INFO] - Training Epoch: 1/2, step 544/574 completed (loss: 0.2565625011920929, acc: 0.9666666388511658)
[2025-02-12 22:02:59,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:59,948][root][INFO] - Training Epoch: 1/2, step 545/574 completed (loss: 0.22169922292232513, acc: 0.9756097793579102)
[2025-02-12 22:03:00,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:00,245][root][INFO] - Training Epoch: 1/2, step 546/574 completed (loss: 0.06070699542760849, acc: 0.9714285731315613)
[2025-02-12 22:03:00,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:00,548][root][INFO] - Training Epoch: 1/2, step 547/574 completed (loss: 0.16215476393699646, acc: 0.9210526347160339)
[2025-02-12 22:03:00,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:00,998][root][INFO] - Training Epoch: 1/2, step 548/574 completed (loss: 0.2567678391933441, acc: 0.8709677457809448)
[2025-02-12 22:03:01,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:01,357][root][INFO] - Training Epoch: 1/2, step 549/574 completed (loss: 0.046686865389347076, acc: 0.9599999785423279)
[2025-02-12 22:03:01,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:01,657][root][INFO] - Training Epoch: 1/2, step 550/574 completed (loss: 0.5690134763717651, acc: 0.8484848737716675)
[2025-02-12 22:03:01,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:01,981][root][INFO] - Training Epoch: 1/2, step 551/574 completed (loss: 0.3060899078845978, acc: 0.875)
[2025-02-12 22:03:02,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:02,282][root][INFO] - Training Epoch: 1/2, step 552/574 completed (loss: 0.41961702704429626, acc: 0.9142857193946838)
[2025-02-12 22:03:02,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:02,635][root][INFO] - Training Epoch: 1/2, step 553/574 completed (loss: 0.5561577081680298, acc: 0.8394160866737366)
[2025-02-12 22:03:02,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:02,972][root][INFO] - Training Epoch: 1/2, step 554/574 completed (loss: 0.36470845341682434, acc: 0.8965517282485962)
[2025-02-12 22:03:03,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:03,287][root][INFO] - Training Epoch: 1/2, step 555/574 completed (loss: 0.6284639239311218, acc: 0.8428571224212646)
[2025-02-12 22:03:03,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:03,618][root][INFO] - Training Epoch: 1/2, step 556/574 completed (loss: 0.6314440369606018, acc: 0.8476821184158325)
[2025-02-12 22:03:03,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:04,019][root][INFO] - Training Epoch: 1/2, step 557/574 completed (loss: 0.4968586266040802, acc: 0.8803418874740601)
[2025-02-12 22:03:04,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:04,349][root][INFO] - Training Epoch: 1/2, step 558/574 completed (loss: 0.2714514136314392, acc: 0.8399999737739563)
[2025-02-12 22:03:04,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:04,684][root][INFO] - Training Epoch: 1/2, step 559/574 completed (loss: 0.5512834787368774, acc: 0.807692289352417)
[2025-02-12 22:03:04,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:05,057][root][INFO] - Training Epoch: 1/2, step 560/574 completed (loss: 0.09431929141283035, acc: 0.9615384340286255)
[2025-02-12 22:03:05,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:05,455][root][INFO] - Training Epoch: 1/2, step 561/574 completed (loss: 0.11231961846351624, acc: 0.9743589758872986)
[2025-02-12 22:03:05,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:05,826][root][INFO] - Training Epoch: 1/2, step 562/574 completed (loss: 0.6521114706993103, acc: 0.8444444537162781)
[2025-02-12 22:03:05,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:06,253][root][INFO] - Training Epoch: 1/2, step 563/574 completed (loss: 0.430522084236145, acc: 0.8961039185523987)
[2025-02-12 22:03:06,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:06,650][root][INFO] - Training Epoch: 1/2, step 564/574 completed (loss: 0.4909926950931549, acc: 0.8333333134651184)
[2025-02-12 22:03:06,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:06,976][root][INFO] - Training Epoch: 1/2, step 565/574 completed (loss: 0.3591606020927429, acc: 0.8965517282485962)
[2025-02-12 22:03:07,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:07,304][root][INFO] - Training Epoch: 1/2, step 566/574 completed (loss: 0.5108937621116638, acc: 0.8571428656578064)
[2025-02-12 22:03:07,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:07,589][root][INFO] - Training Epoch: 1/2, step 567/574 completed (loss: 0.040711410343647, acc: 1.0)
[2025-02-12 22:03:07,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:07,897][root][INFO] - Training Epoch: 1/2, step 568/574 completed (loss: 0.18283461034297943, acc: 0.9259259104728699)
[2025-02-12 22:03:08,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:08,308][root][INFO] - Training Epoch: 1/2, step 569/574 completed (loss: 0.25892868638038635, acc: 0.9465240836143494)
[2025-02-12 22:03:08,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:08,640][root][INFO] - Training Epoch: 1/2, step 570/574 completed (loss: 0.10436107963323593, acc: 0.9838709831237793)
[2025-02-12 22:03:08,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:08,974][root][INFO] - Training Epoch: 1/2, step 571/574 completed (loss: 0.2992790937423706, acc: 0.94017094373703)
[2025-02-12 22:03:09,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:10,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:10,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:10,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:11,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:11,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:11,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:12,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:12,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:13,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:13,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:13,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:14,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:14,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:14,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:15,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:15,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:15,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:16,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:16,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:16,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:17,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:17,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:18,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:18,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:18,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:19,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:19,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:19,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:20,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:20,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:21,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:21,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:21,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:22,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:22,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:22,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:23,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:23,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:23,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:24,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:24,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:24,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:25,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:25,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:25,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:26,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:26,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:26,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:27,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:27,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:27,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:29,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:29,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:29,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:30,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:30,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:30,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:31,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:31,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:31,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:32,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:32,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:33,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:33,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:34,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:34,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:34,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:36,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:36,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:37,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:37,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:37,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:38,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:38,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:39,038][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7633, device='cuda:0') eval_epoch_loss=tensor(0.5672, device='cuda:0') eval_epoch_acc=tensor(0.8423, device='cuda:0')
[2025-02-12 22:03:39,039][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:03:39,039][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:03:39,338][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.5671751499176025/model.pt
[2025-02-12 22:03:39,350][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:03:39,351][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.5671751499176025
[2025-02-12 22:03:39,352][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8423181176185608
[2025-02-12 22:03:39,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:39,779][root][INFO] - Training Epoch: 1/2, step 572/574 completed (loss: 0.3863694667816162, acc: 0.9234693646430969)
[2025-02-12 22:03:39,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:40,140][root][INFO] - Training Epoch: 1/2, step 573/574 completed (loss: 0.5074118971824646, acc: 0.8867924809455872)
[2025-02-12 22:03:40,491][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=2.7424, train_epoch_loss=1.0088, epoch time 366.4349821768701s
[2025-02-12 22:03:40,492][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-12 22:03:40,492][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-02-12 22:03:40,492][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-12 22:03:40,492][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-02-12 22:03:40,492][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-12 22:03:41,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:41,481][root][INFO] - Training Epoch: 2/2, step 0/574 completed (loss: 0.5465433597564697, acc: 0.7407407164573669)
[2025-02-12 22:03:41,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:41,861][root][INFO] - Training Epoch: 2/2, step 1/574 completed (loss: 0.6792596578598022, acc: 0.800000011920929)
[2025-02-12 22:03:41,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:42,220][root][INFO] - Training Epoch: 2/2, step 2/574 completed (loss: 1.1549640893936157, acc: 0.7567567825317383)
[2025-02-12 22:03:42,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:42,563][root][INFO] - Training Epoch: 2/2, step 3/574 completed (loss: 0.6035422682762146, acc: 0.8421052694320679)
[2025-02-12 22:03:42,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:42,920][root][INFO] - Training Epoch: 2/2, step 4/574 completed (loss: 0.8000547885894775, acc: 0.7837837934494019)
[2025-02-12 22:03:43,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:43,261][root][INFO] - Training Epoch: 2/2, step 5/574 completed (loss: 0.47732824087142944, acc: 0.8214285969734192)
[2025-02-12 22:03:43,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:43,614][root][INFO] - Training Epoch: 2/2, step 6/574 completed (loss: 0.9467757940292358, acc: 0.6734693646430969)
[2025-02-12 22:03:43,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:43,962][root][INFO] - Training Epoch: 2/2, step 7/574 completed (loss: 0.5691773891448975, acc: 0.8999999761581421)
[2025-02-12 22:03:44,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:44,347][root][INFO] - Training Epoch: 2/2, step 8/574 completed (loss: 0.20425182580947876, acc: 0.9090909361839294)
[2025-02-12 22:03:44,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:44,741][root][INFO] - Training Epoch: 2/2, step 9/574 completed (loss: 0.16357849538326263, acc: 0.9615384340286255)
[2025-02-12 22:03:44,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:45,125][root][INFO] - Training Epoch: 2/2, step 10/574 completed (loss: 0.3443399667739868, acc: 0.9259259104728699)
[2025-02-12 22:03:45,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:45,518][root][INFO] - Training Epoch: 2/2, step 11/574 completed (loss: 0.5344452261924744, acc: 0.8974359035491943)
[2025-02-12 22:03:45,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:45,861][root][INFO] - Training Epoch: 2/2, step 12/574 completed (loss: 0.170867457985878, acc: 0.939393937587738)
[2025-02-12 22:03:45,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:46,270][root][INFO] - Training Epoch: 2/2, step 13/574 completed (loss: 0.46315476298332214, acc: 0.8260869383811951)
[2025-02-12 22:03:46,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:46,574][root][INFO] - Training Epoch: 2/2, step 14/574 completed (loss: 0.1831027716398239, acc: 0.9803921580314636)
[2025-02-12 22:03:46,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:46,865][root][INFO] - Training Epoch: 2/2, step 15/574 completed (loss: 0.4647703766822815, acc: 0.8979591727256775)
[2025-02-12 22:03:46,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:47,256][root][INFO] - Training Epoch: 2/2, step 16/574 completed (loss: 0.193660706281662, acc: 0.9473684430122375)
[2025-02-12 22:03:47,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:47,623][root][INFO] - Training Epoch: 2/2, step 17/574 completed (loss: 0.45656442642211914, acc: 0.8333333134651184)
[2025-02-12 22:03:47,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:47,989][root][INFO] - Training Epoch: 2/2, step 18/574 completed (loss: 0.615546464920044, acc: 0.8333333134651184)
[2025-02-12 22:03:48,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:48,332][root][INFO] - Training Epoch: 2/2, step 19/574 completed (loss: 0.2737436294555664, acc: 0.8421052694320679)
[2025-02-12 22:03:48,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:48,708][root][INFO] - Training Epoch: 2/2, step 20/574 completed (loss: 0.40601032972335815, acc: 0.9230769276618958)
[2025-02-12 22:03:48,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:48,963][root][INFO] - Training Epoch: 2/2, step 21/574 completed (loss: 0.9861095547676086, acc: 0.8275862336158752)
[2025-02-12 22:03:49,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:49,325][root][INFO] - Training Epoch: 2/2, step 22/574 completed (loss: 0.9260133504867554, acc: 0.7599999904632568)
[2025-02-12 22:03:49,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:49,663][root][INFO] - Training Epoch: 2/2, step 23/574 completed (loss: 1.0607324838638306, acc: 0.8095238208770752)
[2025-02-12 22:03:49,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:50,022][root][INFO] - Training Epoch: 2/2, step 24/574 completed (loss: 0.4976210296154022, acc: 0.8125)
[2025-02-12 22:03:50,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:50,305][root][INFO] - Training Epoch: 2/2, step 25/574 completed (loss: 0.5943254232406616, acc: 0.849056601524353)
[2025-02-12 22:03:50,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:50,608][root][INFO] - Training Epoch: 2/2, step 26/574 completed (loss: 0.90370112657547, acc: 0.698630154132843)
[2025-02-12 22:03:51,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:51,702][root][INFO] - Training Epoch: 2/2, step 27/574 completed (loss: 0.80866938829422, acc: 0.7865612506866455)
[2025-02-12 22:03:51,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52,016][root][INFO] - Training Epoch: 2/2, step 28/574 completed (loss: 0.5089709162712097, acc: 0.8372092843055725)
[2025-02-12 22:03:52,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52,360][root][INFO] - Training Epoch: 2/2, step 29/574 completed (loss: 0.7662896513938904, acc: 0.8433734774589539)
[2025-02-12 22:03:52,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52,692][root][INFO] - Training Epoch: 2/2, step 30/574 completed (loss: 0.6531871557235718, acc: 0.8148148059844971)
[2025-02-12 22:03:52,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52,977][root][INFO] - Training Epoch: 2/2, step 31/574 completed (loss: 0.7676650881767273, acc: 0.7857142686843872)
[2025-02-12 22:03:53,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:53,284][root][INFO] - Training Epoch: 2/2, step 32/574 completed (loss: 0.6048592925071716, acc: 0.8148148059844971)
[2025-02-12 22:03:53,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:53,578][root][INFO] - Training Epoch: 2/2, step 33/574 completed (loss: 0.18491126596927643, acc: 0.95652174949646)
[2025-02-12 22:03:53,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:54,025][root][INFO] - Training Epoch: 2/2, step 34/574 completed (loss: 0.6591989398002625, acc: 0.7815126180648804)
[2025-02-12 22:03:54,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:54,387][root][INFO] - Training Epoch: 2/2, step 35/574 completed (loss: 0.5056885480880737, acc: 0.8524590134620667)
[2025-02-12 22:03:54,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:54,743][root][INFO] - Training Epoch: 2/2, step 36/574 completed (loss: 0.5710830688476562, acc: 0.8253968358039856)
[2025-02-12 22:03:54,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:55,126][root][INFO] - Training Epoch: 2/2, step 37/574 completed (loss: 0.5887320637702942, acc: 0.8813559412956238)
[2025-02-12 22:03:55,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:55,477][root][INFO] - Training Epoch: 2/2, step 38/574 completed (loss: 0.4081082046031952, acc: 0.8850574493408203)
[2025-02-12 22:03:55,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:55,876][root][INFO] - Training Epoch: 2/2, step 39/574 completed (loss: 0.18911926448345184, acc: 1.0)
[2025-02-12 22:03:55,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:56,237][root][INFO] - Training Epoch: 2/2, step 40/574 completed (loss: 0.48609086871147156, acc: 0.807692289352417)
[2025-02-12 22:03:56,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:56,629][root][INFO] - Training Epoch: 2/2, step 41/574 completed (loss: 0.41219228506088257, acc: 0.8783783912658691)
[2025-02-12 22:03:56,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:57,024][root][INFO] - Training Epoch: 2/2, step 42/574 completed (loss: 0.5080272555351257, acc: 0.8153846263885498)
[2025-02-12 22:03:57,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:57,426][root][INFO] - Training Epoch: 2/2, step 43/574 completed (loss: 0.7690302729606628, acc: 0.808080792427063)
[2025-02-12 22:03:57,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:57,831][root][INFO] - Training Epoch: 2/2, step 44/574 completed (loss: 0.3815346658229828, acc: 0.876288652420044)
[2025-02-12 22:03:57,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:58,282][root][INFO] - Training Epoch: 2/2, step 45/574 completed (loss: 0.5209115147590637, acc: 0.875)
[2025-02-12 22:03:58,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:58,674][root][INFO] - Training Epoch: 2/2, step 46/574 completed (loss: 0.3543085753917694, acc: 0.9230769276618958)
[2025-02-12 22:03:58,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:59,058][root][INFO] - Training Epoch: 2/2, step 47/574 completed (loss: 0.26983025670051575, acc: 0.9629629850387573)
[2025-02-12 22:03:59,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:59,423][root][INFO] - Training Epoch: 2/2, step 48/574 completed (loss: 0.43704816699028015, acc: 0.9285714030265808)
[2025-02-12 22:03:59,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:59,738][root][INFO] - Training Epoch: 2/2, step 49/574 completed (loss: 0.07498225569725037, acc: 1.0)
[2025-02-12 22:03:59,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:00,068][root][INFO] - Training Epoch: 2/2, step 50/574 completed (loss: 0.7444141507148743, acc: 0.7719298005104065)
[2025-02-12 22:04:00,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:00,475][root][INFO] - Training Epoch: 2/2, step 51/574 completed (loss: 0.640895426273346, acc: 0.7777777910232544)
[2025-02-12 22:04:00,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:00,809][root][INFO] - Training Epoch: 2/2, step 52/574 completed (loss: 1.2396814823150635, acc: 0.6901408433914185)
[2025-02-12 22:04:01,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:01,259][root][INFO] - Training Epoch: 2/2, step 53/574 completed (loss: 1.5503400564193726, acc: 0.5666666626930237)
[2025-02-12 22:04:01,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:01,495][root][INFO] - Training Epoch: 2/2, step 54/574 completed (loss: 1.2215980291366577, acc: 0.7027027010917664)
[2025-02-12 22:04:01,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:01,847][root][INFO] - Training Epoch: 2/2, step 55/574 completed (loss: 0.18316584825515747, acc: 0.9615384340286255)
[2025-02-12 22:04:03,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:04,446][root][INFO] - Training Epoch: 2/2, step 56/574 completed (loss: 1.034524917602539, acc: 0.6928327679634094)
[2025-02-12 22:04:05,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:05,655][root][INFO] - Training Epoch: 2/2, step 57/574 completed (loss: 1.2235708236694336, acc: 0.6470588445663452)
[2025-02-12 22:04:05,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:06,303][root][INFO] - Training Epoch: 2/2, step 58/574 completed (loss: 0.9962884783744812, acc: 0.6818181872367859)
[2025-02-12 22:04:06,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:06,880][root][INFO] - Training Epoch: 2/2, step 59/574 completed (loss: 0.48373374342918396, acc: 0.875)
[2025-02-12 22:04:07,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:07,444][root][INFO] - Training Epoch: 2/2, step 60/574 completed (loss: 0.9289348721504211, acc: 0.7101449370384216)
[2025-02-12 22:04:07,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:07,845][root][INFO] - Training Epoch: 2/2, step 61/574 completed (loss: 0.8954492807388306, acc: 0.75)
[2025-02-12 22:04:07,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:08,151][root][INFO] - Training Epoch: 2/2, step 62/574 completed (loss: 0.7481579780578613, acc: 0.7647058963775635)
[2025-02-12 22:04:08,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:08,461][root][INFO] - Training Epoch: 2/2, step 63/574 completed (loss: 0.3961782157421112, acc: 0.8888888955116272)
[2025-02-12 22:04:08,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:08,837][root][INFO] - Training Epoch: 2/2, step 64/574 completed (loss: 0.23061954975128174, acc: 0.90625)
[2025-02-12 22:04:08,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:09,153][root][INFO] - Training Epoch: 2/2, step 65/574 completed (loss: 0.14268016815185547, acc: 0.9655172228813171)
[2025-02-12 22:04:09,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:09,477][root][INFO] - Training Epoch: 2/2, step 66/574 completed (loss: 0.8303095698356628, acc: 0.7678571343421936)
[2025-02-12 22:04:09,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:09,796][root][INFO] - Training Epoch: 2/2, step 67/574 completed (loss: 0.6338756680488586, acc: 0.8333333134651184)
[2025-02-12 22:04:09,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10,093][root][INFO] - Training Epoch: 2/2, step 68/574 completed (loss: 0.14997981488704681, acc: 0.9599999785423279)
[2025-02-12 22:04:10,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10,388][root][INFO] - Training Epoch: 2/2, step 69/574 completed (loss: 0.8108113408088684, acc: 0.7222222089767456)
[2025-02-12 22:04:10,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10,685][root][INFO] - Training Epoch: 2/2, step 70/574 completed (loss: 1.178101897239685, acc: 0.6969696879386902)
[2025-02-12 22:04:10,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10,993][root][INFO] - Training Epoch: 2/2, step 71/574 completed (loss: 1.0656843185424805, acc: 0.7132353186607361)
[2025-02-12 22:04:11,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:11,297][root][INFO] - Training Epoch: 2/2, step 72/574 completed (loss: 0.8566171526908875, acc: 0.7936508059501648)
[2025-02-12 22:04:11,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:11,624][root][INFO] - Training Epoch: 2/2, step 73/574 completed (loss: 1.357043981552124, acc: 0.6307692527770996)
[2025-02-12 22:04:11,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:11,948][root][INFO] - Training Epoch: 2/2, step 74/574 completed (loss: 1.287909746170044, acc: 0.6938775777816772)
[2025-02-12 22:04:12,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:12,287][root][INFO] - Training Epoch: 2/2, step 75/574 completed (loss: 1.2824839353561401, acc: 0.6194030046463013)
[2025-02-12 22:04:12,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:12,670][root][INFO] - Training Epoch: 2/2, step 76/574 completed (loss: 1.4569767713546753, acc: 0.6058394312858582)
[2025-02-12 22:04:12,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:12,983][root][INFO] - Training Epoch: 2/2, step 77/574 completed (loss: 0.10892627388238907, acc: 0.9523809552192688)
[2025-02-12 22:04:13,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:13,315][root][INFO] - Training Epoch: 2/2, step 78/574 completed (loss: 0.300266295671463, acc: 0.9166666865348816)
[2025-02-12 22:04:13,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:13,649][root][INFO] - Training Epoch: 2/2, step 79/574 completed (loss: 0.14265425503253937, acc: 0.9696969985961914)
[2025-02-12 22:04:13,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:13,950][root][INFO] - Training Epoch: 2/2, step 80/574 completed (loss: 0.32256177067756653, acc: 0.8846153616905212)
[2025-02-12 22:04:14,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:14,221][root][INFO] - Training Epoch: 2/2, step 81/574 completed (loss: 0.7566695809364319, acc: 0.7115384340286255)
[2025-02-12 22:04:14,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:14,491][root][INFO] - Training Epoch: 2/2, step 82/574 completed (loss: 0.6709110736846924, acc: 0.8461538553237915)
[2025-02-12 22:04:14,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:14,764][root][INFO] - Training Epoch: 2/2, step 83/574 completed (loss: 0.5262057185173035, acc: 0.875)
[2025-02-12 22:04:14,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:15,112][root][INFO] - Training Epoch: 2/2, step 84/574 completed (loss: 0.659031867980957, acc: 0.8115941882133484)
[2025-02-12 22:04:15,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:15,445][root][INFO] - Training Epoch: 2/2, step 85/574 completed (loss: 0.6015159487724304, acc: 0.8199999928474426)
[2025-02-12 22:04:15,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:15,865][root][INFO] - Training Epoch: 2/2, step 86/574 completed (loss: 0.7272269129753113, acc: 0.782608687877655)
[2025-02-12 22:04:16,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:16,331][root][INFO] - Training Epoch: 2/2, step 87/574 completed (loss: 1.3959580659866333, acc: 0.6600000262260437)
[2025-02-12 22:04:16,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:16,687][root][INFO] - Training Epoch: 2/2, step 88/574 completed (loss: 0.8142185211181641, acc: 0.7961165308952332)
[2025-02-12 22:04:17,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:17,764][root][INFO] - Training Epoch: 2/2, step 89/574 completed (loss: 1.0600045919418335, acc: 0.6990291476249695)
[2025-02-12 22:04:18,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:18,581][root][INFO] - Training Epoch: 2/2, step 90/574 completed (loss: 1.0457199811935425, acc: 0.7150537371635437)
[2025-02-12 22:04:19,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:19,388][root][INFO] - Training Epoch: 2/2, step 91/574 completed (loss: 0.9140870571136475, acc: 0.7413793206214905)
[2025-02-12 22:04:19,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:20,131][root][INFO] - Training Epoch: 2/2, step 92/574 completed (loss: 0.8032099604606628, acc: 0.7789473533630371)
[2025-02-12 22:04:20,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:21,119][root][INFO] - Training Epoch: 2/2, step 93/574 completed (loss: 1.3492084741592407, acc: 0.6336633563041687)
[2025-02-12 22:04:21,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:21,421][root][INFO] - Training Epoch: 2/2, step 94/574 completed (loss: 1.2738988399505615, acc: 0.6935483813285828)
[2025-02-12 22:04:21,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:21,763][root][INFO] - Training Epoch: 2/2, step 95/574 completed (loss: 0.9676163196563721, acc: 0.7681159377098083)
[2025-02-12 22:04:21,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:22,093][root][INFO] - Training Epoch: 2/2, step 96/574 completed (loss: 1.2591972351074219, acc: 0.6134454011917114)
[2025-02-12 22:04:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:22,423][root][INFO] - Training Epoch: 2/2, step 97/574 completed (loss: 1.2819459438323975, acc: 0.682692289352417)
[2025-02-12 22:04:22,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:22,808][root][INFO] - Training Epoch: 2/2, step 98/574 completed (loss: 1.266645073890686, acc: 0.6204379796981812)
[2025-02-12 22:04:22,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:23,157][root][INFO] - Training Epoch: 2/2, step 99/574 completed (loss: 1.6733850240707397, acc: 0.5223880410194397)
[2025-02-12 22:04:23,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:23,537][root][INFO] - Training Epoch: 2/2, step 100/574 completed (loss: 0.8509815335273743, acc: 0.699999988079071)
[2025-02-12 22:04:23,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:23,884][root][INFO] - Training Epoch: 2/2, step 101/574 completed (loss: 0.030067892745137215, acc: 1.0)
[2025-02-12 22:04:23,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:24,220][root][INFO] - Training Epoch: 2/2, step 102/574 completed (loss: 0.12918400764465332, acc: 0.95652174949646)
[2025-02-12 22:04:24,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:24,598][root][INFO] - Training Epoch: 2/2, step 103/574 completed (loss: 0.0786270946264267, acc: 0.9545454382896423)
[2025-02-12 22:04:24,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:24,944][root][INFO] - Training Epoch: 2/2, step 104/574 completed (loss: 0.5498583316802979, acc: 0.8448275923728943)
[2025-02-12 22:04:25,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:25,253][root][INFO] - Training Epoch: 2/2, step 105/574 completed (loss: 0.3467012345790863, acc: 0.8604651093482971)
[2025-02-12 22:04:25,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:25,601][root][INFO] - Training Epoch: 2/2, step 106/574 completed (loss: 0.6236713528633118, acc: 0.8399999737739563)
[2025-02-12 22:04:25,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:25,998][root][INFO] - Training Epoch: 2/2, step 107/574 completed (loss: 0.041788533329963684, acc: 1.0)
[2025-02-12 22:04:26,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:26,388][root][INFO] - Training Epoch: 2/2, step 108/574 completed (loss: 0.08369293808937073, acc: 0.9615384340286255)
[2025-02-12 22:04:26,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:26,720][root][INFO] - Training Epoch: 2/2, step 109/574 completed (loss: 0.16650402545928955, acc: 0.9285714030265808)
[2025-02-12 22:04:26,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:27,084][root][INFO] - Training Epoch: 2/2, step 110/574 completed (loss: 0.18964265286922455, acc: 0.9384615421295166)
[2025-02-12 22:04:27,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:27,505][root][INFO] - Training Epoch: 2/2, step 111/574 completed (loss: 0.48918670415878296, acc: 0.8245614171028137)
[2025-02-12 22:04:27,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:27,886][root][INFO] - Training Epoch: 2/2, step 112/574 completed (loss: 0.7392044067382812, acc: 0.8070175647735596)
[2025-02-12 22:04:27,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:28,259][root][INFO] - Training Epoch: 2/2, step 113/574 completed (loss: 0.4216980040073395, acc: 0.8717948794364929)
[2025-02-12 22:04:28,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:28,604][root][INFO] - Training Epoch: 2/2, step 114/574 completed (loss: 0.3017769157886505, acc: 0.8979591727256775)
[2025-02-12 22:04:28,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:28,935][root][INFO] - Training Epoch: 2/2, step 115/574 completed (loss: 0.13237664103507996, acc: 0.9545454382896423)
[2025-02-12 22:04:29,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:29,270][root][INFO] - Training Epoch: 2/2, step 116/574 completed (loss: 0.6063478589057922, acc: 0.841269850730896)
[2025-02-12 22:04:29,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:29,636][root][INFO] - Training Epoch: 2/2, step 117/574 completed (loss: 0.5013893842697144, acc: 0.869918704032898)
[2025-02-12 22:04:29,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:29,970][root][INFO] - Training Epoch: 2/2, step 118/574 completed (loss: 0.3841700851917267, acc: 0.8870967626571655)
[2025-02-12 22:04:30,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:30,815][root][INFO] - Training Epoch: 2/2, step 119/574 completed (loss: 0.5891773700714111, acc: 0.8365018963813782)
[2025-02-12 22:04:30,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:31,169][root][INFO] - Training Epoch: 2/2, step 120/574 completed (loss: 0.4034448564052582, acc: 0.8666666746139526)
[2025-02-12 22:04:31,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:31,574][root][INFO] - Training Epoch: 2/2, step 121/574 completed (loss: 0.5368309020996094, acc: 0.8846153616905212)
[2025-02-12 22:04:31,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:31,883][root][INFO] - Training Epoch: 2/2, step 122/574 completed (loss: 0.14683596789836884, acc: 0.9583333134651184)
[2025-02-12 22:04:32,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:32,285][root][INFO] - Training Epoch: 2/2, step 123/574 completed (loss: 0.3455490171909332, acc: 0.8947368264198303)
[2025-02-12 22:04:32,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:32,672][root][INFO] - Training Epoch: 2/2, step 124/574 completed (loss: 0.9349294304847717, acc: 0.7607361674308777)
[2025-02-12 22:04:32,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33,069][root][INFO] - Training Epoch: 2/2, step 125/574 completed (loss: 0.9344513416290283, acc: 0.7083333134651184)
[2025-02-12 22:04:33,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33,374][root][INFO] - Training Epoch: 2/2, step 126/574 completed (loss: 1.2132247686386108, acc: 0.675000011920929)
[2025-02-12 22:04:33,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33,675][root][INFO] - Training Epoch: 2/2, step 127/574 completed (loss: 0.6058194041252136, acc: 0.8035714030265808)
[2025-02-12 22:04:33,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33,980][root][INFO] - Training Epoch: 2/2, step 128/574 completed (loss: 0.8246767520904541, acc: 0.800000011920929)
[2025-02-12 22:04:34,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:34,397][root][INFO] - Training Epoch: 2/2, step 129/574 completed (loss: 0.9482977390289307, acc: 0.7352941036224365)
[2025-02-12 22:04:34,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:34,825][root][INFO] - Training Epoch: 2/2, step 130/574 completed (loss: 0.6986697316169739, acc: 0.692307710647583)
[2025-02-12 22:04:34,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:35,142][root][INFO] - Training Epoch: 2/2, step 131/574 completed (loss: 0.40675461292266846, acc: 0.8260869383811951)
[2025-02-12 22:04:35,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:35,564][root][INFO] - Training Epoch: 2/2, step 132/574 completed (loss: 0.6923059225082397, acc: 0.8125)
[2025-02-12 22:04:35,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:35,967][root][INFO] - Training Epoch: 2/2, step 133/574 completed (loss: 0.9296131730079651, acc: 0.739130437374115)
[2025-02-12 22:04:36,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:36,244][root][INFO] - Training Epoch: 2/2, step 134/574 completed (loss: 0.9181509017944336, acc: 0.800000011920929)
[2025-02-12 22:04:36,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:36,647][root][INFO] - Training Epoch: 2/2, step 135/574 completed (loss: 0.9244336485862732, acc: 0.7692307829856873)
[2025-02-12 22:04:36,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:37,042][root][INFO] - Training Epoch: 2/2, step 136/574 completed (loss: 0.6458325386047363, acc: 0.8095238208770752)
[2025-02-12 22:04:37,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:37,392][root][INFO] - Training Epoch: 2/2, step 137/574 completed (loss: 1.1063488721847534, acc: 0.5666666626930237)
[2025-02-12 22:04:37,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:37,777][root][INFO] - Training Epoch: 2/2, step 138/574 completed (loss: 0.9654567837715149, acc: 0.739130437374115)
[2025-02-12 22:04:37,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:38,124][root][INFO] - Training Epoch: 2/2, step 139/574 completed (loss: 0.40841224789619446, acc: 0.9047619104385376)
[2025-02-12 22:04:38,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:38,469][root][INFO] - Training Epoch: 2/2, step 140/574 completed (loss: 0.6273311376571655, acc: 0.7307692170143127)
[2025-02-12 22:04:39,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:39,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:40,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:40,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:41,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:41,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:41,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:42,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:42,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:42,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:43,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:43,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:43,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:44,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:44,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:44,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:45,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:45,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:45,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:46,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:46,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:48,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:48,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:49,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:49,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:49,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:50,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:50,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:50,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:51,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:51,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:51,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:53,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:53,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:53,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:54,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:54,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:54,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:55,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:55,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:55,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:56,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:56,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:56,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:57,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:57,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:57,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:58,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:58,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:58,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:59,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:59,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:00,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:00,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:00,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:01,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:01,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:01,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:02,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:02,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:03,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:03,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:03,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:04,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:04,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:05,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:05,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:05,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:06,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:06,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:06,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:07,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:07,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:07,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:08,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:08,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:09,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:09,518][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7016, device='cuda:0') eval_epoch_loss=tensor(0.5316, device='cuda:0') eval_epoch_acc=tensor(0.8524, device='cuda:0')
[2025-02-12 22:05:09,519][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:05:09,520][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:05:09,791][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.5315613150596619/model.pt
[2025-02-12 22:05:09,801][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:05:09,802][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.5315613150596619
[2025-02-12 22:05:09,803][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8524357080459595
[2025-02-12 22:05:09,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:10,212][root][INFO] - Training Epoch: 2/2, step 141/574 completed (loss: 0.9617251753807068, acc: 0.8064516186714172)
[2025-02-12 22:05:10,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:10,500][root][INFO] - Training Epoch: 2/2, step 142/574 completed (loss: 0.936109721660614, acc: 0.6756756901741028)
[2025-02-12 22:05:10,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:11,020][root][INFO] - Training Epoch: 2/2, step 143/574 completed (loss: 0.9952110648155212, acc: 0.7017543911933899)
[2025-02-12 22:05:11,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:11,356][root][INFO] - Training Epoch: 2/2, step 144/574 completed (loss: 0.8649126887321472, acc: 0.7910447716712952)
[2025-02-12 22:05:11,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:11,702][root][INFO] - Training Epoch: 2/2, step 145/574 completed (loss: 0.7012535333633423, acc: 0.8265306353569031)
[2025-02-12 22:05:11,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:12,138][root][INFO] - Training Epoch: 2/2, step 146/574 completed (loss: 1.2406758069992065, acc: 0.6489361524581909)
[2025-02-12 22:05:12,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:12,541][root][INFO] - Training Epoch: 2/2, step 147/574 completed (loss: 1.0663899183273315, acc: 0.7285714149475098)
[2025-02-12 22:05:12,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:12,901][root][INFO] - Training Epoch: 2/2, step 148/574 completed (loss: 1.2900065183639526, acc: 0.6071428656578064)
[2025-02-12 22:05:12,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:13,233][root][INFO] - Training Epoch: 2/2, step 149/574 completed (loss: 0.9667994379997253, acc: 0.782608687877655)
[2025-02-12 22:05:13,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:13,600][root][INFO] - Training Epoch: 2/2, step 150/574 completed (loss: 0.6146264672279358, acc: 0.8620689511299133)
[2025-02-12 22:05:13,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:13,972][root][INFO] - Training Epoch: 2/2, step 151/574 completed (loss: 0.8516055941581726, acc: 0.739130437374115)
[2025-02-12 22:05:14,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:14,405][root][INFO] - Training Epoch: 2/2, step 152/574 completed (loss: 0.9738832712173462, acc: 0.694915235042572)
[2025-02-12 22:05:14,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:14,780][root][INFO] - Training Epoch: 2/2, step 153/574 completed (loss: 1.0723614692687988, acc: 0.719298243522644)
[2025-02-12 22:05:14,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:15,186][root][INFO] - Training Epoch: 2/2, step 154/574 completed (loss: 0.9079880714416504, acc: 0.7567567825317383)
[2025-02-12 22:05:15,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:15,579][root][INFO] - Training Epoch: 2/2, step 155/574 completed (loss: 0.3714846670627594, acc: 0.8928571343421936)
[2025-02-12 22:05:15,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:15,948][root][INFO] - Training Epoch: 2/2, step 156/574 completed (loss: 0.32524409890174866, acc: 0.8695651888847351)
[2025-02-12 22:05:16,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:16,296][root][INFO] - Training Epoch: 2/2, step 157/574 completed (loss: 1.421199083328247, acc: 0.5789473652839661)
[2025-02-12 22:05:17,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:17,831][root][INFO] - Training Epoch: 2/2, step 158/574 completed (loss: 0.9048901200294495, acc: 0.7702702879905701)
[2025-02-12 22:05:17,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:18,194][root][INFO] - Training Epoch: 2/2, step 159/574 completed (loss: 1.3000531196594238, acc: 0.5925925970077515)
[2025-02-12 22:05:18,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:18,640][root][INFO] - Training Epoch: 2/2, step 160/574 completed (loss: 1.059824824333191, acc: 0.7093023061752319)
[2025-02-12 22:05:18,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:19,232][root][INFO] - Training Epoch: 2/2, step 161/574 completed (loss: 1.1231776475906372, acc: 0.6470588445663452)
[2025-02-12 22:05:19,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:19,799][root][INFO] - Training Epoch: 2/2, step 162/574 completed (loss: 1.293740153312683, acc: 0.6516854166984558)
[2025-02-12 22:05:19,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:20,118][root][INFO] - Training Epoch: 2/2, step 163/574 completed (loss: 0.5020838975906372, acc: 0.8409090638160706)
[2025-02-12 22:05:20,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:20,483][root][INFO] - Training Epoch: 2/2, step 164/574 completed (loss: 0.33666127920150757, acc: 0.9523809552192688)
[2025-02-12 22:05:20,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:20,816][root][INFO] - Training Epoch: 2/2, step 165/574 completed (loss: 0.636423647403717, acc: 0.7931034564971924)
[2025-02-12 22:05:20,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:21,161][root][INFO] - Training Epoch: 2/2, step 166/574 completed (loss: 0.20636940002441406, acc: 0.9591836929321289)
[2025-02-12 22:05:21,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:21,500][root][INFO] - Training Epoch: 2/2, step 167/574 completed (loss: 0.2610202431678772, acc: 0.9200000166893005)
[2025-02-12 22:05:21,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:21,911][root][INFO] - Training Epoch: 2/2, step 168/574 completed (loss: 0.6429989337921143, acc: 0.8055555820465088)
[2025-02-12 22:05:21,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:22,241][root][INFO] - Training Epoch: 2/2, step 169/574 completed (loss: 1.0117472410202026, acc: 0.7745097875595093)
[2025-02-12 22:05:22,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:23,289][root][INFO] - Training Epoch: 2/2, step 170/574 completed (loss: 0.7575929760932922, acc: 0.8082191944122314)
[2025-02-12 22:05:23,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:23,639][root][INFO] - Training Epoch: 2/2, step 171/574 completed (loss: 0.31064411997795105, acc: 0.9166666865348816)
[2025-02-12 22:05:23,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:24,046][root][INFO] - Training Epoch: 2/2, step 172/574 completed (loss: 0.5452229380607605, acc: 0.8148148059844971)
[2025-02-12 22:05:24,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:24,414][root][INFO] - Training Epoch: 2/2, step 173/574 completed (loss: 0.835109293460846, acc: 0.7857142686843872)
[2025-02-12 22:05:24,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:24,951][root][INFO] - Training Epoch: 2/2, step 174/574 completed (loss: 1.0994631052017212, acc: 0.7079645991325378)
[2025-02-12 22:05:25,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:25,253][root][INFO] - Training Epoch: 2/2, step 175/574 completed (loss: 0.751186728477478, acc: 0.8260869383811951)
[2025-02-12 22:05:25,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:25,598][root][INFO] - Training Epoch: 2/2, step 176/574 completed (loss: 0.5341917276382446, acc: 0.8295454382896423)
[2025-02-12 22:05:26,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:26,504][root][INFO] - Training Epoch: 2/2, step 177/574 completed (loss: 1.0366398096084595, acc: 0.694656491279602)
[2025-02-12 22:05:26,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:27,181][root][INFO] - Training Epoch: 2/2, step 178/574 completed (loss: 1.096312165260315, acc: 0.6592592597007751)
[2025-02-12 22:05:27,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:27,536][root][INFO] - Training Epoch: 2/2, step 179/574 completed (loss: 0.5439565777778625, acc: 0.8360655903816223)
[2025-02-12 22:05:27,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:27,891][root][INFO] - Training Epoch: 2/2, step 180/574 completed (loss: 0.09670265763998032, acc: 1.0)
[2025-02-12 22:05:27,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:28,236][root][INFO] - Training Epoch: 2/2, step 181/574 completed (loss: 0.31319332122802734, acc: 0.8799999952316284)
[2025-02-12 22:05:28,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:28,545][root][INFO] - Training Epoch: 2/2, step 182/574 completed (loss: 0.1612313836812973, acc: 0.9642857313156128)
[2025-02-12 22:05:28,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:28,849][root][INFO] - Training Epoch: 2/2, step 183/574 completed (loss: 0.4545342028141022, acc: 0.8292682766914368)
[2025-02-12 22:05:28,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:29,169][root][INFO] - Training Epoch: 2/2, step 184/574 completed (loss: 0.50578773021698, acc: 0.8821752071380615)
[2025-02-12 22:05:29,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:29,486][root][INFO] - Training Epoch: 2/2, step 185/574 completed (loss: 0.5092018246650696, acc: 0.8645533323287964)
[2025-02-12 22:05:29,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:29,968][root][INFO] - Training Epoch: 2/2, step 186/574 completed (loss: 0.45948559045791626, acc: 0.8656250238418579)
[2025-02-12 22:05:30,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:30,493][root][INFO] - Training Epoch: 2/2, step 187/574 completed (loss: 0.5115060806274414, acc: 0.8574109077453613)
[2025-02-12 22:05:30,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:30,885][root][INFO] - Training Epoch: 2/2, step 188/574 completed (loss: 0.5444188714027405, acc: 0.8398576378822327)
[2025-02-12 22:05:30,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:31,165][root][INFO] - Training Epoch: 2/2, step 189/574 completed (loss: 0.5124368071556091, acc: 0.8399999737739563)
[2025-02-12 22:05:31,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:31,741][root][INFO] - Training Epoch: 2/2, step 190/574 completed (loss: 0.8502680659294128, acc: 0.6976743936538696)
[2025-02-12 22:05:32,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:32,578][root][INFO] - Training Epoch: 2/2, step 191/574 completed (loss: 1.1023818254470825, acc: 0.7142857313156128)
[2025-02-12 22:05:33,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:33,495][root][INFO] - Training Epoch: 2/2, step 192/574 completed (loss: 1.0424994230270386, acc: 0.6742424368858337)
[2025-02-12 22:05:33,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:34,238][root][INFO] - Training Epoch: 2/2, step 193/574 completed (loss: 0.8481307029724121, acc: 0.7764706015586853)
[2025-02-12 22:05:34,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:35,312][root][INFO] - Training Epoch: 2/2, step 194/574 completed (loss: 0.988685131072998, acc: 0.7222222089767456)
[2025-02-12 22:05:35,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:36,268][root][INFO] - Training Epoch: 2/2, step 195/574 completed (loss: 0.5498021841049194, acc: 0.7903226017951965)
[2025-02-12 22:05:36,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:36,547][root][INFO] - Training Epoch: 2/2, step 196/574 completed (loss: 0.3235122859477997, acc: 0.8928571343421936)
[2025-02-12 22:05:36,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:36,906][root][INFO] - Training Epoch: 2/2, step 197/574 completed (loss: 1.235466718673706, acc: 0.625)
[2025-02-12 22:05:37,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:37,253][root][INFO] - Training Epoch: 2/2, step 198/574 completed (loss: 0.9794105291366577, acc: 0.7352941036224365)
[2025-02-12 22:05:37,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:37,585][root][INFO] - Training Epoch: 2/2, step 199/574 completed (loss: 0.8936046361923218, acc: 0.7647058963775635)
[2025-02-12 22:05:37,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:37,925][root][INFO] - Training Epoch: 2/2, step 200/574 completed (loss: 0.7046128511428833, acc: 0.7627118825912476)
[2025-02-12 22:05:38,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:38,256][root][INFO] - Training Epoch: 2/2, step 201/574 completed (loss: 0.9997929930686951, acc: 0.7313432693481445)
[2025-02-12 22:05:38,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:38,676][root][INFO] - Training Epoch: 2/2, step 202/574 completed (loss: 1.0043940544128418, acc: 0.7184466123580933)
[2025-02-12 22:05:38,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:39,072][root][INFO] - Training Epoch: 2/2, step 203/574 completed (loss: 1.0302443504333496, acc: 0.6984127163887024)
[2025-02-12 22:05:39,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:39,317][root][INFO] - Training Epoch: 2/2, step 204/574 completed (loss: 0.18167974054813385, acc: 0.9450549483299255)
[2025-02-12 22:05:39,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:39,633][root][INFO] - Training Epoch: 2/2, step 205/574 completed (loss: 0.3517152667045593, acc: 0.9058296084403992)
[2025-02-12 22:05:39,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:40,036][root][INFO] - Training Epoch: 2/2, step 206/574 completed (loss: 0.52056884765625, acc: 0.8267716765403748)
[2025-02-12 22:05:40,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:40,356][root][INFO] - Training Epoch: 2/2, step 207/574 completed (loss: 0.4169369637966156, acc: 0.8879310488700867)
[2025-02-12 22:05:40,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:40,708][root][INFO] - Training Epoch: 2/2, step 208/574 completed (loss: 0.4730278253555298, acc: 0.8804348111152649)
[2025-02-12 22:05:40,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41,056][root][INFO] - Training Epoch: 2/2, step 209/574 completed (loss: 0.502404510974884, acc: 0.8521400690078735)
[2025-02-12 22:05:41,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41,358][root][INFO] - Training Epoch: 2/2, step 210/574 completed (loss: 0.258802205324173, acc: 0.9021739363670349)
[2025-02-12 22:05:41,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41,669][root][INFO] - Training Epoch: 2/2, step 211/574 completed (loss: 0.12673063576221466, acc: 1.0)
[2025-02-12 22:05:41,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41,934][root][INFO] - Training Epoch: 2/2, step 212/574 completed (loss: 0.12508808076381683, acc: 0.9642857313156128)
[2025-02-12 22:05:42,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:42,223][root][INFO] - Training Epoch: 2/2, step 213/574 completed (loss: 0.13254643976688385, acc: 0.957446813583374)
[2025-02-12 22:05:42,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:42,904][root][INFO] - Training Epoch: 2/2, step 214/574 completed (loss: 0.17449326813220978, acc: 0.9230769276618958)
[2025-02-12 22:05:43,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:43,231][root][INFO] - Training Epoch: 2/2, step 215/574 completed (loss: 0.17702510952949524, acc: 0.9459459185600281)
[2025-02-12 22:05:43,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:43,532][root][INFO] - Training Epoch: 2/2, step 216/574 completed (loss: 0.22882360219955444, acc: 0.9534883499145508)
[2025-02-12 22:05:43,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:44,061][root][INFO] - Training Epoch: 2/2, step 217/574 completed (loss: 0.24255774915218353, acc: 0.9279279112815857)
[2025-02-12 22:05:44,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:44,450][root][INFO] - Training Epoch: 2/2, step 218/574 completed (loss: 0.11985397338867188, acc: 0.9777777791023254)
[2025-02-12 22:05:44,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:44,814][root][INFO] - Training Epoch: 2/2, step 219/574 completed (loss: 0.4593941867351532, acc: 0.9090909361839294)
[2025-02-12 22:05:44,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:45,224][root][INFO] - Training Epoch: 2/2, step 220/574 completed (loss: 0.11733677983283997, acc: 0.9629629850387573)
[2025-02-12 22:05:45,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:45,572][root][INFO] - Training Epoch: 2/2, step 221/574 completed (loss: 0.07766817510128021, acc: 1.0)
[2025-02-12 22:05:45,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:45,899][root][INFO] - Training Epoch: 2/2, step 222/574 completed (loss: 0.8512443900108337, acc: 0.7307692170143127)
[2025-02-12 22:05:46,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:46,712][root][INFO] - Training Epoch: 2/2, step 223/574 completed (loss: 0.32606303691864014, acc: 0.9021739363670349)
[2025-02-12 22:05:46,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:47,267][root][INFO] - Training Epoch: 2/2, step 224/574 completed (loss: 0.5384947657585144, acc: 0.8579545617103577)
[2025-02-12 22:05:47,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:47,694][root][INFO] - Training Epoch: 2/2, step 225/574 completed (loss: 0.7474306225776672, acc: 0.7872340679168701)
[2025-02-12 22:05:47,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:48,034][root][INFO] - Training Epoch: 2/2, step 226/574 completed (loss: 0.5923975706100464, acc: 0.7924528121948242)
[2025-02-12 22:05:48,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:48,351][root][INFO] - Training Epoch: 2/2, step 227/574 completed (loss: 0.3881518542766571, acc: 0.8666666746139526)
[2025-02-12 22:05:48,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:48,737][root][INFO] - Training Epoch: 2/2, step 228/574 completed (loss: 0.42603108286857605, acc: 0.8604651093482971)
[2025-02-12 22:05:48,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:49,030][root][INFO] - Training Epoch: 2/2, step 229/574 completed (loss: 1.0458165407180786, acc: 0.699999988079071)
[2025-02-12 22:05:49,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:49,464][root][INFO] - Training Epoch: 2/2, step 230/574 completed (loss: 1.706234097480774, acc: 0.5052631497383118)
[2025-02-12 22:05:49,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:49,807][root][INFO] - Training Epoch: 2/2, step 231/574 completed (loss: 1.4204437732696533, acc: 0.644444465637207)
[2025-02-12 22:05:49,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:50,251][root][INFO] - Training Epoch: 2/2, step 232/574 completed (loss: 1.16969895362854, acc: 0.6888889074325562)
[2025-02-12 22:05:50,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:50,741][root][INFO] - Training Epoch: 2/2, step 233/574 completed (loss: 1.6308679580688477, acc: 0.5733944773674011)
[2025-02-12 22:05:50,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:51,242][root][INFO] - Training Epoch: 2/2, step 234/574 completed (loss: 1.0934332609176636, acc: 0.6692307591438293)
[2025-02-12 22:05:51,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:51,542][root][INFO] - Training Epoch: 2/2, step 235/574 completed (loss: 0.6165827512741089, acc: 0.8421052694320679)
[2025-02-12 22:05:51,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:51,856][root][INFO] - Training Epoch: 2/2, step 236/574 completed (loss: 0.48497965931892395, acc: 0.7916666865348816)
[2025-02-12 22:05:51,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:52,169][root][INFO] - Training Epoch: 2/2, step 237/574 completed (loss: 1.4969992637634277, acc: 0.6363636255264282)
[2025-02-12 22:05:52,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:52,497][root][INFO] - Training Epoch: 2/2, step 238/574 completed (loss: 0.42650753259658813, acc: 0.8518518805503845)
[2025-02-12 22:05:52,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:52,837][root][INFO] - Training Epoch: 2/2, step 239/574 completed (loss: 0.8905603289604187, acc: 0.6857143044471741)
[2025-02-12 22:05:52,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:53,265][root][INFO] - Training Epoch: 2/2, step 240/574 completed (loss: 1.0061957836151123, acc: 0.7272727489471436)
[2025-02-12 22:05:53,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:53,622][root][INFO] - Training Epoch: 2/2, step 241/574 completed (loss: 0.6298729777336121, acc: 0.75)
[2025-02-12 22:05:53,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:54,220][root][INFO] - Training Epoch: 2/2, step 242/574 completed (loss: 1.1426116228103638, acc: 0.5967742204666138)
[2025-02-12 22:05:54,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:54,753][root][INFO] - Training Epoch: 2/2, step 243/574 completed (loss: 1.0506218671798706, acc: 0.7045454382896423)
[2025-02-12 22:05:54,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:55,062][root][INFO] - Training Epoch: 2/2, step 244/574 completed (loss: 0.029400072991847992, acc: 1.0)
[2025-02-12 22:05:55,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:55,329][root][INFO] - Training Epoch: 2/2, step 245/574 completed (loss: 0.14849600195884705, acc: 0.9230769276618958)
[2025-02-12 22:05:55,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:55,718][root][INFO] - Training Epoch: 2/2, step 246/574 completed (loss: 0.3116108477115631, acc: 0.9032257795333862)
[2025-02-12 22:05:55,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:56,117][root][INFO] - Training Epoch: 2/2, step 247/574 completed (loss: 0.2665458917617798, acc: 0.949999988079071)
[2025-02-12 22:05:56,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:56,503][root][INFO] - Training Epoch: 2/2, step 248/574 completed (loss: 0.5624580383300781, acc: 0.8648648858070374)
[2025-02-12 22:05:56,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:56,837][root][INFO] - Training Epoch: 2/2, step 249/574 completed (loss: 0.38700780272483826, acc: 0.8918918967247009)
[2025-02-12 22:05:56,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:57,199][root][INFO] - Training Epoch: 2/2, step 250/574 completed (loss: 0.15941251814365387, acc: 0.9459459185600281)
[2025-02-12 22:05:57,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:57,576][root][INFO] - Training Epoch: 2/2, step 251/574 completed (loss: 0.26020294427871704, acc: 0.8970588445663452)
[2025-02-12 22:05:57,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:57,882][root][INFO] - Training Epoch: 2/2, step 252/574 completed (loss: 0.20366279780864716, acc: 0.9024389982223511)
[2025-02-12 22:05:57,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:58,203][root][INFO] - Training Epoch: 2/2, step 253/574 completed (loss: 0.08048105984926224, acc: 1.0)
[2025-02-12 22:05:58,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:58,529][root][INFO] - Training Epoch: 2/2, step 254/574 completed (loss: 0.03683312609791756, acc: 0.9599999785423279)
[2025-02-12 22:05:58,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:58,843][root][INFO] - Training Epoch: 2/2, step 255/574 completed (loss: 0.11865834146738052, acc: 0.9677419066429138)
[2025-02-12 22:05:58,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:59,144][root][INFO] - Training Epoch: 2/2, step 256/574 completed (loss: 0.26938825845718384, acc: 0.9122806787490845)
[2025-02-12 22:05:59,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:59,437][root][INFO] - Training Epoch: 2/2, step 257/574 completed (loss: 0.1456795036792755, acc: 0.9714285731315613)
[2025-02-12 22:05:59,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:59,763][root][INFO] - Training Epoch: 2/2, step 258/574 completed (loss: 0.1694391965866089, acc: 0.9605262875556946)
[2025-02-12 22:06:00,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:00,330][root][INFO] - Training Epoch: 2/2, step 259/574 completed (loss: 0.47656407952308655, acc: 0.9056603908538818)
[2025-02-12 22:06:00,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:00,919][root][INFO] - Training Epoch: 2/2, step 260/574 completed (loss: 0.4919716715812683, acc: 0.8833333253860474)
[2025-02-12 22:06:00,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:01,200][root][INFO] - Training Epoch: 2/2, step 261/574 completed (loss: 0.19504910707473755, acc: 0.8888888955116272)
[2025-02-12 22:06:01,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:01,489][root][INFO] - Training Epoch: 2/2, step 262/574 completed (loss: 0.7513439059257507, acc: 0.8709677457809448)
[2025-02-12 22:06:01,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:01,810][root][INFO] - Training Epoch: 2/2, step 263/574 completed (loss: 1.2729735374450684, acc: 0.746666669845581)
[2025-02-12 22:06:01,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:02,107][root][INFO] - Training Epoch: 2/2, step 264/574 completed (loss: 0.6317715644836426, acc: 0.7708333134651184)
[2025-02-12 22:06:02,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:02,943][root][INFO] - Training Epoch: 2/2, step 265/574 completed (loss: 1.33262038230896, acc: 0.6320000290870667)
[2025-02-12 22:06:03,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:03,251][root][INFO] - Training Epoch: 2/2, step 266/574 completed (loss: 1.432184100151062, acc: 0.6067415475845337)
[2025-02-12 22:06:03,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:03,578][root][INFO] - Training Epoch: 2/2, step 267/574 completed (loss: 1.1361662149429321, acc: 0.6216216087341309)
[2025-02-12 22:06:03,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04,048][root][INFO] - Training Epoch: 2/2, step 268/574 completed (loss: 0.8180915117263794, acc: 0.7413793206214905)
[2025-02-12 22:06:04,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04,334][root][INFO] - Training Epoch: 2/2, step 269/574 completed (loss: 0.23277105391025543, acc: 0.9090909361839294)
[2025-02-12 22:06:04,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04,644][root][INFO] - Training Epoch: 2/2, step 270/574 completed (loss: 0.14011545479297638, acc: 0.9090909361839294)
[2025-02-12 22:06:04,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04,930][root][INFO] - Training Epoch: 2/2, step 271/574 completed (loss: 0.10540248453617096, acc: 0.96875)
[2025-02-12 22:06:05,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:05,228][root][INFO] - Training Epoch: 2/2, step 272/574 completed (loss: 0.13621269166469574, acc: 0.9666666388511658)
[2025-02-12 22:06:05,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:05,577][root][INFO] - Training Epoch: 2/2, step 273/574 completed (loss: 0.2726197838783264, acc: 0.949999988079071)
[2025-02-12 22:06:05,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:05,862][root][INFO] - Training Epoch: 2/2, step 274/574 completed (loss: 0.2514801323413849, acc: 0.9375)
[2025-02-12 22:06:05,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:06,188][root][INFO] - Training Epoch: 2/2, step 275/574 completed (loss: 0.21527309715747833, acc: 0.9333333373069763)
[2025-02-12 22:06:06,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:06,529][root][INFO] - Training Epoch: 2/2, step 276/574 completed (loss: 0.3731170892715454, acc: 0.931034505367279)
[2025-02-12 22:06:06,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:06,922][root][INFO] - Training Epoch: 2/2, step 277/574 completed (loss: 0.3181939721107483, acc: 0.9599999785423279)
[2025-02-12 22:06:07,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:07,238][root][INFO] - Training Epoch: 2/2, step 278/574 completed (loss: 0.3859301507472992, acc: 0.8936170339584351)
[2025-02-12 22:06:07,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:07,570][root][INFO] - Training Epoch: 2/2, step 279/574 completed (loss: 0.6061995625495911, acc: 0.875)
[2025-02-12 22:06:07,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:07,908][root][INFO] - Training Epoch: 2/2, step 280/574 completed (loss: 0.13517098128795624, acc: 0.9772727489471436)
[2025-02-12 22:06:08,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:08,387][root][INFO] - Training Epoch: 2/2, step 281/574 completed (loss: 0.8257825970649719, acc: 0.7951807379722595)
[2025-02-12 22:06:08,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:08,775][root][INFO] - Training Epoch: 2/2, step 282/574 completed (loss: 1.0226471424102783, acc: 0.7592592835426331)
[2025-02-12 22:06:08,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:09,074][root][INFO] - Training Epoch: 2/2, step 283/574 completed (loss: 0.1328066736459732, acc: 0.9473684430122375)
[2025-02-12 22:06:10,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:10,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:10,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:11,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:11,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:12,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:12,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:12,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:13,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:13,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:13,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:14,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:14,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:14,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:15,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:15,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:16,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:16,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:16,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:17,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:17,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:17,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:18,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:18,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:18,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:19,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:19,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:19,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:20,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:20,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:20,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:21,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:21,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:21,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:22,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:22,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:22,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:23,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:23,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:23,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:24,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:24,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:24,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:25,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:25,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:25,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:26,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:26,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:26,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:27,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:27,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:27,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:28,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:28,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:28,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:29,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:29,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:29,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:30,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:30,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:30,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:31,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:31,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:32,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:32,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:32,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:33,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:33,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:33,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:34,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:34,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:35,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:35,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:35,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:36,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:36,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:36,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:37,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:37,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:38,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:38,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:38,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:39,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:39,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:39,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:40,511][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.6684, device='cuda:0') eval_epoch_loss=tensor(0.5119, device='cuda:0') eval_epoch_acc=tensor(0.8582, device='cuda:0')
[2025-02-12 22:06:40,513][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:06:40,513][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:06:40,784][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5118914246559143/model.pt
[2025-02-12 22:06:40,788][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:06:40,789][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.5118914246559143
[2025-02-12 22:06:40,790][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8582029342651367
[2025-02-12 22:06:40,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:41,246][root][INFO] - Training Epoch: 2/2, step 284/574 completed (loss: 0.6641221046447754, acc: 0.8235294222831726)
[2025-02-12 22:06:41,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:41,629][root][INFO] - Training Epoch: 2/2, step 285/574 completed (loss: 0.20304033160209656, acc: 0.925000011920929)
[2025-02-12 22:06:41,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:41,976][root][INFO] - Training Epoch: 2/2, step 286/574 completed (loss: 0.6015849113464355, acc: 0.796875)
[2025-02-12 22:06:42,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:42,335][root][INFO] - Training Epoch: 2/2, step 287/574 completed (loss: 0.5611127614974976, acc: 0.8080000281333923)
[2025-02-12 22:06:42,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:42,687][root][INFO] - Training Epoch: 2/2, step 288/574 completed (loss: 0.3295384645462036, acc: 0.9230769276618958)
[2025-02-12 22:06:42,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43,002][root][INFO] - Training Epoch: 2/2, step 289/574 completed (loss: 0.5553620457649231, acc: 0.8447204828262329)
[2025-02-12 22:06:43,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43,364][root][INFO] - Training Epoch: 2/2, step 290/574 completed (loss: 0.49334925413131714, acc: 0.876288652420044)
[2025-02-12 22:06:43,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43,648][root][INFO] - Training Epoch: 2/2, step 291/574 completed (loss: 0.08546169102191925, acc: 1.0)
[2025-02-12 22:06:43,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43,926][root][INFO] - Training Epoch: 2/2, step 292/574 completed (loss: 0.3515479564666748, acc: 0.8809523582458496)
[2025-02-12 22:06:44,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:44,236][root][INFO] - Training Epoch: 2/2, step 293/574 completed (loss: 0.17178167402744293, acc: 0.9655172228813171)
[2025-02-12 22:06:44,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:44,692][root][INFO] - Training Epoch: 2/2, step 294/574 completed (loss: 0.4478470981121063, acc: 0.8727272748947144)
[2025-02-12 22:06:44,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:45,244][root][INFO] - Training Epoch: 2/2, step 295/574 completed (loss: 0.5731666684150696, acc: 0.8608247637748718)
[2025-02-12 22:06:45,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:45,582][root][INFO] - Training Epoch: 2/2, step 296/574 completed (loss: 0.46430453658103943, acc: 0.8965517282485962)
[2025-02-12 22:06:45,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:45,974][root][INFO] - Training Epoch: 2/2, step 297/574 completed (loss: 0.16166070103645325, acc: 0.9629629850387573)
[2025-02-12 22:06:46,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:46,371][root][INFO] - Training Epoch: 2/2, step 298/574 completed (loss: 0.5888537764549255, acc: 0.7894737124443054)
[2025-02-12 22:06:46,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:46,631][root][INFO] - Training Epoch: 2/2, step 299/574 completed (loss: 0.07121435552835464, acc: 0.9821428656578064)
[2025-02-12 22:06:46,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:46,953][root][INFO] - Training Epoch: 2/2, step 300/574 completed (loss: 0.09735777974128723, acc: 0.96875)
[2025-02-12 22:06:47,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:47,304][root][INFO] - Training Epoch: 2/2, step 301/574 completed (loss: 0.32218286395072937, acc: 0.9245283007621765)
[2025-02-12 22:06:47,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:47,627][root][INFO] - Training Epoch: 2/2, step 302/574 completed (loss: 0.04279404133558273, acc: 1.0)
[2025-02-12 22:06:47,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:47,930][root][INFO] - Training Epoch: 2/2, step 303/574 completed (loss: 0.023933768272399902, acc: 1.0)
[2025-02-12 22:06:48,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:48,228][root][INFO] - Training Epoch: 2/2, step 304/574 completed (loss: 0.09976750612258911, acc: 0.96875)
[2025-02-12 22:06:48,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:48,606][root][INFO] - Training Epoch: 2/2, step 305/574 completed (loss: 0.5351212024688721, acc: 0.8524590134620667)
[2025-02-12 22:06:48,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:48,960][root][INFO] - Training Epoch: 2/2, step 306/574 completed (loss: 0.10068067908287048, acc: 0.9666666388511658)
[2025-02-12 22:06:49,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:49,290][root][INFO] - Training Epoch: 2/2, step 307/574 completed (loss: 0.09207158535718918, acc: 0.9473684430122375)
[2025-02-12 22:06:49,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:49,623][root][INFO] - Training Epoch: 2/2, step 308/574 completed (loss: 0.33395615220069885, acc: 0.8985507488250732)
[2025-02-12 22:06:49,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:50,101][root][INFO] - Training Epoch: 2/2, step 309/574 completed (loss: 0.22040066123008728, acc: 0.9166666865348816)
[2025-02-12 22:06:50,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:50,446][root][INFO] - Training Epoch: 2/2, step 310/574 completed (loss: 0.3198714852333069, acc: 0.891566276550293)
[2025-02-12 22:06:50,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:50,785][root][INFO] - Training Epoch: 2/2, step 311/574 completed (loss: 0.43608924746513367, acc: 0.8461538553237915)
[2025-02-12 22:06:50,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:51,155][root][INFO] - Training Epoch: 2/2, step 312/574 completed (loss: 0.07503421604633331, acc: 0.9693877696990967)
[2025-02-12 22:06:51,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:51,488][root][INFO] - Training Epoch: 2/2, step 313/574 completed (loss: 0.009477376937866211, acc: 1.0)
[2025-02-12 22:06:51,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:51,792][root][INFO] - Training Epoch: 2/2, step 314/574 completed (loss: 0.055905427783727646, acc: 1.0)
[2025-02-12 22:06:51,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:52,111][root][INFO] - Training Epoch: 2/2, step 315/574 completed (loss: 0.31988751888275146, acc: 0.9354838728904724)
[2025-02-12 22:06:52,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:52,407][root][INFO] - Training Epoch: 2/2, step 316/574 completed (loss: 0.1619853526353836, acc: 0.9354838728904724)
[2025-02-12 22:06:52,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:52,735][root][INFO] - Training Epoch: 2/2, step 317/574 completed (loss: 0.2805965840816498, acc: 0.89552241563797)
[2025-02-12 22:06:52,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:53,161][root][INFO] - Training Epoch: 2/2, step 318/574 completed (loss: 0.18096759915351868, acc: 0.9615384340286255)
[2025-02-12 22:06:53,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:53,526][root][INFO] - Training Epoch: 2/2, step 319/574 completed (loss: 0.2952633798122406, acc: 0.9333333373069763)
[2025-02-12 22:06:53,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:53,876][root][INFO] - Training Epoch: 2/2, step 320/574 completed (loss: 0.13583879172801971, acc: 0.9677419066429138)
[2025-02-12 22:06:54,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:54,275][root][INFO] - Training Epoch: 2/2, step 321/574 completed (loss: 0.04466475546360016, acc: 0.9800000190734863)
[2025-02-12 22:06:54,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:54,593][root][INFO] - Training Epoch: 2/2, step 322/574 completed (loss: 0.7091774940490723, acc: 0.7777777910232544)
[2025-02-12 22:06:54,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:54,901][root][INFO] - Training Epoch: 2/2, step 323/574 completed (loss: 0.9248959422111511, acc: 0.7428571581840515)
[2025-02-12 22:06:54,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:55,214][root][INFO] - Training Epoch: 2/2, step 324/574 completed (loss: 1.4262375831604004, acc: 0.6410256624221802)
[2025-02-12 22:06:55,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:55,551][root][INFO] - Training Epoch: 2/2, step 325/574 completed (loss: 1.5646073818206787, acc: 0.6097561120986938)
[2025-02-12 22:06:55,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:55,948][root][INFO] - Training Epoch: 2/2, step 326/574 completed (loss: 0.8338473439216614, acc: 0.7631579041481018)
[2025-02-12 22:06:56,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:56,273][root][INFO] - Training Epoch: 2/2, step 327/574 completed (loss: 0.5034257173538208, acc: 0.9473684430122375)
[2025-02-12 22:06:56,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:56,662][root][INFO] - Training Epoch: 2/2, step 328/574 completed (loss: 0.1017838642001152, acc: 0.9642857313156128)
[2025-02-12 22:06:56,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:56,995][root][INFO] - Training Epoch: 2/2, step 329/574 completed (loss: 0.2540091276168823, acc: 0.9259259104728699)
[2025-02-12 22:06:57,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:57,335][root][INFO] - Training Epoch: 2/2, step 330/574 completed (loss: 0.08657282590866089, acc: 0.96875)
[2025-02-12 22:06:57,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:57,650][root][INFO] - Training Epoch: 2/2, step 331/574 completed (loss: 0.25591105222702026, acc: 0.9354838728904724)
[2025-02-12 22:06:57,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:58,031][root][INFO] - Training Epoch: 2/2, step 332/574 completed (loss: 0.1451268047094345, acc: 0.9649122953414917)
[2025-02-12 22:06:58,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:58,417][root][INFO] - Training Epoch: 2/2, step 333/574 completed (loss: 0.3311879634857178, acc: 0.9375)
[2025-02-12 22:06:58,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:58,869][root][INFO] - Training Epoch: 2/2, step 334/574 completed (loss: 0.1508435755968094, acc: 0.9666666388511658)
[2025-02-12 22:06:58,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:59,198][root][INFO] - Training Epoch: 2/2, step 335/574 completed (loss: 0.24319571256637573, acc: 0.9473684430122375)
[2025-02-12 22:06:59,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:59,493][root][INFO] - Training Epoch: 2/2, step 336/574 completed (loss: 0.8071019649505615, acc: 0.7400000095367432)
[2025-02-12 22:06:59,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:59,821][root][INFO] - Training Epoch: 2/2, step 337/574 completed (loss: 1.3152168989181519, acc: 0.6436781883239746)
[2025-02-12 22:06:59,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:00,230][root][INFO] - Training Epoch: 2/2, step 338/574 completed (loss: 1.2229924201965332, acc: 0.6276595592498779)
[2025-02-12 22:07:00,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:00,565][root][INFO] - Training Epoch: 2/2, step 339/574 completed (loss: 1.2467466592788696, acc: 0.650602400302887)
[2025-02-12 22:07:00,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:00,875][root][INFO] - Training Epoch: 2/2, step 340/574 completed (loss: 0.03962517902255058, acc: 1.0)
[2025-02-12 22:07:00,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:01,154][root][INFO] - Training Epoch: 2/2, step 341/574 completed (loss: 0.7177165150642395, acc: 0.8717948794364929)
[2025-02-12 22:07:01,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:01,525][root][INFO] - Training Epoch: 2/2, step 342/574 completed (loss: 0.574920117855072, acc: 0.8795180916786194)
[2025-02-12 22:07:01,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:01,868][root][INFO] - Training Epoch: 2/2, step 343/574 completed (loss: 0.4471702575683594, acc: 0.8679245114326477)
[2025-02-12 22:07:01,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:02,178][root][INFO] - Training Epoch: 2/2, step 344/574 completed (loss: 0.211822509765625, acc: 0.9367088675498962)
[2025-02-12 22:07:02,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:02,496][root][INFO] - Training Epoch: 2/2, step 345/574 completed (loss: 0.11835005134344101, acc: 0.9411764740943909)
[2025-02-12 22:07:02,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:02,801][root][INFO] - Training Epoch: 2/2, step 346/574 completed (loss: 0.3594485819339752, acc: 0.8805969953536987)
[2025-02-12 22:07:02,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:03,097][root][INFO] - Training Epoch: 2/2, step 347/574 completed (loss: 0.011881815269589424, acc: 1.0)
[2025-02-12 22:07:03,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:03,393][root][INFO] - Training Epoch: 2/2, step 348/574 completed (loss: 0.17829570174217224, acc: 0.9599999785423279)
[2025-02-12 22:07:03,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:03,791][root][INFO] - Training Epoch: 2/2, step 349/574 completed (loss: 0.6856319308280945, acc: 0.8055555820465088)
[2025-02-12 22:07:03,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:04,130][root][INFO] - Training Epoch: 2/2, step 350/574 completed (loss: 0.6999639272689819, acc: 0.7209302186965942)
[2025-02-12 22:07:04,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:04,447][root][INFO] - Training Epoch: 2/2, step 351/574 completed (loss: 0.19377239048480988, acc: 0.9487179517745972)
[2025-02-12 22:07:04,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:04,827][root][INFO] - Training Epoch: 2/2, step 352/574 completed (loss: 0.7606956362724304, acc: 0.7777777910232544)
[2025-02-12 22:07:04,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:05,223][root][INFO] - Training Epoch: 2/2, step 353/574 completed (loss: 0.02366681769490242, acc: 1.0)
[2025-02-12 22:07:05,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:05,610][root][INFO] - Training Epoch: 2/2, step 354/574 completed (loss: 0.7723348140716553, acc: 0.807692289352417)
[2025-02-12 22:07:05,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:05,936][root][INFO] - Training Epoch: 2/2, step 355/574 completed (loss: 0.9502097368240356, acc: 0.7142857313156128)
[2025-02-12 22:07:06,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:06,432][root][INFO] - Training Epoch: 2/2, step 356/574 completed (loss: 0.6371392011642456, acc: 0.8086956739425659)
[2025-02-12 22:07:06,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:06,751][root][INFO] - Training Epoch: 2/2, step 357/574 completed (loss: 0.604884147644043, acc: 0.8152173757553101)
[2025-02-12 22:07:06,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:07,063][root][INFO] - Training Epoch: 2/2, step 358/574 completed (loss: 0.7795040607452393, acc: 0.7755101919174194)
[2025-02-12 22:07:07,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:07,336][root][INFO] - Training Epoch: 2/2, step 359/574 completed (loss: 0.007214905694127083, acc: 1.0)
[2025-02-12 22:07:07,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:07,684][root][INFO] - Training Epoch: 2/2, step 360/574 completed (loss: 0.3611215054988861, acc: 0.9230769276618958)
[2025-02-12 22:07:07,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:08,059][root][INFO] - Training Epoch: 2/2, step 361/574 completed (loss: 0.3069142699241638, acc: 0.9268292784690857)
[2025-02-12 22:07:08,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:08,411][root][INFO] - Training Epoch: 2/2, step 362/574 completed (loss: 0.3105834126472473, acc: 0.9333333373069763)
[2025-02-12 22:07:08,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:08,767][root][INFO] - Training Epoch: 2/2, step 363/574 completed (loss: 0.18656405806541443, acc: 0.9078947305679321)
[2025-02-12 22:07:08,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:09,121][root][INFO] - Training Epoch: 2/2, step 364/574 completed (loss: 0.2535824775695801, acc: 0.8780487775802612)
[2025-02-12 22:07:09,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:09,433][root][INFO] - Training Epoch: 2/2, step 365/574 completed (loss: 0.12354746460914612, acc: 0.9696969985961914)
[2025-02-12 22:07:09,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:09,726][root][INFO] - Training Epoch: 2/2, step 366/574 completed (loss: 0.03692694753408432, acc: 1.0)
[2025-02-12 22:07:09,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:10,056][root][INFO] - Training Epoch: 2/2, step 367/574 completed (loss: 0.023051997646689415, acc: 1.0)
[2025-02-12 22:07:10,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:10,406][root][INFO] - Training Epoch: 2/2, step 368/574 completed (loss: 0.042134907096624374, acc: 1.0)
[2025-02-12 22:07:10,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:10,733][root][INFO] - Training Epoch: 2/2, step 369/574 completed (loss: 0.13840292394161224, acc: 1.0)
[2025-02-12 22:07:11,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:11,371][root][INFO] - Training Epoch: 2/2, step 370/574 completed (loss: 0.4535314738750458, acc: 0.8545454740524292)
[2025-02-12 22:07:11,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:12,243][root][INFO] - Training Epoch: 2/2, step 371/574 completed (loss: 0.38356855511665344, acc: 0.8867924809455872)
[2025-02-12 22:07:12,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:12,580][root][INFO] - Training Epoch: 2/2, step 372/574 completed (loss: 0.19416266679763794, acc: 0.9555555582046509)
[2025-02-12 22:07:12,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:12,908][root][INFO] - Training Epoch: 2/2, step 373/574 completed (loss: 0.2780603766441345, acc: 0.9642857313156128)
[2025-02-12 22:07:13,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:13,266][root][INFO] - Training Epoch: 2/2, step 374/574 completed (loss: 0.1718641221523285, acc: 0.9428571462631226)
[2025-02-12 22:07:13,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:13,693][root][INFO] - Training Epoch: 2/2, step 375/574 completed (loss: 0.004215299617499113, acc: 1.0)
[2025-02-12 22:07:13,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:14,025][root][INFO] - Training Epoch: 2/2, step 376/574 completed (loss: 0.018727807328104973, acc: 1.0)
[2025-02-12 22:07:14,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:14,382][root][INFO] - Training Epoch: 2/2, step 377/574 completed (loss: 0.06777632981538773, acc: 0.9791666865348816)
[2025-02-12 22:07:14,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:14,744][root][INFO] - Training Epoch: 2/2, step 378/574 completed (loss: 0.05204371362924576, acc: 0.9894737005233765)
[2025-02-12 22:07:15,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:15,337][root][INFO] - Training Epoch: 2/2, step 379/574 completed (loss: 0.30612584948539734, acc: 0.9041916131973267)
[2025-02-12 22:07:15,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:15,755][root][INFO] - Training Epoch: 2/2, step 380/574 completed (loss: 0.40126511454582214, acc: 0.8796992301940918)
[2025-02-12 22:07:16,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:17,040][root][INFO] - Training Epoch: 2/2, step 381/574 completed (loss: 0.5343453884124756, acc: 0.855614960193634)
[2025-02-12 22:07:17,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:17,611][root][INFO] - Training Epoch: 2/2, step 382/574 completed (loss: 0.1255699098110199, acc: 0.9639639854431152)
[2025-02-12 22:07:17,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:17,940][root][INFO] - Training Epoch: 2/2, step 383/574 completed (loss: 0.38009366393089294, acc: 0.8928571343421936)
[2025-02-12 22:07:18,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:18,281][root][INFO] - Training Epoch: 2/2, step 384/574 completed (loss: 0.04151174798607826, acc: 1.0)
[2025-02-12 22:07:18,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:18,624][root][INFO] - Training Epoch: 2/2, step 385/574 completed (loss: 0.10150866955518723, acc: 0.96875)
[2025-02-12 22:07:18,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:18,951][root][INFO] - Training Epoch: 2/2, step 386/574 completed (loss: 0.02198476530611515, acc: 1.0)
[2025-02-12 22:07:19,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:19,342][root][INFO] - Training Epoch: 2/2, step 387/574 completed (loss: 0.02035115472972393, acc: 1.0)
[2025-02-12 22:07:19,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:19,663][root][INFO] - Training Epoch: 2/2, step 388/574 completed (loss: 0.017614252865314484, acc: 1.0)
[2025-02-12 22:07:19,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20,015][root][INFO] - Training Epoch: 2/2, step 389/574 completed (loss: 0.004161695018410683, acc: 1.0)
[2025-02-12 22:07:20,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20,348][root][INFO] - Training Epoch: 2/2, step 390/574 completed (loss: 0.2662311792373657, acc: 0.9047619104385376)
[2025-02-12 22:07:20,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20,659][root][INFO] - Training Epoch: 2/2, step 391/574 completed (loss: 1.1490591764450073, acc: 0.6666666865348816)
[2025-02-12 22:07:20,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20,988][root][INFO] - Training Epoch: 2/2, step 392/574 completed (loss: 0.9112418293952942, acc: 0.7572815418243408)
[2025-02-12 22:07:21,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:21,517][root][INFO] - Training Epoch: 2/2, step 393/574 completed (loss: 1.0052626132965088, acc: 0.7867646813392639)
[2025-02-12 22:07:21,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:21,867][root][INFO] - Training Epoch: 2/2, step 394/574 completed (loss: 0.7354065775871277, acc: 0.7933333516120911)
[2025-02-12 22:07:22,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:22,241][root][INFO] - Training Epoch: 2/2, step 395/574 completed (loss: 0.7708996534347534, acc: 0.7777777910232544)
[2025-02-12 22:07:22,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:22,525][root][INFO] - Training Epoch: 2/2, step 396/574 completed (loss: 0.654792308807373, acc: 0.7906976938247681)
[2025-02-12 22:07:22,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:22,821][root][INFO] - Training Epoch: 2/2, step 397/574 completed (loss: 0.23174989223480225, acc: 0.9166666865348816)
[2025-02-12 22:07:22,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:23,117][root][INFO] - Training Epoch: 2/2, step 398/574 completed (loss: 0.3617004454135895, acc: 0.8837209343910217)
[2025-02-12 22:07:23,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:23,402][root][INFO] - Training Epoch: 2/2, step 399/574 completed (loss: 0.17894242703914642, acc: 0.9599999785423279)
[2025-02-12 22:07:23,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:23,932][root][INFO] - Training Epoch: 2/2, step 400/574 completed (loss: 0.3703860342502594, acc: 0.8970588445663452)
[2025-02-12 22:07:24,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:24,225][root][INFO] - Training Epoch: 2/2, step 401/574 completed (loss: 0.5292660593986511, acc: 0.8666666746139526)
[2025-02-12 22:07:24,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:24,512][root][INFO] - Training Epoch: 2/2, step 402/574 completed (loss: 0.41565656661987305, acc: 0.8484848737716675)
[2025-02-12 22:07:24,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:24,892][root][INFO] - Training Epoch: 2/2, step 403/574 completed (loss: 0.21232549846172333, acc: 0.9090909361839294)
[2025-02-12 22:07:25,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:25,250][root][INFO] - Training Epoch: 2/2, step 404/574 completed (loss: 0.12917354702949524, acc: 0.9677419066429138)
[2025-02-12 22:07:25,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:25,589][root][INFO] - Training Epoch: 2/2, step 405/574 completed (loss: 0.16132748126983643, acc: 0.9629629850387573)
[2025-02-12 22:07:25,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:25,912][root][INFO] - Training Epoch: 2/2, step 406/574 completed (loss: 0.022777937352657318, acc: 1.0)
[2025-02-12 22:07:25,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:26,253][root][INFO] - Training Epoch: 2/2, step 407/574 completed (loss: 0.0408414863049984, acc: 1.0)
[2025-02-12 22:07:26,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:26,629][root][INFO] - Training Epoch: 2/2, step 408/574 completed (loss: 0.07246748358011246, acc: 1.0)
[2025-02-12 22:07:26,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:26,965][root][INFO] - Training Epoch: 2/2, step 409/574 completed (loss: 0.05415854975581169, acc: 1.0)
[2025-02-12 22:07:27,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:27,280][root][INFO] - Training Epoch: 2/2, step 410/574 completed (loss: 0.15427479147911072, acc: 0.9482758641242981)
[2025-02-12 22:07:27,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:27,626][root][INFO] - Training Epoch: 2/2, step 411/574 completed (loss: 0.035646598786115646, acc: 1.0)
[2025-02-12 22:07:27,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:27,939][root][INFO] - Training Epoch: 2/2, step 412/574 completed (loss: 0.06741496920585632, acc: 1.0)
[2025-02-12 22:07:28,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:28,241][root][INFO] - Training Epoch: 2/2, step 413/574 completed (loss: 0.18892718851566315, acc: 0.9696969985961914)
[2025-02-12 22:07:28,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:28,560][root][INFO] - Training Epoch: 2/2, step 414/574 completed (loss: 0.03927699103951454, acc: 1.0)
[2025-02-12 22:07:28,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:28,886][root][INFO] - Training Epoch: 2/2, step 415/574 completed (loss: 0.28960996866226196, acc: 0.9215686321258545)
[2025-02-12 22:07:28,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:29,200][root][INFO] - Training Epoch: 2/2, step 416/574 completed (loss: 0.20860902965068817, acc: 0.9230769276618958)
[2025-02-12 22:07:29,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:29,512][root][INFO] - Training Epoch: 2/2, step 417/574 completed (loss: 0.23879525065422058, acc: 0.8888888955116272)
[2025-02-12 22:07:29,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:29,852][root][INFO] - Training Epoch: 2/2, step 418/574 completed (loss: 0.2504664659500122, acc: 0.8999999761581421)
[2025-02-12 22:07:29,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:30,173][root][INFO] - Training Epoch: 2/2, step 419/574 completed (loss: 0.22464290261268616, acc: 0.949999988079071)
[2025-02-12 22:07:30,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:30,511][root][INFO] - Training Epoch: 2/2, step 420/574 completed (loss: 0.09762685745954514, acc: 1.0)
[2025-02-12 22:07:30,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:30,813][root][INFO] - Training Epoch: 2/2, step 421/574 completed (loss: 0.22703711688518524, acc: 0.9666666388511658)
[2025-02-12 22:07:30,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:31,135][root][INFO] - Training Epoch: 2/2, step 422/574 completed (loss: 0.17616984248161316, acc: 0.9375)
[2025-02-12 22:07:31,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:31,481][root][INFO] - Training Epoch: 2/2, step 423/574 completed (loss: 0.3393907845020294, acc: 0.9166666865348816)
[2025-02-12 22:07:31,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:31,833][root][INFO] - Training Epoch: 2/2, step 424/574 completed (loss: 0.38032910227775574, acc: 0.9259259104728699)
[2025-02-12 22:07:31,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:32,136][root][INFO] - Training Epoch: 2/2, step 425/574 completed (loss: 0.10420060902833939, acc: 0.9696969985961914)
[2025-02-12 22:07:32,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:32,440][root][INFO] - Training Epoch: 2/2, step 426/574 completed (loss: 0.010190128348767757, acc: 1.0)
[2025-02-12 22:07:33,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:33,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:34,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:34,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:34,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:35,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:35,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:35,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:36,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:36,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:37,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:37,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:38,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:38,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:38,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:39,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:39,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:39,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:40,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:40,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:40,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:41,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:41,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:41,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:42,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:42,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:43,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:43,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:43,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:44,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:44,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:44,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:46,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:46,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:47,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:47,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:47,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:48,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:48,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:48,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:49,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:49,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:49,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:50,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:50,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:50,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:51,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:51,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:51,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:52,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:52,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:52,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:53,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:53,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:53,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:54,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:54,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:56,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:56,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:57,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:57,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:57,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:58,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:58,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:58,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:59,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:59,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:00,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:00,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:00,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:01,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:01,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:01,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:02,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:02,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:02,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:03,340][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7077, device='cuda:0') eval_epoch_loss=tensor(0.5351, device='cuda:0') eval_epoch_acc=tensor(0.8587, device='cuda:0')
[2025-02-12 22:08:03,343][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:08:03,343][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:08:03,702][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.5351276993751526/model.pt
[2025-02-12 22:08:03,709][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:08:03,710][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8586670756340027
[2025-02-12 22:08:03,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:04,134][root][INFO] - Training Epoch: 2/2, step 427/574 completed (loss: 0.05656805634498596, acc: 1.0)
[2025-02-12 22:08:04,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:04,494][root][INFO] - Training Epoch: 2/2, step 428/574 completed (loss: 0.03479184955358505, acc: 1.0)
[2025-02-12 22:08:04,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:04,827][root][INFO] - Training Epoch: 2/2, step 429/574 completed (loss: 0.038526203483343124, acc: 1.0)
[2025-02-12 22:08:04,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:05,255][root][INFO] - Training Epoch: 2/2, step 430/574 completed (loss: 0.002313123783096671, acc: 1.0)
[2025-02-12 22:08:05,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:05,605][root][INFO] - Training Epoch: 2/2, step 431/574 completed (loss: 0.0066916681826114655, acc: 1.0)
[2025-02-12 22:08:05,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:05,981][root][INFO] - Training Epoch: 2/2, step 432/574 completed (loss: 0.05832420662045479, acc: 0.95652174949646)
[2025-02-12 22:08:06,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:06,340][root][INFO] - Training Epoch: 2/2, step 433/574 completed (loss: 0.1337057203054428, acc: 0.9444444179534912)
[2025-02-12 22:08:06,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:06,637][root][INFO] - Training Epoch: 2/2, step 434/574 completed (loss: 0.0017706049839034677, acc: 1.0)
[2025-02-12 22:08:06,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:06,955][root][INFO] - Training Epoch: 2/2, step 435/574 completed (loss: 0.07274425774812698, acc: 0.9696969985961914)
[2025-02-12 22:08:07,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:07,257][root][INFO] - Training Epoch: 2/2, step 436/574 completed (loss: 0.19466520845890045, acc: 0.9444444179534912)
[2025-02-12 22:08:07,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:07,551][root][INFO] - Training Epoch: 2/2, step 437/574 completed (loss: 0.01695832796394825, acc: 1.0)
[2025-02-12 22:08:07,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:07,845][root][INFO] - Training Epoch: 2/2, step 438/574 completed (loss: 0.04643113166093826, acc: 0.9523809552192688)
[2025-02-12 22:08:07,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:08,155][root][INFO] - Training Epoch: 2/2, step 439/574 completed (loss: 0.6004610061645508, acc: 0.8717948794364929)
[2025-02-12 22:08:08,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:08,612][root][INFO] - Training Epoch: 2/2, step 440/574 completed (loss: 0.4819847047328949, acc: 0.9090909361839294)
[2025-02-12 22:08:08,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:09,289][root][INFO] - Training Epoch: 2/2, step 441/574 completed (loss: 0.8016440272331238, acc: 0.7519999742507935)
[2025-02-12 22:08:09,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:09,702][root][INFO] - Training Epoch: 2/2, step 442/574 completed (loss: 0.7703751921653748, acc: 0.7822580933570862)
[2025-02-12 22:08:10,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:10,356][root][INFO] - Training Epoch: 2/2, step 443/574 completed (loss: 0.4884049594402313, acc: 0.8606964945793152)
[2025-02-12 22:08:10,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:10,713][root][INFO] - Training Epoch: 2/2, step 444/574 completed (loss: 0.09783420711755753, acc: 0.9811320900917053)
[2025-02-12 22:08:10,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:11,172][root][INFO] - Training Epoch: 2/2, step 445/574 completed (loss: 0.12046322226524353, acc: 0.9545454382896423)
[2025-02-12 22:08:11,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:11,467][root][INFO] - Training Epoch: 2/2, step 446/574 completed (loss: 0.2081121802330017, acc: 0.95652174949646)
[2025-02-12 22:08:11,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:11,755][root][INFO] - Training Epoch: 2/2, step 447/574 completed (loss: 0.24258239567279816, acc: 0.9615384340286255)
[2025-02-12 22:08:11,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:12,088][root][INFO] - Training Epoch: 2/2, step 448/574 completed (loss: 0.04636240378022194, acc: 0.9642857313156128)
[2025-02-12 22:08:12,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:12,403][root][INFO] - Training Epoch: 2/2, step 449/574 completed (loss: 0.1596890240907669, acc: 0.9402984976768494)
[2025-02-12 22:08:12,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:12,718][root][INFO] - Training Epoch: 2/2, step 450/574 completed (loss: 0.05674805864691734, acc: 0.9861111044883728)
[2025-02-12 22:08:12,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13,010][root][INFO] - Training Epoch: 2/2, step 451/574 completed (loss: 0.055911142379045486, acc: 0.967391312122345)
[2025-02-12 22:08:13,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13,292][root][INFO] - Training Epoch: 2/2, step 452/574 completed (loss: 0.2545263171195984, acc: 0.9358974099159241)
[2025-02-12 22:08:13,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13,578][root][INFO] - Training Epoch: 2/2, step 453/574 completed (loss: 0.29190585017204285, acc: 0.8947368264198303)
[2025-02-12 22:08:13,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13,874][root][INFO] - Training Epoch: 2/2, step 454/574 completed (loss: 0.18054409325122833, acc: 0.9387755393981934)
[2025-02-12 22:08:13,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:14,163][root][INFO] - Training Epoch: 2/2, step 455/574 completed (loss: 0.21464812755584717, acc: 0.939393937587738)
[2025-02-12 22:08:14,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:14,473][root][INFO] - Training Epoch: 2/2, step 456/574 completed (loss: 0.5940864682197571, acc: 0.8659793734550476)
[2025-02-12 22:08:14,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:14,809][root][INFO] - Training Epoch: 2/2, step 457/574 completed (loss: 0.0697525143623352, acc: 0.9714285731315613)
[2025-02-12 22:08:14,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:15,184][root][INFO] - Training Epoch: 2/2, step 458/574 completed (loss: 0.3054189383983612, acc: 0.9011628031730652)
[2025-02-12 22:08:15,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:15,470][root][INFO] - Training Epoch: 2/2, step 459/574 completed (loss: 0.06920095533132553, acc: 0.9821428656578064)
[2025-02-12 22:08:15,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:15,787][root][INFO] - Training Epoch: 2/2, step 460/574 completed (loss: 0.1675574779510498, acc: 0.9506173133850098)
[2025-02-12 22:08:15,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:16,110][root][INFO] - Training Epoch: 2/2, step 461/574 completed (loss: 0.25090491771698, acc: 0.9166666865348816)
[2025-02-12 22:08:16,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:16,413][root][INFO] - Training Epoch: 2/2, step 462/574 completed (loss: 0.045590590685606, acc: 0.96875)
[2025-02-12 22:08:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:16,746][root][INFO] - Training Epoch: 2/2, step 463/574 completed (loss: 0.3326251208782196, acc: 0.9230769276618958)
[2025-02-12 22:08:16,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:17,035][root][INFO] - Training Epoch: 2/2, step 464/574 completed (loss: 0.13515432178974152, acc: 0.97826087474823)
[2025-02-12 22:08:17,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:17,322][root][INFO] - Training Epoch: 2/2, step 465/574 completed (loss: 0.4068399667739868, acc: 0.8690476417541504)
[2025-02-12 22:08:17,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:17,655][root][INFO] - Training Epoch: 2/2, step 466/574 completed (loss: 0.5269102454185486, acc: 0.8674699068069458)
[2025-02-12 22:08:17,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:18,009][root][INFO] - Training Epoch: 2/2, step 467/574 completed (loss: 0.22186443209648132, acc: 0.9189189076423645)
[2025-02-12 22:08:18,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:18,414][root][INFO] - Training Epoch: 2/2, step 468/574 completed (loss: 0.5379582643508911, acc: 0.893203854560852)
[2025-02-12 22:08:18,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:18,763][root][INFO] - Training Epoch: 2/2, step 469/574 completed (loss: 0.398954838514328, acc: 0.8943089246749878)
[2025-02-12 22:08:18,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:19,069][root][INFO] - Training Epoch: 2/2, step 470/574 completed (loss: 0.0767725482583046, acc: 1.0)
[2025-02-12 22:08:19,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:19,362][root][INFO] - Training Epoch: 2/2, step 471/574 completed (loss: 0.29687970876693726, acc: 0.8928571343421936)
[2025-02-12 22:08:19,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:19,784][root][INFO] - Training Epoch: 2/2, step 472/574 completed (loss: 0.6345800757408142, acc: 0.7941176295280457)
[2025-02-12 22:08:19,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:20,144][root][INFO] - Training Epoch: 2/2, step 473/574 completed (loss: 0.7481784820556641, acc: 0.8165938854217529)
[2025-02-12 22:08:20,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:20,461][root][INFO] - Training Epoch: 2/2, step 474/574 completed (loss: 0.6550170183181763, acc: 0.8229166865348816)
[2025-02-12 22:08:20,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:20,789][root][INFO] - Training Epoch: 2/2, step 475/574 completed (loss: 0.40656688809394836, acc: 0.8650306463241577)
[2025-02-12 22:08:20,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:21,142][root][INFO] - Training Epoch: 2/2, step 476/574 completed (loss: 0.5497060418128967, acc: 0.8201438784599304)
[2025-02-12 22:08:21,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:21,473][root][INFO] - Training Epoch: 2/2, step 477/574 completed (loss: 0.7775352001190186, acc: 0.7638190984725952)
[2025-02-12 22:08:21,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:21,760][root][INFO] - Training Epoch: 2/2, step 478/574 completed (loss: 0.6100679039955139, acc: 0.8611111044883728)
[2025-02-12 22:08:21,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:22,069][root][INFO] - Training Epoch: 2/2, step 479/574 completed (loss: 0.6603885889053345, acc: 0.7878788113594055)
[2025-02-12 22:08:22,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:22,363][root][INFO] - Training Epoch: 2/2, step 480/574 completed (loss: 0.4433930814266205, acc: 0.8518518805503845)
[2025-02-12 22:08:22,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:22,671][root][INFO] - Training Epoch: 2/2, step 481/574 completed (loss: 0.2158162146806717, acc: 0.949999988079071)
[2025-02-12 22:08:22,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:23,020][root][INFO] - Training Epoch: 2/2, step 482/574 completed (loss: 0.45661744475364685, acc: 0.8999999761581421)
[2025-02-12 22:08:23,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:23,409][root][INFO] - Training Epoch: 2/2, step 483/574 completed (loss: 0.5877096056938171, acc: 0.8103448152542114)
[2025-02-12 22:08:23,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:23,738][root][INFO] - Training Epoch: 2/2, step 484/574 completed (loss: 0.1274872124195099, acc: 0.9677419066429138)
[2025-02-12 22:08:23,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24,038][root][INFO] - Training Epoch: 2/2, step 485/574 completed (loss: 0.1355217695236206, acc: 0.9473684430122375)
[2025-02-12 22:08:24,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24,343][root][INFO] - Training Epoch: 2/2, step 486/574 completed (loss: 0.6321579217910767, acc: 0.7777777910232544)
[2025-02-12 22:08:24,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24,666][root][INFO] - Training Epoch: 2/2, step 487/574 completed (loss: 0.4399246573448181, acc: 0.9047619104385376)
[2025-02-12 22:08:24,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24,976][root][INFO] - Training Epoch: 2/2, step 488/574 completed (loss: 0.30871617794036865, acc: 0.9090909361839294)
[2025-02-12 22:08:25,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:25,309][root][INFO] - Training Epoch: 2/2, step 489/574 completed (loss: 0.8162145018577576, acc: 0.800000011920929)
[2025-02-12 22:08:25,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:25,623][root][INFO] - Training Epoch: 2/2, step 490/574 completed (loss: 0.1370909959077835, acc: 0.9666666388511658)
[2025-02-12 22:08:25,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:25,944][root][INFO] - Training Epoch: 2/2, step 491/574 completed (loss: 0.39796286821365356, acc: 0.8965517282485962)
[2025-02-12 22:08:26,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:26,259][root][INFO] - Training Epoch: 2/2, step 492/574 completed (loss: 0.43103304505348206, acc: 0.8627451062202454)
[2025-02-12 22:08:26,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:26,581][root][INFO] - Training Epoch: 2/2, step 493/574 completed (loss: 0.22089961171150208, acc: 0.9655172228813171)
[2025-02-12 22:08:26,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:26,949][root][INFO] - Training Epoch: 2/2, step 494/574 completed (loss: 0.13425035774707794, acc: 1.0)
[2025-02-12 22:08:27,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:27,294][root][INFO] - Training Epoch: 2/2, step 495/574 completed (loss: 0.8619383573532104, acc: 0.7894737124443054)
[2025-02-12 22:08:27,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:27,623][root][INFO] - Training Epoch: 2/2, step 496/574 completed (loss: 0.6562634110450745, acc: 0.8303571343421936)
[2025-02-12 22:08:27,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:27,983][root][INFO] - Training Epoch: 2/2, step 497/574 completed (loss: 0.43168288469314575, acc: 0.898876428604126)
[2025-02-12 22:08:28,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:28,292][root][INFO] - Training Epoch: 2/2, step 498/574 completed (loss: 0.734905481338501, acc: 0.7752808928489685)
[2025-02-12 22:08:28,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:28,600][root][INFO] - Training Epoch: 2/2, step 499/574 completed (loss: 1.1319085359573364, acc: 0.6453900933265686)
[2025-02-12 22:08:28,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:28,933][root][INFO] - Training Epoch: 2/2, step 500/574 completed (loss: 0.6508980393409729, acc: 0.8152173757553101)
[2025-02-12 22:08:29,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:29,284][root][INFO] - Training Epoch: 2/2, step 501/574 completed (loss: 0.008028170093894005, acc: 1.0)
[2025-02-12 22:08:29,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:29,609][root][INFO] - Training Epoch: 2/2, step 502/574 completed (loss: 0.019062228500843048, acc: 1.0)
[2025-02-12 22:08:29,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:29,948][root][INFO] - Training Epoch: 2/2, step 503/574 completed (loss: 0.12278705835342407, acc: 0.9629629850387573)
[2025-02-12 22:08:30,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:30,273][root][INFO] - Training Epoch: 2/2, step 504/574 completed (loss: 0.029645977541804314, acc: 1.0)
[2025-02-12 22:08:30,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:30,607][root][INFO] - Training Epoch: 2/2, step 505/574 completed (loss: 0.5117011666297913, acc: 0.9056603908538818)
[2025-02-12 22:08:30,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:30,914][root][INFO] - Training Epoch: 2/2, step 506/574 completed (loss: 0.45775580406188965, acc: 0.8620689511299133)
[2025-02-12 22:08:31,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:31,498][root][INFO] - Training Epoch: 2/2, step 507/574 completed (loss: 0.8476539850234985, acc: 0.7477477192878723)
[2025-02-12 22:08:31,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:31,938][root][INFO] - Training Epoch: 2/2, step 508/574 completed (loss: 0.43245336413383484, acc: 0.8732394576072693)
[2025-02-12 22:08:32,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:32,248][root][INFO] - Training Epoch: 2/2, step 509/574 completed (loss: 0.08505485951900482, acc: 0.949999988079071)
[2025-02-12 22:08:32,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:32,539][root][INFO] - Training Epoch: 2/2, step 510/574 completed (loss: 0.06983506679534912, acc: 0.9666666388511658)
[2025-02-12 22:08:32,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:32,834][root][INFO] - Training Epoch: 2/2, step 511/574 completed (loss: 0.3776351809501648, acc: 0.8846153616905212)
[2025-02-12 22:08:34,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:35,112][root][INFO] - Training Epoch: 2/2, step 512/574 completed (loss: 0.8713735938072205, acc: 0.7428571581840515)
[2025-02-12 22:08:35,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:35,927][root][INFO] - Training Epoch: 2/2, step 513/574 completed (loss: 0.20979098975658417, acc: 0.9126983880996704)
[2025-02-12 22:08:36,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:36,222][root][INFO] - Training Epoch: 2/2, step 514/574 completed (loss: 0.6381068229675293, acc: 0.8571428656578064)
[2025-02-12 22:08:36,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:36,607][root][INFO] - Training Epoch: 2/2, step 515/574 completed (loss: 0.12118766456842422, acc: 0.9666666388511658)
[2025-02-12 22:08:36,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:37,296][root][INFO] - Training Epoch: 2/2, step 516/574 completed (loss: 0.45740053057670593, acc: 0.8333333134651184)
[2025-02-12 22:08:37,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:37,719][root][INFO] - Training Epoch: 2/2, step 517/574 completed (loss: 0.008763555437326431, acc: 1.0)
[2025-02-12 22:08:37,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:38,104][root][INFO] - Training Epoch: 2/2, step 518/574 completed (loss: 0.04563615098595619, acc: 1.0)
[2025-02-12 22:08:38,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:38,428][root][INFO] - Training Epoch: 2/2, step 519/574 completed (loss: 0.2660158574581146, acc: 0.8999999761581421)
[2025-02-12 22:08:38,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:38,716][root][INFO] - Training Epoch: 2/2, step 520/574 completed (loss: 0.30441415309906006, acc: 0.9259259104728699)
[2025-02-12 22:08:39,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:39,716][root][INFO] - Training Epoch: 2/2, step 521/574 completed (loss: 0.6384173631668091, acc: 0.8008474707603455)
[2025-02-12 22:08:39,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:40,055][root][INFO] - Training Epoch: 2/2, step 522/574 completed (loss: 0.3834778368473053, acc: 0.89552241563797)
[2025-02-12 22:08:40,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:40,435][root][INFO] - Training Epoch: 2/2, step 523/574 completed (loss: 0.3460122048854828, acc: 0.8759124279022217)
[2025-02-12 22:08:40,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:40,995][root][INFO] - Training Epoch: 2/2, step 524/574 completed (loss: 0.5789595246315002, acc: 0.8399999737739563)
[2025-02-12 22:08:41,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:41,292][root][INFO] - Training Epoch: 2/2, step 525/574 completed (loss: 0.04352260380983353, acc: 1.0)
[2025-02-12 22:08:41,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:41,598][root][INFO] - Training Epoch: 2/2, step 526/574 completed (loss: 0.22602325677871704, acc: 0.9038461446762085)
[2025-02-12 22:08:41,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:41,876][root][INFO] - Training Epoch: 2/2, step 527/574 completed (loss: 0.4249337911605835, acc: 0.8571428656578064)
[2025-02-12 22:08:41,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:42,195][root][INFO] - Training Epoch: 2/2, step 528/574 completed (loss: 1.361728549003601, acc: 0.5737704634666443)
[2025-02-12 22:08:42,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:42,493][root][INFO] - Training Epoch: 2/2, step 529/574 completed (loss: 0.41196170449256897, acc: 0.8813559412956238)
[2025-02-12 22:08:42,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:42,791][root][INFO] - Training Epoch: 2/2, step 530/574 completed (loss: 0.9996559619903564, acc: 0.7441860437393188)
[2025-02-12 22:08:42,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:43,198][root][INFO] - Training Epoch: 2/2, step 531/574 completed (loss: 0.4931214153766632, acc: 0.8636363744735718)
[2025-02-12 22:08:43,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:43,604][root][INFO] - Training Epoch: 2/2, step 532/574 completed (loss: 0.7571500539779663, acc: 0.7735849022865295)
[2025-02-12 22:08:43,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:43,948][root][INFO] - Training Epoch: 2/2, step 533/574 completed (loss: 0.5478236675262451, acc: 0.8409090638160706)
[2025-02-12 22:08:44,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:44,254][root][INFO] - Training Epoch: 2/2, step 534/574 completed (loss: 0.4793720543384552, acc: 0.8399999737739563)
[2025-02-12 22:08:44,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:44,560][root][INFO] - Training Epoch: 2/2, step 535/574 completed (loss: 0.41976016759872437, acc: 0.8500000238418579)
[2025-02-12 22:08:44,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:44,890][root][INFO] - Training Epoch: 2/2, step 536/574 completed (loss: 0.07515162974596024, acc: 1.0)
[2025-02-12 22:08:45,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:45,294][root][INFO] - Training Epoch: 2/2, step 537/574 completed (loss: 0.5776805877685547, acc: 0.8615384697914124)
[2025-02-12 22:08:45,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:45,674][root][INFO] - Training Epoch: 2/2, step 538/574 completed (loss: 0.34354594349861145, acc: 0.921875)
[2025-02-12 22:08:45,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:46,074][root][INFO] - Training Epoch: 2/2, step 539/574 completed (loss: 0.340869277715683, acc: 0.90625)
[2025-02-12 22:08:46,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:46,428][root][INFO] - Training Epoch: 2/2, step 540/574 completed (loss: 0.2923389673233032, acc: 0.9090909361839294)
[2025-02-12 22:08:46,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:46,797][root][INFO] - Training Epoch: 2/2, step 541/574 completed (loss: 0.16302786767482758, acc: 0.875)
[2025-02-12 22:08:46,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:47,141][root][INFO] - Training Epoch: 2/2, step 542/574 completed (loss: 0.11688351631164551, acc: 0.9354838728904724)
[2025-02-12 22:08:47,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:47,486][root][INFO] - Training Epoch: 2/2, step 543/574 completed (loss: 0.03467293828725815, acc: 1.0)
[2025-02-12 22:08:47,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:47,817][root][INFO] - Training Epoch: 2/2, step 544/574 completed (loss: 0.06590234488248825, acc: 0.9666666388511658)
[2025-02-12 22:08:47,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:48,152][root][INFO] - Training Epoch: 2/2, step 545/574 completed (loss: 0.11888839304447174, acc: 0.9756097793579102)
[2025-02-12 22:08:48,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:48,456][root][INFO] - Training Epoch: 2/2, step 546/574 completed (loss: 0.023654455319046974, acc: 1.0)
[2025-02-12 22:08:48,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:48,758][root][INFO] - Training Epoch: 2/2, step 547/574 completed (loss: 0.029359964653849602, acc: 0.9736841917037964)
[2025-02-12 22:08:48,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:49,079][root][INFO] - Training Epoch: 2/2, step 548/574 completed (loss: 0.05656478554010391, acc: 1.0)
[2025-02-12 22:08:49,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:49,376][root][INFO] - Training Epoch: 2/2, step 549/574 completed (loss: 0.009645842015743256, acc: 1.0)
[2025-02-12 22:08:49,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:49,680][root][INFO] - Training Epoch: 2/2, step 550/574 completed (loss: 0.3065587282180786, acc: 0.8787878751754761)
[2025-02-12 22:08:49,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:50,011][root][INFO] - Training Epoch: 2/2, step 551/574 completed (loss: 0.12815715372562408, acc: 0.9750000238418579)
[2025-02-12 22:08:50,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:50,389][root][INFO] - Training Epoch: 2/2, step 552/574 completed (loss: 0.1493968814611435, acc: 0.9428571462631226)
[2025-02-12 22:08:50,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:50,733][root][INFO] - Training Epoch: 2/2, step 553/574 completed (loss: 0.4175921082496643, acc: 0.8759124279022217)
[2025-02-12 22:08:50,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:51,041][root][INFO] - Training Epoch: 2/2, step 554/574 completed (loss: 0.17920657992362976, acc: 0.931034505367279)
[2025-02-12 22:08:51,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:51,389][root][INFO] - Training Epoch: 2/2, step 555/574 completed (loss: 0.2794310450553894, acc: 0.9142857193946838)
[2025-02-12 22:08:51,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:51,811][root][INFO] - Training Epoch: 2/2, step 556/574 completed (loss: 0.48230940103530884, acc: 0.8940397500991821)
[2025-02-12 22:08:51,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:52,159][root][INFO] - Training Epoch: 2/2, step 557/574 completed (loss: 0.2661866843700409, acc: 0.9145299196243286)
[2025-02-12 22:08:52,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:52,483][root][INFO] - Training Epoch: 2/2, step 558/574 completed (loss: 0.07797405868768692, acc: 1.0)
[2025-02-12 22:08:52,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:52,764][root][INFO] - Training Epoch: 2/2, step 559/574 completed (loss: 0.1394701898097992, acc: 0.9230769276618958)
[2025-02-12 22:08:52,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:53,133][root][INFO] - Training Epoch: 2/2, step 560/574 completed (loss: 0.06202905997633934, acc: 1.0)
[2025-02-12 22:08:53,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:53,450][root][INFO] - Training Epoch: 2/2, step 561/574 completed (loss: 0.042897846549749374, acc: 1.0)
[2025-02-12 22:08:53,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:53,772][root][INFO] - Training Epoch: 2/2, step 562/574 completed (loss: 0.4327075481414795, acc: 0.8444444537162781)
[2025-02-12 22:08:53,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:54,105][root][INFO] - Training Epoch: 2/2, step 563/574 completed (loss: 0.32117733359336853, acc: 0.8961039185523987)
[2025-02-12 22:08:54,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:54,459][root][INFO] - Training Epoch: 2/2, step 564/574 completed (loss: 0.23630493879318237, acc: 0.9166666865348816)
[2025-02-12 22:08:54,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:54,790][root][INFO] - Training Epoch: 2/2, step 565/574 completed (loss: 0.2583003342151642, acc: 0.9137930870056152)
[2025-02-12 22:08:54,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:55,202][root][INFO] - Training Epoch: 2/2, step 566/574 completed (loss: 0.29658398032188416, acc: 0.9166666865348816)
[2025-02-12 22:08:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:55,600][root][INFO] - Training Epoch: 2/2, step 567/574 completed (loss: 0.025827959179878235, acc: 1.0)
[2025-02-12 22:08:55,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:55,922][root][INFO] - Training Epoch: 2/2, step 568/574 completed (loss: 0.014790809713304043, acc: 1.0)
[2025-02-12 22:08:56,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:56,291][root][INFO] - Training Epoch: 2/2, step 569/574 completed (loss: 0.1838274896144867, acc: 0.9786096215248108)
[2025-02-12 22:08:57,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:57,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:57,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:58,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:58,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:58,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:59,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:59,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:59,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:00,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:00,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:01,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:01,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:01,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:02,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:02,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:02,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:03,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:03,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:03,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:04,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:04,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:04,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:05,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:05,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:05,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:06,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:06,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:07,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:07,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:09,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:09,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:09,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:10,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:10,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:11,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:11,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:11,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:12,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:12,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:12,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:13,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:13,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:13,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:14,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:14,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:14,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:15,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:15,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:15,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:16,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:16,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:16,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:17,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:17,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:17,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:18,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:18,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:18,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:19,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:19,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:19,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:20,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:20,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:21,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:21,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:21,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:22,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:22,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:24,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:24,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:24,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:26,564][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7474, device='cuda:0') eval_epoch_loss=tensor(0.5581, device='cuda:0') eval_epoch_acc=tensor(0.8477, device='cuda:0')
[2025-02-12 22:09:26,565][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:09:26,565][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:09:26,871][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.5581279993057251/model.pt
[2025-02-12 22:09:26,882][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:09:26,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:27,266][root][INFO] - Training Epoch: 2/2, step 570/574 completed (loss: 0.043785177171230316, acc: 0.9838709831237793)
[2025-02-12 22:09:27,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:27,568][root][INFO] - Training Epoch: 2/2, step 571/574 completed (loss: 0.20440471172332764, acc: 0.94017094373703)
[2025-02-12 22:09:27,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:27,906][root][INFO] - Training Epoch: 2/2, step 572/574 completed (loss: 0.3082229197025299, acc: 0.9336734414100647)
[2025-02-12 22:09:28,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:28,239][root][INFO] - Training Epoch: 2/2, step 573/574 completed (loss: 0.3126852512359619, acc: 0.9245283007621765)
[2025-02-12 22:09:28,606][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.6215, train_epoch_loss=0.4834, epoch time 348.11213466897607s
[2025-02-12 22:09:28,606][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-12 22:09:28,606][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-02-12 22:09:28,606][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-12 22:09:28,606][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-02-12 22:09:28,606][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-12 22:09:28,610][root][INFO] - Key: avg_train_prep, Value: 2.1819686889648438
[2025-02-12 22:09:28,611][root][INFO] - Key: avg_train_loss, Value: 0.7461016178131104
[2025-02-12 22:09:28,611][root][INFO] - Key: avg_train_acc, Value: 0.816173791885376
[2025-02-12 22:09:28,611][root][INFO] - Key: avg_eval_prep, Value: 1.9366310834884644
[2025-02-12 22:09:28,611][root][INFO] - Key: avg_eval_loss, Value: 0.6432064175605774
[2025-02-12 22:09:28,611][root][INFO] - Key: avg_eval_acc, Value: 0.8323109149932861
[2025-02-12 22:09:28,611][root][INFO] - Key: avg_epoch_time, Value: 357.2735584229231
[2025-02-12 22:09:28,611][root][INFO] - Key: avg_checkpoint_time, Value: 0.33076154021546245
