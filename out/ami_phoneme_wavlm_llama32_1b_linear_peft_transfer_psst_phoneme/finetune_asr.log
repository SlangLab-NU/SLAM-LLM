[2025-01-02 00:47:40,219][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-02 00:47:40,220][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-02 00:47:40,220][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-02 00:47:40,220][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-02_00-47-39.txt', 'log_interval': 5}
[2025-01-02 00:48:06,081][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-02 00:48:11,281][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:48:11,282][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-02 00:48:11,284][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:48:11,285][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-02 00:48:20,085][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:48:20,087][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-02 00:48:20,087][slam_llm.models.slam_model][INFO] - setup peft...
[2025-01-02 00:48:20,204][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:48:20,206][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-02 00:48:20,312][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-02 00:48:20,312][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-02 00:48:20,312][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792/model.pt
[2025-01-02 00:48:20,492][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-02 00:48:20,496][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-02 00:48:22,211][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-02 00:48:25,046][root][INFO] - --> Training Set Length = 2298
[2025-01-02 00:48:25,055][root][INFO] - --> Validation Set Length = 341
[2025-01-02 00:48:25,055][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:48:25,056][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:48:27,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:28,430][root][INFO] - Training Epoch: 1/2, step 0/574 completed (loss: 2.0960772037506104, acc: 0.48148149251937866)
[2025-01-02 00:48:28,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:29,042][root][INFO] - Training Epoch: 1/2, step 1/574 completed (loss: 0.9558073282241821, acc: 0.800000011920929)
[2025-01-02 00:48:29,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:29,381][root][INFO] - Training Epoch: 1/2, step 2/574 completed (loss: 2.3097050189971924, acc: 0.5135135054588318)
[2025-01-02 00:48:29,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:29,847][root][INFO] - Training Epoch: 1/2, step 3/574 completed (loss: 2.1763036251068115, acc: 0.5789473652839661)
[2025-01-02 00:48:29,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30,228][root][INFO] - Training Epoch: 1/2, step 4/574 completed (loss: 1.6998047828674316, acc: 0.5945945978164673)
[2025-01-02 00:48:30,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30,629][root][INFO] - Training Epoch: 1/2, step 5/574 completed (loss: 0.8240347504615784, acc: 0.8214285969734192)
[2025-01-02 00:48:30,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31,062][root][INFO] - Training Epoch: 1/2, step 6/574 completed (loss: 2.6075844764709473, acc: 0.4693877696990967)
[2025-01-02 00:48:31,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31,385][root][INFO] - Training Epoch: 1/2, step 7/574 completed (loss: 1.3130346536636353, acc: 0.7666666507720947)
[2025-01-02 00:48:31,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31,822][root][INFO] - Training Epoch: 1/2, step 8/574 completed (loss: 1.1516512632369995, acc: 0.7272727489471436)
[2025-01-02 00:48:31,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32,203][root][INFO] - Training Epoch: 1/2, step 9/574 completed (loss: 0.9723425507545471, acc: 0.692307710647583)
[2025-01-02 00:48:32,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32,593][root][INFO] - Training Epoch: 1/2, step 10/574 completed (loss: 1.304939866065979, acc: 0.5555555820465088)
[2025-01-02 00:48:32,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32,956][root][INFO] - Training Epoch: 1/2, step 11/574 completed (loss: 2.1893601417541504, acc: 0.5897436141967773)
[2025-01-02 00:48:33,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33,325][root][INFO] - Training Epoch: 1/2, step 12/574 completed (loss: 1.3780752420425415, acc: 0.7272727489471436)
[2025-01-02 00:48:33,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33,681][root][INFO] - Training Epoch: 1/2, step 13/574 completed (loss: 1.3336013555526733, acc: 0.5652173757553101)
[2025-01-02 00:48:33,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34,086][root][INFO] - Training Epoch: 1/2, step 14/574 completed (loss: 1.912348747253418, acc: 0.5882353186607361)
[2025-01-02 00:48:34,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34,457][root][INFO] - Training Epoch: 1/2, step 15/574 completed (loss: 0.8959335088729858, acc: 0.7142857313156128)
[2025-01-02 00:48:34,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34,805][root][INFO] - Training Epoch: 1/2, step 16/574 completed (loss: 1.910796880722046, acc: 0.6315789222717285)
[2025-01-02 00:48:34,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35,156][root][INFO] - Training Epoch: 1/2, step 17/574 completed (loss: 2.4606688022613525, acc: 0.5)
[2025-01-02 00:48:35,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35,523][root][INFO] - Training Epoch: 1/2, step 18/574 completed (loss: 1.4996267557144165, acc: 0.5833333134651184)
[2025-01-02 00:48:35,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35,843][root][INFO] - Training Epoch: 1/2, step 19/574 completed (loss: 1.468733310699463, acc: 0.7368420958518982)
[2025-01-02 00:48:35,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36,153][root][INFO] - Training Epoch: 1/2, step 20/574 completed (loss: 1.2159637212753296, acc: 0.7307692170143127)
[2025-01-02 00:48:36,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36,554][root][INFO] - Training Epoch: 1/2, step 21/574 completed (loss: 1.7433260679244995, acc: 0.6896551847457886)
[2025-01-02 00:48:36,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36,941][root][INFO] - Training Epoch: 1/2, step 22/574 completed (loss: 2.2058966159820557, acc: 0.47999998927116394)
[2025-01-02 00:48:37,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37,303][root][INFO] - Training Epoch: 1/2, step 23/574 completed (loss: 1.8753631114959717, acc: 0.7142857313156128)
[2025-01-02 00:48:37,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37,589][root][INFO] - Training Epoch: 1/2, step 24/574 completed (loss: 0.8815869688987732, acc: 0.75)
[2025-01-02 00:48:37,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37,932][root][INFO] - Training Epoch: 1/2, step 25/574 completed (loss: 1.5402711629867554, acc: 0.6226415038108826)
[2025-01-02 00:48:38,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:38,322][root][INFO] - Training Epoch: 1/2, step 26/574 completed (loss: 1.955437421798706, acc: 0.5205479264259338)
[2025-01-02 00:48:39,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:39,758][root][INFO] - Training Epoch: 1/2, step 27/574 completed (loss: 1.7114523649215698, acc: 0.5889328122138977)
[2025-01-02 00:48:39,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:40,111][root][INFO] - Training Epoch: 1/2, step 28/574 completed (loss: 1.3421708345413208, acc: 0.7209302186965942)
[2025-01-02 00:48:40,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:40,479][root][INFO] - Training Epoch: 1/2, step 29/574 completed (loss: 1.5003976821899414, acc: 0.6385542154312134)
[2025-01-02 00:48:40,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:40,866][root][INFO] - Training Epoch: 1/2, step 30/574 completed (loss: 1.3822016716003418, acc: 0.7283950448036194)
[2025-01-02 00:48:40,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41,204][root][INFO] - Training Epoch: 1/2, step 31/574 completed (loss: 2.0706331729888916, acc: 0.4642857015132904)
[2025-01-02 00:48:41,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41,604][root][INFO] - Training Epoch: 1/2, step 32/574 completed (loss: 0.9789765477180481, acc: 0.7037037014961243)
[2025-01-02 00:48:41,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42,011][root][INFO] - Training Epoch: 1/2, step 33/574 completed (loss: 1.1695928573608398, acc: 0.8260869383811951)
[2025-01-02 00:48:42,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42,412][root][INFO] - Training Epoch: 1/2, step 34/574 completed (loss: 1.2557156085968018, acc: 0.7310924530029297)
[2025-01-02 00:48:42,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42,753][root][INFO] - Training Epoch: 1/2, step 35/574 completed (loss: 1.345894455909729, acc: 0.7704917788505554)
[2025-01-02 00:48:42,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43,087][root][INFO] - Training Epoch: 1/2, step 36/574 completed (loss: 1.5423274040222168, acc: 0.6984127163887024)
[2025-01-02 00:48:43,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43,420][root][INFO] - Training Epoch: 1/2, step 37/574 completed (loss: 1.38495671749115, acc: 0.6610169410705566)
[2025-01-02 00:48:43,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43,778][root][INFO] - Training Epoch: 1/2, step 38/574 completed (loss: 1.878138780593872, acc: 0.6896551847457886)
[2025-01-02 00:48:43,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44,175][root][INFO] - Training Epoch: 1/2, step 39/574 completed (loss: 3.103090524673462, acc: 0.3333333432674408)
[2025-01-02 00:48:44,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44,553][root][INFO] - Training Epoch: 1/2, step 40/574 completed (loss: 0.8708739876747131, acc: 0.6538461446762085)
[2025-01-02 00:48:44,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45,000][root][INFO] - Training Epoch: 1/2, step 41/574 completed (loss: 0.8193880319595337, acc: 0.7567567825317383)
[2025-01-02 00:48:45,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45,464][root][INFO] - Training Epoch: 1/2, step 42/574 completed (loss: 2.6910288333892822, acc: 0.5384615659713745)
[2025-01-02 00:48:45,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45,955][root][INFO] - Training Epoch: 1/2, step 43/574 completed (loss: 1.6857668161392212, acc: 0.6969696879386902)
[2025-01-02 00:48:46,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46,387][root][INFO] - Training Epoch: 1/2, step 44/574 completed (loss: 2.7896711826324463, acc: 0.6701030731201172)
[2025-01-02 00:48:46,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46,824][root][INFO] - Training Epoch: 1/2, step 45/574 completed (loss: 3.0054476261138916, acc: 0.6617646813392639)
[2025-01-02 00:48:46,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47,173][root][INFO] - Training Epoch: 1/2, step 46/574 completed (loss: 1.5168893337249756, acc: 0.5)
[2025-01-02 00:48:47,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47,540][root][INFO] - Training Epoch: 1/2, step 47/574 completed (loss: 1.363132357597351, acc: 0.6666666865348816)
[2025-01-02 00:48:47,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47,925][root][INFO] - Training Epoch: 1/2, step 48/574 completed (loss: 1.3893510103225708, acc: 0.6785714030265808)
[2025-01-02 00:48:48,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48,284][root][INFO] - Training Epoch: 1/2, step 49/574 completed (loss: 1.018253207206726, acc: 0.8055555820465088)
[2025-01-02 00:48:48,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48,592][root][INFO] - Training Epoch: 1/2, step 50/574 completed (loss: 3.2860264778137207, acc: 0.5438596606254578)
[2025-01-02 00:48:48,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48,884][root][INFO] - Training Epoch: 1/2, step 51/574 completed (loss: 2.516413688659668, acc: 0.5555555820465088)
[2025-01-02 00:48:48,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49,180][root][INFO] - Training Epoch: 1/2, step 52/574 completed (loss: 4.431299686431885, acc: 0.49295774102211)
[2025-01-02 00:48:49,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49,651][root][INFO] - Training Epoch: 1/2, step 53/574 completed (loss: 3.6488871574401855, acc: 0.4000000059604645)
[2025-01-02 00:48:49,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50,036][root][INFO] - Training Epoch: 1/2, step 54/574 completed (loss: 3.7813358306884766, acc: 0.4054054021835327)
[2025-01-02 00:48:50,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50,386][root][INFO] - Training Epoch: 1/2, step 55/574 completed (loss: 0.995570182800293, acc: 0.692307710647583)
[2025-01-02 00:48:52,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:53,524][root][INFO] - Training Epoch: 1/2, step 56/574 completed (loss: 2.2601325511932373, acc: 0.4880546033382416)
[2025-01-02 00:48:54,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:54,893][root][INFO] - Training Epoch: 1/2, step 57/574 completed (loss: 2.0808987617492676, acc: 0.5838779807090759)
[2025-01-02 00:48:55,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:55,570][root][INFO] - Training Epoch: 1/2, step 58/574 completed (loss: 2.7361879348754883, acc: 0.5852272510528564)
[2025-01-02 00:48:55,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56,163][root][INFO] - Training Epoch: 1/2, step 59/574 completed (loss: 1.305859923362732, acc: 0.7426470518112183)
[2025-01-02 00:48:56,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56,753][root][INFO] - Training Epoch: 1/2, step 60/574 completed (loss: 2.338644027709961, acc: 0.6014492511749268)
[2025-01-02 00:48:56,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57,198][root][INFO] - Training Epoch: 1/2, step 61/574 completed (loss: 2.1936802864074707, acc: 0.6000000238418579)
[2025-01-02 00:48:57,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57,611][root][INFO] - Training Epoch: 1/2, step 62/574 completed (loss: 1.6476964950561523, acc: 0.6176470518112183)
[2025-01-02 00:48:57,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58,001][root][INFO] - Training Epoch: 1/2, step 63/574 completed (loss: 3.212573766708374, acc: 0.5277777910232544)
[2025-01-02 00:48:58,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58,371][root][INFO] - Training Epoch: 1/2, step 64/574 completed (loss: 1.2386003732681274, acc: 0.796875)
[2025-01-02 00:48:58,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58,812][root][INFO] - Training Epoch: 1/2, step 65/574 completed (loss: 0.6239901781082153, acc: 0.931034505367279)
[2025-01-02 00:48:58,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59,182][root][INFO] - Training Epoch: 1/2, step 66/574 completed (loss: 3.244312286376953, acc: 0.5178571343421936)
[2025-01-02 00:48:59,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59,564][root][INFO] - Training Epoch: 1/2, step 67/574 completed (loss: 1.9322136640548706, acc: 0.699999988079071)
[2025-01-02 00:48:59,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59,987][root][INFO] - Training Epoch: 1/2, step 68/574 completed (loss: 0.9690605401992798, acc: 0.7200000286102295)
[2025-01-02 00:49:00,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:00,379][root][INFO] - Training Epoch: 1/2, step 69/574 completed (loss: 2.002744674682617, acc: 0.3888888955116272)
[2025-01-02 00:49:00,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:00,837][root][INFO] - Training Epoch: 1/2, step 70/574 completed (loss: 3.609401226043701, acc: 0.39393940567970276)
[2025-01-02 00:49:01,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01,251][root][INFO] - Training Epoch: 1/2, step 71/574 completed (loss: 2.264280319213867, acc: 0.5073529481887817)
[2025-01-02 00:49:01,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01,645][root][INFO] - Training Epoch: 1/2, step 72/574 completed (loss: 1.4374134540557861, acc: 0.6349206566810608)
[2025-01-02 00:49:01,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02,098][root][INFO] - Training Epoch: 1/2, step 73/574 completed (loss: 1.9587557315826416, acc: 0.5384615659713745)
[2025-01-02 00:49:02,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02,414][root][INFO] - Training Epoch: 1/2, step 74/574 completed (loss: 3.47581148147583, acc: 0.4693877696990967)
[2025-01-02 00:49:02,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02,802][root][INFO] - Training Epoch: 1/2, step 75/574 completed (loss: 2.1035680770874023, acc: 0.49253731966018677)
[2025-01-02 00:49:02,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03,233][root][INFO] - Training Epoch: 1/2, step 76/574 completed (loss: 2.5434303283691406, acc: 0.4781021773815155)
[2025-01-02 00:49:03,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03,621][root][INFO] - Training Epoch: 1/2, step 77/574 completed (loss: 0.9223839640617371, acc: 0.6666666865348816)
[2025-01-02 00:49:03,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03,968][root][INFO] - Training Epoch: 1/2, step 78/574 completed (loss: 0.9708287119865417, acc: 0.6666666865348816)
[2025-01-02 00:49:04,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:04,349][root][INFO] - Training Epoch: 1/2, step 79/574 completed (loss: 1.3003275394439697, acc: 0.6666666865348816)
[2025-01-02 00:49:04,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:04,836][root][INFO] - Training Epoch: 1/2, step 80/574 completed (loss: 1.6562825441360474, acc: 0.807692289352417)
[2025-01-02 00:49:04,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05,230][root][INFO] - Training Epoch: 1/2, step 81/574 completed (loss: 2.1471264362335205, acc: 0.6538461446762085)
[2025-01-02 00:49:05,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05,633][root][INFO] - Training Epoch: 1/2, step 82/574 completed (loss: 2.203531503677368, acc: 0.557692289352417)
[2025-01-02 00:49:05,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06,031][root][INFO] - Training Epoch: 1/2, step 83/574 completed (loss: 0.7551875114440918, acc: 0.78125)
[2025-01-02 00:49:06,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06,477][root][INFO] - Training Epoch: 1/2, step 84/574 completed (loss: 2.114417314529419, acc: 0.6231883764266968)
[2025-01-02 00:49:06,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06,863][root][INFO] - Training Epoch: 1/2, step 85/574 completed (loss: 2.4690825939178467, acc: 0.6200000047683716)
[2025-01-02 00:49:06,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07,234][root][INFO] - Training Epoch: 1/2, step 86/574 completed (loss: 0.9847921133041382, acc: 0.739130437374115)
[2025-01-02 00:49:07,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07,746][root][INFO] - Training Epoch: 1/2, step 87/574 completed (loss: 2.9599807262420654, acc: 0.4000000059604645)
[2025-01-02 00:49:07,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:08,094][root][INFO] - Training Epoch: 1/2, step 88/574 completed (loss: 3.0209436416625977, acc: 0.5048543810844421)
[2025-01-02 00:49:08,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:09,277][root][INFO] - Training Epoch: 1/2, step 89/574 completed (loss: 2.6403133869171143, acc: 0.5485436916351318)
[2025-01-02 00:49:09,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:10,163][root][INFO] - Training Epoch: 1/2, step 90/574 completed (loss: 3.2234067916870117, acc: 0.42473119497299194)
[2025-01-02 00:49:10,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11,020][root][INFO] - Training Epoch: 1/2, step 91/574 completed (loss: 2.356367349624634, acc: 0.5387930870056152)
[2025-01-02 00:49:11,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11,763][root][INFO] - Training Epoch: 1/2, step 92/574 completed (loss: 2.2782251834869385, acc: 0.6000000238418579)
[2025-01-02 00:49:12,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:12,790][root][INFO] - Training Epoch: 1/2, step 93/574 completed (loss: 2.8847970962524414, acc: 0.3762376308441162)
[2025-01-02 00:49:12,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13,131][root][INFO] - Training Epoch: 1/2, step 94/574 completed (loss: 2.0657291412353516, acc: 0.5645161271095276)
[2025-01-02 00:49:13,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13,560][root][INFO] - Training Epoch: 1/2, step 95/574 completed (loss: 1.7064288854599, acc: 0.6521739363670349)
[2025-01-02 00:49:13,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13,924][root][INFO] - Training Epoch: 1/2, step 96/574 completed (loss: 2.9077491760253906, acc: 0.462184876203537)
[2025-01-02 00:49:14,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:14,268][root][INFO] - Training Epoch: 1/2, step 97/574 completed (loss: 2.434572219848633, acc: 0.5288461446762085)
[2025-01-02 00:49:14,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:14,652][root][INFO] - Training Epoch: 1/2, step 98/574 completed (loss: 2.4475622177124023, acc: 0.5182482004165649)
[2025-01-02 00:49:14,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:15,040][root][INFO] - Training Epoch: 1/2, step 99/574 completed (loss: 3.269256830215454, acc: 0.34328359365463257)
[2025-01-02 00:49:15,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:15,380][root][INFO] - Training Epoch: 1/2, step 100/574 completed (loss: 1.6858904361724854, acc: 0.6000000238418579)
[2025-01-02 00:49:15,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:15,746][root][INFO] - Training Epoch: 1/2, step 101/574 completed (loss: 1.5157341957092285, acc: 0.7727272510528564)
[2025-01-02 00:49:15,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:16,164][root][INFO] - Training Epoch: 1/2, step 102/574 completed (loss: 0.4701847732067108, acc: 0.8260869383811951)
[2025-01-02 00:49:16,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:16,556][root][INFO] - Training Epoch: 1/2, step 103/574 completed (loss: 0.49472111463546753, acc: 0.8636363744735718)
[2025-01-02 00:49:16,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:16,956][root][INFO] - Training Epoch: 1/2, step 104/574 completed (loss: 1.255368709564209, acc: 0.7586206793785095)
[2025-01-02 00:49:17,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17,328][root][INFO] - Training Epoch: 1/2, step 105/574 completed (loss: 0.6484459042549133, acc: 0.7441860437393188)
[2025-01-02 00:49:17,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17,663][root][INFO] - Training Epoch: 1/2, step 106/574 completed (loss: 0.6560606360435486, acc: 0.7599999904632568)
[2025-01-02 00:49:17,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18,003][root][INFO] - Training Epoch: 1/2, step 107/574 completed (loss: 1.0993772745132446, acc: 0.7647058963775635)
[2025-01-02 00:49:18,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18,410][root][INFO] - Training Epoch: 1/2, step 108/574 completed (loss: 0.6124299168586731, acc: 0.8461538553237915)
[2025-01-02 00:49:18,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18,768][root][INFO] - Training Epoch: 1/2, step 109/574 completed (loss: 0.5734081864356995, acc: 0.9047619104385376)
[2025-01-02 00:49:18,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19,154][root][INFO] - Training Epoch: 1/2, step 110/574 completed (loss: 1.4295260906219482, acc: 0.7692307829856873)
[2025-01-02 00:49:19,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19,606][root][INFO] - Training Epoch: 1/2, step 111/574 completed (loss: 1.2417304515838623, acc: 0.7017543911933899)
[2025-01-02 00:49:19,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20,046][root][INFO] - Training Epoch: 1/2, step 112/574 completed (loss: 2.030348062515259, acc: 0.5789473652839661)
[2025-01-02 00:49:20,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20,434][root][INFO] - Training Epoch: 1/2, step 113/574 completed (loss: 1.3357874155044556, acc: 0.6666666865348816)
[2025-01-02 00:49:20,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20,836][root][INFO] - Training Epoch: 1/2, step 114/574 completed (loss: 1.4531121253967285, acc: 0.7142857313156128)
[2025-01-02 00:49:20,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21,168][root][INFO] - Training Epoch: 1/2, step 115/574 completed (loss: 0.9337542057037354, acc: 0.7727272510528564)
[2025-01-02 00:49:21,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21,592][root][INFO] - Training Epoch: 1/2, step 116/574 completed (loss: 0.9201701879501343, acc: 0.7936508059501648)
[2025-01-02 00:49:21,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21,945][root][INFO] - Training Epoch: 1/2, step 117/574 completed (loss: 0.9825707077980042, acc: 0.7804877758026123)
[2025-01-02 00:49:22,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:22,277][root][INFO] - Training Epoch: 1/2, step 118/574 completed (loss: 1.1930721998214722, acc: 0.8064516186714172)
[2025-01-02 00:49:22,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23,289][root][INFO] - Training Epoch: 1/2, step 119/574 completed (loss: 1.1446245908737183, acc: 0.7186312079429626)
[2025-01-02 00:49:23,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23,655][root][INFO] - Training Epoch: 1/2, step 120/574 completed (loss: 1.3062827587127686, acc: 0.7200000286102295)
[2025-01-02 00:49:23,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24,082][root][INFO] - Training Epoch: 1/2, step 121/574 completed (loss: 2.118635416030884, acc: 0.6538461446762085)
[2025-01-02 00:49:24,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24,397][root][INFO] - Training Epoch: 1/2, step 122/574 completed (loss: 1.2052212953567505, acc: 0.625)
[2025-01-02 00:49:24,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24,737][root][INFO] - Training Epoch: 1/2, step 123/574 completed (loss: 1.535698413848877, acc: 0.6315789222717285)
[2025-01-02 00:49:24,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25,158][root][INFO] - Training Epoch: 1/2, step 124/574 completed (loss: 1.5712023973464966, acc: 0.6564416885375977)
[2025-01-02 00:49:25,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25,644][root][INFO] - Training Epoch: 1/2, step 125/574 completed (loss: 1.4698325395584106, acc: 0.6388888955116272)
[2025-01-02 00:49:25,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26,024][root][INFO] - Training Epoch: 1/2, step 126/574 completed (loss: 1.8144590854644775, acc: 0.5416666865348816)
[2025-01-02 00:49:26,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26,446][root][INFO] - Training Epoch: 1/2, step 127/574 completed (loss: 1.1860154867172241, acc: 0.6845238208770752)
[2025-01-02 00:49:26,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26,835][root][INFO] - Training Epoch: 1/2, step 128/574 completed (loss: 1.415907621383667, acc: 0.620512843132019)
[2025-01-02 00:49:26,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27,296][root][INFO] - Training Epoch: 1/2, step 129/574 completed (loss: 1.498234510421753, acc: 0.654411792755127)
[2025-01-02 00:49:27,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27,732][root][INFO] - Training Epoch: 1/2, step 130/574 completed (loss: 2.1857869625091553, acc: 0.4615384638309479)
[2025-01-02 00:49:27,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28,088][root][INFO] - Training Epoch: 1/2, step 131/574 completed (loss: 2.424856662750244, acc: 0.47826087474823)
[2025-01-02 00:49:28,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28,480][root][INFO] - Training Epoch: 1/2, step 132/574 completed (loss: 2.390357255935669, acc: 0.375)
[2025-01-02 00:49:28,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28,861][root][INFO] - Training Epoch: 1/2, step 133/574 completed (loss: 2.359623432159424, acc: 0.3913043439388275)
[2025-01-02 00:49:28,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29,204][root][INFO] - Training Epoch: 1/2, step 134/574 completed (loss: 2.0650315284729004, acc: 0.5428571701049805)
[2025-01-02 00:49:29,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29,579][root][INFO] - Training Epoch: 1/2, step 135/574 completed (loss: 2.066528797149658, acc: 0.5)
[2025-01-02 00:49:29,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29,986][root][INFO] - Training Epoch: 1/2, step 136/574 completed (loss: 1.7806382179260254, acc: 0.5)
[2025-01-02 00:49:30,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30,363][root][INFO] - Training Epoch: 1/2, step 137/574 completed (loss: 2.223909854888916, acc: 0.3333333432674408)
[2025-01-02 00:49:30,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30,779][root][INFO] - Training Epoch: 1/2, step 138/574 completed (loss: 1.4889857769012451, acc: 0.5652173757553101)
[2025-01-02 00:49:30,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31,190][root][INFO] - Training Epoch: 1/2, step 139/574 completed (loss: 0.734490692615509, acc: 0.761904776096344)
[2025-01-02 00:49:31,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31,566][root][INFO] - Training Epoch: 1/2, step 140/574 completed (loss: 1.386003017425537, acc: 0.5769230723381042)
[2025-01-02 00:49:31,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31,937][root][INFO] - Training Epoch: 1/2, step 141/574 completed (loss: 1.279646396636963, acc: 0.6129032373428345)
[2025-01-02 00:49:32,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:32,311][root][INFO] - Training Epoch: 1/2, step 142/574 completed (loss: 1.2730246782302856, acc: 0.6216216087341309)
[2025-01-02 00:49:33,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:34,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:34,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:37,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:37,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:37,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:39,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:39,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:39,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:40,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:40,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:40,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:41,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:41,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:41,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:42,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:42,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:42,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:43,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:43,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:45,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:45,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:45,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:46,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:46,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:46,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:47,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:47,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:50,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:50,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:50,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:52,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:52,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:52,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:56,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:56,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:56,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:01,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:01,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:01,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:04,435][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.1860, device='cuda:0') eval_epoch_loss=tensor(1.1588, device='cuda:0') eval_epoch_acc=tensor(0.7318, device='cuda:0')
[2025-01-02 00:50:04,436][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:50:04,436][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:50:04,722][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.1587649583816528/model.pt
[2025-01-02 00:50:04,731][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:50:04,732][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.1587649583816528
[2025-01-02 00:50:04,733][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7317560911178589
[2025-01-02 00:50:04,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05,351][root][INFO] - Training Epoch: 1/2, step 143/574 completed (loss: 1.8134150505065918, acc: 0.6140350699424744)
[2025-01-02 00:50:05,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05,724][root][INFO] - Training Epoch: 1/2, step 144/574 completed (loss: 1.498374581336975, acc: 0.6492537260055542)
[2025-01-02 00:50:05,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06,136][root][INFO] - Training Epoch: 1/2, step 145/574 completed (loss: 1.3837168216705322, acc: 0.6734693646430969)
[2025-01-02 00:50:06,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06,621][root][INFO] - Training Epoch: 1/2, step 146/574 completed (loss: 1.9408624172210693, acc: 0.478723406791687)
[2025-01-02 00:50:06,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06,943][root][INFO] - Training Epoch: 1/2, step 147/574 completed (loss: 2.0081305503845215, acc: 0.6142857074737549)
[2025-01-02 00:50:07,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07,317][root][INFO] - Training Epoch: 1/2, step 148/574 completed (loss: 2.2850470542907715, acc: 0.3928571343421936)
[2025-01-02 00:50:07,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07,694][root][INFO] - Training Epoch: 1/2, step 149/574 completed (loss: 1.6809298992156982, acc: 0.6521739363670349)
[2025-01-02 00:50:07,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07,992][root][INFO] - Training Epoch: 1/2, step 150/574 completed (loss: 1.7353630065917969, acc: 0.5517241358757019)
[2025-01-02 00:50:08,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:08,381][root][INFO] - Training Epoch: 1/2, step 151/574 completed (loss: 1.994886040687561, acc: 0.52173912525177)
[2025-01-02 00:50:08,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:08,820][root][INFO] - Training Epoch: 1/2, step 152/574 completed (loss: 1.4604899883270264, acc: 0.6610169410705566)
[2025-01-02 00:50:08,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09,193][root][INFO] - Training Epoch: 1/2, step 153/574 completed (loss: 1.7408629655838013, acc: 0.5438596606254578)
[2025-01-02 00:50:09,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09,579][root][INFO] - Training Epoch: 1/2, step 154/574 completed (loss: 1.7372745275497437, acc: 0.5810810923576355)
[2025-01-02 00:50:09,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09,977][root][INFO] - Training Epoch: 1/2, step 155/574 completed (loss: 1.487965703010559, acc: 0.7857142686843872)
[2025-01-02 00:50:10,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10,303][root][INFO] - Training Epoch: 1/2, step 156/574 completed (loss: 1.0978997945785522, acc: 0.695652186870575)
[2025-01-02 00:50:10,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10,675][root][INFO] - Training Epoch: 1/2, step 157/574 completed (loss: 3.616783618927002, acc: 0.21052631735801697)
[2025-01-02 00:50:11,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12,581][root][INFO] - Training Epoch: 1/2, step 158/574 completed (loss: 3.1195712089538574, acc: 0.3918918967247009)
[2025-01-02 00:50:12,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12,927][root][INFO] - Training Epoch: 1/2, step 159/574 completed (loss: 2.2524983882904053, acc: 0.46296295523643494)
[2025-01-02 00:50:13,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:13,387][root][INFO] - Training Epoch: 1/2, step 160/574 completed (loss: 2.8257505893707275, acc: 0.40697672963142395)
[2025-01-02 00:50:13,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14,022][root][INFO] - Training Epoch: 1/2, step 161/574 completed (loss: 2.983236074447632, acc: 0.3176470696926117)
[2025-01-02 00:50:14,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14,597][root][INFO] - Training Epoch: 1/2, step 162/574 completed (loss: 2.823718786239624, acc: 0.42696627974510193)
[2025-01-02 00:50:14,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14,961][root][INFO] - Training Epoch: 1/2, step 163/574 completed (loss: 1.4401568174362183, acc: 0.7272727489471436)
[2025-01-02 00:50:15,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15,316][root][INFO] - Training Epoch: 1/2, step 164/574 completed (loss: 0.9065024852752686, acc: 0.761904776096344)
[2025-01-02 00:50:15,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15,730][root][INFO] - Training Epoch: 1/2, step 165/574 completed (loss: 1.239507794380188, acc: 0.6206896305084229)
[2025-01-02 00:50:15,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:16,096][root][INFO] - Training Epoch: 1/2, step 166/574 completed (loss: 0.5904043316841125, acc: 0.8571428656578064)
[2025-01-02 00:50:16,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:16,450][root][INFO] - Training Epoch: 1/2, step 167/574 completed (loss: 0.3325038552284241, acc: 0.8999999761581421)
[2025-01-02 00:50:16,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:16,933][root][INFO] - Training Epoch: 1/2, step 168/574 completed (loss: 1.3747507333755493, acc: 0.7222222089767456)
[2025-01-02 00:50:17,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:17,348][root][INFO] - Training Epoch: 1/2, step 169/574 completed (loss: 1.265617847442627, acc: 0.686274528503418)
[2025-01-02 00:50:17,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18,493][root][INFO] - Training Epoch: 1/2, step 170/574 completed (loss: 1.7625277042388916, acc: 0.6095890402793884)
[2025-01-02 00:50:18,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18,892][root][INFO] - Training Epoch: 1/2, step 171/574 completed (loss: 1.0873425006866455, acc: 0.6666666865348816)
[2025-01-02 00:50:18,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19,224][root][INFO] - Training Epoch: 1/2, step 172/574 completed (loss: 1.9721812009811401, acc: 0.4444444477558136)
[2025-01-02 00:50:19,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19,626][root][INFO] - Training Epoch: 1/2, step 173/574 completed (loss: 1.8524646759033203, acc: 0.5357142686843872)
[2025-01-02 00:50:19,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20,260][root][INFO] - Training Epoch: 1/2, step 174/574 completed (loss: 1.4003978967666626, acc: 0.7079645991325378)
[2025-01-02 00:50:20,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20,602][root][INFO] - Training Epoch: 1/2, step 175/574 completed (loss: 1.6508293151855469, acc: 0.5652173757553101)
[2025-01-02 00:50:20,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20,947][root][INFO] - Training Epoch: 1/2, step 176/574 completed (loss: 1.0691766738891602, acc: 0.75)
[2025-01-02 00:50:21,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:21,919][root][INFO] - Training Epoch: 1/2, step 177/574 completed (loss: 1.798761010169983, acc: 0.5572519302368164)
[2025-01-02 00:50:22,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:22,736][root][INFO] - Training Epoch: 1/2, step 178/574 completed (loss: 1.934437870979309, acc: 0.5111111402511597)
[2025-01-02 00:50:22,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23,094][root][INFO] - Training Epoch: 1/2, step 179/574 completed (loss: 1.2355029582977295, acc: 0.7213114500045776)
[2025-01-02 00:50:23,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23,421][root][INFO] - Training Epoch: 1/2, step 180/574 completed (loss: 0.36355093121528625, acc: 0.9583333134651184)
[2025-01-02 00:50:23,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23,733][root][INFO] - Training Epoch: 1/2, step 181/574 completed (loss: 0.38092154264450073, acc: 0.8799999952316284)
[2025-01-02 00:50:23,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24,081][root][INFO] - Training Epoch: 1/2, step 182/574 completed (loss: 0.8015670776367188, acc: 0.8214285969734192)
[2025-01-02 00:50:24,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24,504][root][INFO] - Training Epoch: 1/2, step 183/574 completed (loss: 0.7150432467460632, acc: 0.8048780560493469)
[2025-01-02 00:50:24,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24,913][root][INFO] - Training Epoch: 1/2, step 184/574 completed (loss: 1.2877682447433472, acc: 0.773413896560669)
[2025-01-02 00:50:25,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:25,298][root][INFO] - Training Epoch: 1/2, step 185/574 completed (loss: 0.818790078163147, acc: 0.8097983002662659)
[2025-01-02 00:50:25,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:25,842][root][INFO] - Training Epoch: 1/2, step 186/574 completed (loss: 0.8776870965957642, acc: 0.784375011920929)
[2025-01-02 00:50:25,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26,396][root][INFO] - Training Epoch: 1/2, step 187/574 completed (loss: 0.8266293406486511, acc: 0.7879924774169922)
[2025-01-02 00:50:26,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26,818][root][INFO] - Training Epoch: 1/2, step 188/574 completed (loss: 1.072155475616455, acc: 0.725978672504425)
[2025-01-02 00:50:26,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:27,178][root][INFO] - Training Epoch: 1/2, step 189/574 completed (loss: 1.1914957761764526, acc: 0.6399999856948853)
[2025-01-02 00:50:27,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:27,816][root][INFO] - Training Epoch: 1/2, step 190/574 completed (loss: 1.5692896842956543, acc: 0.6395348906517029)
[2025-01-02 00:50:28,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28,703][root][INFO] - Training Epoch: 1/2, step 191/574 completed (loss: 2.317563772201538, acc: 0.4841269850730896)
[2025-01-02 00:50:29,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:29,695][root][INFO] - Training Epoch: 1/2, step 192/574 completed (loss: 1.7144280672073364, acc: 0.560606062412262)
[2025-01-02 00:50:29,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:30,464][root][INFO] - Training Epoch: 1/2, step 193/574 completed (loss: 1.7672395706176758, acc: 0.5764706134796143)
[2025-01-02 00:50:30,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:31,588][root][INFO] - Training Epoch: 1/2, step 194/574 completed (loss: 1.9358303546905518, acc: 0.5432098507881165)
[2025-01-02 00:50:31,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32,565][root][INFO] - Training Epoch: 1/2, step 195/574 completed (loss: 1.9517812728881836, acc: 0.5322580933570862)
[2025-01-02 00:50:32,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32,900][root][INFO] - Training Epoch: 1/2, step 196/574 completed (loss: 0.8494869470596313, acc: 0.7857142686843872)
[2025-01-02 00:50:33,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:33,272][root][INFO] - Training Epoch: 1/2, step 197/574 completed (loss: 2.130155324935913, acc: 0.5)
[2025-01-02 00:50:33,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:33,642][root][INFO] - Training Epoch: 1/2, step 198/574 completed (loss: 1.6383460760116577, acc: 0.6470588445663452)
[2025-01-02 00:50:33,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34,031][root][INFO] - Training Epoch: 1/2, step 199/574 completed (loss: 1.5701415538787842, acc: 0.6985294222831726)
[2025-01-02 00:50:34,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34,497][root][INFO] - Training Epoch: 1/2, step 200/574 completed (loss: 0.9776029586791992, acc: 0.7118644118309021)
[2025-01-02 00:50:34,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34,898][root][INFO] - Training Epoch: 1/2, step 201/574 completed (loss: 1.6180869340896606, acc: 0.6194030046463013)
[2025-01-02 00:50:35,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:35,285][root][INFO] - Training Epoch: 1/2, step 202/574 completed (loss: 1.7725379467010498, acc: 0.6213592290878296)
[2025-01-02 00:50:35,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:35,656][root][INFO] - Training Epoch: 1/2, step 203/574 completed (loss: 1.4669471979141235, acc: 0.6507936716079712)
[2025-01-02 00:50:35,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:36,072][root][INFO] - Training Epoch: 1/2, step 204/574 completed (loss: 0.6210635900497437, acc: 0.8901098966598511)
[2025-01-02 00:50:36,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:36,488][root][INFO] - Training Epoch: 1/2, step 205/574 completed (loss: 0.8536784648895264, acc: 0.8206278085708618)
[2025-01-02 00:50:36,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:36,959][root][INFO] - Training Epoch: 1/2, step 206/574 completed (loss: 0.8669939637184143, acc: 0.7795275449752808)
[2025-01-02 00:50:37,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:37,321][root][INFO] - Training Epoch: 1/2, step 207/574 completed (loss: 1.0235031843185425, acc: 0.7715517282485962)
[2025-01-02 00:50:37,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:37,669][root][INFO] - Training Epoch: 1/2, step 208/574 completed (loss: 0.7585262060165405, acc: 0.8079710006713867)
[2025-01-02 00:50:37,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:38,111][root][INFO] - Training Epoch: 1/2, step 209/574 completed (loss: 0.8811317086219788, acc: 0.801556408405304)
[2025-01-02 00:50:38,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:38,496][root][INFO] - Training Epoch: 1/2, step 210/574 completed (loss: 0.6278790235519409, acc: 0.8586956262588501)
[2025-01-02 00:50:38,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:38,849][root][INFO] - Training Epoch: 1/2, step 211/574 completed (loss: 1.1895579099655151, acc: 0.739130437374115)
[2025-01-02 00:50:38,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:39,221][root][INFO] - Training Epoch: 1/2, step 212/574 completed (loss: 0.484697163105011, acc: 0.8214285969734192)
[2025-01-02 00:50:39,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:39,602][root][INFO] - Training Epoch: 1/2, step 213/574 completed (loss: 0.8280224204063416, acc: 0.8297872543334961)
[2025-01-02 00:50:39,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:40,397][root][INFO] - Training Epoch: 1/2, step 214/574 completed (loss: 0.9563409686088562, acc: 0.800000011920929)
[2025-01-02 00:50:40,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:40,745][root][INFO] - Training Epoch: 1/2, step 215/574 completed (loss: 0.34853604435920715, acc: 0.9054054021835327)
[2025-01-02 00:50:40,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:41,104][root][INFO] - Training Epoch: 1/2, step 216/574 completed (loss: 0.7035624384880066, acc: 0.8720930218696594)
[2025-01-02 00:50:41,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:41,658][root][INFO] - Training Epoch: 1/2, step 217/574 completed (loss: 0.9452805519104004, acc: 0.7747747898101807)
[2025-01-02 00:50:41,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:42,125][root][INFO] - Training Epoch: 1/2, step 218/574 completed (loss: 0.6040908694267273, acc: 0.8444444537162781)
[2025-01-02 00:50:42,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:42,522][root][INFO] - Training Epoch: 1/2, step 219/574 completed (loss: 0.7040678262710571, acc: 0.8181818127632141)
[2025-01-02 00:50:42,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:42,836][root][INFO] - Training Epoch: 1/2, step 220/574 completed (loss: 0.7230144143104553, acc: 0.7407407164573669)
[2025-01-02 00:50:42,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:43,215][root][INFO] - Training Epoch: 1/2, step 221/574 completed (loss: 0.37387025356292725, acc: 0.8799999952316284)
[2025-01-02 00:50:43,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:43,592][root][INFO] - Training Epoch: 1/2, step 222/574 completed (loss: 1.1307110786437988, acc: 0.5961538553237915)
[2025-01-02 00:50:43,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:44,440][root][INFO] - Training Epoch: 1/2, step 223/574 completed (loss: 0.9880131483078003, acc: 0.7771739363670349)
[2025-01-02 00:50:44,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:45,018][root][INFO] - Training Epoch: 1/2, step 224/574 completed (loss: 1.0222002267837524, acc: 0.7727272510528564)
[2025-01-02 00:50:45,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:45,476][root][INFO] - Training Epoch: 1/2, step 225/574 completed (loss: 1.1571173667907715, acc: 0.7021276354789734)
[2025-01-02 00:50:45,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:45,856][root][INFO] - Training Epoch: 1/2, step 226/574 completed (loss: 1.382325530052185, acc: 0.6415094137191772)
[2025-01-02 00:50:45,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:46,205][root][INFO] - Training Epoch: 1/2, step 227/574 completed (loss: 0.9724611043930054, acc: 0.7166666388511658)
[2025-01-02 00:50:46,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:46,506][root][INFO] - Training Epoch: 1/2, step 228/574 completed (loss: 1.2304739952087402, acc: 0.7209302186965942)
[2025-01-02 00:50:46,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:46,963][root][INFO] - Training Epoch: 1/2, step 229/574 completed (loss: 2.3603641986846924, acc: 0.46666666865348816)
[2025-01-02 00:50:47,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:47,367][root][INFO] - Training Epoch: 1/2, step 230/574 completed (loss: 2.317147970199585, acc: 0.4000000059604645)
[2025-01-02 00:50:47,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:47,756][root][INFO] - Training Epoch: 1/2, step 231/574 completed (loss: 2.1054043769836426, acc: 0.4888888895511627)
[2025-01-02 00:50:47,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:48,218][root][INFO] - Training Epoch: 1/2, step 232/574 completed (loss: 2.07835054397583, acc: 0.4444444477558136)
[2025-01-02 00:50:48,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:48,752][root][INFO] - Training Epoch: 1/2, step 233/574 completed (loss: 2.375926971435547, acc: 0.4678899049758911)
[2025-01-02 00:50:48,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:49,268][root][INFO] - Training Epoch: 1/2, step 234/574 completed (loss: 2.162447452545166, acc: 0.4692307710647583)
[2025-01-02 00:50:49,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:49,577][root][INFO] - Training Epoch: 1/2, step 235/574 completed (loss: 1.0101045370101929, acc: 0.6842105388641357)
[2025-01-02 00:50:49,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:49,888][root][INFO] - Training Epoch: 1/2, step 236/574 completed (loss: 0.9620233178138733, acc: 0.7083333134651184)
[2025-01-02 00:50:49,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:50,252][root][INFO] - Training Epoch: 1/2, step 237/574 completed (loss: 1.7603141069412231, acc: 0.5909090638160706)
[2025-01-02 00:50:50,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:50,691][root][INFO] - Training Epoch: 1/2, step 238/574 completed (loss: 1.333779215812683, acc: 0.7407407164573669)
[2025-01-02 00:50:50,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:51,059][root][INFO] - Training Epoch: 1/2, step 239/574 completed (loss: 1.4452977180480957, acc: 0.5714285969734192)
[2025-01-02 00:50:51,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:51,415][root][INFO] - Training Epoch: 1/2, step 240/574 completed (loss: 1.847219467163086, acc: 0.5454545617103577)
[2025-01-02 00:50:51,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:51,737][root][INFO] - Training Epoch: 1/2, step 241/574 completed (loss: 1.1860886812210083, acc: 0.7045454382896423)
[2025-01-02 00:50:51,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:52,379][root][INFO] - Training Epoch: 1/2, step 242/574 completed (loss: 2.13842511177063, acc: 0.4354838728904724)
[2025-01-02 00:50:52,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:52,952][root][INFO] - Training Epoch: 1/2, step 243/574 completed (loss: 1.9427618980407715, acc: 0.4545454680919647)
[2025-01-02 00:50:53,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:53,321][root][INFO] - Training Epoch: 1/2, step 244/574 completed (loss: 0.1500348448753357, acc: 1.0)
[2025-01-02 00:50:53,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:53,658][root][INFO] - Training Epoch: 1/2, step 245/574 completed (loss: 0.7284564971923828, acc: 0.7692307829856873)
[2025-01-02 00:50:53,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:54,014][root][INFO] - Training Epoch: 1/2, step 246/574 completed (loss: 0.2368825525045395, acc: 0.8709677457809448)
[2025-01-02 00:50:54,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:54,349][root][INFO] - Training Epoch: 1/2, step 247/574 completed (loss: 0.4904192388057709, acc: 0.8500000238418579)
[2025-01-02 00:50:54,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:54,774][root][INFO] - Training Epoch: 1/2, step 248/574 completed (loss: 1.2458175420761108, acc: 0.7567567825317383)
[2025-01-02 00:50:54,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:55,183][root][INFO] - Training Epoch: 1/2, step 249/574 completed (loss: 0.7290998697280884, acc: 0.7837837934494019)
[2025-01-02 00:50:55,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:55,613][root][INFO] - Training Epoch: 1/2, step 250/574 completed (loss: 0.6867018342018127, acc: 0.8918918967247009)
[2025-01-02 00:50:55,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:56,028][root][INFO] - Training Epoch: 1/2, step 251/574 completed (loss: 0.8632074594497681, acc: 0.779411792755127)
[2025-01-02 00:50:56,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:56,412][root][INFO] - Training Epoch: 1/2, step 252/574 completed (loss: 0.7273715138435364, acc: 0.8780487775802612)
[2025-01-02 00:50:56,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:56,799][root][INFO] - Training Epoch: 1/2, step 253/574 completed (loss: 0.7396548390388489, acc: 0.800000011920929)
[2025-01-02 00:50:56,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:57,193][root][INFO] - Training Epoch: 1/2, step 254/574 completed (loss: 0.12249826639890671, acc: 0.9599999785423279)
[2025-01-02 00:50:57,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:57,626][root][INFO] - Training Epoch: 1/2, step 255/574 completed (loss: 0.8832330107688904, acc: 0.7096773982048035)
[2025-01-02 00:50:57,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:57,994][root][INFO] - Training Epoch: 1/2, step 256/574 completed (loss: 0.8939158320426941, acc: 0.859649121761322)
[2025-01-02 00:50:58,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:58,341][root][INFO] - Training Epoch: 1/2, step 257/574 completed (loss: 0.3832823634147644, acc: 0.8999999761581421)
[2025-01-02 00:50:58,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:58,718][root][INFO] - Training Epoch: 1/2, step 258/574 completed (loss: 0.26641228795051575, acc: 0.9210526347160339)
[2025-01-02 00:50:58,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:59,331][root][INFO] - Training Epoch: 1/2, step 259/574 completed (loss: 0.652482807636261, acc: 0.801886796951294)
[2025-01-02 00:50:59,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:59,958][root][INFO] - Training Epoch: 1/2, step 260/574 completed (loss: 0.715645968914032, acc: 0.8166666626930237)
[2025-01-02 00:51:00,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:00,385][root][INFO] - Training Epoch: 1/2, step 261/574 completed (loss: 0.4881190061569214, acc: 0.8888888955116272)
[2025-01-02 00:51:00,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:00,796][root][INFO] - Training Epoch: 1/2, step 262/574 completed (loss: 1.4605789184570312, acc: 0.5806451439857483)
[2025-01-02 00:51:00,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:01,184][root][INFO] - Training Epoch: 1/2, step 263/574 completed (loss: 1.6504240036010742, acc: 0.5733333230018616)
[2025-01-02 00:51:01,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:01,515][root][INFO] - Training Epoch: 1/2, step 264/574 completed (loss: 0.9298611283302307, acc: 0.7291666865348816)
[2025-01-02 00:51:01,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:02,458][root][INFO] - Training Epoch: 1/2, step 265/574 completed (loss: 1.9743893146514893, acc: 0.527999997138977)
[2025-01-02 00:51:02,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:02,873][root][INFO] - Training Epoch: 1/2, step 266/574 completed (loss: 1.7134501934051514, acc: 0.5393258333206177)
[2025-01-02 00:51:03,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:03,267][root][INFO] - Training Epoch: 1/2, step 267/574 completed (loss: 1.6308140754699707, acc: 0.6486486196517944)
[2025-01-02 00:51:03,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:03,742][root][INFO] - Training Epoch: 1/2, step 268/574 completed (loss: 1.2329596281051636, acc: 0.6896551847457886)
[2025-01-02 00:51:03,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:04,083][root][INFO] - Training Epoch: 1/2, step 269/574 completed (loss: 0.8255137801170349, acc: 0.7727272510528564)
[2025-01-02 00:51:04,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:04,461][root][INFO] - Training Epoch: 1/2, step 270/574 completed (loss: 0.49926406145095825, acc: 0.8636363744735718)
[2025-01-02 00:51:04,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:04,801][root][INFO] - Training Epoch: 1/2, step 271/574 completed (loss: 0.383171409368515, acc: 0.90625)
[2025-01-02 00:51:04,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:05,188][root][INFO] - Training Epoch: 1/2, step 272/574 completed (loss: 0.19739072024822235, acc: 0.9333333373069763)
[2025-01-02 00:51:05,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:05,633][root][INFO] - Training Epoch: 1/2, step 273/574 completed (loss: 0.5852798223495483, acc: 0.8666666746139526)
[2025-01-02 00:51:05,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:05,970][root][INFO] - Training Epoch: 1/2, step 274/574 completed (loss: 0.3210498094558716, acc: 0.875)
[2025-01-02 00:51:06,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:06,357][root][INFO] - Training Epoch: 1/2, step 275/574 completed (loss: 0.5108688473701477, acc: 0.8999999761581421)
[2025-01-02 00:51:06,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:06,780][root][INFO] - Training Epoch: 1/2, step 276/574 completed (loss: 0.49975743889808655, acc: 0.7931034564971924)
[2025-01-02 00:51:06,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:07,131][root][INFO] - Training Epoch: 1/2, step 277/574 completed (loss: 0.43017205595970154, acc: 0.9200000166893005)
[2025-01-02 00:51:07,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:07,507][root][INFO] - Training Epoch: 1/2, step 278/574 completed (loss: 0.7208014130592346, acc: 0.8297872543334961)
[2025-01-02 00:51:07,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:07,932][root][INFO] - Training Epoch: 1/2, step 279/574 completed (loss: 0.5917996168136597, acc: 0.875)
[2025-01-02 00:51:08,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:08,317][root][INFO] - Training Epoch: 1/2, step 280/574 completed (loss: 0.34996676445007324, acc: 0.9090909361839294)
[2025-01-02 00:51:08,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:08,770][root][INFO] - Training Epoch: 1/2, step 281/574 completed (loss: 1.0159531831741333, acc: 0.7108433842658997)
[2025-01-02 00:51:08,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:09,129][root][INFO] - Training Epoch: 1/2, step 282/574 completed (loss: 1.196498155593872, acc: 0.6666666865348816)
[2025-01-02 00:51:09,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:09,467][root][INFO] - Training Epoch: 1/2, step 283/574 completed (loss: 0.4300616979598999, acc: 0.8157894611358643)
[2025-01-02 00:51:09,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:09,851][root][INFO] - Training Epoch: 1/2, step 284/574 completed (loss: 1.184145212173462, acc: 0.7058823704719543)
[2025-01-02 00:51:10,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:10,262][root][INFO] - Training Epoch: 1/2, step 285/574 completed (loss: 0.38941946625709534, acc: 0.925000011920929)
[2025-01-02 00:51:10,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:11,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:11,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:12,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:12,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:12,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:12,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:13,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:13,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:13,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:14,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:14,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:15,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:15,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:15,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:16,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:16,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:16,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:17,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:17,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:17,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:18,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:18,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:18,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:19,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:19,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:19,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:20,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:20,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:20,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:21,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:21,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:21,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:22,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:22,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:22,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:23,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:23,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:23,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:24,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:24,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:24,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:25,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:25,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:25,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:26,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:26,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:26,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:27,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:27,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:27,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:28,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:28,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:29,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:29,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:29,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:30,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:30,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:30,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:31,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:31,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:31,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:32,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:32,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:32,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:33,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:33,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:34,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:34,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:34,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:35,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:35,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:35,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:35,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:36,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:36,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:36,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:37,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:37,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:37,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:38,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:38,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:39,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:39,647][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2296, device='cuda:0') eval_epoch_loss=tensor(0.8018, device='cuda:0') eval_epoch_acc=tensor(0.7935, device='cuda:0')
[2025-01-02 00:51:39,649][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:51:39,649][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:51:39,949][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.8018431067466736/model.pt
[2025-01-02 00:51:39,958][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:51:39,959][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.8018431067466736
[2025-01-02 00:51:39,960][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7934848666191101
[2025-01-02 00:51:40,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:40,343][root][INFO] - Training Epoch: 1/2, step 286/574 completed (loss: 0.7133402824401855, acc: 0.8046875)
[2025-01-02 00:51:40,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:40,695][root][INFO] - Training Epoch: 1/2, step 287/574 completed (loss: 0.938919723033905, acc: 0.7440000176429749)
[2025-01-02 00:51:40,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:41,026][root][INFO] - Training Epoch: 1/2, step 288/574 completed (loss: 0.6217103600502014, acc: 0.8791208863258362)
[2025-01-02 00:51:41,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:41,343][root][INFO] - Training Epoch: 1/2, step 289/574 completed (loss: 0.9461178779602051, acc: 0.7888198494911194)
[2025-01-02 00:51:41,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:41,760][root][INFO] - Training Epoch: 1/2, step 290/574 completed (loss: 0.8469858765602112, acc: 0.8144329786300659)
[2025-01-02 00:51:41,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:42,121][root][INFO] - Training Epoch: 1/2, step 291/574 completed (loss: 0.3925360441207886, acc: 0.9090909361839294)
[2025-01-02 00:51:42,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:42,464][root][INFO] - Training Epoch: 1/2, step 292/574 completed (loss: 0.9269307255744934, acc: 0.738095223903656)
[2025-01-02 00:51:42,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:42,848][root][INFO] - Training Epoch: 1/2, step 293/574 completed (loss: 0.7629358172416687, acc: 0.8275862336158752)
[2025-01-02 00:51:43,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:43,349][root][INFO] - Training Epoch: 1/2, step 294/574 completed (loss: 0.7498307228088379, acc: 0.8181818127632141)
[2025-01-02 00:51:43,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:43,931][root][INFO] - Training Epoch: 1/2, step 295/574 completed (loss: 0.7496804594993591, acc: 0.8144329786300659)
[2025-01-02 00:51:44,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:44,322][root][INFO] - Training Epoch: 1/2, step 296/574 completed (loss: 0.7951002717018127, acc: 0.7931034564971924)
[2025-01-02 00:51:44,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:44,731][root][INFO] - Training Epoch: 1/2, step 297/574 completed (loss: 0.69985431432724, acc: 0.7777777910232544)
[2025-01-02 00:51:44,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:45,118][root][INFO] - Training Epoch: 1/2, step 298/574 completed (loss: 1.152602195739746, acc: 0.6842105388641357)
[2025-01-02 00:51:45,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:45,492][root][INFO] - Training Epoch: 1/2, step 299/574 completed (loss: 0.20089881122112274, acc: 0.9642857313156128)
[2025-01-02 00:51:45,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:45,819][root][INFO] - Training Epoch: 1/2, step 300/574 completed (loss: 0.38449785113334656, acc: 0.90625)
[2025-01-02 00:51:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:46,144][root][INFO] - Training Epoch: 1/2, step 301/574 completed (loss: 0.742695689201355, acc: 0.849056601524353)
[2025-01-02 00:51:46,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:46,549][root][INFO] - Training Epoch: 1/2, step 302/574 completed (loss: 0.5086500644683838, acc: 0.8867924809455872)
[2025-01-02 00:51:46,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:46,940][root][INFO] - Training Epoch: 1/2, step 303/574 completed (loss: 0.24362541735172272, acc: 0.9411764740943909)
[2025-01-02 00:51:47,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:47,335][root][INFO] - Training Epoch: 1/2, step 304/574 completed (loss: 0.6728290915489197, acc: 0.78125)
[2025-01-02 00:51:47,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:47,713][root][INFO] - Training Epoch: 1/2, step 305/574 completed (loss: 0.8190322518348694, acc: 0.7377049326896667)
[2025-01-02 00:51:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:48,019][root][INFO] - Training Epoch: 1/2, step 306/574 completed (loss: 0.6786878108978271, acc: 0.8666666746139526)
[2025-01-02 00:51:48,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:48,328][root][INFO] - Training Epoch: 1/2, step 307/574 completed (loss: 0.3414623737335205, acc: 0.9473684430122375)
[2025-01-02 00:51:48,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:48,637][root][INFO] - Training Epoch: 1/2, step 308/574 completed (loss: 0.5928993821144104, acc: 0.8550724387168884)
[2025-01-02 00:51:48,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:49,064][root][INFO] - Training Epoch: 1/2, step 309/574 completed (loss: 0.3838421702384949, acc: 0.9305555820465088)
[2025-01-02 00:51:49,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:49,382][root][INFO] - Training Epoch: 1/2, step 310/574 completed (loss: 0.5014948844909668, acc: 0.8192771077156067)
[2025-01-02 00:51:49,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:49,696][root][INFO] - Training Epoch: 1/2, step 311/574 completed (loss: 0.6396040320396423, acc: 0.807692289352417)
[2025-01-02 00:51:49,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:50,053][root][INFO] - Training Epoch: 1/2, step 312/574 completed (loss: 0.3136441111564636, acc: 0.9489796161651611)
[2025-01-02 00:51:50,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:50,351][root][INFO] - Training Epoch: 1/2, step 313/574 completed (loss: 0.3466693162918091, acc: 0.875)
[2025-01-02 00:51:50,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:50,655][root][INFO] - Training Epoch: 1/2, step 314/574 completed (loss: 0.2773747444152832, acc: 0.9166666865348816)
[2025-01-02 00:51:50,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:50,986][root][INFO] - Training Epoch: 1/2, step 315/574 completed (loss: 0.5001742839813232, acc: 0.8709677457809448)
[2025-01-02 00:51:51,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:51,293][root][INFO] - Training Epoch: 1/2, step 316/574 completed (loss: 1.004585862159729, acc: 0.774193525314331)
[2025-01-02 00:51:51,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:51,655][root][INFO] - Training Epoch: 1/2, step 317/574 completed (loss: 0.6103706359863281, acc: 0.89552241563797)
[2025-01-02 00:51:51,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:52,027][root][INFO] - Training Epoch: 1/2, step 318/574 completed (loss: 0.25268417596817017, acc: 0.9134615659713745)
[2025-01-02 00:51:52,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:52,387][root][INFO] - Training Epoch: 1/2, step 319/574 completed (loss: 0.5681647658348083, acc: 0.8222222328186035)
[2025-01-02 00:51:52,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:52,701][root][INFO] - Training Epoch: 1/2, step 320/574 completed (loss: 0.2045537233352661, acc: 0.9354838728904724)
[2025-01-02 00:51:52,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:53,016][root][INFO] - Training Epoch: 1/2, step 321/574 completed (loss: 0.262297123670578, acc: 0.9399999976158142)
[2025-01-02 00:51:53,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:53,323][root][INFO] - Training Epoch: 1/2, step 322/574 completed (loss: 1.0110899209976196, acc: 0.6666666865348816)
[2025-01-02 00:51:53,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:53,611][root][INFO] - Training Epoch: 1/2, step 323/574 completed (loss: 2.029653549194336, acc: 0.4285714328289032)
[2025-01-02 00:51:53,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:53,933][root][INFO] - Training Epoch: 1/2, step 324/574 completed (loss: 2.1398723125457764, acc: 0.5128205418586731)
[2025-01-02 00:51:54,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:54,254][root][INFO] - Training Epoch: 1/2, step 325/574 completed (loss: 1.9501781463623047, acc: 0.5609756112098694)
[2025-01-02 00:51:54,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:54,565][root][INFO] - Training Epoch: 1/2, step 326/574 completed (loss: 1.3116763830184937, acc: 0.6315789222717285)
[2025-01-02 00:51:54,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:54,906][root][INFO] - Training Epoch: 1/2, step 327/574 completed (loss: 0.9053950309753418, acc: 0.8421052694320679)
[2025-01-02 00:51:55,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:55,241][root][INFO] - Training Epoch: 1/2, step 328/574 completed (loss: 0.4220568537712097, acc: 0.8928571343421936)
[2025-01-02 00:51:55,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:55,622][root][INFO] - Training Epoch: 1/2, step 329/574 completed (loss: 0.550256073474884, acc: 0.8518518805503845)
[2025-01-02 00:51:55,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:55,980][root][INFO] - Training Epoch: 1/2, step 330/574 completed (loss: 0.33458518981933594, acc: 0.9375)
[2025-01-02 00:51:56,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:56,330][root][INFO] - Training Epoch: 1/2, step 331/574 completed (loss: 0.6434490084648132, acc: 0.8387096524238586)
[2025-01-02 00:51:56,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:56,716][root][INFO] - Training Epoch: 1/2, step 332/574 completed (loss: 0.3747450113296509, acc: 0.9122806787490845)
[2025-01-02 00:51:56,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:57,097][root][INFO] - Training Epoch: 1/2, step 333/574 completed (loss: 0.28387224674224854, acc: 0.9375)
[2025-01-02 00:51:57,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:57,459][root][INFO] - Training Epoch: 1/2, step 334/574 completed (loss: 0.3169019818305969, acc: 0.8999999761581421)
[2025-01-02 00:51:57,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:57,795][root][INFO] - Training Epoch: 1/2, step 335/574 completed (loss: 1.0097763538360596, acc: 0.6315789222717285)
[2025-01-02 00:51:57,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:58,121][root][INFO] - Training Epoch: 1/2, step 336/574 completed (loss: 1.2959824800491333, acc: 0.6399999856948853)
[2025-01-02 00:51:58,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:58,491][root][INFO] - Training Epoch: 1/2, step 337/574 completed (loss: 1.685233235359192, acc: 0.6321839094161987)
[2025-01-02 00:51:58,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:58,854][root][INFO] - Training Epoch: 1/2, step 338/574 completed (loss: 1.656711220741272, acc: 0.5531914830207825)
[2025-01-02 00:51:58,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:59,172][root][INFO] - Training Epoch: 1/2, step 339/574 completed (loss: 1.5962456464767456, acc: 0.5783132314682007)
[2025-01-02 00:51:59,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:59,421][root][INFO] - Training Epoch: 1/2, step 340/574 completed (loss: 0.6154646873474121, acc: 0.8260869383811951)
[2025-01-02 00:51:59,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:59,691][root][INFO] - Training Epoch: 1/2, step 341/574 completed (loss: 0.9448167085647583, acc: 0.7948718070983887)
[2025-01-02 00:51:59,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:00,002][root][INFO] - Training Epoch: 1/2, step 342/574 completed (loss: 0.7898755073547363, acc: 0.8313252925872803)
[2025-01-02 00:52:00,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:00,312][root][INFO] - Training Epoch: 1/2, step 343/574 completed (loss: 0.7986016869544983, acc: 0.7924528121948242)
[2025-01-02 00:52:00,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:00,635][root][INFO] - Training Epoch: 1/2, step 344/574 completed (loss: 0.3656769096851349, acc: 0.8987341523170471)
[2025-01-02 00:52:00,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:00,951][root][INFO] - Training Epoch: 1/2, step 345/574 completed (loss: 0.19269923865795135, acc: 0.9215686321258545)
[2025-01-02 00:52:01,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:01,264][root][INFO] - Training Epoch: 1/2, step 346/574 completed (loss: 0.7071539759635925, acc: 0.8208954930305481)
[2025-01-02 00:52:01,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:01,580][root][INFO] - Training Epoch: 1/2, step 347/574 completed (loss: 0.3336485028266907, acc: 0.8999999761581421)
[2025-01-02 00:52:01,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:01,883][root][INFO] - Training Epoch: 1/2, step 348/574 completed (loss: 0.9717156291007996, acc: 0.7599999904632568)
[2025-01-02 00:52:02,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:02,297][root][INFO] - Training Epoch: 1/2, step 349/574 completed (loss: 1.1842131614685059, acc: 0.6944444179534912)
[2025-01-02 00:52:02,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:02,603][root][INFO] - Training Epoch: 1/2, step 350/574 completed (loss: 1.1962127685546875, acc: 0.604651153087616)
[2025-01-02 00:52:02,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:02,924][root][INFO] - Training Epoch: 1/2, step 351/574 completed (loss: 0.716668426990509, acc: 0.7692307829856873)
[2025-01-02 00:52:03,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:03,302][root][INFO] - Training Epoch: 1/2, step 352/574 completed (loss: 1.3925206661224365, acc: 0.5777778029441833)
[2025-01-02 00:52:03,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:03,600][root][INFO] - Training Epoch: 1/2, step 353/574 completed (loss: 0.24560818076133728, acc: 0.8695651888847351)
[2025-01-02 00:52:03,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:03,872][root][INFO] - Training Epoch: 1/2, step 354/574 completed (loss: 0.9506394267082214, acc: 0.692307710647583)
[2025-01-02 00:52:03,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:04,186][root][INFO] - Training Epoch: 1/2, step 355/574 completed (loss: 1.100253939628601, acc: 0.7032967209815979)
[2025-01-02 00:52:04,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:04,705][root][INFO] - Training Epoch: 1/2, step 356/574 completed (loss: 1.0854213237762451, acc: 0.643478274345398)
[2025-01-02 00:52:04,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:05,059][root][INFO] - Training Epoch: 1/2, step 357/574 completed (loss: 1.038211703300476, acc: 0.6739130616188049)
[2025-01-02 00:52:05,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:05,405][root][INFO] - Training Epoch: 1/2, step 358/574 completed (loss: 0.9624885320663452, acc: 0.7142857313156128)
[2025-01-02 00:52:05,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:05,724][root][INFO] - Training Epoch: 1/2, step 359/574 completed (loss: 0.16297800838947296, acc: 0.9583333134651184)
[2025-01-02 00:52:05,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:06,017][root][INFO] - Training Epoch: 1/2, step 360/574 completed (loss: 0.8440206050872803, acc: 0.7307692170143127)
[2025-01-02 00:52:06,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:06,312][root][INFO] - Training Epoch: 1/2, step 361/574 completed (loss: 0.8651100993156433, acc: 0.8048780560493469)
[2025-01-02 00:52:06,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:06,628][root][INFO] - Training Epoch: 1/2, step 362/574 completed (loss: 0.40794023871421814, acc: 0.8888888955116272)
[2025-01-02 00:52:06,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:06,946][root][INFO] - Training Epoch: 1/2, step 363/574 completed (loss: 0.44035449624061584, acc: 0.8947368264198303)
[2025-01-02 00:52:07,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:07,248][root][INFO] - Training Epoch: 1/2, step 364/574 completed (loss: 0.502977192401886, acc: 0.8292682766914368)
[2025-01-02 00:52:07,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:07,612][root][INFO] - Training Epoch: 1/2, step 365/574 completed (loss: 0.6338353157043457, acc: 0.8484848737716675)
[2025-01-02 00:52:07,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:07,982][root][INFO] - Training Epoch: 1/2, step 366/574 completed (loss: 0.10678545385599136, acc: 0.9583333134651184)
[2025-01-02 00:52:08,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:08,308][root][INFO] - Training Epoch: 1/2, step 367/574 completed (loss: 0.5802980661392212, acc: 0.8260869383811951)
[2025-01-02 00:52:08,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:08,658][root][INFO] - Training Epoch: 1/2, step 368/574 completed (loss: 0.4063591957092285, acc: 0.9285714030265808)
[2025-01-02 00:52:08,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:09,022][root][INFO] - Training Epoch: 1/2, step 369/574 completed (loss: 0.7018551230430603, acc: 0.8125)
[2025-01-02 00:52:09,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:09,675][root][INFO] - Training Epoch: 1/2, step 370/574 completed (loss: 0.8217401504516602, acc: 0.7515151500701904)
[2025-01-02 00:52:10,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:10,598][root][INFO] - Training Epoch: 1/2, step 371/574 completed (loss: 0.6454081535339355, acc: 0.8207547068595886)
[2025-01-02 00:52:10,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:10,941][root][INFO] - Training Epoch: 1/2, step 372/574 completed (loss: 0.29657143354415894, acc: 0.9111111164093018)
[2025-01-02 00:52:11,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:11,240][root][INFO] - Training Epoch: 1/2, step 373/574 completed (loss: 0.5367110967636108, acc: 0.9285714030265808)
[2025-01-02 00:52:11,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:11,580][root][INFO] - Training Epoch: 1/2, step 374/574 completed (loss: 0.36891013383865356, acc: 0.8857142925262451)
[2025-01-02 00:52:11,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:11,898][root][INFO] - Training Epoch: 1/2, step 375/574 completed (loss: 0.05150402709841728, acc: 0.9599999785423279)
[2025-01-02 00:52:12,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:12,222][root][INFO] - Training Epoch: 1/2, step 376/574 completed (loss: 0.3780227601528168, acc: 0.8695651888847351)
[2025-01-02 00:52:12,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:12,560][root][INFO] - Training Epoch: 1/2, step 377/574 completed (loss: 0.299203485250473, acc: 0.9166666865348816)
[2025-01-02 00:52:12,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:12,950][root][INFO] - Training Epoch: 1/2, step 378/574 completed (loss: 0.15972286462783813, acc: 0.9368420839309692)
[2025-01-02 00:52:13,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:13,562][root][INFO] - Training Epoch: 1/2, step 379/574 completed (loss: 0.384162575006485, acc: 0.8982036113739014)
[2025-01-02 00:52:13,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:13,970][root][INFO] - Training Epoch: 1/2, step 380/574 completed (loss: 0.4682667851448059, acc: 0.8646616339683533)
[2025-01-02 00:52:14,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:15,365][root][INFO] - Training Epoch: 1/2, step 381/574 completed (loss: 0.9218704104423523, acc: 0.7486631274223328)
[2025-01-02 00:52:15,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:15,917][root][INFO] - Training Epoch: 1/2, step 382/574 completed (loss: 0.31011661887168884, acc: 0.8828828930854797)
[2025-01-02 00:52:15,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:16,204][root][INFO] - Training Epoch: 1/2, step 383/574 completed (loss: 0.8903903961181641, acc: 0.7857142686843872)
[2025-01-02 00:52:16,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:16,485][root][INFO] - Training Epoch: 1/2, step 384/574 completed (loss: 0.14951176941394806, acc: 0.9642857313156128)
[2025-01-02 00:52:16,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:16,782][root][INFO] - Training Epoch: 1/2, step 385/574 completed (loss: 0.40998971462249756, acc: 0.9375)
[2025-01-02 00:52:16,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:17,093][root][INFO] - Training Epoch: 1/2, step 386/574 completed (loss: 0.141343355178833, acc: 0.9722222089767456)
[2025-01-02 00:52:17,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:17,396][root][INFO] - Training Epoch: 1/2, step 387/574 completed (loss: 0.17742978036403656, acc: 0.9736841917037964)
[2025-01-02 00:52:17,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:17,703][root][INFO] - Training Epoch: 1/2, step 388/574 completed (loss: 0.21739600598812103, acc: 0.9545454382896423)
[2025-01-02 00:52:17,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:17,989][root][INFO] - Training Epoch: 1/2, step 389/574 completed (loss: 0.10823611915111542, acc: 0.949999988079071)
[2025-01-02 00:52:18,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:18,228][root][INFO] - Training Epoch: 1/2, step 390/574 completed (loss: 0.7503102421760559, acc: 0.761904776096344)
[2025-01-02 00:52:18,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:18,622][root][INFO] - Training Epoch: 1/2, step 391/574 completed (loss: 1.4271928071975708, acc: 0.5925925970077515)
[2025-01-02 00:52:18,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:18,954][root][INFO] - Training Epoch: 1/2, step 392/574 completed (loss: 1.173766016960144, acc: 0.6893203854560852)
[2025-01-02 00:52:19,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:19,506][root][INFO] - Training Epoch: 1/2, step 393/574 completed (loss: 1.2662763595581055, acc: 0.7279411554336548)
[2025-01-02 00:52:19,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:19,893][root][INFO] - Training Epoch: 1/2, step 394/574 completed (loss: 1.1018272638320923, acc: 0.6666666865348816)
[2025-01-02 00:52:20,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:20,273][root][INFO] - Training Epoch: 1/2, step 395/574 completed (loss: 1.2174232006072998, acc: 0.6736111044883728)
[2025-01-02 00:52:20,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:20,599][root][INFO] - Training Epoch: 1/2, step 396/574 completed (loss: 1.0854159593582153, acc: 0.6744186282157898)
[2025-01-02 00:52:20,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:20,947][root][INFO] - Training Epoch: 1/2, step 397/574 completed (loss: 0.47100651264190674, acc: 0.875)
[2025-01-02 00:52:21,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:21,335][root][INFO] - Training Epoch: 1/2, step 398/574 completed (loss: 0.6326809525489807, acc: 0.8139534592628479)
[2025-01-02 00:52:21,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:21,685][root][INFO] - Training Epoch: 1/2, step 399/574 completed (loss: 0.2768159806728363, acc: 0.8799999952316284)
[2025-01-02 00:52:21,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:22,263][root][INFO] - Training Epoch: 1/2, step 400/574 completed (loss: 0.6974911689758301, acc: 0.8088235259056091)
[2025-01-02 00:52:22,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:22,668][root][INFO] - Training Epoch: 1/2, step 401/574 completed (loss: 0.6122077703475952, acc: 0.8399999737739563)
[2025-01-02 00:52:22,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:23,033][root][INFO] - Training Epoch: 1/2, step 402/574 completed (loss: 0.6454145312309265, acc: 0.8484848737716675)
[2025-01-02 00:52:23,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:23,386][root][INFO] - Training Epoch: 1/2, step 403/574 completed (loss: 0.7245442271232605, acc: 0.8484848737716675)
[2025-01-02 00:52:23,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:23,716][root][INFO] - Training Epoch: 1/2, step 404/574 completed (loss: 0.7485206723213196, acc: 0.8064516186714172)
[2025-01-02 00:52:23,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:24,126][root][INFO] - Training Epoch: 1/2, step 405/574 completed (loss: 0.30024129152297974, acc: 0.8888888955116272)
[2025-01-02 00:52:24,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:24,512][root][INFO] - Training Epoch: 1/2, step 406/574 completed (loss: 0.3979929983615875, acc: 0.9200000166893005)
[2025-01-02 00:52:24,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:24,892][root][INFO] - Training Epoch: 1/2, step 407/574 completed (loss: 0.4039112329483032, acc: 0.8888888955116272)
[2025-01-02 00:52:25,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:25,322][root][INFO] - Training Epoch: 1/2, step 408/574 completed (loss: 0.46460455656051636, acc: 0.8518518805503845)
[2025-01-02 00:52:25,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:25,668][root][INFO] - Training Epoch: 1/2, step 409/574 completed (loss: 0.2226782888174057, acc: 0.9230769276618958)
[2025-01-02 00:52:25,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:26,047][root][INFO] - Training Epoch: 1/2, step 410/574 completed (loss: 0.3452410399913788, acc: 0.9137930870056152)
[2025-01-02 00:52:26,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:26,406][root][INFO] - Training Epoch: 1/2, step 411/574 completed (loss: 0.23620833456516266, acc: 0.9285714030265808)
[2025-01-02 00:52:26,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:26,779][root][INFO] - Training Epoch: 1/2, step 412/574 completed (loss: 0.293478786945343, acc: 0.9333333373069763)
[2025-01-02 00:52:26,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:27,137][root][INFO] - Training Epoch: 1/2, step 413/574 completed (loss: 0.5485002994537354, acc: 0.8484848737716675)
[2025-01-02 00:52:27,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:27,488][root][INFO] - Training Epoch: 1/2, step 414/574 completed (loss: 0.3288912773132324, acc: 0.9090909361839294)
[2025-01-02 00:52:27,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:27,880][root][INFO] - Training Epoch: 1/2, step 415/574 completed (loss: 0.5830181837081909, acc: 0.843137264251709)
[2025-01-02 00:52:28,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:28,296][root][INFO] - Training Epoch: 1/2, step 416/574 completed (loss: 0.4768317639827728, acc: 0.8461538553237915)
[2025-01-02 00:52:28,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:28,697][root][INFO] - Training Epoch: 1/2, step 417/574 completed (loss: 0.48771873116493225, acc: 0.9444444179534912)
[2025-01-02 00:52:28,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:29,093][root][INFO] - Training Epoch: 1/2, step 418/574 completed (loss: 0.47181424498558044, acc: 0.8999999761581421)
[2025-01-02 00:52:29,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:29,503][root][INFO] - Training Epoch: 1/2, step 419/574 completed (loss: 0.6667318344116211, acc: 0.800000011920929)
[2025-01-02 00:52:29,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:29,916][root][INFO] - Training Epoch: 1/2, step 420/574 completed (loss: 0.3836306929588318, acc: 0.9047619104385376)
[2025-01-02 00:52:30,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:30,322][root][INFO] - Training Epoch: 1/2, step 421/574 completed (loss: 0.513346791267395, acc: 0.8666666746139526)
[2025-01-02 00:52:30,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:30,701][root][INFO] - Training Epoch: 1/2, step 422/574 completed (loss: 0.8621380925178528, acc: 0.8125)
[2025-01-02 00:52:30,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:31,085][root][INFO] - Training Epoch: 1/2, step 423/574 completed (loss: 1.2168723344802856, acc: 0.6666666865348816)
[2025-01-02 00:52:31,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:31,446][root][INFO] - Training Epoch: 1/2, step 424/574 completed (loss: 0.6466858386993408, acc: 0.8518518805503845)
[2025-01-02 00:52:31,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:31,774][root][INFO] - Training Epoch: 1/2, step 425/574 completed (loss: 0.40104812383651733, acc: 0.939393937587738)
[2025-01-02 00:52:31,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:32,109][root][INFO] - Training Epoch: 1/2, step 426/574 completed (loss: 0.32516908645629883, acc: 0.9130434989929199)
[2025-01-02 00:52:32,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:32,441][root][INFO] - Training Epoch: 1/2, step 427/574 completed (loss: 0.4965033531188965, acc: 0.837837815284729)
[2025-01-02 00:52:32,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:32,793][root][INFO] - Training Epoch: 1/2, step 428/574 completed (loss: 0.40503641963005066, acc: 0.9629629850387573)
[2025-01-02 00:52:33,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:33,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:34,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:34,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:34,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:35,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:35,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:35,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:36,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:36,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:36,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:37,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:37,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:37,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:38,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:38,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:38,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:39,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:39,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:39,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:40,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:40,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:41,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:41,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:41,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:41,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:42,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:42,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:42,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:43,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:43,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:43,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:44,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:44,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:44,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:45,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:45,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:45,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:46,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:46,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:46,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:47,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:47,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:47,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:48,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:48,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:48,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:49,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:49,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:49,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:49,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:50,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:50,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:50,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:51,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:51,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:51,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:52,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:52,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:52,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:53,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:53,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:54,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:54,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:54,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:55,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:55,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:55,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:55,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:56,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:56,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:57,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:57,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:57,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:57,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:58,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:58,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:58,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:59,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:59,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:59,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:00,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:00,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:00,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:01,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:01,682][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9682, device='cuda:0') eval_epoch_loss=tensor(0.6771, device='cuda:0') eval_epoch_acc=tensor(0.8153, device='cuda:0')
[2025-01-02 00:53:01,683][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:53:01,683][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:53:02,011][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.6771058440208435/model.pt
[2025-01-02 00:53:02,021][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:53:02,022][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6771058440208435
[2025-01-02 00:53:02,023][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8153413534164429
[2025-01-02 00:53:02,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:02,486][root][INFO] - Training Epoch: 1/2, step 429/574 completed (loss: 0.3641608655452728, acc: 0.9130434989929199)
[2025-01-02 00:53:02,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:02,884][root][INFO] - Training Epoch: 1/2, step 430/574 completed (loss: 0.06076667830348015, acc: 1.0)
[2025-01-02 00:53:03,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:03,254][root][INFO] - Training Epoch: 1/2, step 431/574 completed (loss: 0.12006128579378128, acc: 0.9629629850387573)
[2025-01-02 00:53:03,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:03,647][root][INFO] - Training Epoch: 1/2, step 432/574 completed (loss: 0.4525335133075714, acc: 0.8260869383811951)
[2025-01-02 00:53:03,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:04,094][root][INFO] - Training Epoch: 1/2, step 433/574 completed (loss: 0.4776234030723572, acc: 0.8888888955116272)
[2025-01-02 00:53:04,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:04,456][root][INFO] - Training Epoch: 1/2, step 434/574 completed (loss: 0.021821053698658943, acc: 1.0)
[2025-01-02 00:53:04,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:04,857][root][INFO] - Training Epoch: 1/2, step 435/574 completed (loss: 0.05752258002758026, acc: 0.9696969985961914)
[2025-01-02 00:53:05,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:05,288][root][INFO] - Training Epoch: 1/2, step 436/574 completed (loss: 0.4197756350040436, acc: 0.8611111044883728)
[2025-01-02 00:53:05,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:05,683][root][INFO] - Training Epoch: 1/2, step 437/574 completed (loss: 0.10681992024183273, acc: 0.9545454382896423)
[2025-01-02 00:53:05,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:05,995][root][INFO] - Training Epoch: 1/2, step 438/574 completed (loss: 0.22729913890361786, acc: 0.9523809552192688)
[2025-01-02 00:53:06,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:06,369][root][INFO] - Training Epoch: 1/2, step 439/574 completed (loss: 0.867083728313446, acc: 0.8205128312110901)
[2025-01-02 00:53:06,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:06,881][root][INFO] - Training Epoch: 1/2, step 440/574 completed (loss: 0.6109676361083984, acc: 0.8636363744735718)
[2025-01-02 00:53:07,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:07,685][root][INFO] - Training Epoch: 1/2, step 441/574 completed (loss: 0.9477338790893555, acc: 0.7440000176429749)
[2025-01-02 00:53:07,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:08,131][root][INFO] - Training Epoch: 1/2, step 442/574 completed (loss: 0.9112265110015869, acc: 0.7419354915618896)
[2025-01-02 00:53:08,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:08,804][root][INFO] - Training Epoch: 1/2, step 443/574 completed (loss: 0.5460476279258728, acc: 0.8606964945793152)
[2025-01-02 00:53:08,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:09,188][root][INFO] - Training Epoch: 1/2, step 444/574 completed (loss: 0.24874818325042725, acc: 0.8867924809455872)
[2025-01-02 00:53:09,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:09,639][root][INFO] - Training Epoch: 1/2, step 445/574 completed (loss: 0.4065081775188446, acc: 0.8636363744735718)
[2025-01-02 00:53:09,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:10,006][root][INFO] - Training Epoch: 1/2, step 446/574 completed (loss: 0.975254476070404, acc: 0.739130437374115)
[2025-01-02 00:53:10,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:10,436][root][INFO] - Training Epoch: 1/2, step 447/574 completed (loss: 0.7577883005142212, acc: 0.807692289352417)
[2025-01-02 00:53:10,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:10,769][root][INFO] - Training Epoch: 1/2, step 448/574 completed (loss: 0.3152664601802826, acc: 0.8928571343421936)
[2025-01-02 00:53:10,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:11,093][root][INFO] - Training Epoch: 1/2, step 449/574 completed (loss: 0.24105840921401978, acc: 0.9701492786407471)
[2025-01-02 00:53:11,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:11,461][root][INFO] - Training Epoch: 1/2, step 450/574 completed (loss: 0.1788587123155594, acc: 0.9722222089767456)
[2025-01-02 00:53:11,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:11,826][root][INFO] - Training Epoch: 1/2, step 451/574 completed (loss: 0.1374981552362442, acc: 0.945652186870575)
[2025-01-02 00:53:11,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:12,196][root][INFO] - Training Epoch: 1/2, step 452/574 completed (loss: 0.388200581073761, acc: 0.8589743375778198)
[2025-01-02 00:53:12,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:12,580][root][INFO] - Training Epoch: 1/2, step 453/574 completed (loss: 0.5084468722343445, acc: 0.8552631735801697)
[2025-01-02 00:53:12,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:12,945][root][INFO] - Training Epoch: 1/2, step 454/574 completed (loss: 0.3190035820007324, acc: 0.918367326259613)
[2025-01-02 00:53:13,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:13,342][root][INFO] - Training Epoch: 1/2, step 455/574 completed (loss: 0.5045404434204102, acc: 0.8484848737716675)
[2025-01-02 00:53:13,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:13,733][root][INFO] - Training Epoch: 1/2, step 456/574 completed (loss: 0.8163053393363953, acc: 0.8350515365600586)
[2025-01-02 00:53:13,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:14,113][root][INFO] - Training Epoch: 1/2, step 457/574 completed (loss: 0.09693556278944016, acc: 0.9714285731315613)
[2025-01-02 00:53:14,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:14,527][root][INFO] - Training Epoch: 1/2, step 458/574 completed (loss: 0.5391938090324402, acc: 0.8779069781303406)
[2025-01-02 00:53:14,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:14,884][root][INFO] - Training Epoch: 1/2, step 459/574 completed (loss: 0.12435595691204071, acc: 0.9285714030265808)
[2025-01-02 00:53:15,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:15,272][root][INFO] - Training Epoch: 1/2, step 460/574 completed (loss: 0.42872878909111023, acc: 0.9012345671653748)
[2025-01-02 00:53:15,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:15,616][root][INFO] - Training Epoch: 1/2, step 461/574 completed (loss: 0.643793523311615, acc: 0.8611111044883728)
[2025-01-02 00:53:15,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:15,965][root][INFO] - Training Epoch: 1/2, step 462/574 completed (loss: 0.33965006470680237, acc: 0.875)
[2025-01-02 00:53:16,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:16,344][root][INFO] - Training Epoch: 1/2, step 463/574 completed (loss: 0.7679691910743713, acc: 0.8846153616905212)
[2025-01-02 00:53:16,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:16,643][root][INFO] - Training Epoch: 1/2, step 464/574 completed (loss: 0.4882411062717438, acc: 0.8260869383811951)
[2025-01-02 00:53:16,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:16,981][root][INFO] - Training Epoch: 1/2, step 465/574 completed (loss: 0.5134040117263794, acc: 0.8333333134651184)
[2025-01-02 00:53:17,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:17,398][root][INFO] - Training Epoch: 1/2, step 466/574 completed (loss: 0.5929955840110779, acc: 0.8674699068069458)
[2025-01-02 00:53:17,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:17,786][root][INFO] - Training Epoch: 1/2, step 467/574 completed (loss: 0.44548362493515015, acc: 0.8828828930854797)
[2025-01-02 00:53:17,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:18,194][root][INFO] - Training Epoch: 1/2, step 468/574 completed (loss: 0.928087055683136, acc: 0.7864077687263489)
[2025-01-02 00:53:18,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:18,607][root][INFO] - Training Epoch: 1/2, step 469/574 completed (loss: 0.7196878790855408, acc: 0.8130081295967102)
[2025-01-02 00:53:18,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:18,977][root][INFO] - Training Epoch: 1/2, step 470/574 completed (loss: 0.4021291732788086, acc: 0.875)
[2025-01-02 00:53:19,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:19,328][root][INFO] - Training Epoch: 1/2, step 471/574 completed (loss: 0.8555709719657898, acc: 0.7142857313156128)
[2025-01-02 00:53:19,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:19,785][root][INFO] - Training Epoch: 1/2, step 472/574 completed (loss: 1.0367742776870728, acc: 0.6764705777168274)
[2025-01-02 00:53:19,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:20,224][root][INFO] - Training Epoch: 1/2, step 473/574 completed (loss: 0.8818895220756531, acc: 0.7641921639442444)
[2025-01-02 00:53:20,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:20,619][root][INFO] - Training Epoch: 1/2, step 474/574 completed (loss: 0.8725702166557312, acc: 0.78125)
[2025-01-02 00:53:20,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:20,995][root][INFO] - Training Epoch: 1/2, step 475/574 completed (loss: 0.5445415377616882, acc: 0.8527607321739197)
[2025-01-02 00:53:21,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:21,353][root][INFO] - Training Epoch: 1/2, step 476/574 completed (loss: 0.5550224184989929, acc: 0.8417266011238098)
[2025-01-02 00:53:21,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:21,760][root][INFO] - Training Epoch: 1/2, step 477/574 completed (loss: 1.0850963592529297, acc: 0.6783919334411621)
[2025-01-02 00:53:21,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:22,134][root][INFO] - Training Epoch: 1/2, step 478/574 completed (loss: 0.9768196940422058, acc: 0.6666666865348816)
[2025-01-02 00:53:22,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:22,468][root][INFO] - Training Epoch: 1/2, step 479/574 completed (loss: 0.97879558801651, acc: 0.7878788113594055)
[2025-01-02 00:53:22,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:22,867][root][INFO] - Training Epoch: 1/2, step 480/574 completed (loss: 0.9146249294281006, acc: 0.8148148059844971)
[2025-01-02 00:53:22,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:23,287][root][INFO] - Training Epoch: 1/2, step 481/574 completed (loss: 0.7424988746643066, acc: 0.75)
[2025-01-02 00:53:23,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:23,695][root][INFO] - Training Epoch: 1/2, step 482/574 completed (loss: 1.6608874797821045, acc: 0.5)
[2025-01-02 00:53:23,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:24,139][root][INFO] - Training Epoch: 1/2, step 483/574 completed (loss: 1.204069972038269, acc: 0.6379310488700867)
[2025-01-02 00:53:24,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:24,464][root][INFO] - Training Epoch: 1/2, step 484/574 completed (loss: 0.41571348905563354, acc: 0.9032257795333862)
[2025-01-02 00:53:24,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:24,798][root][INFO] - Training Epoch: 1/2, step 485/574 completed (loss: 1.0789213180541992, acc: 0.7368420958518982)
[2025-01-02 00:53:24,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:25,149][root][INFO] - Training Epoch: 1/2, step 486/574 completed (loss: 1.6705228090286255, acc: 0.5925925970077515)
[2025-01-02 00:53:25,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:25,506][root][INFO] - Training Epoch: 1/2, step 487/574 completed (loss: 0.8875300288200378, acc: 0.761904776096344)
[2025-01-02 00:53:25,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:25,834][root][INFO] - Training Epoch: 1/2, step 488/574 completed (loss: 1.1082123517990112, acc: 0.8181818127632141)
[2025-01-02 00:53:25,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:26,226][root][INFO] - Training Epoch: 1/2, step 489/574 completed (loss: 1.2932225465774536, acc: 0.6615384817123413)
[2025-01-02 00:53:26,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:26,582][root][INFO] - Training Epoch: 1/2, step 490/574 completed (loss: 0.7888116240501404, acc: 0.8333333134651184)
[2025-01-02 00:53:26,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:26,913][root][INFO] - Training Epoch: 1/2, step 491/574 completed (loss: 1.1024073362350464, acc: 0.7241379022598267)
[2025-01-02 00:53:27,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:27,247][root][INFO] - Training Epoch: 1/2, step 492/574 completed (loss: 0.6809874176979065, acc: 0.7843137383460999)
[2025-01-02 00:53:27,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:27,639][root][INFO] - Training Epoch: 1/2, step 493/574 completed (loss: 0.6305983066558838, acc: 0.7241379022598267)
[2025-01-02 00:53:27,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:27,959][root][INFO] - Training Epoch: 1/2, step 494/574 completed (loss: 0.8309297561645508, acc: 0.7368420958518982)
[2025-01-02 00:53:28,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:28,285][root][INFO] - Training Epoch: 1/2, step 495/574 completed (loss: 1.1149412393569946, acc: 0.7368420958518982)
[2025-01-02 00:53:28,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:28,658][root][INFO] - Training Epoch: 1/2, step 496/574 completed (loss: 0.7501225471496582, acc: 0.8035714030265808)
[2025-01-02 00:53:28,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:29,075][root][INFO] - Training Epoch: 1/2, step 497/574 completed (loss: 0.5937519073486328, acc: 0.8426966071128845)
[2025-01-02 00:53:29,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:29,427][root][INFO] - Training Epoch: 1/2, step 498/574 completed (loss: 0.9805591702461243, acc: 0.7191011309623718)
[2025-01-02 00:53:29,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:29,785][root][INFO] - Training Epoch: 1/2, step 499/574 completed (loss: 1.3711857795715332, acc: 0.5957446694374084)
[2025-01-02 00:53:29,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:30,139][root][INFO] - Training Epoch: 1/2, step 500/574 completed (loss: 1.023349404335022, acc: 0.695652186870575)
[2025-01-02 00:53:30,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:30,461][root][INFO] - Training Epoch: 1/2, step 501/574 completed (loss: 0.0917326956987381, acc: 1.0)
[2025-01-02 00:53:30,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:30,811][root][INFO] - Training Epoch: 1/2, step 502/574 completed (loss: 0.39086538553237915, acc: 0.9230769276618958)
[2025-01-02 00:53:30,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:31,206][root][INFO] - Training Epoch: 1/2, step 503/574 completed (loss: 0.49976783990859985, acc: 0.8518518805503845)
[2025-01-02 00:53:31,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:31,594][root][INFO] - Training Epoch: 1/2, step 504/574 completed (loss: 0.5509083271026611, acc: 0.7777777910232544)
[2025-01-02 00:53:31,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:31,990][root][INFO] - Training Epoch: 1/2, step 505/574 completed (loss: 0.8207904696464539, acc: 0.8301886916160583)
[2025-01-02 00:53:32,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:32,349][root][INFO] - Training Epoch: 1/2, step 506/574 completed (loss: 0.7271933555603027, acc: 0.7931034564971924)
[2025-01-02 00:53:32,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:33,000][root][INFO] - Training Epoch: 1/2, step 507/574 completed (loss: 1.2702800035476685, acc: 0.6576576828956604)
[2025-01-02 00:53:33,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:33,466][root][INFO] - Training Epoch: 1/2, step 508/574 completed (loss: 0.9949659109115601, acc: 0.7605633735656738)
[2025-01-02 00:53:33,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:33,807][root][INFO] - Training Epoch: 1/2, step 509/574 completed (loss: 0.3264072835445404, acc: 0.8500000238418579)
[2025-01-02 00:53:33,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:34,179][root][INFO] - Training Epoch: 1/2, step 510/574 completed (loss: 0.5821549296379089, acc: 0.800000011920929)
[2025-01-02 00:53:34,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:34,521][root][INFO] - Training Epoch: 1/2, step 511/574 completed (loss: 0.9107742309570312, acc: 0.7692307829856873)
[2025-01-02 00:53:35,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:37,234][root][INFO] - Training Epoch: 1/2, step 512/574 completed (loss: 1.4150891304016113, acc: 0.6499999761581421)
[2025-01-02 00:53:37,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:38,062][root][INFO] - Training Epoch: 1/2, step 513/574 completed (loss: 0.5315220952033997, acc: 0.841269850730896)
[2025-01-02 00:53:38,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:38,422][root][INFO] - Training Epoch: 1/2, step 514/574 completed (loss: 0.7925068736076355, acc: 0.75)
[2025-01-02 00:53:38,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:38,779][root][INFO] - Training Epoch: 1/2, step 515/574 completed (loss: 0.2504706084728241, acc: 0.9333333373069763)
[2025-01-02 00:53:39,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:39,581][root][INFO] - Training Epoch: 1/2, step 516/574 completed (loss: 0.8504893779754639, acc: 0.7638888955116272)
[2025-01-02 00:53:39,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:39,902][root][INFO] - Training Epoch: 1/2, step 517/574 completed (loss: 0.020902005955576897, acc: 1.0)
[2025-01-02 00:53:39,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:40,215][root][INFO] - Training Epoch: 1/2, step 518/574 completed (loss: 0.2858227789402008, acc: 0.9032257795333862)
[2025-01-02 00:53:40,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:40,552][root][INFO] - Training Epoch: 1/2, step 519/574 completed (loss: 0.7010858058929443, acc: 0.75)
[2025-01-02 00:53:40,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:40,899][root][INFO] - Training Epoch: 1/2, step 520/574 completed (loss: 0.6893262267112732, acc: 0.7407407164573669)
[2025-01-02 00:53:41,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:41,950][root][INFO] - Training Epoch: 1/2, step 521/574 completed (loss: 0.7542321681976318, acc: 0.7711864113807678)
[2025-01-02 00:53:42,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:42,293][root][INFO] - Training Epoch: 1/2, step 522/574 completed (loss: 0.36840349435806274, acc: 0.888059675693512)
[2025-01-02 00:53:42,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:42,669][root][INFO] - Training Epoch: 1/2, step 523/574 completed (loss: 0.500443696975708, acc: 0.8540145754814148)
[2025-01-02 00:53:42,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:43,281][root][INFO] - Training Epoch: 1/2, step 524/574 completed (loss: 0.7667062878608704, acc: 0.800000011920929)
[2025-01-02 00:53:43,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:43,613][root][INFO] - Training Epoch: 1/2, step 525/574 completed (loss: 0.1687181144952774, acc: 0.9629629850387573)
[2025-01-02 00:53:43,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:44,014][root][INFO] - Training Epoch: 1/2, step 526/574 completed (loss: 0.3103504776954651, acc: 0.9230769276618958)
[2025-01-02 00:53:44,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:44,403][root][INFO] - Training Epoch: 1/2, step 527/574 completed (loss: 1.0301889181137085, acc: 0.7142857313156128)
[2025-01-02 00:53:44,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:44,791][root][INFO] - Training Epoch: 1/2, step 528/574 completed (loss: 2.1609182357788086, acc: 0.4590163826942444)
[2025-01-02 00:53:44,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:45,205][root][INFO] - Training Epoch: 1/2, step 529/574 completed (loss: 0.5018726587295532, acc: 0.8305084705352783)
[2025-01-02 00:53:45,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:45,543][root][INFO] - Training Epoch: 1/2, step 530/574 completed (loss: 1.4571330547332764, acc: 0.6279069781303406)
[2025-01-02 00:53:45,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:45,934][root][INFO] - Training Epoch: 1/2, step 531/574 completed (loss: 1.2594181299209595, acc: 0.6818181872367859)
[2025-01-02 00:53:46,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:46,296][root][INFO] - Training Epoch: 1/2, step 532/574 completed (loss: 1.245910882949829, acc: 0.6037735939025879)
[2025-01-02 00:53:46,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:46,721][root][INFO] - Training Epoch: 1/2, step 533/574 completed (loss: 1.0800869464874268, acc: 0.6818181872367859)
[2025-01-02 00:53:46,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:47,096][root][INFO] - Training Epoch: 1/2, step 534/574 completed (loss: 0.8499565720558167, acc: 0.6399999856948853)
[2025-01-02 00:53:47,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:47,503][root][INFO] - Training Epoch: 1/2, step 535/574 completed (loss: 0.8485628366470337, acc: 0.800000011920929)
[2025-01-02 00:53:47,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:47,883][root][INFO] - Training Epoch: 1/2, step 536/574 completed (loss: 0.40819665789604187, acc: 0.9090909361839294)
[2025-01-02 00:53:48,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:48,283][root][INFO] - Training Epoch: 1/2, step 537/574 completed (loss: 0.8768196105957031, acc: 0.7538461685180664)
[2025-01-02 00:53:48,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:48,665][root][INFO] - Training Epoch: 1/2, step 538/574 completed (loss: 0.8168632388114929, acc: 0.703125)
[2025-01-02 00:53:48,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:49,086][root][INFO] - Training Epoch: 1/2, step 539/574 completed (loss: 0.7566508054733276, acc: 0.75)
[2025-01-02 00:53:49,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:49,455][root][INFO] - Training Epoch: 1/2, step 540/574 completed (loss: 1.0074880123138428, acc: 0.6666666865348816)
[2025-01-02 00:53:49,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:49,816][root][INFO] - Training Epoch: 1/2, step 541/574 completed (loss: 0.7631595134735107, acc: 0.6875)
[2025-01-02 00:53:49,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:50,140][root][INFO] - Training Epoch: 1/2, step 542/574 completed (loss: 0.2262110412120819, acc: 0.9032257795333862)
[2025-01-02 00:53:50,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:50,492][root][INFO] - Training Epoch: 1/2, step 543/574 completed (loss: 0.13598136603832245, acc: 0.95652174949646)
[2025-01-02 00:53:50,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:50,841][root][INFO] - Training Epoch: 1/2, step 544/574 completed (loss: 0.4070983827114105, acc: 0.8666666746139526)
[2025-01-02 00:53:50,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:51,204][root][INFO] - Training Epoch: 1/2, step 545/574 completed (loss: 0.19471165537834167, acc: 0.9756097793579102)
[2025-01-02 00:53:51,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:51,582][root][INFO] - Training Epoch: 1/2, step 546/574 completed (loss: 0.04183049872517586, acc: 1.0)
[2025-01-02 00:53:51,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:51,946][root][INFO] - Training Epoch: 1/2, step 547/574 completed (loss: 0.18244227766990662, acc: 0.9210526347160339)
[2025-01-02 00:53:52,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:52,270][root][INFO] - Training Epoch: 1/2, step 548/574 completed (loss: 0.4191432595252991, acc: 0.9354838728904724)
[2025-01-02 00:53:52,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:52,621][root][INFO] - Training Epoch: 1/2, step 549/574 completed (loss: 0.029523940756917, acc: 1.0)
[2025-01-02 00:53:52,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:53,026][root][INFO] - Training Epoch: 1/2, step 550/574 completed (loss: 0.5884313583374023, acc: 0.8181818127632141)
[2025-01-02 00:53:53,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:53,422][root][INFO] - Training Epoch: 1/2, step 551/574 completed (loss: 0.2310602217912674, acc: 0.8999999761581421)
[2025-01-02 00:53:53,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:53,818][root][INFO] - Training Epoch: 1/2, step 552/574 completed (loss: 0.3228304982185364, acc: 0.9285714030265808)
[2025-01-02 00:53:53,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:54,212][root][INFO] - Training Epoch: 1/2, step 553/574 completed (loss: 0.5484403967857361, acc: 0.8540145754814148)
[2025-01-02 00:53:54,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:54,566][root][INFO] - Training Epoch: 1/2, step 554/574 completed (loss: 0.39863935112953186, acc: 0.9034482836723328)
[2025-01-02 00:53:54,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:54,965][root][INFO] - Training Epoch: 1/2, step 555/574 completed (loss: 0.48040857911109924, acc: 0.8785714507102966)
[2025-01-02 00:53:55,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:55,349][root][INFO] - Training Epoch: 1/2, step 556/574 completed (loss: 0.5649575591087341, acc: 0.860927164554596)
[2025-01-02 00:53:55,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:55,695][root][INFO] - Training Epoch: 1/2, step 557/574 completed (loss: 0.44429126381874084, acc: 0.8803418874740601)
[2025-01-02 00:53:55,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:56,048][root][INFO] - Training Epoch: 1/2, step 558/574 completed (loss: 0.3694975674152374, acc: 0.9200000166893005)
[2025-01-02 00:53:56,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:56,437][root][INFO] - Training Epoch: 1/2, step 559/574 completed (loss: 0.5265846252441406, acc: 0.9230769276618958)
[2025-01-02 00:53:56,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:56,780][root][INFO] - Training Epoch: 1/2, step 560/574 completed (loss: 0.19219937920570374, acc: 0.9230769276618958)
[2025-01-02 00:53:56,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:57,162][root][INFO] - Training Epoch: 1/2, step 561/574 completed (loss: 0.2284938395023346, acc: 0.9487179517745972)
[2025-01-02 00:53:57,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:57,518][root][INFO] - Training Epoch: 1/2, step 562/574 completed (loss: 0.5660887956619263, acc: 0.8666666746139526)
[2025-01-02 00:53:57,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:57,862][root][INFO] - Training Epoch: 1/2, step 563/574 completed (loss: 0.4803484082221985, acc: 0.8961039185523987)
[2025-01-02 00:53:57,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:58,223][root][INFO] - Training Epoch: 1/2, step 564/574 completed (loss: 0.6056312918663025, acc: 0.7708333134651184)
[2025-01-02 00:53:58,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:58,637][root][INFO] - Training Epoch: 1/2, step 565/574 completed (loss: 0.3252857029438019, acc: 0.8793103694915771)
[2025-01-02 00:53:58,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:58,993][root][INFO] - Training Epoch: 1/2, step 566/574 completed (loss: 0.4695014953613281, acc: 0.9047619104385376)
[2025-01-02 00:53:59,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:59,317][root][INFO] - Training Epoch: 1/2, step 567/574 completed (loss: 0.0831461101770401, acc: 0.9736841917037964)
[2025-01-02 00:53:59,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:59,700][root][INFO] - Training Epoch: 1/2, step 568/574 completed (loss: 0.1456563025712967, acc: 0.9629629850387573)
[2025-01-02 00:53:59,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:00,115][root][INFO] - Training Epoch: 1/2, step 569/574 completed (loss: 0.2599075734615326, acc: 0.9411764740943909)
[2025-01-02 00:54:00,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:00,469][root][INFO] - Training Epoch: 1/2, step 570/574 completed (loss: 0.022779352962970734, acc: 1.0)
[2025-01-02 00:54:00,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:00,809][root][INFO] - Training Epoch: 1/2, step 571/574 completed (loss: 0.2806324064731598, acc: 0.9145299196243286)
[2025-01-02 00:54:01,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:01,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:02,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:02,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:02,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:03,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:03,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:03,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:04,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:04,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:04,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:05,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:05,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:05,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:06,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:06,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:06,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:07,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:07,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:07,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:08,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:08,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:08,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:09,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:09,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:09,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:10,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:10,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:11,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:11,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:11,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:12,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:12,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:12,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:13,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:13,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:13,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:14,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:14,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:14,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:14,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:15,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:15,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:15,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:16,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:16,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:17,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:17,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:17,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:18,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:18,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:18,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:18,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:19,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:19,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:20,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:20,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:20,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:21,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:21,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:21,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:22,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:22,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:22,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:23,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:23,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:23,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:24,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:24,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:25,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:25,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:25,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:26,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:26,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:26,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:26,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:27,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:27,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:27,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:28,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:28,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:28,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:29,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:29,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:30,199][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8498, device='cuda:0') eval_epoch_loss=tensor(0.6151, device='cuda:0') eval_epoch_acc=tensor(0.8272, device='cuda:0')
[2025-01-02 00:54:30,201][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:54:30,201][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:54:30,501][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.6150627732276917/model.pt
[2025-01-02 00:54:30,511][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:54:30,512][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6150627732276917
[2025-01-02 00:54:30,513][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8272204399108887
[2025-01-02 00:54:30,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:30,913][root][INFO] - Training Epoch: 1/2, step 572/574 completed (loss: 0.4740603268146515, acc: 0.8622449040412903)
[2025-01-02 00:54:31,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:31,326][root][INFO] - Training Epoch: 1/2, step 573/574 completed (loss: 0.5246187448501587, acc: 0.8742138147354126)
[2025-01-02 00:54:31,874][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=2.9199, train_epoch_loss=1.0715, epoch time 366.81205443292856s
[2025-01-02 00:54:31,874][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-02 00:54:31,874][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-02 00:54:31,874][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-02 00:54:31,874][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-01-02 00:54:31,875][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-01-02 00:54:32,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:32,792][root][INFO] - Training Epoch: 2/2, step 0/574 completed (loss: 0.7231180667877197, acc: 0.8148148059844971)
[2025-01-02 00:54:32,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:33,119][root][INFO] - Training Epoch: 2/2, step 1/574 completed (loss: 0.649641752243042, acc: 0.8399999737739563)
[2025-01-02 00:54:33,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:33,553][root][INFO] - Training Epoch: 2/2, step 2/574 completed (loss: 1.1465201377868652, acc: 0.7567567825317383)
[2025-01-02 00:54:33,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:33,922][root][INFO] - Training Epoch: 2/2, step 3/574 completed (loss: 0.7361151576042175, acc: 0.8157894611358643)
[2025-01-02 00:54:34,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:34,234][root][INFO] - Training Epoch: 2/2, step 4/574 completed (loss: 0.979326605796814, acc: 0.7567567825317383)
[2025-01-02 00:54:34,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:34,569][root][INFO] - Training Epoch: 2/2, step 5/574 completed (loss: 0.4224259853363037, acc: 0.7857142686843872)
[2025-01-02 00:54:34,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:34,967][root][INFO] - Training Epoch: 2/2, step 6/574 completed (loss: 1.10263991355896, acc: 0.6326530575752258)
[2025-01-02 00:54:35,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:35,281][root][INFO] - Training Epoch: 2/2, step 7/574 completed (loss: 0.6630503535270691, acc: 0.8333333134651184)
[2025-01-02 00:54:35,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:35,647][root][INFO] - Training Epoch: 2/2, step 8/574 completed (loss: 0.2707246243953705, acc: 0.8636363744735718)
[2025-01-02 00:54:35,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:36,044][root][INFO] - Training Epoch: 2/2, step 9/574 completed (loss: 0.2121051698923111, acc: 0.9230769276618958)
[2025-01-02 00:54:36,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:36,389][root][INFO] - Training Epoch: 2/2, step 10/574 completed (loss: 0.39529338479042053, acc: 0.8888888955116272)
[2025-01-02 00:54:36,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:36,737][root][INFO] - Training Epoch: 2/2, step 11/574 completed (loss: 0.5803205370903015, acc: 0.7948718070983887)
[2025-01-02 00:54:36,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:37,112][root][INFO] - Training Epoch: 2/2, step 12/574 completed (loss: 0.21457840502262115, acc: 0.9090909361839294)
[2025-01-02 00:54:37,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:37,458][root][INFO] - Training Epoch: 2/2, step 13/574 completed (loss: 0.4645301103591919, acc: 0.8695651888847351)
[2025-01-02 00:54:37,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:37,849][root][INFO] - Training Epoch: 2/2, step 14/574 completed (loss: 0.27892598509788513, acc: 0.9411764740943909)
[2025-01-02 00:54:37,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:38,230][root][INFO] - Training Epoch: 2/2, step 15/574 completed (loss: 0.4562045931816101, acc: 0.8775510191917419)
[2025-01-02 00:54:38,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:38,567][root][INFO] - Training Epoch: 2/2, step 16/574 completed (loss: 0.3851414620876312, acc: 0.8947368264198303)
[2025-01-02 00:54:38,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:38,949][root][INFO] - Training Epoch: 2/2, step 17/574 completed (loss: 0.6929232478141785, acc: 0.7916666865348816)
[2025-01-02 00:54:39,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:39,305][root][INFO] - Training Epoch: 2/2, step 18/574 completed (loss: 0.7077023386955261, acc: 0.75)
[2025-01-02 00:54:39,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:39,623][root][INFO] - Training Epoch: 2/2, step 19/574 completed (loss: 0.6637623906135559, acc: 0.7894737124443054)
[2025-01-02 00:54:39,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:39,989][root][INFO] - Training Epoch: 2/2, step 20/574 completed (loss: 0.4518876373767853, acc: 0.807692289352417)
[2025-01-02 00:54:40,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:40,334][root][INFO] - Training Epoch: 2/2, step 21/574 completed (loss: 0.9278722405433655, acc: 0.8275862336158752)
[2025-01-02 00:54:40,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:40,722][root][INFO] - Training Epoch: 2/2, step 22/574 completed (loss: 1.1051141023635864, acc: 0.6399999856948853)
[2025-01-02 00:54:40,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:41,063][root][INFO] - Training Epoch: 2/2, step 23/574 completed (loss: 0.9907076954841614, acc: 0.8095238208770752)
[2025-01-02 00:54:41,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:41,472][root][INFO] - Training Epoch: 2/2, step 24/574 completed (loss: 0.2211509346961975, acc: 1.0)
[2025-01-02 00:54:41,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:41,863][root][INFO] - Training Epoch: 2/2, step 25/574 completed (loss: 0.7874521017074585, acc: 0.7735849022865295)
[2025-01-02 00:54:41,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:42,260][root][INFO] - Training Epoch: 2/2, step 26/574 completed (loss: 0.9719119071960449, acc: 0.7260273694992065)
[2025-01-02 00:54:42,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:43,548][root][INFO] - Training Epoch: 2/2, step 27/574 completed (loss: 1.2275364398956299, acc: 0.6798418760299683)
[2025-01-02 00:54:43,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:43,941][root][INFO] - Training Epoch: 2/2, step 28/574 completed (loss: 0.569291353225708, acc: 0.7906976938247681)
[2025-01-02 00:54:44,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:44,319][root][INFO] - Training Epoch: 2/2, step 29/574 completed (loss: 0.7836110591888428, acc: 0.7951807379722595)
[2025-01-02 00:54:44,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:44,682][root][INFO] - Training Epoch: 2/2, step 30/574 completed (loss: 0.7489690184593201, acc: 0.790123462677002)
[2025-01-02 00:54:44,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:45,020][root][INFO] - Training Epoch: 2/2, step 31/574 completed (loss: 0.7943070530891418, acc: 0.75)
[2025-01-02 00:54:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:45,364][root][INFO] - Training Epoch: 2/2, step 32/574 completed (loss: 0.586036205291748, acc: 0.8148148059844971)
[2025-01-02 00:54:45,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:45,750][root][INFO] - Training Epoch: 2/2, step 33/574 completed (loss: 0.29730501770973206, acc: 0.9130434989929199)
[2025-01-02 00:54:45,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:46,096][root][INFO] - Training Epoch: 2/2, step 34/574 completed (loss: 0.7098436951637268, acc: 0.7731092572212219)
[2025-01-02 00:54:46,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:46,405][root][INFO] - Training Epoch: 2/2, step 35/574 completed (loss: 0.4235098958015442, acc: 0.868852436542511)
[2025-01-02 00:54:46,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:46,792][root][INFO] - Training Epoch: 2/2, step 36/574 completed (loss: 0.6151689887046814, acc: 0.8253968358039856)
[2025-01-02 00:54:46,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:47,139][root][INFO] - Training Epoch: 2/2, step 37/574 completed (loss: 0.6996534466743469, acc: 0.8305084705352783)
[2025-01-02 00:54:47,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:47,500][root][INFO] - Training Epoch: 2/2, step 38/574 completed (loss: 0.5137492418289185, acc: 0.8735632300376892)
[2025-01-02 00:54:47,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:47,789][root][INFO] - Training Epoch: 2/2, step 39/574 completed (loss: 0.6664717793464661, acc: 0.8095238208770752)
[2025-01-02 00:54:47,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:48,074][root][INFO] - Training Epoch: 2/2, step 40/574 completed (loss: 0.6076970100402832, acc: 0.8461538553237915)
[2025-01-02 00:54:48,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:48,441][root][INFO] - Training Epoch: 2/2, step 41/574 completed (loss: 0.42434537410736084, acc: 0.8918918967247009)
[2025-01-02 00:54:48,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:48,776][root][INFO] - Training Epoch: 2/2, step 42/574 completed (loss: 0.6768885254859924, acc: 0.8153846263885498)
[2025-01-02 00:54:48,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:49,150][root][INFO] - Training Epoch: 2/2, step 43/574 completed (loss: 0.7155275344848633, acc: 0.8787878751754761)
[2025-01-02 00:54:49,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:49,571][root][INFO] - Training Epoch: 2/2, step 44/574 completed (loss: 0.5066174864768982, acc: 0.8556700944900513)
[2025-01-02 00:54:49,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:49,969][root][INFO] - Training Epoch: 2/2, step 45/574 completed (loss: 0.5404360294342041, acc: 0.8308823704719543)
[2025-01-02 00:54:50,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:50,369][root][INFO] - Training Epoch: 2/2, step 46/574 completed (loss: 0.4704631567001343, acc: 0.7307692170143127)
[2025-01-02 00:54:50,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:50,742][root][INFO] - Training Epoch: 2/2, step 47/574 completed (loss: 0.45833688974380493, acc: 0.9259259104728699)
[2025-01-02 00:54:50,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:51,138][root][INFO] - Training Epoch: 2/2, step 48/574 completed (loss: 0.46940016746520996, acc: 0.8928571343421936)
[2025-01-02 00:54:51,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:51,510][root][INFO] - Training Epoch: 2/2, step 49/574 completed (loss: 0.11122690141201019, acc: 1.0)
[2025-01-02 00:54:51,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:51,862][root][INFO] - Training Epoch: 2/2, step 50/574 completed (loss: 0.9263852834701538, acc: 0.7543859481811523)
[2025-01-02 00:54:51,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:52,233][root][INFO] - Training Epoch: 2/2, step 51/574 completed (loss: 0.8567588925361633, acc: 0.7460317611694336)
[2025-01-02 00:54:52,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:52,590][root][INFO] - Training Epoch: 2/2, step 52/574 completed (loss: 1.2568247318267822, acc: 0.7042253613471985)
[2025-01-02 00:54:52,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:53,025][root][INFO] - Training Epoch: 2/2, step 53/574 completed (loss: 1.515528678894043, acc: 0.5533333420753479)
[2025-01-02 00:54:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:53,388][root][INFO] - Training Epoch: 2/2, step 54/574 completed (loss: 1.2401864528656006, acc: 0.5945945978164673)
[2025-01-02 00:54:53,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:53,791][root][INFO] - Training Epoch: 2/2, step 55/574 completed (loss: 0.17075668275356293, acc: 0.9615384340286255)
[2025-01-02 00:54:55,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:56,829][root][INFO] - Training Epoch: 2/2, step 56/574 completed (loss: 1.3190815448760986, acc: 0.6621160507202148)
[2025-01-02 00:54:57,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:58,189][root][INFO] - Training Epoch: 2/2, step 57/574 completed (loss: 1.3646368980407715, acc: 0.6318082809448242)
[2025-01-02 00:54:58,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:58,808][root][INFO] - Training Epoch: 2/2, step 58/574 completed (loss: 0.9608904123306274, acc: 0.6931818127632141)
[2025-01-02 00:54:58,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:59,372][root][INFO] - Training Epoch: 2/2, step 59/574 completed (loss: 0.361925333738327, acc: 0.8897058963775635)
[2025-01-02 00:54:59,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:59,927][root][INFO] - Training Epoch: 2/2, step 60/574 completed (loss: 0.9535802006721497, acc: 0.7028985619544983)
[2025-01-02 00:55:00,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:00,385][root][INFO] - Training Epoch: 2/2, step 61/574 completed (loss: 0.6793094873428345, acc: 0.8125)
[2025-01-02 00:55:00,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:00,755][root][INFO] - Training Epoch: 2/2, step 62/574 completed (loss: 0.6873179078102112, acc: 0.7941176295280457)
[2025-01-02 00:55:00,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:01,161][root][INFO] - Training Epoch: 2/2, step 63/574 completed (loss: 0.504499077796936, acc: 0.8611111044883728)
[2025-01-02 00:55:01,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:01,504][root][INFO] - Training Epoch: 2/2, step 64/574 completed (loss: 0.3354988694190979, acc: 0.890625)
[2025-01-02 00:55:01,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:01,805][root][INFO] - Training Epoch: 2/2, step 65/574 completed (loss: 0.19097597897052765, acc: 0.8965517282485962)
[2025-01-02 00:55:01,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:02,178][root][INFO] - Training Epoch: 2/2, step 66/574 completed (loss: 1.039798617362976, acc: 0.8035714030265808)
[2025-01-02 00:55:02,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:02,508][root][INFO] - Training Epoch: 2/2, step 67/574 completed (loss: 0.7465070486068726, acc: 0.7666666507720947)
[2025-01-02 00:55:02,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:02,884][root][INFO] - Training Epoch: 2/2, step 68/574 completed (loss: 0.12499275803565979, acc: 0.9599999785423279)
[2025-01-02 00:55:03,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:03,278][root][INFO] - Training Epoch: 2/2, step 69/574 completed (loss: 0.8263016939163208, acc: 0.8333333134651184)
[2025-01-02 00:55:03,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:03,617][root][INFO] - Training Epoch: 2/2, step 70/574 completed (loss: 1.1255970001220703, acc: 0.7272727489471436)
[2025-01-02 00:55:03,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:04,001][root][INFO] - Training Epoch: 2/2, step 71/574 completed (loss: 1.029616355895996, acc: 0.6764705777168274)
[2025-01-02 00:55:04,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:04,380][root][INFO] - Training Epoch: 2/2, step 72/574 completed (loss: 0.8431432247161865, acc: 0.761904776096344)
[2025-01-02 00:55:04,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:04,798][root][INFO] - Training Epoch: 2/2, step 73/574 completed (loss: 1.2484021186828613, acc: 0.6615384817123413)
[2025-01-02 00:55:04,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:05,150][root][INFO] - Training Epoch: 2/2, step 74/574 completed (loss: 1.2407405376434326, acc: 0.6938775777816772)
[2025-01-02 00:55:05,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:05,534][root][INFO] - Training Epoch: 2/2, step 75/574 completed (loss: 1.3095612525939941, acc: 0.6492537260055542)
[2025-01-02 00:55:05,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:05,937][root][INFO] - Training Epoch: 2/2, step 76/574 completed (loss: 1.453957200050354, acc: 0.5985401272773743)
[2025-01-02 00:55:06,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:06,234][root][INFO] - Training Epoch: 2/2, step 77/574 completed (loss: 0.12811186909675598, acc: 0.9523809552192688)
[2025-01-02 00:55:06,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:06,540][root][INFO] - Training Epoch: 2/2, step 78/574 completed (loss: 0.3658629357814789, acc: 0.9166666865348816)
[2025-01-02 00:55:06,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:06,918][root][INFO] - Training Epoch: 2/2, step 79/574 completed (loss: 0.15707874298095703, acc: 0.9696969985961914)
[2025-01-02 00:55:07,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:07,273][root][INFO] - Training Epoch: 2/2, step 80/574 completed (loss: 0.5179454684257507, acc: 0.8461538553237915)
[2025-01-02 00:55:07,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:07,615][root][INFO] - Training Epoch: 2/2, step 81/574 completed (loss: 0.8063960671424866, acc: 0.7692307829856873)
[2025-01-02 00:55:07,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:07,937][root][INFO] - Training Epoch: 2/2, step 82/574 completed (loss: 0.8986015915870667, acc: 0.7884615659713745)
[2025-01-02 00:55:08,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:08,264][root][INFO] - Training Epoch: 2/2, step 83/574 completed (loss: 0.3998402953147888, acc: 0.90625)
[2025-01-02 00:55:08,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:08,584][root][INFO] - Training Epoch: 2/2, step 84/574 completed (loss: 0.5871400833129883, acc: 0.8405796885490417)
[2025-01-02 00:55:08,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:08,974][root][INFO] - Training Epoch: 2/2, step 85/574 completed (loss: 0.7811019420623779, acc: 0.7599999904632568)
[2025-01-02 00:55:09,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:09,364][root][INFO] - Training Epoch: 2/2, step 86/574 completed (loss: 0.6710648536682129, acc: 0.8695651888847351)
[2025-01-02 00:55:09,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:09,898][root][INFO] - Training Epoch: 2/2, step 87/574 completed (loss: 0.9491678476333618, acc: 0.699999988079071)
[2025-01-02 00:55:09,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:10,287][root][INFO] - Training Epoch: 2/2, step 88/574 completed (loss: 0.7933663725852966, acc: 0.7766990065574646)
[2025-01-02 00:55:10,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:11,415][root][INFO] - Training Epoch: 2/2, step 89/574 completed (loss: 0.9878706932067871, acc: 0.7572815418243408)
[2025-01-02 00:55:11,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:12,228][root][INFO] - Training Epoch: 2/2, step 90/574 completed (loss: 1.175472617149353, acc: 0.6881720423698425)
[2025-01-02 00:55:12,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:13,022][root][INFO] - Training Epoch: 2/2, step 91/574 completed (loss: 1.1253305673599243, acc: 0.7025862336158752)
[2025-01-02 00:55:13,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:13,764][root][INFO] - Training Epoch: 2/2, step 92/574 completed (loss: 0.7204925417900085, acc: 0.7684210538864136)
[2025-01-02 00:55:14,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:14,749][root][INFO] - Training Epoch: 2/2, step 93/574 completed (loss: 1.596341609954834, acc: 0.5445544719696045)
[2025-01-02 00:55:14,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:15,052][root][INFO] - Training Epoch: 2/2, step 94/574 completed (loss: 1.203361988067627, acc: 0.6612903475761414)
[2025-01-02 00:55:15,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:15,401][root][INFO] - Training Epoch: 2/2, step 95/574 completed (loss: 0.8612808585166931, acc: 0.739130437374115)
[2025-01-02 00:55:15,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:15,822][root][INFO] - Training Epoch: 2/2, step 96/574 completed (loss: 1.2297399044036865, acc: 0.6638655662536621)
[2025-01-02 00:55:15,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:16,197][root][INFO] - Training Epoch: 2/2, step 97/574 completed (loss: 1.2366034984588623, acc: 0.6730769276618958)
[2025-01-02 00:55:16,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:16,594][root][INFO] - Training Epoch: 2/2, step 98/574 completed (loss: 1.262500524520874, acc: 0.6496350169181824)
[2025-01-02 00:55:16,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:16,977][root][INFO] - Training Epoch: 2/2, step 99/574 completed (loss: 1.7324788570404053, acc: 0.5373134613037109)
[2025-01-02 00:55:17,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:17,347][root][INFO] - Training Epoch: 2/2, step 100/574 completed (loss: 0.6492132544517517, acc: 0.8500000238418579)
[2025-01-02 00:55:17,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:17,676][root][INFO] - Training Epoch: 2/2, step 101/574 completed (loss: 0.045216165482997894, acc: 1.0)
[2025-01-02 00:55:17,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:17,992][root][INFO] - Training Epoch: 2/2, step 102/574 completed (loss: 0.1891157478094101, acc: 0.95652174949646)
[2025-01-02 00:55:18,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:18,333][root][INFO] - Training Epoch: 2/2, step 103/574 completed (loss: 0.15250152349472046, acc: 0.9545454382896423)
[2025-01-02 00:55:18,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:18,714][root][INFO] - Training Epoch: 2/2, step 104/574 completed (loss: 0.5868111252784729, acc: 0.8620689511299133)
[2025-01-02 00:55:18,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:19,083][root][INFO] - Training Epoch: 2/2, step 105/574 completed (loss: 0.41153836250305176, acc: 0.8604651093482971)
[2025-01-02 00:55:19,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:19,418][root][INFO] - Training Epoch: 2/2, step 106/574 completed (loss: 0.3816153407096863, acc: 0.800000011920929)
[2025-01-02 00:55:19,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:19,734][root][INFO] - Training Epoch: 2/2, step 107/574 completed (loss: 0.047822773456573486, acc: 1.0)
[2025-01-02 00:55:19,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:20,134][root][INFO] - Training Epoch: 2/2, step 108/574 completed (loss: 0.08831703662872314, acc: 0.9615384340286255)
[2025-01-02 00:55:20,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:20,502][root][INFO] - Training Epoch: 2/2, step 109/574 completed (loss: 0.15895909070968628, acc: 0.9047619104385376)
[2025-01-02 00:55:20,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:20,919][root][INFO] - Training Epoch: 2/2, step 110/574 completed (loss: 0.19468580186367035, acc: 0.9230769276618958)
[2025-01-02 00:55:21,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:21,390][root][INFO] - Training Epoch: 2/2, step 111/574 completed (loss: 0.46172794699668884, acc: 0.8070175647735596)
[2025-01-02 00:55:21,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:21,810][root][INFO] - Training Epoch: 2/2, step 112/574 completed (loss: 0.8402725458145142, acc: 0.7894737124443054)
[2025-01-02 00:55:21,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:22,173][root][INFO] - Training Epoch: 2/2, step 113/574 completed (loss: 0.5420817732810974, acc: 0.8461538553237915)
[2025-01-02 00:55:22,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:22,542][root][INFO] - Training Epoch: 2/2, step 114/574 completed (loss: 0.4349680542945862, acc: 0.8979591727256775)
[2025-01-02 00:55:22,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:22,838][root][INFO] - Training Epoch: 2/2, step 115/574 completed (loss: 0.07733450829982758, acc: 1.0)
[2025-01-02 00:55:22,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:23,192][root][INFO] - Training Epoch: 2/2, step 116/574 completed (loss: 0.7487492561340332, acc: 0.8253968358039856)
[2025-01-02 00:55:23,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:23,511][root][INFO] - Training Epoch: 2/2, step 117/574 completed (loss: 0.4039965569972992, acc: 0.9024389982223511)
[2025-01-02 00:55:23,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:23,887][root][INFO] - Training Epoch: 2/2, step 118/574 completed (loss: 0.2604483962059021, acc: 0.9354838728904724)
[2025-01-02 00:55:24,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:24,786][root][INFO] - Training Epoch: 2/2, step 119/574 completed (loss: 0.6046273708343506, acc: 0.8403041958808899)
[2025-01-02 00:55:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:25,169][root][INFO] - Training Epoch: 2/2, step 120/574 completed (loss: 0.44544097781181335, acc: 0.8399999737739563)
[2025-01-02 00:55:25,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:25,586][root][INFO] - Training Epoch: 2/2, step 121/574 completed (loss: 0.4741714298725128, acc: 0.8846153616905212)
[2025-01-02 00:55:25,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:25,902][root][INFO] - Training Epoch: 2/2, step 122/574 completed (loss: 0.39088332653045654, acc: 0.7916666865348816)
[2025-01-02 00:55:26,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:26,279][root][INFO] - Training Epoch: 2/2, step 123/574 completed (loss: 0.5760103464126587, acc: 0.8421052694320679)
[2025-01-02 00:55:26,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:26,686][root][INFO] - Training Epoch: 2/2, step 124/574 completed (loss: 1.0775781869888306, acc: 0.6809815764427185)
[2025-01-02 00:55:26,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:27,114][root][INFO] - Training Epoch: 2/2, step 125/574 completed (loss: 0.9917475581169128, acc: 0.7152777910232544)
[2025-01-02 00:55:27,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:27,475][root][INFO] - Training Epoch: 2/2, step 126/574 completed (loss: 1.2260617017745972, acc: 0.6333333253860474)
[2025-01-02 00:55:27,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:27,845][root][INFO] - Training Epoch: 2/2, step 127/574 completed (loss: 0.7096078395843506, acc: 0.7916666865348816)
[2025-01-02 00:55:27,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:28,230][root][INFO] - Training Epoch: 2/2, step 128/574 completed (loss: 0.905936062335968, acc: 0.7384615540504456)
[2025-01-02 00:55:28,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:28,629][root][INFO] - Training Epoch: 2/2, step 129/574 completed (loss: 1.0428380966186523, acc: 0.6691176295280457)
[2025-01-02 00:55:28,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:28,930][root][INFO] - Training Epoch: 2/2, step 130/574 completed (loss: 0.6261478662490845, acc: 0.7307692170143127)
[2025-01-02 00:55:29,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:29,232][root][INFO] - Training Epoch: 2/2, step 131/574 completed (loss: 0.7334689497947693, acc: 0.8260869383811951)
[2025-01-02 00:55:29,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:29,536][root][INFO] - Training Epoch: 2/2, step 132/574 completed (loss: 1.0746251344680786, acc: 0.78125)
[2025-01-02 00:55:29,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:29,906][root][INFO] - Training Epoch: 2/2, step 133/574 completed (loss: 1.406457781791687, acc: 0.5652173757553101)
[2025-01-02 00:55:30,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:30,292][root][INFO] - Training Epoch: 2/2, step 134/574 completed (loss: 1.0991497039794922, acc: 0.6571428775787354)
[2025-01-02 00:55:30,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:30,612][root][INFO] - Training Epoch: 2/2, step 135/574 completed (loss: 1.2559508085250854, acc: 0.7307692170143127)
[2025-01-02 00:55:30,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:30,960][root][INFO] - Training Epoch: 2/2, step 136/574 completed (loss: 0.8341069221496582, acc: 0.738095223903656)
[2025-01-02 00:55:31,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:31,321][root][INFO] - Training Epoch: 2/2, step 137/574 completed (loss: 1.3879932165145874, acc: 0.5333333611488342)
[2025-01-02 00:55:31,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:31,679][root][INFO] - Training Epoch: 2/2, step 138/574 completed (loss: 1.0024160146713257, acc: 0.739130437374115)
[2025-01-02 00:55:31,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:32,061][root][INFO] - Training Epoch: 2/2, step 139/574 completed (loss: 0.4731098413467407, acc: 0.8571428656578064)
[2025-01-02 00:55:32,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:32,424][root][INFO] - Training Epoch: 2/2, step 140/574 completed (loss: 0.6718343496322632, acc: 0.7692307829856873)
[2025-01-02 00:55:33,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:33,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:33,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:34,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:34,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:35,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:35,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:35,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:35,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:36,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:36,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:37,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:37,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:37,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:38,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:38,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:38,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:38,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:39,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:39,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:40,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:40,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:40,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:40,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:41,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:41,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:41,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:42,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:42,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:42,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:43,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:43,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:43,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:44,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:44,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:44,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:45,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:45,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:45,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:46,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:46,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:46,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:47,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:47,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:47,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:48,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:48,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:48,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:49,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:49,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:49,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:50,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:50,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:50,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:51,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:51,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:51,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:52,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:52,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:52,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:52,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:53,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:53,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:54,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:54,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:54,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:55,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:55,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:56,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:56,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:56,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:57,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:57,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:57,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:58,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:58,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:58,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:59,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:59,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:59,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:00,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:00,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:00,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:01,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:01,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:02,047][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8478, device='cuda:0') eval_epoch_loss=tensor(0.6140, device='cuda:0') eval_epoch_acc=tensor(0.8282, device='cuda:0')
[2025-01-02 00:56:02,048][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:56:02,048][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:56:02,328][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.6139749884605408/model.pt
[2025-01-02 00:56:02,338][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:56:02,339][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6139749884605408
[2025-01-02 00:56:02,340][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8282060027122498
[2025-01-02 00:56:02,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:02,709][root][INFO] - Training Epoch: 2/2, step 141/574 completed (loss: 0.876643717288971, acc: 0.7096773982048035)
[2025-01-02 00:56:02,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:03,052][root][INFO] - Training Epoch: 2/2, step 142/574 completed (loss: 0.865031361579895, acc: 0.7027027010917664)
[2025-01-02 00:56:03,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:03,608][root][INFO] - Training Epoch: 2/2, step 143/574 completed (loss: 0.8410071730613708, acc: 0.7368420958518982)
[2025-01-02 00:56:03,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:03,958][root][INFO] - Training Epoch: 2/2, step 144/574 completed (loss: 1.0204029083251953, acc: 0.7164179086685181)
[2025-01-02 00:56:04,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:04,362][root][INFO] - Training Epoch: 2/2, step 145/574 completed (loss: 0.9282163381576538, acc: 0.6836734414100647)
[2025-01-02 00:56:04,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:04,793][root][INFO] - Training Epoch: 2/2, step 146/574 completed (loss: 1.402605414390564, acc: 0.5744680762290955)
[2025-01-02 00:56:04,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:05,081][root][INFO] - Training Epoch: 2/2, step 147/574 completed (loss: 1.02421236038208, acc: 0.699999988079071)
[2025-01-02 00:56:05,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:05,396][root][INFO] - Training Epoch: 2/2, step 148/574 completed (loss: 1.4952064752578735, acc: 0.5357142686843872)
[2025-01-02 00:56:05,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:05,772][root][INFO] - Training Epoch: 2/2, step 149/574 completed (loss: 1.1751821041107178, acc: 0.695652186870575)
[2025-01-02 00:56:05,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:06,115][root][INFO] - Training Epoch: 2/2, step 150/574 completed (loss: 0.943602979183197, acc: 0.6896551847457886)
[2025-01-02 00:56:06,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:06,469][root][INFO] - Training Epoch: 2/2, step 151/574 completed (loss: 1.2229703664779663, acc: 0.717391312122345)
[2025-01-02 00:56:06,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:06,869][root][INFO] - Training Epoch: 2/2, step 152/574 completed (loss: 0.9402305483818054, acc: 0.7118644118309021)
[2025-01-02 00:56:06,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:07,208][root][INFO] - Training Epoch: 2/2, step 153/574 completed (loss: 1.202122688293457, acc: 0.7017543911933899)
[2025-01-02 00:56:07,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:07,535][root][INFO] - Training Epoch: 2/2, step 154/574 completed (loss: 0.9407734274864197, acc: 0.7162162065505981)
[2025-01-02 00:56:07,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:07,839][root][INFO] - Training Epoch: 2/2, step 155/574 completed (loss: 0.6111810803413391, acc: 0.75)
[2025-01-02 00:56:07,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:08,192][root][INFO] - Training Epoch: 2/2, step 156/574 completed (loss: 0.8000865578651428, acc: 0.739130437374115)
[2025-01-02 00:56:08,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:08,503][root][INFO] - Training Epoch: 2/2, step 157/574 completed (loss: 2.484286069869995, acc: 0.31578946113586426)
[2025-01-02 00:56:09,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:10,139][root][INFO] - Training Epoch: 2/2, step 158/574 completed (loss: 1.4157530069351196, acc: 0.6351351141929626)
[2025-01-02 00:56:10,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:10,450][root][INFO] - Training Epoch: 2/2, step 159/574 completed (loss: 1.5808396339416504, acc: 0.5185185074806213)
[2025-01-02 00:56:10,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:10,850][root][INFO] - Training Epoch: 2/2, step 160/574 completed (loss: 1.5997896194458008, acc: 0.569767415523529)
[2025-01-02 00:56:10,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:11,430][root][INFO] - Training Epoch: 2/2, step 161/574 completed (loss: 1.7125619649887085, acc: 0.47058823704719543)
[2025-01-02 00:56:11,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:11,979][root][INFO] - Training Epoch: 2/2, step 162/574 completed (loss: 1.797761082649231, acc: 0.5730336904525757)
[2025-01-02 00:56:12,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:12,360][root][INFO] - Training Epoch: 2/2, step 163/574 completed (loss: 0.571933388710022, acc: 0.8863636255264282)
[2025-01-02 00:56:12,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:12,715][root][INFO] - Training Epoch: 2/2, step 164/574 completed (loss: 0.6185380816459656, acc: 0.8095238208770752)
[2025-01-02 00:56:12,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:13,055][root][INFO] - Training Epoch: 2/2, step 165/574 completed (loss: 1.0522539615631104, acc: 0.6206896305084229)
[2025-01-02 00:56:13,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:13,417][root][INFO] - Training Epoch: 2/2, step 166/574 completed (loss: 0.2540077865123749, acc: 0.918367326259613)
[2025-01-02 00:56:13,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:13,785][root][INFO] - Training Epoch: 2/2, step 167/574 completed (loss: 0.16915267705917358, acc: 0.9800000190734863)
[2025-01-02 00:56:13,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:14,212][root][INFO] - Training Epoch: 2/2, step 168/574 completed (loss: 0.5954102277755737, acc: 0.8472222089767456)
[2025-01-02 00:56:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:14,563][root][INFO] - Training Epoch: 2/2, step 169/574 completed (loss: 1.0577218532562256, acc: 0.7745097875595093)
[2025-01-02 00:56:14,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:15,584][root][INFO] - Training Epoch: 2/2, step 170/574 completed (loss: 0.9719147086143494, acc: 0.732876718044281)
[2025-01-02 00:56:15,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:15,892][root][INFO] - Training Epoch: 2/2, step 171/574 completed (loss: 0.23085017502307892, acc: 0.9583333134651184)
[2025-01-02 00:56:16,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:16,287][root][INFO] - Training Epoch: 2/2, step 172/574 completed (loss: 0.7605273127555847, acc: 0.7777777910232544)
[2025-01-02 00:56:16,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:16,651][root][INFO] - Training Epoch: 2/2, step 173/574 completed (loss: 0.7788206338882446, acc: 0.7857142686843872)
[2025-01-02 00:56:16,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:17,185][root][INFO] - Training Epoch: 2/2, step 174/574 completed (loss: 1.2153898477554321, acc: 0.7168141603469849)
[2025-01-02 00:56:17,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:17,519][root][INFO] - Training Epoch: 2/2, step 175/574 completed (loss: 0.7879306674003601, acc: 0.8115941882133484)
[2025-01-02 00:56:17,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:17,850][root][INFO] - Training Epoch: 2/2, step 176/574 completed (loss: 0.6231535077095032, acc: 0.7954545617103577)
[2025-01-02 00:56:18,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:18,756][root][INFO] - Training Epoch: 2/2, step 177/574 completed (loss: 1.2204844951629639, acc: 0.6412213444709778)
[2025-01-02 00:56:18,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:19,419][root][INFO] - Training Epoch: 2/2, step 178/574 completed (loss: 1.1517798900604248, acc: 0.6518518328666687)
[2025-01-02 00:56:19,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:19,769][root][INFO] - Training Epoch: 2/2, step 179/574 completed (loss: 0.7362900376319885, acc: 0.7868852615356445)
[2025-01-02 00:56:19,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:20,114][root][INFO] - Training Epoch: 2/2, step 180/574 completed (loss: 0.03454921394586563, acc: 1.0)
[2025-01-02 00:56:20,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:20,450][root][INFO] - Training Epoch: 2/2, step 181/574 completed (loss: 0.380308598279953, acc: 0.9200000166893005)
[2025-01-02 00:56:20,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:20,813][root][INFO] - Training Epoch: 2/2, step 182/574 completed (loss: 0.2701563835144043, acc: 0.9285714030265808)
[2025-01-02 00:56:20,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:21,131][root][INFO] - Training Epoch: 2/2, step 183/574 completed (loss: 0.27739056944847107, acc: 0.9024389982223511)
[2025-01-02 00:56:21,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:21,475][root][INFO] - Training Epoch: 2/2, step 184/574 completed (loss: 0.5805449485778809, acc: 0.8640483617782593)
[2025-01-02 00:56:21,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:21,780][root][INFO] - Training Epoch: 2/2, step 185/574 completed (loss: 0.4880637526512146, acc: 0.8731988668441772)
[2025-01-02 00:56:21,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:22,253][root][INFO] - Training Epoch: 2/2, step 186/574 completed (loss: 0.4484666883945465, acc: 0.862500011920929)
[2025-01-02 00:56:22,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:22,778][root][INFO] - Training Epoch: 2/2, step 187/574 completed (loss: 0.5162120461463928, acc: 0.8667917251586914)
[2025-01-02 00:56:22,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:23,188][root][INFO] - Training Epoch: 2/2, step 188/574 completed (loss: 0.5800586342811584, acc: 0.8398576378822327)
[2025-01-02 00:56:23,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:23,477][root][INFO] - Training Epoch: 2/2, step 189/574 completed (loss: 0.6601507663726807, acc: 0.800000011920929)
[2025-01-02 00:56:23,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:24,021][root][INFO] - Training Epoch: 2/2, step 190/574 completed (loss: 0.8720588088035583, acc: 0.7441860437393188)
[2025-01-02 00:56:24,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:24,808][root][INFO] - Training Epoch: 2/2, step 191/574 completed (loss: 1.3587286472320557, acc: 0.6269841194152832)
[2025-01-02 00:56:25,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:25,714][root][INFO] - Training Epoch: 2/2, step 192/574 completed (loss: 0.931972861289978, acc: 0.7348484992980957)
[2025-01-02 00:56:25,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:26,450][root][INFO] - Training Epoch: 2/2, step 193/574 completed (loss: 0.8985546231269836, acc: 0.7764706015586853)
[2025-01-02 00:56:26,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:27,526][root][INFO] - Training Epoch: 2/2, step 194/574 completed (loss: 1.0320154428482056, acc: 0.6975308656692505)
[2025-01-02 00:56:27,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:28,472][root][INFO] - Training Epoch: 2/2, step 195/574 completed (loss: 0.6293377876281738, acc: 0.7903226017951965)
[2025-01-02 00:56:28,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:28,753][root][INFO] - Training Epoch: 2/2, step 196/574 completed (loss: 0.3189895749092102, acc: 0.9285714030265808)
[2025-01-02 00:56:28,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:29,131][root][INFO] - Training Epoch: 2/2, step 197/574 completed (loss: 1.219294548034668, acc: 0.699999988079071)
[2025-01-02 00:56:29,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:29,530][root][INFO] - Training Epoch: 2/2, step 198/574 completed (loss: 1.0198333263397217, acc: 0.720588207244873)
[2025-01-02 00:56:29,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:29,904][root][INFO] - Training Epoch: 2/2, step 199/574 completed (loss: 0.9773448705673218, acc: 0.7352941036224365)
[2025-01-02 00:56:30,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:30,283][root][INFO] - Training Epoch: 2/2, step 200/574 completed (loss: 0.7358606457710266, acc: 0.7881355881690979)
[2025-01-02 00:56:30,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:30,663][root][INFO] - Training Epoch: 2/2, step 201/574 completed (loss: 1.0154271125793457, acc: 0.7388059496879578)
[2025-01-02 00:56:30,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:31,067][root][INFO] - Training Epoch: 2/2, step 202/574 completed (loss: 1.0175431966781616, acc: 0.7184466123580933)
[2025-01-02 00:56:31,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:31,393][root][INFO] - Training Epoch: 2/2, step 203/574 completed (loss: 0.8780295252799988, acc: 0.7301587462425232)
[2025-01-02 00:56:31,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:31,759][root][INFO] - Training Epoch: 2/2, step 204/574 completed (loss: 0.2467876672744751, acc: 0.9230769276618958)
[2025-01-02 00:56:31,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:32,135][root][INFO] - Training Epoch: 2/2, step 205/574 completed (loss: 0.36284470558166504, acc: 0.9013453125953674)
[2025-01-02 00:56:32,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:32,574][root][INFO] - Training Epoch: 2/2, step 206/574 completed (loss: 0.5227242708206177, acc: 0.8267716765403748)
[2025-01-02 00:56:32,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:32,936][root][INFO] - Training Epoch: 2/2, step 207/574 completed (loss: 0.43331995606422424, acc: 0.8793103694915771)
[2025-01-02 00:56:33,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:33,291][root][INFO] - Training Epoch: 2/2, step 208/574 completed (loss: 0.4911005198955536, acc: 0.8731883764266968)
[2025-01-02 00:56:33,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:33,694][root][INFO] - Training Epoch: 2/2, step 209/574 completed (loss: 0.46542298793792725, acc: 0.8638132214546204)
[2025-01-02 00:56:33,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:34,085][root][INFO] - Training Epoch: 2/2, step 210/574 completed (loss: 0.40847665071487427, acc: 0.8586956262588501)
[2025-01-02 00:56:34,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:34,459][root][INFO] - Training Epoch: 2/2, step 211/574 completed (loss: 0.37977832555770874, acc: 0.8260869383811951)
[2025-01-02 00:56:34,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:34,819][root][INFO] - Training Epoch: 2/2, step 212/574 completed (loss: 0.13551726937294006, acc: 1.0)
[2025-01-02 00:56:34,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:35,170][root][INFO] - Training Epoch: 2/2, step 213/574 completed (loss: 0.18119129538536072, acc: 0.936170220375061)
[2025-01-02 00:56:35,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:35,846][root][INFO] - Training Epoch: 2/2, step 214/574 completed (loss: 0.22623366117477417, acc: 0.9538461565971375)
[2025-01-02 00:56:35,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:36,167][root][INFO] - Training Epoch: 2/2, step 215/574 completed (loss: 0.20519381761550903, acc: 0.9459459185600281)
[2025-01-02 00:56:36,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:36,529][root][INFO] - Training Epoch: 2/2, step 216/574 completed (loss: 0.19159063696861267, acc: 0.930232584476471)
[2025-01-02 00:56:36,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:37,057][root][INFO] - Training Epoch: 2/2, step 217/574 completed (loss: 0.30070626735687256, acc: 0.8918918967247009)
[2025-01-02 00:56:37,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:37,436][root][INFO] - Training Epoch: 2/2, step 218/574 completed (loss: 0.1842557042837143, acc: 0.9444444179534912)
[2025-01-02 00:56:37,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:37,777][root][INFO] - Training Epoch: 2/2, step 219/574 completed (loss: 0.2604680359363556, acc: 0.9696969985961914)
[2025-01-02 00:56:37,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:38,134][root][INFO] - Training Epoch: 2/2, step 220/574 completed (loss: 0.16986936330795288, acc: 0.9259259104728699)
[2025-01-02 00:56:38,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:38,550][root][INFO] - Training Epoch: 2/2, step 221/574 completed (loss: 0.14676253497600555, acc: 0.9599999785423279)
[2025-01-02 00:56:38,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:38,963][root][INFO] - Training Epoch: 2/2, step 222/574 completed (loss: 0.7589895725250244, acc: 0.7884615659713745)
[2025-01-02 00:56:39,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:39,723][root][INFO] - Training Epoch: 2/2, step 223/574 completed (loss: 0.4630100131034851, acc: 0.864130437374115)
[2025-01-02 00:56:39,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:40,257][root][INFO] - Training Epoch: 2/2, step 224/574 completed (loss: 0.6081923842430115, acc: 0.8011363744735718)
[2025-01-02 00:56:40,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:40,692][root][INFO] - Training Epoch: 2/2, step 225/574 completed (loss: 0.8810501098632812, acc: 0.7659574747085571)
[2025-01-02 00:56:40,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:40,999][root][INFO] - Training Epoch: 2/2, step 226/574 completed (loss: 0.7321946024894714, acc: 0.8113207817077637)
[2025-01-02 00:56:41,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:41,364][root][INFO] - Training Epoch: 2/2, step 227/574 completed (loss: 0.524941623210907, acc: 0.8333333134651184)
[2025-01-02 00:56:41,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:41,724][root][INFO] - Training Epoch: 2/2, step 228/574 completed (loss: 0.3652336597442627, acc: 0.8837209343910217)
[2025-01-02 00:56:41,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:42,032][root][INFO] - Training Epoch: 2/2, step 229/574 completed (loss: 1.378831148147583, acc: 0.5666666626930237)
[2025-01-02 00:56:42,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:42,363][root][INFO] - Training Epoch: 2/2, step 230/574 completed (loss: 1.8728528022766113, acc: 0.5157894492149353)
[2025-01-02 00:56:42,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:42,698][root][INFO] - Training Epoch: 2/2, step 231/574 completed (loss: 1.4642080068588257, acc: 0.6222222447395325)
[2025-01-02 00:56:42,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:43,096][root][INFO] - Training Epoch: 2/2, step 232/574 completed (loss: 1.5650192499160767, acc: 0.5888888835906982)
[2025-01-02 00:56:43,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:43,579][root][INFO] - Training Epoch: 2/2, step 233/574 completed (loss: 1.9031126499176025, acc: 0.5458715558052063)
[2025-01-02 00:56:43,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:44,043][root][INFO] - Training Epoch: 2/2, step 234/574 completed (loss: 1.662280559539795, acc: 0.5384615659713745)
[2025-01-02 00:56:44,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:44,342][root][INFO] - Training Epoch: 2/2, step 235/574 completed (loss: 0.4055086076259613, acc: 0.8421052694320679)
[2025-01-02 00:56:44,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:44,689][root][INFO] - Training Epoch: 2/2, step 236/574 completed (loss: 0.5104288458824158, acc: 0.875)
[2025-01-02 00:56:44,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:45,058][root][INFO] - Training Epoch: 2/2, step 237/574 completed (loss: 1.2599656581878662, acc: 0.6818181872367859)
[2025-01-02 00:56:45,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:45,412][root][INFO] - Training Epoch: 2/2, step 238/574 completed (loss: 0.6056321263313293, acc: 0.8518518805503845)
[2025-01-02 00:56:45,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:45,787][root][INFO] - Training Epoch: 2/2, step 239/574 completed (loss: 1.006569266319275, acc: 0.6571428775787354)
[2025-01-02 00:56:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:46,205][root][INFO] - Training Epoch: 2/2, step 240/574 completed (loss: 1.218105435371399, acc: 0.7272727489471436)
[2025-01-02 00:56:46,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:46,575][root][INFO] - Training Epoch: 2/2, step 241/574 completed (loss: 0.8318269848823547, acc: 0.7045454382896423)
[2025-01-02 00:56:46,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:47,169][root][INFO] - Training Epoch: 2/2, step 242/574 completed (loss: 1.2167140245437622, acc: 0.5645161271095276)
[2025-01-02 00:56:47,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:47,690][root][INFO] - Training Epoch: 2/2, step 243/574 completed (loss: 1.3457378149032593, acc: 0.6136363744735718)
[2025-01-02 00:56:47,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:48,016][root][INFO] - Training Epoch: 2/2, step 244/574 completed (loss: 0.009466135874390602, acc: 1.0)
[2025-01-02 00:56:48,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:48,345][root][INFO] - Training Epoch: 2/2, step 245/574 completed (loss: 0.7120263576507568, acc: 0.7692307829856873)
[2025-01-02 00:56:48,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:48,685][root][INFO] - Training Epoch: 2/2, step 246/574 completed (loss: 0.1642380952835083, acc: 0.9354838728904724)
[2025-01-02 00:56:48,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:49,062][root][INFO] - Training Epoch: 2/2, step 247/574 completed (loss: 0.19824036955833435, acc: 0.949999988079071)
[2025-01-02 00:56:49,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:49,434][root][INFO] - Training Epoch: 2/2, step 248/574 completed (loss: 0.24734966456890106, acc: 0.9189189076423645)
[2025-01-02 00:56:49,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:49,785][root][INFO] - Training Epoch: 2/2, step 249/574 completed (loss: 0.3997933566570282, acc: 0.9189189076423645)
[2025-01-02 00:56:49,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:50,132][root][INFO] - Training Epoch: 2/2, step 250/574 completed (loss: 0.05795931816101074, acc: 1.0)
[2025-01-02 00:56:50,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:50,460][root][INFO] - Training Epoch: 2/2, step 251/574 completed (loss: 0.3516204059123993, acc: 0.8676470518112183)
[2025-01-02 00:56:50,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:50,800][root][INFO] - Training Epoch: 2/2, step 252/574 completed (loss: 0.19199621677398682, acc: 0.9268292784690857)
[2025-01-02 00:56:50,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:51,155][root][INFO] - Training Epoch: 2/2, step 253/574 completed (loss: 0.06284138560295105, acc: 1.0)
[2025-01-02 00:56:51,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:51,493][root][INFO] - Training Epoch: 2/2, step 254/574 completed (loss: 0.02956382930278778, acc: 1.0)
[2025-01-02 00:56:51,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:51,807][root][INFO] - Training Epoch: 2/2, step 255/574 completed (loss: 0.21800248324871063, acc: 0.9354838728904724)
[2025-01-02 00:56:51,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:52,121][root][INFO] - Training Epoch: 2/2, step 256/574 completed (loss: 0.35734009742736816, acc: 0.8947368264198303)
[2025-01-02 00:56:52,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:52,406][root][INFO] - Training Epoch: 2/2, step 257/574 completed (loss: 0.16741229593753815, acc: 0.9571428298950195)
[2025-01-02 00:56:52,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:52,706][root][INFO] - Training Epoch: 2/2, step 258/574 completed (loss: 0.21026611328125, acc: 0.9342105388641357)
[2025-01-02 00:56:52,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:53,263][root][INFO] - Training Epoch: 2/2, step 259/574 completed (loss: 0.5356441736221313, acc: 0.8773584961891174)
[2025-01-02 00:56:53,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:53,839][root][INFO] - Training Epoch: 2/2, step 260/574 completed (loss: 0.5286609530448914, acc: 0.8666666746139526)
[2025-01-02 00:56:53,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:54,126][root][INFO] - Training Epoch: 2/2, step 261/574 completed (loss: 0.24532373249530792, acc: 0.9444444179534912)
[2025-01-02 00:56:54,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:54,424][root][INFO] - Training Epoch: 2/2, step 262/574 completed (loss: 0.7299139499664307, acc: 0.8064516186714172)
[2025-01-02 00:56:54,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:54,740][root][INFO] - Training Epoch: 2/2, step 263/574 completed (loss: 1.3018015623092651, acc: 0.7200000286102295)
[2025-01-02 00:56:54,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:55,088][root][INFO] - Training Epoch: 2/2, step 264/574 completed (loss: 0.6774318814277649, acc: 0.75)
[2025-01-02 00:56:55,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:55,970][root][INFO] - Training Epoch: 2/2, step 265/574 completed (loss: 1.4481042623519897, acc: 0.6079999804496765)
[2025-01-02 00:56:56,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:56,278][root][INFO] - Training Epoch: 2/2, step 266/574 completed (loss: 1.4610896110534668, acc: 0.584269642829895)
[2025-01-02 00:56:56,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:56,609][root][INFO] - Training Epoch: 2/2, step 267/574 completed (loss: 1.0899839401245117, acc: 0.6891891956329346)
[2025-01-02 00:56:56,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:57,038][root][INFO] - Training Epoch: 2/2, step 268/574 completed (loss: 0.8556970953941345, acc: 0.7758620977401733)
[2025-01-02 00:56:57,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:57,323][root][INFO] - Training Epoch: 2/2, step 269/574 completed (loss: 0.3000393211841583, acc: 0.9545454382896423)
[2025-01-02 00:56:57,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:57,630][root][INFO] - Training Epoch: 2/2, step 270/574 completed (loss: 0.2533828020095825, acc: 0.9090909361839294)
[2025-01-02 00:56:57,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:57,918][root][INFO] - Training Epoch: 2/2, step 271/574 completed (loss: 0.1668897271156311, acc: 0.9375)
[2025-01-02 00:56:58,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:58,223][root][INFO] - Training Epoch: 2/2, step 272/574 completed (loss: 0.11101207137107849, acc: 0.9666666388511658)
[2025-01-02 00:56:58,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:58,614][root][INFO] - Training Epoch: 2/2, step 273/574 completed (loss: 0.4320007264614105, acc: 0.9166666865348816)
[2025-01-02 00:56:58,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:58,910][root][INFO] - Training Epoch: 2/2, step 274/574 completed (loss: 0.15820792317390442, acc: 0.96875)
[2025-01-02 00:56:58,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:59,246][root][INFO] - Training Epoch: 2/2, step 275/574 completed (loss: 0.40408405661582947, acc: 0.8999999761581421)
[2025-01-02 00:56:59,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:59,578][root][INFO] - Training Epoch: 2/2, step 276/574 completed (loss: 0.4310581684112549, acc: 0.931034505367279)
[2025-01-02 00:56:59,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:56:59,866][root][INFO] - Training Epoch: 2/2, step 277/574 completed (loss: 0.3124639391899109, acc: 0.9200000166893005)
[2025-01-02 00:56:59,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:00,222][root][INFO] - Training Epoch: 2/2, step 278/574 completed (loss: 0.3710465431213379, acc: 0.8936170339584351)
[2025-01-02 00:57:00,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:00,551][root][INFO] - Training Epoch: 2/2, step 279/574 completed (loss: 0.5788417458534241, acc: 0.8958333134651184)
[2025-01-02 00:57:00,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:00,901][root][INFO] - Training Epoch: 2/2, step 280/574 completed (loss: 0.18992438912391663, acc: 0.9545454382896423)
[2025-01-02 00:57:01,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:01,323][root][INFO] - Training Epoch: 2/2, step 281/574 completed (loss: 0.9850272536277771, acc: 0.6746987700462341)
[2025-01-02 00:57:01,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:01,663][root][INFO] - Training Epoch: 2/2, step 282/574 completed (loss: 0.926472008228302, acc: 0.7407407164573669)
[2025-01-02 00:57:01,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:01,934][root][INFO] - Training Epoch: 2/2, step 283/574 completed (loss: 0.1294594705104828, acc: 0.9736841917037964)
[2025-01-02 00:57:02,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:03,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:03,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:03,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:03,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:04,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:04,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:05,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:05,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:05,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:06,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:06,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:07,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:07,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:07,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:08,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:08,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:08,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:09,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:09,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:09,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:10,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:10,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:10,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:10,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:11,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:11,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:11,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:12,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:12,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:12,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:13,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:13,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:13,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:13,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:14,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:14,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:14,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:15,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:15,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:15,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:16,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:16,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:16,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:16,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:17,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:17,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:17,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:18,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:18,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:19,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:19,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:19,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:19,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:20,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:20,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:20,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:21,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:21,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:21,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:22,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:22,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:22,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:23,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:23,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:24,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:24,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:24,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:25,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:25,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:26,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:26,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:27,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:27,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:27,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:28,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:28,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:28,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:29,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:29,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:29,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:30,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:30,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:31,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:31,759][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7495, device='cuda:0') eval_epoch_loss=tensor(0.5593, device='cuda:0') eval_epoch_acc=tensor(0.8443, device='cuda:0')
[2025-01-02 00:57:31,761][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:57:31,761][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:57:32,103][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5593151450157166/model.pt
[2025-01-02 00:57:32,110][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:57:32,111][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.5593151450157166
[2025-01-02 00:57:32,112][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.844327449798584
[2025-01-02 00:57:32,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:32,504][root][INFO] - Training Epoch: 2/2, step 284/574 completed (loss: 0.5412508845329285, acc: 0.7647058963775635)
[2025-01-02 00:57:32,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:32,779][root][INFO] - Training Epoch: 2/2, step 285/574 completed (loss: 0.3323267102241516, acc: 0.925000011920929)
[2025-01-02 00:57:32,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:33,087][root][INFO] - Training Epoch: 2/2, step 286/574 completed (loss: 0.4929717481136322, acc: 0.84375)
[2025-01-02 00:57:33,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:33,401][root][INFO] - Training Epoch: 2/2, step 287/574 completed (loss: 0.5930265784263611, acc: 0.8479999899864197)
[2025-01-02 00:57:33,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:33,694][root][INFO] - Training Epoch: 2/2, step 288/574 completed (loss: 0.39059311151504517, acc: 0.8791208863258362)
[2025-01-02 00:57:33,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:34,033][root][INFO] - Training Epoch: 2/2, step 289/574 completed (loss: 0.5143517255783081, acc: 0.8198757767677307)
[2025-01-02 00:57:34,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:34,385][root][INFO] - Training Epoch: 2/2, step 290/574 completed (loss: 0.5462695956230164, acc: 0.8711340427398682)
[2025-01-02 00:57:34,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:34,729][root][INFO] - Training Epoch: 2/2, step 291/574 completed (loss: 0.03106418065726757, acc: 1.0)
[2025-01-02 00:57:34,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:35,098][root][INFO] - Training Epoch: 2/2, step 292/574 completed (loss: 0.5216459035873413, acc: 0.7857142686843872)
[2025-01-02 00:57:35,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:35,519][root][INFO] - Training Epoch: 2/2, step 293/574 completed (loss: 0.15010055899620056, acc: 0.982758641242981)
[2025-01-02 00:57:35,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:35,998][root][INFO] - Training Epoch: 2/2, step 294/574 completed (loss: 0.6668810844421387, acc: 0.8181818127632141)
[2025-01-02 00:57:36,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:36,556][root][INFO] - Training Epoch: 2/2, step 295/574 completed (loss: 0.5098810791969299, acc: 0.8659793734550476)
[2025-01-02 00:57:36,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:36,935][root][INFO] - Training Epoch: 2/2, step 296/574 completed (loss: 0.4709482192993164, acc: 0.8275862336158752)
[2025-01-02 00:57:37,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:37,318][root][INFO] - Training Epoch: 2/2, step 297/574 completed (loss: 0.1292087882757187, acc: 0.9629629850387573)
[2025-01-02 00:57:37,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:37,680][root][INFO] - Training Epoch: 2/2, step 298/574 completed (loss: 0.6469883918762207, acc: 0.8421052694320679)
[2025-01-02 00:57:37,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:37,982][root][INFO] - Training Epoch: 2/2, step 299/574 completed (loss: 0.07572179287672043, acc: 1.0)
[2025-01-02 00:57:38,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:38,366][root][INFO] - Training Epoch: 2/2, step 300/574 completed (loss: 0.08445451408624649, acc: 0.96875)
[2025-01-02 00:57:38,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:38,759][root][INFO] - Training Epoch: 2/2, step 301/574 completed (loss: 0.4532957375049591, acc: 0.9056603908538818)
[2025-01-02 00:57:38,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:39,084][root][INFO] - Training Epoch: 2/2, step 302/574 completed (loss: 0.05788774415850639, acc: 0.9622641801834106)
[2025-01-02 00:57:39,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:39,437][root][INFO] - Training Epoch: 2/2, step 303/574 completed (loss: 0.129244863986969, acc: 0.970588207244873)
[2025-01-02 00:57:39,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:39,739][root][INFO] - Training Epoch: 2/2, step 304/574 completed (loss: 0.33557045459747314, acc: 0.90625)
[2025-01-02 00:57:39,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:40,069][root][INFO] - Training Epoch: 2/2, step 305/574 completed (loss: 0.4933326542377472, acc: 0.868852436542511)
[2025-01-02 00:57:40,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:40,463][root][INFO] - Training Epoch: 2/2, step 306/574 completed (loss: 0.14449195563793182, acc: 0.9333333373069763)
[2025-01-02 00:57:40,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:40,794][root][INFO] - Training Epoch: 2/2, step 307/574 completed (loss: 0.11083944141864777, acc: 0.9473684430122375)
[2025-01-02 00:57:40,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:41,183][root][INFO] - Training Epoch: 2/2, step 308/574 completed (loss: 0.33412501215934753, acc: 0.8985507488250732)
[2025-01-02 00:57:41,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:41,662][root][INFO] - Training Epoch: 2/2, step 309/574 completed (loss: 0.2021215409040451, acc: 0.9444444179534912)
[2025-01-02 00:57:41,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:42,050][root][INFO] - Training Epoch: 2/2, step 310/574 completed (loss: 0.19727279245853424, acc: 0.9759036302566528)
[2025-01-02 00:57:42,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:42,393][root][INFO] - Training Epoch: 2/2, step 311/574 completed (loss: 0.4019736051559448, acc: 0.8461538553237915)
[2025-01-02 00:57:42,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:42,735][root][INFO] - Training Epoch: 2/2, step 312/574 completed (loss: 0.16015344858169556, acc: 0.9489796161651611)
[2025-01-02 00:57:42,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:43,068][root][INFO] - Training Epoch: 2/2, step 313/574 completed (loss: 0.025543212890625, acc: 1.0)
[2025-01-02 00:57:43,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:43,403][root][INFO] - Training Epoch: 2/2, step 314/574 completed (loss: 0.07799387723207474, acc: 0.9583333134651184)
[2025-01-02 00:57:43,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:43,729][root][INFO] - Training Epoch: 2/2, step 315/574 completed (loss: 0.40926092863082886, acc: 0.9032257795333862)
[2025-01-02 00:57:43,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:44,087][root][INFO] - Training Epoch: 2/2, step 316/574 completed (loss: 0.5109040141105652, acc: 0.8709677457809448)
[2025-01-02 00:57:44,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:44,509][root][INFO] - Training Epoch: 2/2, step 317/574 completed (loss: 0.28331464529037476, acc: 0.9253731369972229)
[2025-01-02 00:57:44,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:44,883][root][INFO] - Training Epoch: 2/2, step 318/574 completed (loss: 0.12045420706272125, acc: 0.9711538553237915)
[2025-01-02 00:57:44,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:45,262][root][INFO] - Training Epoch: 2/2, step 319/574 completed (loss: 0.2532806098461151, acc: 0.9333333373069763)
[2025-01-02 00:57:45,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:45,622][root][INFO] - Training Epoch: 2/2, step 320/574 completed (loss: 0.14428399503231049, acc: 0.9516128897666931)
[2025-01-02 00:57:45,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:45,950][root][INFO] - Training Epoch: 2/2, step 321/574 completed (loss: 0.059866178780794144, acc: 1.0)
[2025-01-02 00:57:46,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:46,301][root][INFO] - Training Epoch: 2/2, step 322/574 completed (loss: 0.8333402276039124, acc: 0.7037037014961243)
[2025-01-02 00:57:46,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:46,634][root][INFO] - Training Epoch: 2/2, step 323/574 completed (loss: 1.6227775812149048, acc: 0.5714285969734192)
[2025-01-02 00:57:46,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:46,950][root][INFO] - Training Epoch: 2/2, step 324/574 completed (loss: 1.2547047138214111, acc: 0.7435897588729858)
[2025-01-02 00:57:47,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:47,259][root][INFO] - Training Epoch: 2/2, step 325/574 completed (loss: 1.615490436553955, acc: 0.5121951103210449)
[2025-01-02 00:57:47,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:47,685][root][INFO] - Training Epoch: 2/2, step 326/574 completed (loss: 1.1688216924667358, acc: 0.6578947305679321)
[2025-01-02 00:57:47,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:48,046][root][INFO] - Training Epoch: 2/2, step 327/574 completed (loss: 0.5695875883102417, acc: 0.8421052694320679)
[2025-01-02 00:57:48,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:48,388][root][INFO] - Training Epoch: 2/2, step 328/574 completed (loss: 0.24347780644893646, acc: 0.9285714030265808)
[2025-01-02 00:57:48,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:48,707][root][INFO] - Training Epoch: 2/2, step 329/574 completed (loss: 0.3918660581111908, acc: 0.8888888955116272)
[2025-01-02 00:57:48,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:49,067][root][INFO] - Training Epoch: 2/2, step 330/574 completed (loss: 0.1340237259864807, acc: 0.96875)
[2025-01-02 00:57:49,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:49,364][root][INFO] - Training Epoch: 2/2, step 331/574 completed (loss: 0.28814584016799927, acc: 0.9516128897666931)
[2025-01-02 00:57:49,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:49,754][root][INFO] - Training Epoch: 2/2, step 332/574 completed (loss: 0.18408173322677612, acc: 0.9649122953414917)
[2025-01-02 00:57:49,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:50,111][root][INFO] - Training Epoch: 2/2, step 333/574 completed (loss: 0.1851252168416977, acc: 0.90625)
[2025-01-02 00:57:50,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:50,455][root][INFO] - Training Epoch: 2/2, step 334/574 completed (loss: 0.15855376422405243, acc: 0.9666666388511658)
[2025-01-02 00:57:50,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:50,759][root][INFO] - Training Epoch: 2/2, step 335/574 completed (loss: 0.4918799102306366, acc: 0.7894737124443054)
[2025-01-02 00:57:50,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:51,067][root][INFO] - Training Epoch: 2/2, step 336/574 completed (loss: 1.029370903968811, acc: 0.7599999904632568)
[2025-01-02 00:57:51,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:51,467][root][INFO] - Training Epoch: 2/2, step 337/574 completed (loss: 1.3373278379440308, acc: 0.6666666865348816)
[2025-01-02 00:57:51,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:51,834][root][INFO] - Training Epoch: 2/2, step 338/574 completed (loss: 1.4397650957107544, acc: 0.5531914830207825)
[2025-01-02 00:57:51,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:52,190][root][INFO] - Training Epoch: 2/2, step 339/574 completed (loss: 1.3831828832626343, acc: 0.6265060305595398)
[2025-01-02 00:57:52,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:52,552][root][INFO] - Training Epoch: 2/2, step 340/574 completed (loss: 0.11621300876140594, acc: 0.95652174949646)
[2025-01-02 00:57:52,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:52,915][root][INFO] - Training Epoch: 2/2, step 341/574 completed (loss: 0.546983003616333, acc: 0.8461538553237915)
[2025-01-02 00:57:53,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:53,234][root][INFO] - Training Epoch: 2/2, step 342/574 completed (loss: 0.4907447099685669, acc: 0.8795180916786194)
[2025-01-02 00:57:53,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:53,575][root][INFO] - Training Epoch: 2/2, step 343/574 completed (loss: 0.599746823310852, acc: 0.7924528121948242)
[2025-01-02 00:57:53,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:53,870][root][INFO] - Training Epoch: 2/2, step 344/574 completed (loss: 0.2285204976797104, acc: 0.9240506291389465)
[2025-01-02 00:57:53,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:54,179][root][INFO] - Training Epoch: 2/2, step 345/574 completed (loss: 0.11796701699495316, acc: 0.9607843160629272)
[2025-01-02 00:57:54,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:54,523][root][INFO] - Training Epoch: 2/2, step 346/574 completed (loss: 0.42618557810783386, acc: 0.8805969953536987)
[2025-01-02 00:57:54,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:54,851][root][INFO] - Training Epoch: 2/2, step 347/574 completed (loss: 0.020976422354578972, acc: 1.0)
[2025-01-02 00:57:54,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:55,184][root][INFO] - Training Epoch: 2/2, step 348/574 completed (loss: 0.28896164894104004, acc: 0.8799999952316284)
[2025-01-02 00:57:55,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:55,624][root][INFO] - Training Epoch: 2/2, step 349/574 completed (loss: 0.8533537983894348, acc: 0.7222222089767456)
[2025-01-02 00:57:55,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:55,962][root][INFO] - Training Epoch: 2/2, step 350/574 completed (loss: 0.7766033411026001, acc: 0.6976743936538696)
[2025-01-02 00:57:56,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:56,287][root][INFO] - Training Epoch: 2/2, step 351/574 completed (loss: 0.20596560835838318, acc: 0.9743589758872986)
[2025-01-02 00:57:56,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:56,664][root][INFO] - Training Epoch: 2/2, step 352/574 completed (loss: 0.915604293346405, acc: 0.7111111283302307)
[2025-01-02 00:57:56,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:57,004][root][INFO] - Training Epoch: 2/2, step 353/574 completed (loss: 0.03768797963857651, acc: 1.0)
[2025-01-02 00:57:57,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:57,298][root][INFO] - Training Epoch: 2/2, step 354/574 completed (loss: 0.5837283730506897, acc: 0.807692289352417)
[2025-01-02 00:57:57,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:57,619][root][INFO] - Training Epoch: 2/2, step 355/574 completed (loss: 0.8552464246749878, acc: 0.7582417726516724)
[2025-01-02 00:57:57,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:58,114][root][INFO] - Training Epoch: 2/2, step 356/574 completed (loss: 0.7262927293777466, acc: 0.782608687877655)
[2025-01-02 00:57:58,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:58,490][root][INFO] - Training Epoch: 2/2, step 357/574 completed (loss: 0.6262885928153992, acc: 0.8152173757553101)
[2025-01-02 00:57:58,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:58,832][root][INFO] - Training Epoch: 2/2, step 358/574 completed (loss: 0.7661339640617371, acc: 0.7551020383834839)
[2025-01-02 00:57:58,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:59,171][root][INFO] - Training Epoch: 2/2, step 359/574 completed (loss: 0.009196952916681767, acc: 1.0)
[2025-01-02 00:57:59,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:59,558][root][INFO] - Training Epoch: 2/2, step 360/574 completed (loss: 0.2981449365615845, acc: 0.9230769276618958)
[2025-01-02 00:57:59,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:57:59,891][root][INFO] - Training Epoch: 2/2, step 361/574 completed (loss: 0.46566247940063477, acc: 0.8536585569381714)
[2025-01-02 00:58:00,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:00,271][root][INFO] - Training Epoch: 2/2, step 362/574 completed (loss: 0.2720329463481903, acc: 0.9333333373069763)
[2025-01-02 00:58:00,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:00,614][root][INFO] - Training Epoch: 2/2, step 363/574 completed (loss: 0.1544230431318283, acc: 0.9736841917037964)
[2025-01-02 00:58:00,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:00,968][root][INFO] - Training Epoch: 2/2, step 364/574 completed (loss: 0.24227987229824066, acc: 0.8780487775802612)
[2025-01-02 00:58:01,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:01,306][root][INFO] - Training Epoch: 2/2, step 365/574 completed (loss: 0.18872585892677307, acc: 0.939393937587738)
[2025-01-02 00:58:01,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:01,643][root][INFO] - Training Epoch: 2/2, step 366/574 completed (loss: 0.021450510248541832, acc: 1.0)
[2025-01-02 00:58:01,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:02,007][root][INFO] - Training Epoch: 2/2, step 367/574 completed (loss: 0.05652664974331856, acc: 1.0)
[2025-01-02 00:58:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:02,366][root][INFO] - Training Epoch: 2/2, step 368/574 completed (loss: 0.22049497067928314, acc: 0.9642857313156128)
[2025-01-02 00:58:02,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:02,696][root][INFO] - Training Epoch: 2/2, step 369/574 completed (loss: 0.22020459175109863, acc: 0.90625)
[2025-01-02 00:58:02,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:03,309][root][INFO] - Training Epoch: 2/2, step 370/574 completed (loss: 0.44379276037216187, acc: 0.8606060743331909)
[2025-01-02 00:58:03,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:04,121][root][INFO] - Training Epoch: 2/2, step 371/574 completed (loss: 0.3554072380065918, acc: 0.8962264060974121)
[2025-01-02 00:58:04,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:04,418][root][INFO] - Training Epoch: 2/2, step 372/574 completed (loss: 0.2105870395898819, acc: 0.9222221970558167)
[2025-01-02 00:58:04,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:04,761][root][INFO] - Training Epoch: 2/2, step 373/574 completed (loss: 0.31189194321632385, acc: 0.9642857313156128)
[2025-01-02 00:58:04,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:05,162][root][INFO] - Training Epoch: 2/2, step 374/574 completed (loss: 0.17113985121250153, acc: 0.9142857193946838)
[2025-01-02 00:58:05,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:05,515][root][INFO] - Training Epoch: 2/2, step 375/574 completed (loss: 0.0034413819666951895, acc: 1.0)
[2025-01-02 00:58:05,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:05,820][root][INFO] - Training Epoch: 2/2, step 376/574 completed (loss: 0.015287349000573158, acc: 1.0)
[2025-01-02 00:58:05,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:06,149][root][INFO] - Training Epoch: 2/2, step 377/574 completed (loss: 0.23654256761074066, acc: 0.9375)
[2025-01-02 00:58:06,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:06,463][root][INFO] - Training Epoch: 2/2, step 378/574 completed (loss: 0.03585712984204292, acc: 0.9789473414421082)
[2025-01-02 00:58:06,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:07,032][root][INFO] - Training Epoch: 2/2, step 379/574 completed (loss: 0.2805216312408447, acc: 0.910179615020752)
[2025-01-02 00:58:07,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:07,474][root][INFO] - Training Epoch: 2/2, step 380/574 completed (loss: 0.3818877339363098, acc: 0.9172932505607605)
[2025-01-02 00:58:07,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:08,694][root][INFO] - Training Epoch: 2/2, step 381/574 completed (loss: 0.6781755089759827, acc: 0.8395721912384033)
[2025-01-02 00:58:08,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:09,253][root][INFO] - Training Epoch: 2/2, step 382/574 completed (loss: 0.11074457317590714, acc: 0.9639639854431152)
[2025-01-02 00:58:09,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:09,544][root][INFO] - Training Epoch: 2/2, step 383/574 completed (loss: 0.494294673204422, acc: 0.8928571343421936)
[2025-01-02 00:58:09,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:09,877][root][INFO] - Training Epoch: 2/2, step 384/574 completed (loss: 0.04991251230239868, acc: 0.9642857313156128)
[2025-01-02 00:58:09,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:10,279][root][INFO] - Training Epoch: 2/2, step 385/574 completed (loss: 0.19041451811790466, acc: 0.96875)
[2025-01-02 00:58:10,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:10,611][root][INFO] - Training Epoch: 2/2, step 386/574 completed (loss: 0.0318920724093914, acc: 0.9722222089767456)
[2025-01-02 00:58:10,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:10,928][root][INFO] - Training Epoch: 2/2, step 387/574 completed (loss: 0.044182512909173965, acc: 0.9736841917037964)
[2025-01-02 00:58:11,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:11,232][root][INFO] - Training Epoch: 2/2, step 388/574 completed (loss: 0.03280426189303398, acc: 1.0)
[2025-01-02 00:58:11,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:11,567][root][INFO] - Training Epoch: 2/2, step 389/574 completed (loss: 0.008810734376311302, acc: 1.0)
[2025-01-02 00:58:11,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:11,882][root][INFO] - Training Epoch: 2/2, step 390/574 completed (loss: 0.29876312613487244, acc: 0.9047619104385376)
[2025-01-02 00:58:12,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:12,242][root][INFO] - Training Epoch: 2/2, step 391/574 completed (loss: 1.1517484188079834, acc: 0.6666666865348816)
[2025-01-02 00:58:12,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:12,642][root][INFO] - Training Epoch: 2/2, step 392/574 completed (loss: 0.9683669209480286, acc: 0.7572815418243408)
[2025-01-02 00:58:12,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:13,160][root][INFO] - Training Epoch: 2/2, step 393/574 completed (loss: 1.1713297367095947, acc: 0.7720588445663452)
[2025-01-02 00:58:13,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:13,513][root][INFO] - Training Epoch: 2/2, step 394/574 completed (loss: 0.8272575736045837, acc: 0.753333330154419)
[2025-01-02 00:58:13,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:13,961][root][INFO] - Training Epoch: 2/2, step 395/574 completed (loss: 0.8937017917633057, acc: 0.7430555820465088)
[2025-01-02 00:58:14,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:14,338][root][INFO] - Training Epoch: 2/2, step 396/574 completed (loss: 0.6322294473648071, acc: 0.8139534592628479)
[2025-01-02 00:58:14,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:14,729][root][INFO] - Training Epoch: 2/2, step 397/574 completed (loss: 0.22082293033599854, acc: 0.875)
[2025-01-02 00:58:14,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:15,087][root][INFO] - Training Epoch: 2/2, step 398/574 completed (loss: 0.3559688627719879, acc: 0.8604651093482971)
[2025-01-02 00:58:15,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:15,394][root][INFO] - Training Epoch: 2/2, step 399/574 completed (loss: 0.08478705585002899, acc: 1.0)
[2025-01-02 00:58:15,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:15,919][root][INFO] - Training Epoch: 2/2, step 400/574 completed (loss: 0.3679804801940918, acc: 0.8970588445663452)
[2025-01-02 00:58:16,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:16,202][root][INFO] - Training Epoch: 2/2, step 401/574 completed (loss: 0.710224986076355, acc: 0.8266666531562805)
[2025-01-02 00:58:16,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:16,453][root][INFO] - Training Epoch: 2/2, step 402/574 completed (loss: 0.42213860154151917, acc: 0.8787878751754761)
[2025-01-02 00:58:16,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:16,735][root][INFO] - Training Epoch: 2/2, step 403/574 completed (loss: 0.3964497148990631, acc: 0.8181818127632141)
[2025-01-02 00:58:16,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:17,111][root][INFO] - Training Epoch: 2/2, step 404/574 completed (loss: 0.14436747133731842, acc: 1.0)
[2025-01-02 00:58:17,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:17,453][root][INFO] - Training Epoch: 2/2, step 405/574 completed (loss: 0.20223215222358704, acc: 0.9259259104728699)
[2025-01-02 00:58:17,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:17,827][root][INFO] - Training Epoch: 2/2, step 406/574 completed (loss: 0.16438227891921997, acc: 0.9200000166893005)
[2025-01-02 00:58:17,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:18,182][root][INFO] - Training Epoch: 2/2, step 407/574 completed (loss: 0.10852836072444916, acc: 0.9722222089767456)
[2025-01-02 00:58:18,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:18,480][root][INFO] - Training Epoch: 2/2, step 408/574 completed (loss: 0.21954546868801117, acc: 0.8888888955116272)
[2025-01-02 00:58:18,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:18,843][root][INFO] - Training Epoch: 2/2, step 409/574 completed (loss: 0.10143221169710159, acc: 1.0)
[2025-01-02 00:58:18,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:19,214][root][INFO] - Training Epoch: 2/2, step 410/574 completed (loss: 0.11005029827356339, acc: 0.982758641242981)
[2025-01-02 00:58:19,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:19,529][root][INFO] - Training Epoch: 2/2, step 411/574 completed (loss: 0.03669329360127449, acc: 1.0)
[2025-01-02 00:58:19,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:19,892][root][INFO] - Training Epoch: 2/2, step 412/574 completed (loss: 0.07065154612064362, acc: 1.0)
[2025-01-02 00:58:20,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:20,288][root][INFO] - Training Epoch: 2/2, step 413/574 completed (loss: 0.21374410390853882, acc: 0.9696969985961914)
[2025-01-02 00:58:20,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:20,643][root][INFO] - Training Epoch: 2/2, step 414/574 completed (loss: 0.04287725314497948, acc: 1.0)
[2025-01-02 00:58:20,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:20,998][root][INFO] - Training Epoch: 2/2, step 415/574 completed (loss: 0.4165697991847992, acc: 0.8823529481887817)
[2025-01-02 00:58:21,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:21,406][root][INFO] - Training Epoch: 2/2, step 416/574 completed (loss: 0.2903718948364258, acc: 0.8846153616905212)
[2025-01-02 00:58:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:21,773][root][INFO] - Training Epoch: 2/2, step 417/574 completed (loss: 0.21097750961780548, acc: 0.8888888955116272)
[2025-01-02 00:58:21,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:22,147][root][INFO] - Training Epoch: 2/2, step 418/574 completed (loss: 0.2829020619392395, acc: 0.949999988079071)
[2025-01-02 00:58:22,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:22,462][root][INFO] - Training Epoch: 2/2, step 419/574 completed (loss: 0.1631116271018982, acc: 0.949999988079071)
[2025-01-02 00:58:22,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:22,866][root][INFO] - Training Epoch: 2/2, step 420/574 completed (loss: 0.13423819839954376, acc: 1.0)
[2025-01-02 00:58:22,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:23,244][root][INFO] - Training Epoch: 2/2, step 421/574 completed (loss: 0.21517053246498108, acc: 0.9666666388511658)
[2025-01-02 00:58:23,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:23,637][root][INFO] - Training Epoch: 2/2, step 422/574 completed (loss: 0.316868394613266, acc: 0.875)
[2025-01-02 00:58:23,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:24,001][root][INFO] - Training Epoch: 2/2, step 423/574 completed (loss: 0.44161006808280945, acc: 0.8611111044883728)
[2025-01-02 00:58:24,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:24,302][root][INFO] - Training Epoch: 2/2, step 424/574 completed (loss: 0.25888392329216003, acc: 0.9629629850387573)
[2025-01-02 00:58:24,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:24,628][root][INFO] - Training Epoch: 2/2, step 425/574 completed (loss: 0.12247422337532043, acc: 0.939393937587738)
[2025-01-02 00:58:24,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:24,969][root][INFO] - Training Epoch: 2/2, step 426/574 completed (loss: 0.011697228997945786, acc: 1.0)
[2025-01-02 00:58:25,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:26,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:26,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:26,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:27,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:27,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:27,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:28,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:28,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:29,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:29,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:29,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:30,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:30,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:31,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:31,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:31,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:32,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:32,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:33,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:33,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:33,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:34,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:34,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:34,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:35,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:35,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:35,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:36,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:36,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:36,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:37,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:37,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:37,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:38,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:38,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:38,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:39,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:39,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:40,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:40,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:40,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:40,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:41,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:41,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:41,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:42,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:42,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:42,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:43,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:43,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:43,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:44,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:44,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:44,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:45,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:45,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:45,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:46,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:46,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:47,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:47,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:47,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:48,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:48,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:48,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:49,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:49,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:50,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:50,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:50,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:51,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:51,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:51,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:51,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:52,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:52,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:52,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:53,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:53,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:53,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:54,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:54,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:54,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:55,637][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7664, device='cuda:0') eval_epoch_loss=tensor(0.5690, device='cuda:0') eval_epoch_acc=tensor(0.8503, device='cuda:0')
[2025-01-02 00:58:55,639][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:58:55,639][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:58:56,015][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.5689716935157776/model.pt
[2025-01-02 00:58:56,020][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:58:56,020][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8503390550613403
[2025-01-02 00:58:56,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:56,506][root][INFO] - Training Epoch: 2/2, step 427/574 completed (loss: 0.16441889107227325, acc: 0.9189189076423645)
[2025-01-02 00:58:56,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:56,835][root][INFO] - Training Epoch: 2/2, step 428/574 completed (loss: 0.018822001293301582, acc: 1.0)
[2025-01-02 00:58:56,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:57,121][root][INFO] - Training Epoch: 2/2, step 429/574 completed (loss: 0.15422631800174713, acc: 0.95652174949646)
[2025-01-02 00:58:57,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:57,421][root][INFO] - Training Epoch: 2/2, step 430/574 completed (loss: 0.004581226967275143, acc: 1.0)
[2025-01-02 00:58:57,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:57,795][root][INFO] - Training Epoch: 2/2, step 431/574 completed (loss: 0.008228803984820843, acc: 1.0)
[2025-01-02 00:58:57,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:58,121][root][INFO] - Training Epoch: 2/2, step 432/574 completed (loss: 0.08340184390544891, acc: 0.95652174949646)
[2025-01-02 00:58:58,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:58,539][root][INFO] - Training Epoch: 2/2, step 433/574 completed (loss: 0.2923888564109802, acc: 0.8888888955116272)
[2025-01-02 00:58:58,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:58,935][root][INFO] - Training Epoch: 2/2, step 434/574 completed (loss: 0.003220687620341778, acc: 1.0)
[2025-01-02 00:58:59,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:59,346][root][INFO] - Training Epoch: 2/2, step 435/574 completed (loss: 0.05148453637957573, acc: 0.9696969985961914)
[2025-01-02 00:58:59,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:58:59,737][root][INFO] - Training Epoch: 2/2, step 436/574 completed (loss: 0.24264150857925415, acc: 0.9166666865348816)
[2025-01-02 00:58:59,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:00,135][root][INFO] - Training Epoch: 2/2, step 437/574 completed (loss: 0.02539062686264515, acc: 1.0)
[2025-01-02 00:59:00,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:00,534][root][INFO] - Training Epoch: 2/2, step 438/574 completed (loss: 0.006714326795190573, acc: 1.0)
[2025-01-02 00:59:00,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:00,856][root][INFO] - Training Epoch: 2/2, step 439/574 completed (loss: 0.4427894055843353, acc: 0.8974359035491943)
[2025-01-02 00:59:01,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:01,326][root][INFO] - Training Epoch: 2/2, step 440/574 completed (loss: 0.5345101356506348, acc: 0.8787878751754761)
[2025-01-02 00:59:01,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:02,016][root][INFO] - Training Epoch: 2/2, step 441/574 completed (loss: 0.6568292379379272, acc: 0.8080000281333923)
[2025-01-02 00:59:02,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:02,414][root][INFO] - Training Epoch: 2/2, step 442/574 completed (loss: 0.8061410784721375, acc: 0.7903226017951965)
[2025-01-02 00:59:02,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:03,059][root][INFO] - Training Epoch: 2/2, step 443/574 completed (loss: 0.4390260577201843, acc: 0.8706467747688293)
[2025-01-02 00:59:03,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:03,444][root][INFO] - Training Epoch: 2/2, step 444/574 completed (loss: 0.1368190497159958, acc: 0.9622641801834106)
[2025-01-02 00:59:03,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:03,872][root][INFO] - Training Epoch: 2/2, step 445/574 completed (loss: 0.2512086033821106, acc: 0.8863636255264282)
[2025-01-02 00:59:03,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:04,251][root][INFO] - Training Epoch: 2/2, step 446/574 completed (loss: 0.45468205213546753, acc: 0.9130434989929199)
[2025-01-02 00:59:04,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:04,509][root][INFO] - Training Epoch: 2/2, step 447/574 completed (loss: 0.41466382145881653, acc: 0.9230769276618958)
[2025-01-02 00:59:04,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:04,783][root][INFO] - Training Epoch: 2/2, step 448/574 completed (loss: 0.17519208788871765, acc: 0.9642857313156128)
[2025-01-02 00:59:04,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:05,111][root][INFO] - Training Epoch: 2/2, step 449/574 completed (loss: 0.1362098753452301, acc: 0.9850746393203735)
[2025-01-02 00:59:05,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:05,468][root][INFO] - Training Epoch: 2/2, step 450/574 completed (loss: 0.10064470022916794, acc: 0.9861111044883728)
[2025-01-02 00:59:05,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:05,863][root][INFO] - Training Epoch: 2/2, step 451/574 completed (loss: 0.08846772462129593, acc: 0.95652174949646)
[2025-01-02 00:59:05,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:06,210][root][INFO] - Training Epoch: 2/2, step 452/574 completed (loss: 0.1853947937488556, acc: 0.9358974099159241)
[2025-01-02 00:59:06,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:06,585][root][INFO] - Training Epoch: 2/2, step 453/574 completed (loss: 0.2779460847377777, acc: 0.9078947305679321)
[2025-01-02 00:59:06,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:07,000][root][INFO] - Training Epoch: 2/2, step 454/574 completed (loss: 0.10563186556100845, acc: 0.9795918464660645)
[2025-01-02 00:59:07,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:07,405][root][INFO] - Training Epoch: 2/2, step 455/574 completed (loss: 0.19199271500110626, acc: 0.939393937587738)
[2025-01-02 00:59:07,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:07,734][root][INFO] - Training Epoch: 2/2, step 456/574 completed (loss: 0.6238383650779724, acc: 0.8350515365600586)
[2025-01-02 00:59:07,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:08,007][root][INFO] - Training Epoch: 2/2, step 457/574 completed (loss: 0.028229277580976486, acc: 0.9857142567634583)
[2025-01-02 00:59:08,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:08,395][root][INFO] - Training Epoch: 2/2, step 458/574 completed (loss: 0.330547958612442, acc: 0.9069767594337463)
[2025-01-02 00:59:08,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:08,711][root][INFO] - Training Epoch: 2/2, step 459/574 completed (loss: 0.048510972410440445, acc: 1.0)
[2025-01-02 00:59:08,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:09,097][root][INFO] - Training Epoch: 2/2, step 460/574 completed (loss: 0.22861196100711823, acc: 0.9382715821266174)
[2025-01-02 00:59:09,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:09,426][root][INFO] - Training Epoch: 2/2, step 461/574 completed (loss: 0.31957176327705383, acc: 0.8611111044883728)
[2025-01-02 00:59:09,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:09,806][root][INFO] - Training Epoch: 2/2, step 462/574 completed (loss: 0.07354956120252609, acc: 1.0)
[2025-01-02 00:59:09,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:10,177][root][INFO] - Training Epoch: 2/2, step 463/574 completed (loss: 0.5665643811225891, acc: 0.8846153616905212)
[2025-01-02 00:59:10,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:10,551][root][INFO] - Training Epoch: 2/2, step 464/574 completed (loss: 0.2368270605802536, acc: 0.9347826242446899)
[2025-01-02 00:59:10,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:10,849][root][INFO] - Training Epoch: 2/2, step 465/574 completed (loss: 0.3414594531059265, acc: 0.8928571343421936)
[2025-01-02 00:59:10,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:11,243][root][INFO] - Training Epoch: 2/2, step 466/574 completed (loss: 0.46798062324523926, acc: 0.8554216623306274)
[2025-01-02 00:59:11,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:11,646][root][INFO] - Training Epoch: 2/2, step 467/574 completed (loss: 0.20382873713970184, acc: 0.954954981803894)
[2025-01-02 00:59:11,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:12,063][root][INFO] - Training Epoch: 2/2, step 468/574 completed (loss: 0.6462069749832153, acc: 0.8543689250946045)
[2025-01-02 00:59:12,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:12,433][root][INFO] - Training Epoch: 2/2, step 469/574 completed (loss: 0.4484723210334778, acc: 0.8861788511276245)
[2025-01-02 00:59:12,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:12,734][root][INFO] - Training Epoch: 2/2, step 470/574 completed (loss: 0.14034488797187805, acc: 0.9583333134651184)
[2025-01-02 00:59:12,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:13,082][root][INFO] - Training Epoch: 2/2, step 471/574 completed (loss: 0.3507983088493347, acc: 0.8571428656578064)
[2025-01-02 00:59:13,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:13,502][root][INFO] - Training Epoch: 2/2, step 472/574 completed (loss: 0.6193076372146606, acc: 0.7941176295280457)
[2025-01-02 00:59:13,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:13,848][root][INFO] - Training Epoch: 2/2, step 473/574 completed (loss: 0.7512615919113159, acc: 0.7947598099708557)
[2025-01-02 00:59:13,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:14,220][root][INFO] - Training Epoch: 2/2, step 474/574 completed (loss: 0.677523136138916, acc: 0.8333333134651184)
[2025-01-02 00:59:14,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:14,551][root][INFO] - Training Epoch: 2/2, step 475/574 completed (loss: 0.4608851671218872, acc: 0.8527607321739197)
[2025-01-02 00:59:14,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:14,852][root][INFO] - Training Epoch: 2/2, step 476/574 completed (loss: 0.4004223644733429, acc: 0.8920863270759583)
[2025-01-02 00:59:14,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:15,201][root][INFO] - Training Epoch: 2/2, step 477/574 completed (loss: 0.8882582187652588, acc: 0.7185929417610168)
[2025-01-02 00:59:15,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:15,529][root][INFO] - Training Epoch: 2/2, step 478/574 completed (loss: 0.5419687032699585, acc: 0.8055555820465088)
[2025-01-02 00:59:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:15,857][root][INFO] - Training Epoch: 2/2, step 479/574 completed (loss: 0.5840634107589722, acc: 0.8484848737716675)
[2025-01-02 00:59:15,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:16,179][root][INFO] - Training Epoch: 2/2, step 480/574 completed (loss: 0.23736076056957245, acc: 0.8518518805503845)
[2025-01-02 00:59:16,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:16,587][root][INFO] - Training Epoch: 2/2, step 481/574 completed (loss: 0.3737758994102478, acc: 0.8999999761581421)
[2025-01-02 00:59:16,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:16,922][root][INFO] - Training Epoch: 2/2, step 482/574 completed (loss: 0.5490801930427551, acc: 0.800000011920929)
[2025-01-02 00:59:17,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:17,313][root][INFO] - Training Epoch: 2/2, step 483/574 completed (loss: 0.7037799954414368, acc: 0.7931034564971924)
[2025-01-02 00:59:17,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:17,625][root][INFO] - Training Epoch: 2/2, step 484/574 completed (loss: 0.3086532652378082, acc: 0.9354838728904724)
[2025-01-02 00:59:17,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:17,986][root][INFO] - Training Epoch: 2/2, step 485/574 completed (loss: 0.3425683081150055, acc: 0.8947368264198303)
[2025-01-02 00:59:18,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:18,364][root][INFO] - Training Epoch: 2/2, step 486/574 completed (loss: 0.6594330668449402, acc: 0.7407407164573669)
[2025-01-02 00:59:18,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:18,740][root][INFO] - Training Epoch: 2/2, step 487/574 completed (loss: 0.4456416666507721, acc: 0.9047619104385376)
[2025-01-02 00:59:18,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:19,095][root][INFO] - Training Epoch: 2/2, step 488/574 completed (loss: 0.6138007044792175, acc: 0.7727272510528564)
[2025-01-02 00:59:19,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:19,434][root][INFO] - Training Epoch: 2/2, step 489/574 completed (loss: 1.0156627893447876, acc: 0.7230769395828247)
[2025-01-02 00:59:19,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:19,670][root][INFO] - Training Epoch: 2/2, step 490/574 completed (loss: 0.20533473789691925, acc: 0.9666666388511658)
[2025-01-02 00:59:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:20,019][root][INFO] - Training Epoch: 2/2, step 491/574 completed (loss: 0.5657045841217041, acc: 0.7931034564971924)
[2025-01-02 00:59:20,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:20,370][root][INFO] - Training Epoch: 2/2, step 492/574 completed (loss: 0.5839804410934448, acc: 0.8235294222831726)
[2025-01-02 00:59:20,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:20,730][root][INFO] - Training Epoch: 2/2, step 493/574 completed (loss: 0.3526719808578491, acc: 0.8620689511299133)
[2025-01-02 00:59:20,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:21,023][root][INFO] - Training Epoch: 2/2, step 494/574 completed (loss: 0.370297372341156, acc: 0.8947368264198303)
[2025-01-02 00:59:21,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:21,350][root][INFO] - Training Epoch: 2/2, step 495/574 completed (loss: 0.9697408080101013, acc: 0.7894737124443054)
[2025-01-02 00:59:21,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:21,771][root][INFO] - Training Epoch: 2/2, step 496/574 completed (loss: 0.6131982803344727, acc: 0.8214285969734192)
[2025-01-02 00:59:21,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:22,210][root][INFO] - Training Epoch: 2/2, step 497/574 completed (loss: 0.3634483814239502, acc: 0.898876428604126)
[2025-01-02 00:59:22,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:22,550][root][INFO] - Training Epoch: 2/2, step 498/574 completed (loss: 0.8188408613204956, acc: 0.7528089880943298)
[2025-01-02 00:59:22,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:22,957][root][INFO] - Training Epoch: 2/2, step 499/574 completed (loss: 1.1393544673919678, acc: 0.6382978558540344)
[2025-01-02 00:59:23,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:23,346][root][INFO] - Training Epoch: 2/2, step 500/574 completed (loss: 0.6786867380142212, acc: 0.8478260636329651)
[2025-01-02 00:59:23,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:23,681][root][INFO] - Training Epoch: 2/2, step 501/574 completed (loss: 0.014359541237354279, acc: 1.0)
[2025-01-02 00:59:23,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:24,069][root][INFO] - Training Epoch: 2/2, step 502/574 completed (loss: 0.10392101854085922, acc: 0.9615384340286255)
[2025-01-02 00:59:24,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:24,460][root][INFO] - Training Epoch: 2/2, step 503/574 completed (loss: 0.1289098709821701, acc: 0.9629629850387573)
[2025-01-02 00:59:24,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:24,794][root][INFO] - Training Epoch: 2/2, step 504/574 completed (loss: 0.10950896143913269, acc: 1.0)
[2025-01-02 00:59:24,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:25,092][root][INFO] - Training Epoch: 2/2, step 505/574 completed (loss: 0.5504011511802673, acc: 0.8867924809455872)
[2025-01-02 00:59:25,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:25,399][root][INFO] - Training Epoch: 2/2, step 506/574 completed (loss: 0.5768020749092102, acc: 0.8275862336158752)
[2025-01-02 00:59:25,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:25,998][root][INFO] - Training Epoch: 2/2, step 507/574 completed (loss: 0.9563944935798645, acc: 0.7657657861709595)
[2025-01-02 00:59:26,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:26,424][root][INFO] - Training Epoch: 2/2, step 508/574 completed (loss: 0.7454186677932739, acc: 0.7605633735656738)
[2025-01-02 00:59:26,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:26,773][root][INFO] - Training Epoch: 2/2, step 509/574 completed (loss: 0.15356385707855225, acc: 0.949999988079071)
[2025-01-02 00:59:26,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:27,172][root][INFO] - Training Epoch: 2/2, step 510/574 completed (loss: 0.3024649918079376, acc: 0.8999999761581421)
[2025-01-02 00:59:27,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:27,520][root][INFO] - Training Epoch: 2/2, step 511/574 completed (loss: 0.5131297707557678, acc: 0.8461538553237915)
[2025-01-02 00:59:28,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:30,175][root][INFO] - Training Epoch: 2/2, step 512/574 completed (loss: 0.9866135716438293, acc: 0.699999988079071)
[2025-01-02 00:59:30,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:30,943][root][INFO] - Training Epoch: 2/2, step 513/574 completed (loss: 0.25253763794898987, acc: 0.9126983880996704)
[2025-01-02 00:59:31,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:31,224][root][INFO] - Training Epoch: 2/2, step 514/574 completed (loss: 0.5671519637107849, acc: 0.8214285969734192)
[2025-01-02 00:59:31,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:31,527][root][INFO] - Training Epoch: 2/2, step 515/574 completed (loss: 0.059102676808834076, acc: 1.0)
[2025-01-02 00:59:31,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:32,216][root][INFO] - Training Epoch: 2/2, step 516/574 completed (loss: 0.5113428235054016, acc: 0.875)
[2025-01-02 00:59:32,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:32,561][root][INFO] - Training Epoch: 2/2, step 517/574 completed (loss: 0.018662258982658386, acc: 1.0)
[2025-01-02 00:59:32,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:32,924][root][INFO] - Training Epoch: 2/2, step 518/574 completed (loss: 0.04923074319958687, acc: 1.0)
[2025-01-02 00:59:33,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:33,224][root][INFO] - Training Epoch: 2/2, step 519/574 completed (loss: 0.2920606732368469, acc: 0.949999988079071)
[2025-01-02 00:59:33,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:33,528][root][INFO] - Training Epoch: 2/2, step 520/574 completed (loss: 0.5376539826393127, acc: 0.7777777910232544)
[2025-01-02 00:59:33,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:34,514][root][INFO] - Training Epoch: 2/2, step 521/574 completed (loss: 0.6232041716575623, acc: 0.8093220591545105)
[2025-01-02 00:59:34,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:34,885][root][INFO] - Training Epoch: 2/2, step 522/574 completed (loss: 0.22478735446929932, acc: 0.9477611780166626)
[2025-01-02 00:59:34,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:35,235][root][INFO] - Training Epoch: 2/2, step 523/574 completed (loss: 0.3884393870830536, acc: 0.8978102207183838)
[2025-01-02 00:59:35,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:35,787][root][INFO] - Training Epoch: 2/2, step 524/574 completed (loss: 0.6250175833702087, acc: 0.8550000190734863)
[2025-01-02 00:59:35,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:36,090][root][INFO] - Training Epoch: 2/2, step 525/574 completed (loss: 0.08184961974620819, acc: 0.9629629850387573)
[2025-01-02 00:59:36,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:36,367][root][INFO] - Training Epoch: 2/2, step 526/574 completed (loss: 0.16549068689346313, acc: 0.9230769276618958)
[2025-01-02 00:59:36,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:36,664][root][INFO] - Training Epoch: 2/2, step 527/574 completed (loss: 0.3551086485385895, acc: 0.9047619104385376)
[2025-01-02 00:59:36,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:36,964][root][INFO] - Training Epoch: 2/2, step 528/574 completed (loss: 1.5692837238311768, acc: 0.5901639461517334)
[2025-01-02 00:59:37,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:37,263][root][INFO] - Training Epoch: 2/2, step 529/574 completed (loss: 0.40101662278175354, acc: 0.8474576473236084)
[2025-01-02 00:59:37,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:37,643][root][INFO] - Training Epoch: 2/2, step 530/574 completed (loss: 1.0446200370788574, acc: 0.7209302186965942)
[2025-01-02 00:59:37,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:37,955][root][INFO] - Training Epoch: 2/2, step 531/574 completed (loss: 0.6408603191375732, acc: 0.7954545617103577)
[2025-01-02 00:59:38,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:38,270][root][INFO] - Training Epoch: 2/2, step 532/574 completed (loss: 0.8293625712394714, acc: 0.7735849022865295)
[2025-01-02 00:59:38,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:38,641][root][INFO] - Training Epoch: 2/2, step 533/574 completed (loss: 0.6980609893798828, acc: 0.8409090638160706)
[2025-01-02 00:59:38,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:39,002][root][INFO] - Training Epoch: 2/2, step 534/574 completed (loss: 0.34803473949432373, acc: 0.8799999952316284)
[2025-01-02 00:59:39,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:39,389][root][INFO] - Training Epoch: 2/2, step 535/574 completed (loss: 0.4373463988304138, acc: 0.8999999761581421)
[2025-01-02 00:59:39,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:39,722][root][INFO] - Training Epoch: 2/2, step 536/574 completed (loss: 0.15486985445022583, acc: 0.9090909361839294)
[2025-01-02 00:59:39,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:40,163][root][INFO] - Training Epoch: 2/2, step 537/574 completed (loss: 0.6562545299530029, acc: 0.8461538553237915)
[2025-01-02 00:59:40,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:40,518][root][INFO] - Training Epoch: 2/2, step 538/574 completed (loss: 0.40500083565711975, acc: 0.90625)
[2025-01-02 00:59:40,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:40,912][root][INFO] - Training Epoch: 2/2, step 539/574 completed (loss: 0.46411141753196716, acc: 0.8125)
[2025-01-02 00:59:40,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:41,279][root][INFO] - Training Epoch: 2/2, step 540/574 completed (loss: 0.5704007148742676, acc: 0.7878788113594055)
[2025-01-02 00:59:41,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:41,637][root][INFO] - Training Epoch: 2/2, step 541/574 completed (loss: 0.21913886070251465, acc: 0.9375)
[2025-01-02 00:59:41,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:42,024][root][INFO] - Training Epoch: 2/2, step 542/574 completed (loss: 0.056755345314741135, acc: 1.0)
[2025-01-02 00:59:42,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:42,347][root][INFO] - Training Epoch: 2/2, step 543/574 completed (loss: 0.02376217395067215, acc: 1.0)
[2025-01-02 00:59:42,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:42,718][root][INFO] - Training Epoch: 2/2, step 544/574 completed (loss: 0.07402932643890381, acc: 0.9666666388511658)
[2025-01-02 00:59:42,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:43,056][root][INFO] - Training Epoch: 2/2, step 545/574 completed (loss: 0.12977375090122223, acc: 0.9756097793579102)
[2025-01-02 00:59:43,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:43,381][root][INFO] - Training Epoch: 2/2, step 546/574 completed (loss: 0.012247883714735508, acc: 1.0)
[2025-01-02 00:59:43,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:43,715][root][INFO] - Training Epoch: 2/2, step 547/574 completed (loss: 0.04977301135659218, acc: 0.9736841917037964)
[2025-01-02 00:59:43,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:44,107][root][INFO] - Training Epoch: 2/2, step 548/574 completed (loss: 0.17789135873317719, acc: 0.9677419066429138)
[2025-01-02 00:59:44,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:44,447][root][INFO] - Training Epoch: 2/2, step 549/574 completed (loss: 0.0022468925453722477, acc: 1.0)
[2025-01-02 00:59:44,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:44,791][root][INFO] - Training Epoch: 2/2, step 550/574 completed (loss: 0.27527135610580444, acc: 0.9090909361839294)
[2025-01-02 00:59:44,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:45,113][root][INFO] - Training Epoch: 2/2, step 551/574 completed (loss: 0.11969618499279022, acc: 0.949999988079071)
[2025-01-02 00:59:45,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:45,428][root][INFO] - Training Epoch: 2/2, step 552/574 completed (loss: 0.1670842468738556, acc: 0.9428571462631226)
[2025-01-02 00:59:45,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:45,731][root][INFO] - Training Epoch: 2/2, step 553/574 completed (loss: 0.42532384395599365, acc: 0.8759124279022217)
[2025-01-02 00:59:45,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:46,078][root][INFO] - Training Epoch: 2/2, step 554/574 completed (loss: 0.17110493779182434, acc: 0.9448275566101074)
[2025-01-02 00:59:46,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:46,383][root][INFO] - Training Epoch: 2/2, step 555/574 completed (loss: 0.27967432141304016, acc: 0.9357143044471741)
[2025-01-02 00:59:46,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:46,699][root][INFO] - Training Epoch: 2/2, step 556/574 completed (loss: 0.3937378227710724, acc: 0.9072847962379456)
[2025-01-02 00:59:46,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:47,032][root][INFO] - Training Epoch: 2/2, step 557/574 completed (loss: 0.17512854933738708, acc: 0.9316239356994629)
[2025-01-02 00:59:47,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:47,336][root][INFO] - Training Epoch: 2/2, step 558/574 completed (loss: 0.13547146320343018, acc: 0.9599999785423279)
[2025-01-02 00:59:47,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:47,649][root][INFO] - Training Epoch: 2/2, step 559/574 completed (loss: 0.12517312169075012, acc: 0.9615384340286255)
[2025-01-02 00:59:47,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:47,937][root][INFO] - Training Epoch: 2/2, step 560/574 completed (loss: 0.022614626213908195, acc: 1.0)
[2025-01-02 00:59:48,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:48,232][root][INFO] - Training Epoch: 2/2, step 561/574 completed (loss: 0.15006113052368164, acc: 0.9743589758872986)
[2025-01-02 00:59:48,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:48,579][root][INFO] - Training Epoch: 2/2, step 562/574 completed (loss: 0.4980916678905487, acc: 0.8888888955116272)
[2025-01-02 00:59:48,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:48,910][root][INFO] - Training Epoch: 2/2, step 563/574 completed (loss: 0.41194429993629456, acc: 0.8961039185523987)
[2025-01-02 00:59:49,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:49,238][root][INFO] - Training Epoch: 2/2, step 564/574 completed (loss: 0.21702790260314941, acc: 0.9375)
[2025-01-02 00:59:49,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:49,566][root][INFO] - Training Epoch: 2/2, step 565/574 completed (loss: 0.24363790452480316, acc: 0.931034505367279)
[2025-01-02 00:59:49,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:49,884][root][INFO] - Training Epoch: 2/2, step 566/574 completed (loss: 0.2657749056816101, acc: 0.9404761791229248)
[2025-01-02 00:59:49,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:50,181][root][INFO] - Training Epoch: 2/2, step 567/574 completed (loss: 0.027856584638357162, acc: 1.0)
[2025-01-02 00:59:50,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:50,483][root][INFO] - Training Epoch: 2/2, step 568/574 completed (loss: 0.04219166934490204, acc: 1.0)
[2025-01-02 00:59:50,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:50,852][root][INFO] - Training Epoch: 2/2, step 569/574 completed (loss: 0.1577347069978714, acc: 0.9625668525695801)
[2025-01-02 00:59:51,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:52,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:52,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:52,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:53,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:53,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:53,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:54,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:54,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:54,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:55,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:55,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:55,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:56,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:56,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:56,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:56,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:57,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:57,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:58,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:58,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:58,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:59,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:59,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:59,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:59:59,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:00,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:00,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:00,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:01,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:01,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:01,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:02,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:02,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:02,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:03,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:03,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:03,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:04,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:04,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:04,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:05,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:05,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:05,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:06,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:06,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:06,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:07,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:07,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:08,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:08,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:08,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:08,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:09,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:09,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:09,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:10,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:10,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:10,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:11,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:11,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:12,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:12,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:12,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:13,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:13,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:13,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:14,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:14,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:15,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:15,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:15,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:16,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:16,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:16,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:17,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:17,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:17,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:18,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:18,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:18,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:19,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:19,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:19,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:20,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:20,751][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8501, device='cuda:0') eval_epoch_loss=tensor(0.6152, device='cuda:0') eval_epoch_acc=tensor(0.8397, device='cuda:0')
[2025-01-02 01:00:20,752][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 01:00:20,753][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 01:00:21,148][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.6152417659759521/model.pt
[2025-01-02 01:00:21,155][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 01:00:21,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:21,459][root][INFO] - Training Epoch: 2/2, step 570/574 completed (loss: 0.011627973057329655, acc: 1.0)
[2025-01-02 01:00:21,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:21,829][root][INFO] - Training Epoch: 2/2, step 571/574 completed (loss: 0.15352807939052582, acc: 0.9316239356994629)
[2025-01-02 01:00:21,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:22,165][root][INFO] - Training Epoch: 2/2, step 572/574 completed (loss: 0.4354414939880371, acc: 0.8571428656578064)
[2025-01-02 01:00:22,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 01:00:22,519][root][INFO] - Training Epoch: 2/2, step 573/574 completed (loss: 0.36369141936302185, acc: 0.9119496941566467)
[2025-01-02 01:00:22,991][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.7171, train_epoch_loss=0.5406, epoch time 351.1152784638107s
[2025-01-02 01:00:22,991][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-02 01:00:22,991][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-02 01:00:22,991][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-02 01:00:22,991][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-01-02 01:00:22,992][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-01-02 01:00:22,995][root][INFO] - Key: avg_train_prep, Value: 2.318479061126709
[2025-01-02 01:00:22,997][root][INFO] - Key: avg_train_loss, Value: 0.8060842752456665
[2025-01-02 01:00:22,997][root][INFO] - Key: avg_train_acc, Value: 0.7959412336349487
[2025-01-02 01:00:22,997][root][INFO] - Key: avg_eval_prep, Value: 2.055922269821167
[2025-01-02 01:00:22,997][root][INFO] - Key: avg_eval_loss, Value: 0.7012850046157837
[2025-01-02 01:00:22,997][root][INFO] - Key: avg_eval_acc, Value: 0.8162913918495178
[2025-01-02 01:00:22,998][root][INFO] - Key: avg_epoch_time, Value: 358.9636664483696
[2025-01-02 01:00:22,998][root][INFO] - Key: avg_checkpoint_time, Value: 0.3351983977481723
[2025-01-06 00:59:04,431][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-06 00:59:04,432][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-06 00:59:04,432][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-06 00:59:04,432][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-06_00-59-03.txt', 'log_interval': 5}
[2025-01-06 00:59:32,309][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-06 00:59:37,571][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37,573][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-06 00:59:37,575][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37,576][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-06 00:59:47,658][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47,660][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-06 00:59:47,660][slam_llm.models.slam_model][INFO] - setup peft...
[2025-01-06 00:59:47,779][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47,781][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-06 00:59:47,883][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-06 00:59:47,883][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-06 00:59:47,884][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792/model.pt
[2025-01-06 00:59:48,183][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-06 00:59:48,188][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-06 00:59:49,995][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-06 00:59:53,257][root][INFO] - --> Training Set Length = 2298
[2025-01-06 00:59:53,264][root][INFO] - --> Validation Set Length = 341
[2025-01-06 00:59:53,265][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:53,266][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:55,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:56,921][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 2.0960772037506104, acc: 0.48148149251937866)
[2025-01-06 00:59:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57,533][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 0.9558073282241821, acc: 0.800000011920929)
[2025-01-06 00:59:57,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57,919][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 2.3097050189971924, acc: 0.5135135054588318)
[2025-01-06 00:59:58,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58,340][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 2.1763036251068115, acc: 0.5789473652839661)
[2025-01-06 00:59:58,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58,719][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 1.6998047828674316, acc: 0.5945945978164673)
[2025-01-06 00:59:58,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59,084][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 0.8240347504615784, acc: 0.8214285969734192)
[2025-01-06 00:59:59,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59,490][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 2.6075844764709473, acc: 0.4693877696990967)
[2025-01-06 00:59:59,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59,837][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 1.3130346536636353, acc: 0.7666666507720947)
[2025-01-06 00:59:59,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00,206][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 1.1516512632369995, acc: 0.7272727489471436)
[2025-01-06 01:00:00,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00,559][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 0.9723425507545471, acc: 0.692307710647583)
[2025-01-06 01:00:00,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00,890][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 1.304939866065979, acc: 0.5555555820465088)
[2025-01-06 01:00:01,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01,241][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 2.1893601417541504, acc: 0.5897436141967773)
[2025-01-06 01:00:01,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01,644][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 1.3780752420425415, acc: 0.7272727489471436)
[2025-01-06 01:00:01,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02,014][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 1.3336013555526733, acc: 0.5652173757553101)
[2025-01-06 01:00:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02,458][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 1.912348747253418, acc: 0.5882353186607361)
[2025-01-06 01:00:02,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02,843][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 0.8959335088729858, acc: 0.7142857313156128)
[2025-01-06 01:00:02,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03,126][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 1.910796880722046, acc: 0.6315789222717285)
[2025-01-06 01:00:03,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03,471][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 2.4606688022613525, acc: 0.5)
[2025-01-06 01:00:03,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03,888][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 1.4996267557144165, acc: 0.5833333134651184)
[2025-01-06 01:00:04,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04,260][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 1.468733310699463, acc: 0.7368420958518982)
[2025-01-06 01:00:04,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04,627][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 1.2159637212753296, acc: 0.7307692170143127)
[2025-01-06 01:00:04,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05,018][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 1.7433260679244995, acc: 0.6896551847457886)
[2025-01-06 01:00:05,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05,407][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 2.2058966159820557, acc: 0.47999998927116394)
[2025-01-06 01:00:05,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05,805][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 1.8753631114959717, acc: 0.7142857313156128)
[2025-01-06 01:00:05,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06,224][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 0.8815869688987732, acc: 0.75)
[2025-01-06 01:00:06,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06,682][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 1.5402711629867554, acc: 0.6226415038108826)
[2025-01-06 01:00:06,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:07,049][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 1.955437421798706, acc: 0.5205479264259338)
[2025-01-06 01:00:07,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08,407][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 1.7114523649215698, acc: 0.5889328122138977)
[2025-01-06 01:00:08,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08,807][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 1.3423949480056763, acc: 0.7209302186965942)
[2025-01-06 01:00:08,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09,220][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 1.500946283340454, acc: 0.6385542154312134)
[2025-01-06 01:00:09,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09,626][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 1.382470965385437, acc: 0.7283950448036194)
[2025-01-06 01:00:09,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10,023][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 2.0714735984802246, acc: 0.4642857015132904)
[2025-01-06 01:00:10,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10,396][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 0.9791938662528992, acc: 0.7037037014961243)
[2025-01-06 01:00:10,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10,827][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 1.1704047918319702, acc: 0.8260869383811951)
[2025-01-06 01:00:11,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11,295][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 1.2558906078338623, acc: 0.7310924530029297)
[2025-01-06 01:00:11,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11,711][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 1.3465101718902588, acc: 0.7704917788505554)
[2025-01-06 01:00:11,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12,072][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 1.5427273511886597, acc: 0.6984127163887024)
[2025-01-06 01:00:12,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12,408][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 1.3849573135375977, acc: 0.6610169410705566)
[2025-01-06 01:00:12,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12,822][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 1.877794623374939, acc: 0.6896551847457886)
[2025-01-06 01:00:12,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13,248][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 3.1021664142608643, acc: 0.3333333432674408)
[2025-01-06 01:00:13,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13,667][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 0.8725772500038147, acc: 0.6538461446762085)
[2025-01-06 01:00:13,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14,106][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 0.8198295831680298, acc: 0.7567567825317383)
[2025-01-06 01:00:14,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14,517][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 2.6914615631103516, acc: 0.5384615659713745)
[2025-01-06 01:00:14,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15,020][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 1.6860326528549194, acc: 0.6969696879386902)
[2025-01-06 01:00:15,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15,466][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 2.789707660675049, acc: 0.6701030731201172)
[2025-01-06 01:00:15,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15,898][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 3.0055675506591797, acc: 0.6617646813392639)
[2025-01-06 01:00:16,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16,288][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 1.5173671245574951, acc: 0.5)
[2025-01-06 01:00:16,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16,638][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 1.3630419969558716, acc: 0.6666666865348816)
[2025-01-06 01:00:16,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17,021][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 1.3895180225372314, acc: 0.6785714030265808)
[2025-01-06 01:00:17,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17,398][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 1.0181450843811035, acc: 0.8055555820465088)
[2025-01-06 01:00:17,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17,735][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 3.2852566242218018, acc: 0.5438596606254578)
[2025-01-06 01:00:17,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18,034][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 2.5162854194641113, acc: 0.5555555820465088)
[2025-01-06 01:00:18,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18,396][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 4.431297779083252, acc: 0.49295774102211)
[2025-01-06 01:00:18,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18,897][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 3.6485249996185303, acc: 0.4000000059604645)
[2025-01-06 01:00:19,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19,372][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 3.78129243850708, acc: 0.4054054021835327)
[2025-01-06 01:00:19,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19,750][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 0.9954131245613098, acc: 0.692307710647583)
[2025-01-06 01:00:21,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:22,987][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 2.2599079608917236, acc: 0.4846416413784027)
[2025-01-06 01:00:23,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:24,313][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 2.0813755989074707, acc: 0.5838779807090759)
[2025-01-06 01:00:24,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:24,977][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 2.736443042755127, acc: 0.5852272510528564)
[2025-01-06 01:00:25,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:25,552][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 1.3058348894119263, acc: 0.7426470518112183)
[2025-01-06 01:00:25,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26,127][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 2.3382418155670166, acc: 0.6014492511749268)
[2025-01-06 01:00:26,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26,564][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 2.1934895515441895, acc: 0.6000000238418579)
[2025-01-06 01:00:26,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26,930][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 1.6479579210281372, acc: 0.6176470518112183)
[2025-01-06 01:00:27,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27,310][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 3.2112069129943848, acc: 0.5277777910232544)
[2025-01-06 01:00:27,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27,759][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 1.2382947206497192, acc: 0.796875)
[2025-01-06 01:00:27,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28,172][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 0.6247630715370178, acc: 0.931034505367279)
[2025-01-06 01:00:28,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28,531][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 3.242856502532959, acc: 0.5178571343421936)
[2025-01-06 01:00:28,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28,869][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 1.932536244392395, acc: 0.699999988079071)
[2025-01-06 01:00:28,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29,200][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 0.9694967865943909, acc: 0.7200000286102295)
[2025-01-06 01:00:29,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29,564][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 2.0032596588134766, acc: 0.3888888955116272)
[2025-01-06 01:00:29,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29,922][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 3.6090400218963623, acc: 0.39393940567970276)
[2025-01-06 01:00:30,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30,314][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 2.2645766735076904, acc: 0.5073529481887817)
[2025-01-06 01:00:30,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30,672][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 1.4376345872879028, acc: 0.6349206566810608)
[2025-01-06 01:00:30,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31,054][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 1.9589128494262695, acc: 0.5384615659713745)
[2025-01-06 01:00:31,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31,409][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 3.4755287170410156, acc: 0.4693877696990967)
[2025-01-06 01:00:31,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31,759][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 2.1028525829315186, acc: 0.49253731966018677)
[2025-01-06 01:00:31,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32,198][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 2.5429904460906982, acc: 0.4781021773815155)
[2025-01-06 01:00:32,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32,599][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 0.9221081733703613, acc: 0.6666666865348816)
[2025-01-06 01:00:32,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32,968][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 0.9714252352714539, acc: 0.6666666865348816)
[2025-01-06 01:00:33,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33,383][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 1.3005354404449463, acc: 0.6666666865348816)
[2025-01-06 01:00:33,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33,759][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 1.6556757688522339, acc: 0.807692289352417)
[2025-01-06 01:00:33,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34,106][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 2.1469831466674805, acc: 0.6538461446762085)
[2025-01-06 01:00:34,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34,445][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 2.2033472061157227, acc: 0.557692289352417)
[2025-01-06 01:00:34,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34,773][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 0.7546709775924683, acc: 0.78125)
[2025-01-06 01:00:34,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35,184][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 2.1145565509796143, acc: 0.6231883764266968)
[2025-01-06 01:00:35,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35,560][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 2.4699556827545166, acc: 0.6200000047683716)
[2025-01-06 01:00:35,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35,949][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 0.9845025539398193, acc: 0.739130437374115)
[2025-01-06 01:00:36,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36,459][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 2.959163188934326, acc: 0.4000000059604645)
[2025-01-06 01:00:36,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36,889][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.0210726261138916, acc: 0.5048543810844421)
[2025-01-06 01:00:37,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:38,069][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 2.64070200920105, acc: 0.5485436916351318)
[2025-01-06 01:00:38,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:38,915][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 3.2235021591186523, acc: 0.42473119497299194)
[2025-01-06 01:00:39,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:39,715][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 2.356602430343628, acc: 0.5387930870056152)
[2025-01-06 01:00:39,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:40,454][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 2.27778697013855, acc: 0.6000000238418579)
[2025-01-06 01:00:40,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41,439][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 2.8838136196136475, acc: 0.3762376308441162)
[2025-01-06 01:00:41,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41,764][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 2.0652685165405273, acc: 0.5645161271095276)
[2025-01-06 01:00:41,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42,145][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 1.7059935331344604, acc: 0.6521739363670349)
[2025-01-06 01:00:42,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42,498][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 2.907857894897461, acc: 0.462184876203537)
[2025-01-06 01:00:42,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42,888][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 2.4340083599090576, acc: 0.5288461446762085)
[2025-01-06 01:00:43,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43,299][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 2.447782516479492, acc: 0.5182482004165649)
[2025-01-06 01:00:43,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43,660][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 3.2687087059020996, acc: 0.34328359365463257)
[2025-01-06 01:00:43,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44,053][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 1.685002326965332, acc: 0.6000000238418579)
[2025-01-06 01:00:44,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44,440][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 1.5151630640029907, acc: 0.7727272510528564)
[2025-01-06 01:00:44,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44,858][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 0.4701274335384369, acc: 0.8260869383811951)
[2025-01-06 01:00:44,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45,217][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 0.49450138211250305, acc: 0.8636363744735718)
[2025-01-06 01:00:45,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45,568][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 1.2553813457489014, acc: 0.7586206793785095)
[2025-01-06 01:00:45,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45,861][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 0.6483151912689209, acc: 0.7441860437393188)
[2025-01-06 01:00:45,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46,193][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 0.6564112305641174, acc: 0.7599999904632568)
[2025-01-06 01:00:46,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46,575][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 1.098104476928711, acc: 0.7647058963775635)
[2025-01-06 01:00:46,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46,932][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 0.6119911670684814, acc: 0.8461538553237915)
[2025-01-06 01:00:47,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47,352][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 0.5733845829963684, acc: 0.9047619104385376)
[2025-01-06 01:00:47,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47,808][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 1.4297422170639038, acc: 0.7692307829856873)
[2025-01-06 01:00:47,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48,243][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 1.242030143737793, acc: 0.7017543911933899)
[2025-01-06 01:00:48,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48,664][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 2.030719757080078, acc: 0.5789473652839661)
[2025-01-06 01:00:48,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49,036][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 1.3355225324630737, acc: 0.6666666865348816)
[2025-01-06 01:00:49,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49,410][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 1.4531559944152832, acc: 0.7142857313156128)
[2025-01-06 01:00:49,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49,767][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 0.9336803555488586, acc: 0.7727272510528564)
[2025-01-06 01:00:49,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50,134][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 0.9202200174331665, acc: 0.7936508059501648)
[2025-01-06 01:00:50,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50,514][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 0.9818724989891052, acc: 0.7804877758026123)
[2025-01-06 01:00:50,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50,934][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 1.192895531654358, acc: 0.8064516186714172)
[2025-01-06 01:00:51,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:51,800][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 1.144753098487854, acc: 0.7186312079429626)
[2025-01-06 01:00:51,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52,230][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 1.305894136428833, acc: 0.7200000286102295)
[2025-01-06 01:00:52,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52,662][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 2.1191201210021973, acc: 0.6538461446762085)
[2025-01-06 01:00:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52,979][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 1.2040207386016846, acc: 0.625)
[2025-01-06 01:00:53,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53,378][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 1.5368326902389526, acc: 0.6315789222717285)
[2025-01-06 01:00:53,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53,753][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 1.5712474584579468, acc: 0.6625766754150391)
[2025-01-06 01:00:53,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54,159][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 1.4695100784301758, acc: 0.6388888955116272)
[2025-01-06 01:00:54,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54,607][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 1.814400315284729, acc: 0.5416666865348816)
[2025-01-06 01:00:54,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55,056][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 1.1863170862197876, acc: 0.6845238208770752)
[2025-01-06 01:00:55,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55,522][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 1.4157365560531616, acc: 0.620512843132019)
[2025-01-06 01:00:55,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55,954][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 1.4988969564437866, acc: 0.654411792755127)
[2025-01-06 01:00:56,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56,275][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 2.1824846267700195, acc: 0.4615384638309479)
[2025-01-06 01:00:56,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56,658][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.4251701831817627, acc: 0.47826087474823)
[2025-01-06 01:00:56,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57,017][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 2.391698122024536, acc: 0.375)
[2025-01-06 01:00:57,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57,444][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 2.359903335571289, acc: 0.3913043439388275)
[2025-01-06 01:00:57,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57,832][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 2.0656535625457764, acc: 0.5428571701049805)
[2025-01-06 01:00:57,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58,251][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 2.0671348571777344, acc: 0.5)
[2025-01-06 01:00:58,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58,637][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 1.7794500589370728, acc: 0.5)
[2025-01-06 01:00:58,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58,989][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 2.2228643894195557, acc: 0.3333333432674408)
[2025-01-06 01:00:59,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59,358][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 1.490444540977478, acc: 0.5652173757553101)
[2025-01-06 01:00:59,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59,709][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 0.7352169752120972, acc: 0.761904776096344)
[2025-01-06 01:00:59,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00,068][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 1.3862265348434448, acc: 0.5769230723381042)
[2025-01-06 01:01:00,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00,395][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 1.2803184986114502, acc: 0.6129032373428345)
[2025-01-06 01:01:00,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00,792][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 1.2730916738510132, acc: 0.6216216087341309)
[2025-01-06 01:01:01,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33,969][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.1857, device='cuda:0') eval_epoch_loss=tensor(1.1587, device='cuda:0') eval_epoch_acc=tensor(0.7317, device='cuda:0')
[2025-01-06 01:01:33,970][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:01:33,970][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:01:34,452][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.1586825847625732/model.pt
[2025-01-06 01:01:34,456][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:01:34,457][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.1586825847625732
[2025-01-06 01:01:34,457][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7316946983337402
[2025-01-06 01:01:34,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35,030][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 1.8133007287979126, acc: 0.6140350699424744)
[2025-01-06 01:01:35,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35,424][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 1.498198390007019, acc: 0.6492537260055542)
[2025-01-06 01:01:35,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35,798][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 1.383817434310913, acc: 0.6734693646430969)
[2025-01-06 01:01:35,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36,258][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 1.9412192106246948, acc: 0.478723406791687)
[2025-01-06 01:01:36,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36,599][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 2.0077733993530273, acc: 0.6142857074737549)
[2025-01-06 01:01:36,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36,980][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 2.2853736877441406, acc: 0.3928571343421936)
[2025-01-06 01:01:37,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37,311][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 1.6805156469345093, acc: 0.6521739363670349)
[2025-01-06 01:01:37,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37,688][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 1.7355554103851318, acc: 0.5517241358757019)
[2025-01-06 01:01:37,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38,101][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 1.9950246810913086, acc: 0.52173912525177)
[2025-01-06 01:01:38,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38,504][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 1.4595916271209717, acc: 0.6610169410705566)
[2025-01-06 01:01:38,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38,863][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 1.740472435951233, acc: 0.5438596606254578)
[2025-01-06 01:01:39,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39,267][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 1.7371227741241455, acc: 0.5810810923576355)
[2025-01-06 01:01:39,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39,661][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 1.4880610704421997, acc: 0.7857142686843872)
[2025-01-06 01:01:39,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:40,038][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 1.0979986190795898, acc: 0.695652186870575)
[2025-01-06 01:01:40,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:40,359][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 3.6154866218566895, acc: 0.21052631735801697)
[2025-01-06 01:01:41,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42,067][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 3.11930775642395, acc: 0.3918918967247009)
[2025-01-06 01:01:42,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42,494][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 2.2521164417266846, acc: 0.46296295523643494)
[2025-01-06 01:01:42,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42,952][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 2.8270204067230225, acc: 0.40697672963142395)
[2025-01-06 01:01:43,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:43,603][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 2.9823381900787354, acc: 0.3176470696926117)
[2025-01-06 01:01:43,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44,208][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 2.8240461349487305, acc: 0.42696627974510193)
[2025-01-06 01:01:44,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44,595][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 1.4406166076660156, acc: 0.7272727489471436)
[2025-01-06 01:01:44,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44,944][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 0.9069750905036926, acc: 0.761904776096344)
[2025-01-06 01:01:45,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45,360][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 1.2419666051864624, acc: 0.6206896305084229)
[2025-01-06 01:01:45,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45,767][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 0.5901293754577637, acc: 0.8571428656578064)
[2025-01-06 01:01:45,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46,126][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 0.3323826491832733, acc: 0.8999999761581421)
[2025-01-06 01:01:46,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46,528][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 1.3737258911132812, acc: 0.7222222089767456)
[2025-01-06 01:01:46,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46,876][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 1.2656753063201904, acc: 0.686274528503418)
[2025-01-06 01:01:47,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:47,972][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 1.7622241973876953, acc: 0.6095890402793884)
[2025-01-06 01:01:48,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48,316][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 1.0892925262451172, acc: 0.6666666865348816)
[2025-01-06 01:01:48,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48,619][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 1.9727182388305664, acc: 0.4444444477558136)
[2025-01-06 01:01:48,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48,957][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 1.8523234128952026, acc: 0.5357142686843872)
[2025-01-06 01:01:49,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49,541][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 1.400709867477417, acc: 0.7079645991325378)
[2025-01-06 01:01:49,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49,930][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 1.6509157419204712, acc: 0.5652173757553101)
[2025-01-06 01:01:50,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:50,330][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 1.069796085357666, acc: 0.75)
[2025-01-06 01:01:50,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:51,284][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 1.7988947629928589, acc: 0.5572519302368164)
[2025-01-06 01:01:51,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:51,970][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 1.934402346611023, acc: 0.5111111402511597)
[2025-01-06 01:01:52,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52,289][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 1.2353763580322266, acc: 0.7213114500045776)
[2025-01-06 01:01:52,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52,610][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 0.3638155460357666, acc: 0.9583333134651184)
[2025-01-06 01:01:52,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52,932][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 0.3809497356414795, acc: 0.8799999952316284)
[2025-01-06 01:01:53,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53,281][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 0.8016209006309509, acc: 0.8214285969734192)
[2025-01-06 01:01:53,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53,705][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 0.7147901654243469, acc: 0.8048780560493469)
[2025-01-06 01:01:53,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54,147][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 1.2877061367034912, acc: 0.773413896560669)
[2025-01-06 01:01:54,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54,533][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 0.8187381625175476, acc: 0.8097983002662659)
[2025-01-06 01:01:54,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55,054][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 0.8777696490287781, acc: 0.784375011920929)
[2025-01-06 01:01:55,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55,599][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 0.8265831470489502, acc: 0.7879924774169922)
[2025-01-06 01:01:55,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56,025][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 1.0721641778945923, acc: 0.725978672504425)
[2025-01-06 01:01:56,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56,427][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 1.1922845840454102, acc: 0.6399999856948853)
[2025-01-06 01:01:56,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:57,035][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 1.5684839487075806, acc: 0.6395348906517029)
[2025-01-06 01:01:57,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:57,844][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 2.316472291946411, acc: 0.4841269850730896)
[2025-01-06 01:01:58,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:58,757][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 1.714808702468872, acc: 0.560606062412262)
[2025-01-06 01:01:58,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:59,499][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 1.7674024105072021, acc: 0.5764706134796143)
[2025-01-06 01:01:59,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:00,576][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 1.9359149932861328, acc: 0.5432098507881165)
[2025-01-06 01:02:00,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01,524][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 1.951974630355835, acc: 0.5322580933570862)
[2025-01-06 01:02:01,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01,904][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 0.8484710454940796, acc: 0.7857142686843872)
[2025-01-06 01:02:02,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02,240][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 2.130281925201416, acc: 0.5)
[2025-01-06 01:02:02,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02,589][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 1.6386348009109497, acc: 0.6470588445663452)
[2025-01-06 01:02:02,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03,019][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 1.5695605278015137, acc: 0.6985294222831726)
[2025-01-06 01:02:03,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03,401][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 0.9777063131332397, acc: 0.7118644118309021)
[2025-01-06 01:02:03,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03,741][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 1.6179800033569336, acc: 0.6194030046463013)
[2025-01-06 01:02:03,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04,156][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 1.7721519470214844, acc: 0.6213592290878296)
[2025-01-06 01:02:04,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04,593][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 1.4674711227416992, acc: 0.6507936716079712)
[2025-01-06 01:02:04,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05,007][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 0.6210256814956665, acc: 0.8901098966598511)
[2025-01-06 01:02:05,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05,427][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 0.8537670373916626, acc: 0.8206278085708618)
[2025-01-06 01:02:05,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05,850][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 0.8675282001495361, acc: 0.7795275449752808)
[2025-01-06 01:02:06,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06,293][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 1.023289442062378, acc: 0.7715517282485962)
[2025-01-06 01:02:06,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06,654][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 0.7584750652313232, acc: 0.8079710006713867)
[2025-01-06 01:02:06,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07,056][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 0.881391704082489, acc: 0.801556408405304)
[2025-01-06 01:02:07,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07,479][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 0.6284053921699524, acc: 0.8586956262588501)
[2025-01-06 01:02:07,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07,822][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 1.1885472536087036, acc: 0.739130437374115)
[2025-01-06 01:02:07,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:08,210][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 0.4852323830127716, acc: 0.8214285969734192)
[2025-01-06 01:02:08,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:08,603][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 0.8277787566184998, acc: 0.8297872543334961)
[2025-01-06 01:02:08,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09,285][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 0.9558125734329224, acc: 0.800000011920929)
[2025-01-06 01:02:09,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09,630][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 0.34890785813331604, acc: 0.9054054021835327)
[2025-01-06 01:02:09,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09,994][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 0.7036503553390503, acc: 0.8720930218696594)
[2025-01-06 01:02:10,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10,537][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 0.9447864890098572, acc: 0.7747747898101807)
[2025-01-06 01:02:10,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10,940][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 0.6039091944694519, acc: 0.8444444537162781)
[2025-01-06 01:02:11,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11,326][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 0.704095721244812, acc: 0.8181818127632141)
[2025-01-06 01:02:11,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11,693][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 0.7219918370246887, acc: 0.7407407164573669)
[2025-01-06 01:02:11,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:12,099][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 0.37386173009872437, acc: 0.8799999952316284)
[2025-01-06 01:02:12,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:12,511][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 1.1299134492874146, acc: 0.5961538553237915)
[2025-01-06 01:02:12,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13,293][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 0.9880853891372681, acc: 0.7771739363670349)
[2025-01-06 01:02:13,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13,842][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 1.0224581956863403, acc: 0.7727272510528564)
[2025-01-06 01:02:13,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14,314][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 1.157251238822937, acc: 0.7021276354789734)
[2025-01-06 01:02:14,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14,700][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 1.3819166421890259, acc: 0.6415094137191772)
[2025-01-06 01:02:14,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15,086][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 0.9733750224113464, acc: 0.7166666388511658)
[2025-01-06 01:02:15,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15,492][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 1.2306827306747437, acc: 0.7209302186965942)
[2025-01-06 01:02:15,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15,884][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 2.3621790409088135, acc: 0.4333333373069763)
[2025-01-06 01:02:16,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16,275][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 2.3176932334899902, acc: 0.4000000059604645)
[2025-01-06 01:02:16,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16,651][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 2.104884386062622, acc: 0.4888888895511627)
[2025-01-06 01:02:16,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17,111][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 2.078317880630493, acc: 0.4444444477558136)
[2025-01-06 01:02:17,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17,627][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 2.3752667903900146, acc: 0.4678899049758911)
[2025-01-06 01:02:17,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18,135][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 2.1621525287628174, acc: 0.4692307710647583)
[2025-01-06 01:02:18,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18,506][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 1.0092352628707886, acc: 0.6842105388641357)
[2025-01-06 01:02:18,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18,856][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 0.9620973467826843, acc: 0.7083333134651184)
[2025-01-06 01:02:18,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19,213][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 1.7593655586242676, acc: 0.5909090638160706)
[2025-01-06 01:02:19,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19,562][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 1.333633303642273, acc: 0.7407407164573669)
[2025-01-06 01:02:19,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19,932][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 1.4449703693389893, acc: 0.5714285969734192)
[2025-01-06 01:02:20,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:20,292][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 1.8470721244812012, acc: 0.5454545617103577)
[2025-01-06 01:02:20,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:20,650][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 1.1872295141220093, acc: 0.7045454382896423)
[2025-01-06 01:02:20,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21,254][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 2.138962984085083, acc: 0.4354838728904724)
[2025-01-06 01:02:21,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21,802][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 1.941954493522644, acc: 0.4545454680919647)
[2025-01-06 01:02:21,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22,136][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 0.14991600811481476, acc: 1.0)
[2025-01-06 01:02:22,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22,508][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 0.7288596630096436, acc: 0.7692307829856873)
[2025-01-06 01:02:22,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22,866][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 0.23707932233810425, acc: 0.8709677457809448)
[2025-01-06 01:02:22,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23,246][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 0.49087876081466675, acc: 0.8500000238418579)
[2025-01-06 01:02:23,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23,643][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 1.2458029985427856, acc: 0.7567567825317383)
[2025-01-06 01:02:23,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23,970][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 0.7291963696479797, acc: 0.7837837934494019)
[2025-01-06 01:02:24,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24,355][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 0.6870085597038269, acc: 0.8918918967247009)
[2025-01-06 01:02:24,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24,716][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 0.8631909489631653, acc: 0.779411792755127)
[2025-01-06 01:02:24,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25,083][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 0.7271607518196106, acc: 0.8780487775802612)
[2025-01-06 01:02:25,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25,480][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 0.7405747771263123, acc: 0.800000011920929)
[2025-01-06 01:02:25,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25,814][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 0.12232787907123566, acc: 0.9599999785423279)
[2025-01-06 01:02:25,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26,219][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 0.8839313983917236, acc: 0.7096773982048035)
[2025-01-06 01:02:26,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26,631][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 0.8938177824020386, acc: 0.859649121761322)
[2025-01-06 01:02:26,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26,984][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 0.3828529417514801, acc: 0.8999999761581421)
[2025-01-06 01:02:27,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:27,369][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 0.266270250082016, acc: 0.9210526347160339)
[2025-01-06 01:02:27,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:27,960][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 0.6527619361877441, acc: 0.8113207817077637)
[2025-01-06 01:02:28,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28,549][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 0.7154539227485657, acc: 0.8166666626930237)
[2025-01-06 01:02:28,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28,876][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 0.48788580298423767, acc: 0.8888888955116272)
[2025-01-06 01:02:29,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29,227][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 1.4611833095550537, acc: 0.5806451439857483)
[2025-01-06 01:02:29,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29,605][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 1.6506986618041992, acc: 0.5733333230018616)
[2025-01-06 01:02:29,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30,020][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 0.9303960204124451, acc: 0.7291666865348816)
[2025-01-06 01:02:30,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30,891][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 1.9742937088012695, acc: 0.527999997138977)
[2025-01-06 01:02:30,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31,273][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 1.7131458520889282, acc: 0.5393258333206177)
[2025-01-06 01:02:31,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31,645][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 1.6324859857559204, acc: 0.6486486196517944)
[2025-01-06 01:02:31,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32,103][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 1.23255455493927, acc: 0.6896551847457886)
[2025-01-06 01:02:32,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32,475][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 0.8253955245018005, acc: 0.7727272510528564)
[2025-01-06 01:02:32,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32,865][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 0.4990929663181305, acc: 0.8636363744735718)
[2025-01-06 01:02:32,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33,230][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 0.3829019069671631, acc: 0.90625)
[2025-01-06 01:02:33,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33,597][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 0.19782447814941406, acc: 0.9333333373069763)
[2025-01-06 01:02:33,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34,038][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 0.5849542021751404, acc: 0.8666666746139526)
[2025-01-06 01:02:34,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34,399][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 0.3207775950431824, acc: 0.875)
[2025-01-06 01:02:34,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34,759][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 0.510931134223938, acc: 0.8999999761581421)
[2025-01-06 01:02:34,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35,084][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 0.5002748370170593, acc: 0.7931034564971924)
[2025-01-06 01:02:35,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35,434][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 0.4310816526412964, acc: 0.9200000166893005)
[2025-01-06 01:02:35,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35,828][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 0.7206261157989502, acc: 0.8297872543334961)
[2025-01-06 01:02:35,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36,226][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 0.5919608473777771, acc: 0.875)
[2025-01-06 01:02:36,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36,565][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 0.3500055968761444, acc: 0.9090909361839294)
[2025-01-06 01:02:36,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37,016][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 1.015984058380127, acc: 0.7108433842658997)
[2025-01-06 01:02:37,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37,416][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 1.1961537599563599, acc: 0.6666666865348816)
[2025-01-06 01:02:37,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37,755][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 0.42980632185935974, acc: 0.8157894611358643)
[2025-01-06 01:02:37,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:38,127][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 1.1846035718917847, acc: 0.7058823704719543)
[2025-01-06 01:02:38,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:38,510][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 0.389043927192688, acc: 0.925000011920929)
[2025-01-06 01:02:39,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:08,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:08,933][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2298, device='cuda:0') eval_epoch_loss=tensor(0.8019, device='cuda:0') eval_epoch_acc=tensor(0.7935, device='cuda:0')
[2025-01-06 01:03:08,934][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:03:08,934][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:03:09,253][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.8019208908081055/model.pt
[2025-01-06 01:03:09,261][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:03:09,263][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.8019208908081055
[2025-01-06 01:03:09,263][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7934848666191101
[2025-01-06 01:03:09,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09,720][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 0.7133325338363647, acc: 0.8046875)
[2025-01-06 01:03:09,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10,083][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 0.9387662410736084, acc: 0.7440000176429749)
[2025-01-06 01:03:10,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10,487][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 0.6216768622398376, acc: 0.8791208863258362)
[2025-01-06 01:03:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10,871][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 0.9460833072662354, acc: 0.7888198494911194)
[2025-01-06 01:03:11,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11,245][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 0.8468931317329407, acc: 0.8144329786300659)
[2025-01-06 01:03:11,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11,574][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 0.3929460346698761, acc: 0.9090909361839294)
[2025-01-06 01:03:11,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11,881][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 0.9273978471755981, acc: 0.738095223903656)
[2025-01-06 01:03:12,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12,284][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 0.7628114223480225, acc: 0.8275862336158752)
[2025-01-06 01:03:12,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12,783][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 0.7498511672019958, acc: 0.8181818127632141)
[2025-01-06 01:03:12,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13,341][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 0.7499509453773499, acc: 0.8144329786300659)
[2025-01-06 01:03:13,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13,658][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 0.7951481342315674, acc: 0.7931034564971924)
[2025-01-06 01:03:13,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14,070][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 0.7010436654090881, acc: 0.7777777910232544)
[2025-01-06 01:03:14,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14,425][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 1.152280569076538, acc: 0.6842105388641357)
[2025-01-06 01:03:14,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14,781][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 0.2008243054151535, acc: 0.9642857313156128)
[2025-01-06 01:03:14,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15,120][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 0.3846021294593811, acc: 0.90625)
[2025-01-06 01:03:15,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15,457][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 0.741578221321106, acc: 0.849056601524353)
[2025-01-06 01:03:15,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15,784][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 0.5088059306144714, acc: 0.8867924809455872)
[2025-01-06 01:03:15,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16,152][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 0.24371017515659332, acc: 0.9411764740943909)
[2025-01-06 01:03:16,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16,547][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 0.6731700301170349, acc: 0.78125)
[2025-01-06 01:03:16,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16,897][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 0.819186270236969, acc: 0.7377049326896667)
[2025-01-06 01:03:16,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17,213][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 0.6796799302101135, acc: 0.8666666746139526)
[2025-01-06 01:03:17,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17,576][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 0.3412132263183594, acc: 0.9473684430122375)
[2025-01-06 01:03:17,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17,984][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 0.5922541618347168, acc: 0.8550724387168884)
[2025-01-06 01:03:18,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18,443][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 0.3839297294616699, acc: 0.9305555820465088)
[2025-01-06 01:03:18,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18,825][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 0.501461386680603, acc: 0.8192771077156067)
[2025-01-06 01:03:18,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19,205][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 0.6396678686141968, acc: 0.807692289352417)
[2025-01-06 01:03:19,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19,608][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 0.31354498863220215, acc: 0.9489796161651611)
[2025-01-06 01:03:19,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19,975][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 0.34681394696235657, acc: 0.875)
[2025-01-06 01:03:20,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20,308][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 0.27741196751594543, acc: 0.9166666865348816)
[2025-01-06 01:03:20,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20,636][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 0.5003311634063721, acc: 0.8709677457809448)
[2025-01-06 01:03:20,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21,008][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 1.0043245553970337, acc: 0.774193525314331)
[2025-01-06 01:03:21,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21,377][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 0.6104412078857422, acc: 0.89552241563797)
[2025-01-06 01:03:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21,745][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 0.252848744392395, acc: 0.9134615659713745)
[2025-01-06 01:03:21,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22,070][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 0.5686008930206299, acc: 0.8222222328186035)
[2025-01-06 01:03:22,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22,400][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 0.20443867146968842, acc: 0.9354838728904724)
[2025-01-06 01:03:22,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22,758][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 0.26219359040260315, acc: 0.9399999976158142)
[2025-01-06 01:03:22,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23,119][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 1.0115891695022583, acc: 0.6666666865348816)
[2025-01-06 01:03:23,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23,509][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 2.0313608646392822, acc: 0.4285714328289032)
[2025-01-06 01:03:23,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23,892][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 2.13932466506958, acc: 0.5128205418586731)
[2025-01-06 01:03:24,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24,286][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 1.9493122100830078, acc: 0.5609756112098694)
[2025-01-06 01:03:24,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24,688][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 1.3116586208343506, acc: 0.6315789222717285)
[2025-01-06 01:03:24,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25,056][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 0.9057068824768066, acc: 0.8421052694320679)
[2025-01-06 01:03:25,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25,494][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 0.42213064432144165, acc: 0.8928571343421936)
[2025-01-06 01:03:25,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25,872][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 0.5507743954658508, acc: 0.8518518805503845)
[2025-01-06 01:03:26,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26,239][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 0.3359340727329254, acc: 0.9375)
[2025-01-06 01:03:26,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26,597][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 0.6429754495620728, acc: 0.8387096524238586)
[2025-01-06 01:03:26,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26,968][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 0.37479129433631897, acc: 0.9122806787490845)
[2025-01-06 01:03:27,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27,298][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 0.28325536847114563, acc: 0.9375)
[2025-01-06 01:03:27,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27,635][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 0.3177732825279236, acc: 0.8999999761581421)
[2025-01-06 01:03:27,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27,911][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 1.0092910528182983, acc: 0.6315789222717285)
[2025-01-06 01:03:28,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28,246][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 1.2960195541381836, acc: 0.6399999856948853)
[2025-01-06 01:03:28,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28,625][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 1.6848795413970947, acc: 0.6321839094161987)
[2025-01-06 01:03:28,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29,003][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 1.6567370891571045, acc: 0.5531914830207825)
[2025-01-06 01:03:29,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29,331][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 1.5951972007751465, acc: 0.5783132314682007)
[2025-01-06 01:03:29,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29,648][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 0.6147688031196594, acc: 0.8260869383811951)
[2025-01-06 01:03:29,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29,992][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 0.9457850456237793, acc: 0.7948718070983887)
[2025-01-06 01:03:30,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30,372][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 0.7894880771636963, acc: 0.8313252925872803)
[2025-01-06 01:03:30,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30,698][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 0.7989235520362854, acc: 0.7924528121948242)
[2025-01-06 01:03:30,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31,086][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 0.36584559082984924, acc: 0.8987341523170471)
[2025-01-06 01:03:31,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31,484][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 0.1923258900642395, acc: 0.9215686321258545)
[2025-01-06 01:03:31,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31,851][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 0.7073538303375244, acc: 0.8208954930305481)
[2025-01-06 01:03:31,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32,159][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 0.3340352773666382, acc: 0.8999999761581421)
[2025-01-06 01:03:32,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32,548][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 0.9714672565460205, acc: 0.7599999904632568)
[2025-01-06 01:03:32,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32,960][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 1.1837072372436523, acc: 0.6944444179534912)
[2025-01-06 01:03:33,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33,275][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 1.1957793235778809, acc: 0.604651153087616)
[2025-01-06 01:03:33,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33,612][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 0.7172326445579529, acc: 0.7692307829856873)
[2025-01-06 01:03:33,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34,025][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 1.3927744626998901, acc: 0.5777778029441833)
[2025-01-06 01:03:34,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34,392][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 0.2453639805316925, acc: 0.8695651888847351)
[2025-01-06 01:03:34,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34,740][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 0.9511269927024841, acc: 0.692307710647583)
[2025-01-06 01:03:34,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35,123][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 1.1009798049926758, acc: 0.7032967209815979)
[2025-01-06 01:03:35,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35,685][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 1.0857340097427368, acc: 0.6521739363670349)
[2025-01-06 01:03:35,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36,071][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 1.0368748903274536, acc: 0.6739130616188049)
[2025-01-06 01:03:36,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36,473][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 0.9623553156852722, acc: 0.7142857313156128)
[2025-01-06 01:03:36,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36,809][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 0.162997767329216, acc: 0.9583333134651184)
[2025-01-06 01:03:36,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37,115][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 0.8441877961158752, acc: 0.7307692170143127)
[2025-01-06 01:03:37,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37,503][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 0.8637880682945251, acc: 0.8048780560493469)
[2025-01-06 01:03:37,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37,898][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 0.4070178270339966, acc: 0.8888888955116272)
[2025-01-06 01:03:38,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38,273][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 0.43989071249961853, acc: 0.8947368264198303)
[2025-01-06 01:03:38,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38,620][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 0.502916157245636, acc: 0.8292682766914368)
[2025-01-06 01:03:38,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38,940][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 0.634061336517334, acc: 0.8484848737716675)
[2025-01-06 01:03:39,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39,296][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 0.10641727596521378, acc: 0.9583333134651184)
[2025-01-06 01:03:39,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39,624][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 0.580049991607666, acc: 0.8260869383811951)
[2025-01-06 01:03:39,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40,003][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 0.4057846963405609, acc: 0.9285714030265808)
[2025-01-06 01:03:40,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40,382][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 0.7013149857521057, acc: 0.8125)
[2025-01-06 01:03:40,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41,031][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 0.8213034868240356, acc: 0.7515151500701904)
[2025-01-06 01:03:41,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41,953][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 0.6460599899291992, acc: 0.8207547068595886)
[2025-01-06 01:03:42,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42,302][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 0.2965221703052521, acc: 0.9111111164093018)
[2025-01-06 01:03:42,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42,636][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 0.5364047884941101, acc: 0.9285714030265808)
[2025-01-06 01:03:42,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42,974][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 0.36913201212882996, acc: 0.8857142925262451)
[2025-01-06 01:03:43,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43,285][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.051501356065273285, acc: 0.9599999785423279)
[2025-01-06 01:03:43,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43,662][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 0.37806397676467896, acc: 0.8695651888847351)
[2025-01-06 01:03:43,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43,996][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 0.2991867661476135, acc: 0.9166666865348816)
[2025-01-06 01:03:44,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44,363][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 0.1597858965396881, acc: 0.9368420839309692)
[2025-01-06 01:03:44,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44,946][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 0.38388875126838684, acc: 0.8982036113739014)
[2025-01-06 01:03:45,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:45,372][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 0.46795400977134705, acc: 0.8646616339683533)
[2025-01-06 01:03:45,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:46,635][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 0.9217604398727417, acc: 0.7433155179023743)
[2025-01-06 01:03:46,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47,191][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 0.3108471930027008, acc: 0.8828828930854797)
[2025-01-06 01:03:47,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47,489][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 0.8907738327980042, acc: 0.7857142686843872)
[2025-01-06 01:03:47,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47,784][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 0.14951595664024353, acc: 0.9642857313156128)
[2025-01-06 01:03:47,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48,162][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 0.40960273146629333, acc: 0.9375)
[2025-01-06 01:03:48,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48,517][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 0.1412639021873474, acc: 0.9722222089767456)
[2025-01-06 01:03:48,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48,875][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 0.17698317766189575, acc: 0.9736841917037964)
[2025-01-06 01:03:48,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49,223][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 0.21772700548171997, acc: 0.9545454382896423)
[2025-01-06 01:03:49,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49,632][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 0.10819413512945175, acc: 0.949999988079071)
[2025-01-06 01:03:49,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50,020][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 0.7510315179824829, acc: 0.761904776096344)
[2025-01-06 01:03:50,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50,363][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 1.4274235963821411, acc: 0.5925925970077515)
[2025-01-06 01:03:50,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50,779][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 1.1736772060394287, acc: 0.6893203854560852)
[2025-01-06 01:03:50,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51,345][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 1.2658659219741821, acc: 0.7279411554336548)
[2025-01-06 01:03:51,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51,744][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 1.101735234260559, acc: 0.6666666865348816)
[2025-01-06 01:03:51,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52,150][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 1.218258023262024, acc: 0.6736111044883728)
[2025-01-06 01:03:52,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52,494][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 1.085007667541504, acc: 0.6744186282157898)
[2025-01-06 01:03:52,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52,842][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 0.4723799228668213, acc: 0.875)
[2025-01-06 01:03:52,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53,249][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 0.6331213712692261, acc: 0.8139534592628479)
[2025-01-06 01:03:53,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53,649][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 0.2770223319530487, acc: 0.8799999952316284)
[2025-01-06 01:03:53,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54,216][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 0.6974082589149475, acc: 0.8088235259056091)
[2025-01-06 01:03:54,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54,595][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 0.6118826866149902, acc: 0.8399999737739563)
[2025-01-06 01:03:54,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54,973][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 0.6457494497299194, acc: 0.8484848737716675)
[2025-01-06 01:03:55,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55,350][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 0.7245611548423767, acc: 0.8484848737716675)
[2025-01-06 01:03:55,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55,695][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 0.7485826015472412, acc: 0.8064516186714172)
[2025-01-06 01:03:55,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56,069][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 0.30111196637153625, acc: 0.8888888955116272)
[2025-01-06 01:03:56,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56,452][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 0.3989086151123047, acc: 0.9200000166893005)
[2025-01-06 01:03:56,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56,844][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 0.40391266345977783, acc: 0.8888888955116272)
[2025-01-06 01:03:56,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57,186][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 0.4643043279647827, acc: 0.8518518805503845)
[2025-01-06 01:03:57,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57,574][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 0.22214621305465698, acc: 0.9230769276618958)
[2025-01-06 01:03:57,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58,003][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 0.3456108570098877, acc: 0.9137930870056152)
[2025-01-06 01:03:58,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58,364][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 0.23725877702236176, acc: 0.9285714030265808)
[2025-01-06 01:03:58,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58,766][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 0.29249563813209534, acc: 0.9333333373069763)
[2025-01-06 01:03:58,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59,168][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 0.5479106903076172, acc: 0.8484848737716675)
[2025-01-06 01:03:59,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59,492][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 0.3262665867805481, acc: 0.9090909361839294)
[2025-01-06 01:03:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59,918][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 0.583116352558136, acc: 0.843137264251709)
[2025-01-06 01:04:00,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00,285][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 0.47532960772514343, acc: 0.8461538553237915)
[2025-01-06 01:04:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00,598][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 0.4862183928489685, acc: 0.9444444179534912)
[2025-01-06 01:04:00,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01,002][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 0.47190576791763306, acc: 0.8999999761581421)
[2025-01-06 01:04:01,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01,373][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 0.6672390699386597, acc: 0.800000011920929)
[2025-01-06 01:04:01,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01,781][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.37585124373435974, acc: 0.9047619104385376)
[2025-01-06 01:04:01,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02,131][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 0.5132392048835754, acc: 0.8666666746139526)
[2025-01-06 01:04:02,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02,515][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 0.8660603165626526, acc: 0.78125)
[2025-01-06 01:04:02,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02,920][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 1.2172640562057495, acc: 0.6666666865348816)
[2025-01-06 01:04:03,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03,316][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 0.6477375626564026, acc: 0.8518518805503845)
[2025-01-06 01:04:03,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03,673][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 0.39961814880371094, acc: 0.939393937587738)
[2025-01-06 01:04:03,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04,065][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 0.3247663080692291, acc: 0.9130434989929199)
[2025-01-06 01:04:04,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04,416][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 0.49659693241119385, acc: 0.837837815284729)
[2025-01-06 01:04:04,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04,792][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 0.40429338812828064, acc: 0.9629629850387573)
[2025-01-06 01:04:05,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:05,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35,669][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9671, device='cuda:0') eval_epoch_loss=tensor(0.6766, device='cuda:0') eval_epoch_acc=tensor(0.8174, device='cuda:0')
[2025-01-06 01:04:35,670][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:04:35,670][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:04:35,975][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.6765642166137695/model.pt
[2025-01-06 01:04:35,979][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:04:35,979][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6765642166137695
[2025-01-06 01:04:35,980][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8173673748970032
[2025-01-06 01:04:36,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36,331][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 0.3658028542995453, acc: 0.9130434989929199)
[2025-01-06 01:04:36,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36,661][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.06116054579615593, acc: 1.0)
[2025-01-06 01:04:36,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37,070][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 0.11992242932319641, acc: 0.9629629850387573)
[2025-01-06 01:04:37,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37,440][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 0.4546505808830261, acc: 0.8260869383811951)
[2025-01-06 01:04:37,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37,829][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 0.47723668813705444, acc: 0.8888888955116272)
[2025-01-06 01:04:37,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38,253][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.021968500688672066, acc: 1.0)
[2025-01-06 01:04:38,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38,620][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 0.05711772292852402, acc: 0.9696969985961914)
[2025-01-06 01:04:38,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38,957][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 0.41889750957489014, acc: 0.8611111044883728)
[2025-01-06 01:04:39,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39,283][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 0.10279800742864609, acc: 0.9545454382896423)
[2025-01-06 01:04:39,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39,569][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.22673489153385162, acc: 0.9523809552192688)
[2025-01-06 01:04:39,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39,965][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 0.8678648471832275, acc: 0.8205128312110901)
[2025-01-06 01:04:40,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:40,482][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 0.6144784092903137, acc: 0.8636363744735718)
[2025-01-06 01:04:40,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41,273][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 0.9506895542144775, acc: 0.7440000176429749)
[2025-01-06 01:04:41,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41,692][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 0.9117076396942139, acc: 0.7419354915618896)
[2025-01-06 01:04:41,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:42,396][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 0.5459526181221008, acc: 0.8606964945793152)
[2025-01-06 01:04:42,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:42,788][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 0.24805817008018494, acc: 0.8867924809455872)
[2025-01-06 01:04:42,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43,245][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 0.4075601398944855, acc: 0.8636363744735718)
[2025-01-06 01:04:43,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43,563][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 0.9767727255821228, acc: 0.739130437374115)
[2025-01-06 01:04:43,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43,908][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 0.7544057369232178, acc: 0.807692289352417)
[2025-01-06 01:04:44,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44,252][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 0.3137170374393463, acc: 0.8928571343421936)
[2025-01-06 01:04:44,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44,578][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 0.24006490409374237, acc: 0.9701492786407471)
[2025-01-06 01:04:44,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44,988][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 0.17883466184139252, acc: 0.9722222089767456)
[2025-01-06 01:04:45,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45,372][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 0.13756370544433594, acc: 0.945652186870575)
[2025-01-06 01:04:45,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45,773][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 0.38889241218566895, acc: 0.8589743375778198)
[2025-01-06 01:04:45,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46,210][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 0.5080274939537048, acc: 0.8552631735801697)
[2025-01-06 01:04:46,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46,622][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 0.3115530014038086, acc: 0.918367326259613)
[2025-01-06 01:04:46,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47,028][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 0.505463182926178, acc: 0.8484848737716675)
[2025-01-06 01:04:47,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47,400][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 0.8159522414207458, acc: 0.8350515365600586)
[2025-01-06 01:04:47,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47,733][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 0.09703460335731506, acc: 0.9714285731315613)
[2025-01-06 01:04:47,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48,133][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 0.5424477458000183, acc: 0.8779069781303406)
[2025-01-06 01:04:48,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48,470][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 0.12435363233089447, acc: 0.9285714030265808)
[2025-01-06 01:04:48,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48,815][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 0.4258556663990021, acc: 0.9012345671653748)
[2025-01-06 01:04:48,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49,197][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 0.644440770149231, acc: 0.8611111044883728)
[2025-01-06 01:04:49,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49,579][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 0.33825036883354187, acc: 0.875)
[2025-01-06 01:04:49,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49,889][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 0.7706355452537537, acc: 0.8846153616905212)
[2025-01-06 01:04:49,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50,159][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 0.48913922905921936, acc: 0.8260869383811951)
[2025-01-06 01:04:50,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50,470][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 0.5132101774215698, acc: 0.8333333134651184)
[2025-01-06 01:04:50,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50,861][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 0.5932947993278503, acc: 0.8674699068069458)
[2025-01-06 01:04:51,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51,316][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 0.44416871666908264, acc: 0.8828828930854797)
[2025-01-06 01:04:51,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51,691][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 0.927232027053833, acc: 0.7766990065574646)
[2025-01-06 01:04:51,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52,026][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 0.7204468250274658, acc: 0.8130081295967102)
[2025-01-06 01:04:52,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52,328][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 0.40125688910484314, acc: 0.875)
[2025-01-06 01:04:52,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52,639][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 0.8564974665641785, acc: 0.7142857313156128)
[2025-01-06 01:04:52,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53,081][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 1.038540005683899, acc: 0.6764705777168274)
[2025-01-06 01:04:53,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53,458][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 0.883495032787323, acc: 0.7641921639442444)
[2025-01-06 01:04:53,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53,878][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 0.8723722100257874, acc: 0.78125)
[2025-01-06 01:04:54,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54,215][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 0.5445380806922913, acc: 0.8527607321739197)
[2025-01-06 01:04:54,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54,542][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 0.554931640625, acc: 0.8417266011238098)
[2025-01-06 01:04:54,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54,934][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 1.0867276191711426, acc: 0.6783919334411621)
[2025-01-06 01:04:55,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55,316][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 0.9756627678871155, acc: 0.6666666865348816)
[2025-01-06 01:04:55,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55,696][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 0.9773929715156555, acc: 0.7878788113594055)
[2025-01-06 01:04:55,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56,060][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 0.9156703352928162, acc: 0.8148148059844971)
[2025-01-06 01:04:56,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56,432][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 0.7452800869941711, acc: 0.75)
[2025-01-06 01:04:56,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56,810][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 1.654907464981079, acc: 0.5)
[2025-01-06 01:04:57,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57,254][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.1997735500335693, acc: 0.6206896305084229)
[2025-01-06 01:04:57,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57,605][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 0.4169554114341736, acc: 0.9032257795333862)
[2025-01-06 01:04:57,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57,978][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 1.0760122537612915, acc: 0.7368420958518982)
[2025-01-06 01:04:58,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58,323][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 1.6697596311569214, acc: 0.5925925970077515)
[2025-01-06 01:04:58,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58,647][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 0.8864482045173645, acc: 0.761904776096344)
[2025-01-06 01:04:58,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58,996][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 1.0888912677764893, acc: 0.8181818127632141)
[2025-01-06 01:04:59,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59,370][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 1.2934883832931519, acc: 0.6615384817123413)
[2025-01-06 01:04:59,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59,730][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 0.7877187132835388, acc: 0.8333333134651184)
[2025-01-06 01:04:59,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00,080][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 1.1029647588729858, acc: 0.7241379022598267)
[2025-01-06 01:05:00,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00,454][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 0.6794244647026062, acc: 0.7843137383460999)
[2025-01-06 01:05:00,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00,819][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 0.6318362951278687, acc: 0.7241379022598267)
[2025-01-06 01:05:00,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01,186][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.8278921842575073, acc: 0.7368420958518982)
[2025-01-06 01:05:01,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01,517][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 1.1137114763259888, acc: 0.7368420958518982)
[2025-01-06 01:05:01,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01,940][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 0.7488706707954407, acc: 0.8035714030265808)
[2025-01-06 01:05:02,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02,358][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 0.5937235951423645, acc: 0.8426966071128845)
[2025-01-06 01:05:02,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02,782][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 0.9788896441459656, acc: 0.7191011309623718)
[2025-01-06 01:05:02,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03,151][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 1.3724110126495361, acc: 0.5957446694374084)
[2025-01-06 01:05:03,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03,537][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 1.0241119861602783, acc: 0.695652186870575)
[2025-01-06 01:05:03,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03,913][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.09170521795749664, acc: 1.0)
[2025-01-06 01:05:04,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04,306][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 0.3901512920856476, acc: 0.9230769276618958)
[2025-01-06 01:05:04,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04,632][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 0.5045039057731628, acc: 0.8518518805503845)
[2025-01-06 01:05:04,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04,945][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 0.5534527897834778, acc: 0.7777777910232544)
[2025-01-06 01:05:05,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05,311][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 0.8159187436103821, acc: 0.8301886916160583)
[2025-01-06 01:05:05,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05,663][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 0.730451226234436, acc: 0.7931034564971924)
[2025-01-06 01:05:05,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06,327][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 1.268875241279602, acc: 0.6576576828956604)
[2025-01-06 01:05:06,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06,802][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 0.9963908791542053, acc: 0.7464788556098938)
[2025-01-06 01:05:06,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07,164][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.32577764987945557, acc: 0.8500000238418579)
[2025-01-06 01:05:07,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07,456][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.578191876411438, acc: 0.800000011920929)
[2025-01-06 01:05:07,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07,840][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 0.9120505452156067, acc: 0.7692307829856873)
[2025-01-06 01:05:09,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:10,552][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 1.4155980348587036, acc: 0.6428571343421936)
[2025-01-06 01:05:10,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:11,323][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 0.5361999273300171, acc: 0.841269850730896)
[2025-01-06 01:05:11,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:11,697][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 0.7895970940589905, acc: 0.75)
[2025-01-06 01:05:11,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:12,088][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 0.24980181455612183, acc: 0.9333333373069763)
[2025-01-06 01:05:12,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:12,825][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 0.8449305295944214, acc: 0.7638888955116272)
[2025-01-06 01:05:12,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13,191][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.020985933020710945, acc: 1.0)
[2025-01-06 01:05:13,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13,534][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 0.285753458738327, acc: 0.9032257795333862)
[2025-01-06 01:05:13,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13,906][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 0.720779538154602, acc: 0.75)
[2025-01-06 01:05:14,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:14,224][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 0.6868394017219543, acc: 0.7407407164573669)
[2025-01-06 01:05:14,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15,263][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 0.7544456720352173, acc: 0.7711864113807678)
[2025-01-06 01:05:15,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15,615][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 0.3670450448989868, acc: 0.888059675693512)
[2025-01-06 01:05:15,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16,008][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 0.4992264211177826, acc: 0.8540145754814148)
[2025-01-06 01:05:16,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16,587][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 0.7697305083274841, acc: 0.800000011920929)
[2025-01-06 01:05:16,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16,918][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 0.1681254357099533, acc: 0.9629629850387573)
[2025-01-06 01:05:17,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17,272][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 0.3108213245868683, acc: 0.9230769276618958)
[2025-01-06 01:05:17,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17,588][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 1.0260882377624512, acc: 0.7142857313156128)
[2025-01-06 01:05:17,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17,947][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 2.161731004714966, acc: 0.4590163826942444)
[2025-01-06 01:05:18,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18,314][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 0.5015478730201721, acc: 0.8305084705352783)
[2025-01-06 01:05:18,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18,665][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 1.4623866081237793, acc: 0.6279069781303406)
[2025-01-06 01:05:18,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19,049][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 1.2591017484664917, acc: 0.6818181872367859)
[2025-01-06 01:05:19,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19,405][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 1.2514292001724243, acc: 0.6226415038108826)
[2025-01-06 01:05:19,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19,753][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 1.0817354917526245, acc: 0.6818181872367859)
[2025-01-06 01:05:19,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20,118][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 0.8553323149681091, acc: 0.6399999856948853)
[2025-01-06 01:05:20,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20,485][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 0.8430142402648926, acc: 0.800000011920929)
[2025-01-06 01:05:20,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20,907][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 0.4091193377971649, acc: 0.9090909361839294)
[2025-01-06 01:05:21,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21,337][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 0.8808736205101013, acc: 0.7538461685180664)
[2025-01-06 01:05:21,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21,723][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 0.8204123973846436, acc: 0.703125)
[2025-01-06 01:05:21,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22,162][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 0.7529906630516052, acc: 0.75)
[2025-01-06 01:05:22,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22,507][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 0.9976338744163513, acc: 0.6666666865348816)
[2025-01-06 01:05:22,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22,888][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.7829928994178772, acc: 0.6875)
[2025-01-06 01:05:22,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23,210][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 0.22586704790592194, acc: 0.9032257795333862)
[2025-01-06 01:05:23,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23,545][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.13436445593833923, acc: 0.95652174949646)
[2025-01-06 01:05:23,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23,905][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 0.40904441475868225, acc: 0.8666666746139526)
[2025-01-06 01:05:24,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24,314][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 0.1952754259109497, acc: 0.9756097793579102)
[2025-01-06 01:05:24,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24,694][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 0.04242555797100067, acc: 1.0)
[2025-01-06 01:05:24,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25,045][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 0.18120919167995453, acc: 0.9210526347160339)
[2025-01-06 01:05:25,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25,376][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 0.4153086841106415, acc: 0.9354838728904724)
[2025-01-06 01:05:25,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25,696][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.029889078810811043, acc: 1.0)
[2025-01-06 01:05:25,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25,996][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 0.5840049982070923, acc: 0.8181818127632141)
[2025-01-06 01:05:26,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26,364][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 0.23057611286640167, acc: 0.8999999761581421)
[2025-01-06 01:05:26,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26,689][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 0.32205086946487427, acc: 0.9285714030265808)
[2025-01-06 01:05:26,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27,040][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 0.5501851439476013, acc: 0.8540145754814148)
[2025-01-06 01:05:27,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27,467][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 0.3995909094810486, acc: 0.8896551728248596)
[2025-01-06 01:05:27,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27,828][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 0.4819357693195343, acc: 0.8785714507102966)
[2025-01-06 01:05:27,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28,163][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 0.5657161474227905, acc: 0.860927164554596)
[2025-01-06 01:05:28,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28,532][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 0.4444589912891388, acc: 0.8803418874740601)
[2025-01-06 01:05:28,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28,869][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.37140074372291565, acc: 0.9200000166893005)
[2025-01-06 01:05:28,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29,230][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 0.5242212414741516, acc: 0.9230769276618958)
[2025-01-06 01:05:29,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29,587][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 0.19299966096878052, acc: 0.9230769276618958)
[2025-01-06 01:05:29,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29,967][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 0.22862288355827332, acc: 0.9487179517745972)
[2025-01-06 01:05:30,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30,378][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 0.5631411075592041, acc: 0.8666666746139526)
[2025-01-06 01:05:30,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30,723][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 0.4802827835083008, acc: 0.8831169009208679)
[2025-01-06 01:05:30,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31,056][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 0.6044090986251831, acc: 0.7708333134651184)
[2025-01-06 01:05:31,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31,473][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 0.32538947463035583, acc: 0.8793103694915771)
[2025-01-06 01:05:31,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31,796][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 0.46807920932769775, acc: 0.9047619104385376)
[2025-01-06 01:05:31,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32,103][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 0.08282721042633057, acc: 0.9736841917037964)
[2025-01-06 01:05:32,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32,473][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 0.14750856161117554, acc: 0.9629629850387573)
[2025-01-06 01:05:32,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32,878][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 0.2587588429450989, acc: 0.9358288645744324)
[2025-01-06 01:05:32,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33,164][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 0.022803906351327896, acc: 1.0)
[2025-01-06 01:05:33,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33,442][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 0.2815634608268738, acc: 0.9145299196243286)
[2025-01-06 01:05:34,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03,759][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8495, device='cuda:0') eval_epoch_loss=tensor(0.6149, device='cuda:0') eval_epoch_acc=tensor(0.8259, device='cuda:0')
[2025-01-06 01:06:03,760][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:06:03,760][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:06:04,028][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.6149211525917053/model.pt
[2025-01-06 01:06:04,032][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:06:04,033][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6149211525917053
[2025-01-06 01:06:04,034][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8259221911430359
[2025-01-06 01:06:04,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04,358][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 0.4742846190929413, acc: 0.8571428656578064)
[2025-01-06 01:06:04,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04,720][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 0.5229641199111938, acc: 0.8742138147354126)
[2025-01-06 01:06:05,244][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=2.9198, train_epoch_loss=1.0715, epoch time 371.9666225835681s
[2025-01-06 01:06:05,244][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:06:05,245][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:06:05,245][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:06:05,245][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-01-06 01:06:05,245][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:06:05,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06,108][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 0.7208008766174316, acc: 0.8148148059844971)
[2025-01-06 01:06:06,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06,420][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 0.6486573815345764, acc: 0.800000011920929)
[2025-01-06 01:06:06,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06,731][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 1.1583443880081177, acc: 0.7297297120094299)
[2025-01-06 01:06:06,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07,090][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 0.7375479340553284, acc: 0.8157894611358643)
[2025-01-06 01:06:07,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07,424][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 0.9777830839157104, acc: 0.7567567825317383)
[2025-01-06 01:06:07,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07,715][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 0.42351865768432617, acc: 0.7857142686843872)
[2025-01-06 01:06:07,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:08,048][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 1.0758931636810303, acc: 0.6326530575752258)
[2025-01-06 01:06:08,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:08,411][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 0.6642690300941467, acc: 0.8333333134651184)
[2025-01-06 01:06:08,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:08,783][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.2705538272857666, acc: 0.8636363744735718)
[2025-01-06 01:06:08,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09,165][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.21263140439987183, acc: 0.9230769276618958)
[2025-01-06 01:06:09,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09,524][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 0.39379459619522095, acc: 0.8888888955116272)
[2025-01-06 01:06:09,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09,886][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 0.5820255875587463, acc: 0.7948718070983887)
[2025-01-06 01:06:10,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10,275][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 0.21597599983215332, acc: 0.9090909361839294)
[2025-01-06 01:06:10,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10,674][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 0.46526774764060974, acc: 0.8695651888847351)
[2025-01-06 01:06:10,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11,027][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 0.2789430022239685, acc: 0.9411764740943909)
[2025-01-06 01:06:11,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11,362][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 0.4552028775215149, acc: 0.8979591727256775)
[2025-01-06 01:06:11,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11,708][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.38318929076194763, acc: 0.8947368264198303)
[2025-01-06 01:06:11,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12,086][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 0.7053587436676025, acc: 0.7916666865348816)
[2025-01-06 01:06:12,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12,441][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 0.7032404541969299, acc: 0.75)
[2025-01-06 01:06:12,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12,795][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 0.6624778509140015, acc: 0.7894737124443054)
[2025-01-06 01:06:12,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13,154][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 0.4520837068557739, acc: 0.807692289352417)
[2025-01-06 01:06:13,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13,487][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 0.931245744228363, acc: 0.8275862336158752)
[2025-01-06 01:06:13,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13,831][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 1.107893705368042, acc: 0.6399999856948853)
[2025-01-06 01:06:13,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14,205][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 0.9914445281028748, acc: 0.8095238208770752)
[2025-01-06 01:06:14,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14,552][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 0.22374595701694489, acc: 1.0)
[2025-01-06 01:06:14,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14,875][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 0.7920220494270325, acc: 0.7735849022865295)
[2025-01-06 01:06:14,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:15,214][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 0.9703331589698792, acc: 0.7260273694992065)
[2025-01-06 01:06:15,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16,493][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 1.2191190719604492, acc: 0.6798418760299683)
[2025-01-06 01:06:16,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16,795][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 0.5640444159507751, acc: 0.7906976938247681)
[2025-01-06 01:06:16,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17,143][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 0.7830657958984375, acc: 0.7951807379722595)
[2025-01-06 01:06:17,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17,522][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 0.7488556504249573, acc: 0.7777777910232544)
[2025-01-06 01:06:17,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17,903][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 0.7940849661827087, acc: 0.75)
[2025-01-06 01:06:17,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18,241][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 0.5730765461921692, acc: 0.8148148059844971)
[2025-01-06 01:06:18,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18,627][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 0.2982548773288727, acc: 0.9130434989929199)
[2025-01-06 01:06:18,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18,990][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 0.7088443040847778, acc: 0.7731092572212219)
[2025-01-06 01:06:19,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:19,313][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 0.4227296710014343, acc: 0.868852436542511)
[2025-01-06 01:06:19,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:19,713][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 0.6198167204856873, acc: 0.8253968358039856)
[2025-01-06 01:06:19,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20,053][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 0.6703565716743469, acc: 0.8305084705352783)
[2025-01-06 01:06:20,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20,432][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 0.5116943120956421, acc: 0.8735632300376892)
[2025-01-06 01:06:20,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20,714][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 0.6663711071014404, acc: 0.761904776096344)
[2025-01-06 01:06:20,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21,014][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 0.6128401160240173, acc: 0.8461538553237915)
[2025-01-06 01:06:21,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21,434][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 0.4262574315071106, acc: 0.8918918967247009)
[2025-01-06 01:06:21,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21,825][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 0.6774780750274658, acc: 0.8153846263885498)
[2025-01-06 01:06:21,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22,243][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 0.7154717445373535, acc: 0.8787878751754761)
[2025-01-06 01:06:22,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22,664][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 0.5029227137565613, acc: 0.8556700944900513)
[2025-01-06 01:06:22,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23,054][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 0.5411576628684998, acc: 0.8308823704719543)
[2025-01-06 01:06:23,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23,421][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.47124770283699036, acc: 0.7307692170143127)
[2025-01-06 01:06:23,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23,763][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.45721790194511414, acc: 0.9259259104728699)
[2025-01-06 01:06:23,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24,089][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 0.46174582839012146, acc: 0.8928571343421936)
[2025-01-06 01:06:24,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24,442][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 0.11038201302289963, acc: 1.0)
[2025-01-06 01:06:24,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24,771][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 0.9210165143013, acc: 0.7543859481811523)
[2025-01-06 01:06:24,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25,114][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 0.8519516587257385, acc: 0.7460317611694336)
[2025-01-06 01:06:25,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25,509][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 1.2543967962265015, acc: 0.7183098793029785)
[2025-01-06 01:06:25,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25,965][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 1.5136117935180664, acc: 0.5400000214576721)
[2025-01-06 01:06:26,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26,277][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 1.2429218292236328, acc: 0.5945945978164673)
[2025-01-06 01:06:26,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26,581][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.171199768781662, acc: 0.9615384340286255)
[2025-01-06 01:06:28,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:29,698][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 1.333075761795044, acc: 0.658703088760376)
[2025-01-06 01:06:30,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:30,891][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 1.3744534254074097, acc: 0.6296296119689941)
[2025-01-06 01:06:31,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:31,537][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 0.958892822265625, acc: 0.6931818127632141)
[2025-01-06 01:06:31,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:32,103][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 0.3630378544330597, acc: 0.8897058963775635)
[2025-01-06 01:06:32,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:32,662][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 0.9544416666030884, acc: 0.7028985619544983)
[2025-01-06 01:06:32,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33,119][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 0.6780011653900146, acc: 0.8125)
[2025-01-06 01:06:33,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33,502][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 0.6941309571266174, acc: 0.7941176295280457)
[2025-01-06 01:06:33,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33,910][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 0.5019770264625549, acc: 0.8611111044883728)
[2025-01-06 01:06:34,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34,279][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 0.3379836678504944, acc: 0.890625)
[2025-01-06 01:06:34,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34,543][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.19241288304328918, acc: 0.8965517282485962)
[2025-01-06 01:06:34,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34,912][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 1.0396617650985718, acc: 0.8035714030265808)
[2025-01-06 01:06:35,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35,304][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 0.7483240962028503, acc: 0.7666666507720947)
[2025-01-06 01:06:35,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35,670][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.1256123185157776, acc: 0.9599999785423279)
[2025-01-06 01:06:35,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36,047][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 0.8254468441009521, acc: 0.8333333134651184)
[2025-01-06 01:06:36,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36,393][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 1.1265501976013184, acc: 0.7272727489471436)
[2025-01-06 01:06:36,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36,760][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 1.0208508968353271, acc: 0.6764705777168274)
[2025-01-06 01:06:36,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37,112][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 0.8443939089775085, acc: 0.761904776096344)
[2025-01-06 01:06:37,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37,469][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 1.2488926649093628, acc: 0.656410276889801)
[2025-01-06 01:06:37,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37,829][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 1.2419986724853516, acc: 0.6836734414100647)
[2025-01-06 01:06:37,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38,185][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 1.3080780506134033, acc: 0.6492537260055542)
[2025-01-06 01:06:38,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38,592][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 1.4568694829940796, acc: 0.5948905348777771)
[2025-01-06 01:06:38,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38,904][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.12921856343746185, acc: 0.9523809552192688)
[2025-01-06 01:06:38,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39,203][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.3656540811061859, acc: 0.9166666865348816)
[2025-01-06 01:06:39,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39,568][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 0.15819032490253448, acc: 1.0)
[2025-01-06 01:06:39,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39,944][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 0.519710898399353, acc: 0.8461538553237915)
[2025-01-06 01:06:40,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40,334][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 0.8100433945655823, acc: 0.7692307829856873)
[2025-01-06 01:06:40,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40,671][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 0.9017245173454285, acc: 0.7884615659713745)
[2025-01-06 01:06:40,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41,040][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 0.40031886100769043, acc: 0.9375)
[2025-01-06 01:06:41,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41,433][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 0.5862709879875183, acc: 0.8405796885490417)
[2025-01-06 01:06:41,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41,827][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 0.7837892770767212, acc: 0.7799999713897705)
[2025-01-06 01:06:41,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42,199][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 0.6702028512954712, acc: 0.8695651888847351)
[2025-01-06 01:06:42,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42,685][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 0.9467266798019409, acc: 0.6800000071525574)
[2025-01-06 01:06:42,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43,038][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 0.792012095451355, acc: 0.7864077687263489)
[2025-01-06 01:06:43,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44,092][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 0.9849845170974731, acc: 0.762135922908783)
[2025-01-06 01:06:44,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44,915][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 1.1861587762832642, acc: 0.6881720423698425)
[2025-01-06 01:06:45,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:45,714][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 1.124176025390625, acc: 0.6896551847457886)
[2025-01-06 01:06:45,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:46,453][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 0.7182541489601135, acc: 0.7684210538864136)
[2025-01-06 01:06:46,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:47,438][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 1.6024290323257446, acc: 0.5247524976730347)
[2025-01-06 01:06:47,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:47,720][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 1.2008962631225586, acc: 0.6612903475761414)
[2025-01-06 01:06:47,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48,024][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 0.8567084670066833, acc: 0.739130437374115)
[2025-01-06 01:06:48,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48,351][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 1.226864218711853, acc: 0.6638655662536621)
[2025-01-06 01:06:48,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48,718][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 1.2365658283233643, acc: 0.6730769276618958)
[2025-01-06 01:06:48,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49,130][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 1.2611770629882812, acc: 0.6496350169181824)
[2025-01-06 01:06:49,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49,483][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 1.7294172048568726, acc: 0.5373134613037109)
[2025-01-06 01:06:49,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49,823][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 0.6444767117500305, acc: 0.8500000238418579)
[2025-01-06 01:06:49,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50,207][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.04562711343169212, acc: 1.0)
[2025-01-06 01:06:50,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50,547][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 0.19107136130332947, acc: 0.95652174949646)
[2025-01-06 01:06:50,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50,895][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 0.15379564464092255, acc: 0.9545454382896423)
[2025-01-06 01:06:50,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51,212][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 0.586373507976532, acc: 0.8620689511299133)
[2025-01-06 01:06:51,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51,526][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 0.41103410720825195, acc: 0.8604651093482971)
[2025-01-06 01:06:51,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51,897][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 0.3832707107067108, acc: 0.800000011920929)
[2025-01-06 01:06:51,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52,240][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.047014642506837845, acc: 1.0)
[2025-01-06 01:06:52,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52,623][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.08774011582136154, acc: 0.9615384340286255)
[2025-01-06 01:06:52,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52,976][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 0.1532578021287918, acc: 0.9047619104385376)
[2025-01-06 01:06:53,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53,322][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 0.19310109317302704, acc: 0.9230769276618958)
[2025-01-06 01:06:53,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53,766][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 0.45355817675590515, acc: 0.8070175647735596)
[2025-01-06 01:06:53,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54,165][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 0.8395028114318848, acc: 0.7894737124443054)
[2025-01-06 01:06:54,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54,541][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 0.5369480848312378, acc: 0.8461538553237915)
[2025-01-06 01:06:54,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54,942][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 0.43218475580215454, acc: 0.8979591727256775)
[2025-01-06 01:06:55,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55,320][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.07857315987348557, acc: 1.0)
[2025-01-06 01:06:55,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55,711][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 0.7473755478858948, acc: 0.8253968358039856)
[2025-01-06 01:06:55,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56,115][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 0.4034920930862427, acc: 0.9024389982223511)
[2025-01-06 01:06:56,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56,451][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 0.2575814723968506, acc: 0.9354838728904724)
[2025-01-06 01:06:56,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57,332][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 0.6026501655578613, acc: 0.8441064357757568)
[2025-01-06 01:06:57,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57,710][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 0.4445089101791382, acc: 0.8399999737739563)
[2025-01-06 01:06:57,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58,139][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 0.4735879898071289, acc: 0.8846153616905212)
[2025-01-06 01:06:58,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58,483][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.39043399691581726, acc: 0.7916666865348816)
[2025-01-06 01:06:58,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58,827][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 0.569234311580658, acc: 0.8421052694320679)
[2025-01-06 01:06:58,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59,185][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 1.0754709243774414, acc: 0.6932515501976013)
[2025-01-06 01:06:59,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59,547][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 0.9913610816001892, acc: 0.7152777910232544)
[2025-01-06 01:06:59,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59,940][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 1.2290102243423462, acc: 0.6333333253860474)
[2025-01-06 01:07:00,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00,319][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 0.706498384475708, acc: 0.7916666865348816)
[2025-01-06 01:07:00,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00,710][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 0.9105314612388611, acc: 0.7333333492279053)
[2025-01-06 01:07:00,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01,126][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 1.0406720638275146, acc: 0.6764705777168274)
[2025-01-06 01:07:01,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01,484][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 0.6227989196777344, acc: 0.7692307829856873)
[2025-01-06 01:07:01,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01,834][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.7247493863105774, acc: 0.8695651888847351)
[2025-01-06 01:07:01,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02,196][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 1.062666416168213, acc: 0.78125)
[2025-01-06 01:07:02,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02,539][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 1.4002079963684082, acc: 0.5652173757553101)
[2025-01-06 01:07:02,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02,850][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 1.096394658088684, acc: 0.6571428775787354)
[2025-01-06 01:07:02,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03,192][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 1.2522703409194946, acc: 0.7307692170143127)
[2025-01-06 01:07:03,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03,513][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 0.8345078229904175, acc: 0.738095223903656)
[2025-01-06 01:07:03,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03,849][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 1.3781911134719849, acc: 0.5333333611488342)
[2025-01-06 01:07:03,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04,174][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 1.002128005027771, acc: 0.739130437374115)
[2025-01-06 01:07:04,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04,510][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 0.4716627597808838, acc: 0.8571428656578064)
[2025-01-06 01:07:04,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04,927][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 0.6792459487915039, acc: 0.7692307829856873)
[2025-01-06 01:07:05,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35,488][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8467, device='cuda:0') eval_epoch_loss=tensor(0.6134, device='cuda:0') eval_epoch_acc=tensor(0.8283, device='cuda:0')
[2025-01-06 01:07:35,489][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:07:35,490][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:07:35,732][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.6133849024772644/model.pt
[2025-01-06 01:07:35,735][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:07:35,736][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6133849024772644
[2025-01-06 01:07:35,736][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8282821774482727
[2025-01-06 01:07:35,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36,145][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 0.873360812664032, acc: 0.7096773982048035)
[2025-01-06 01:07:36,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36,479][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 0.8661066293716431, acc: 0.7027027010917664)
[2025-01-06 01:07:36,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37,015][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 0.8386752605438232, acc: 0.7280701994895935)
[2025-01-06 01:07:37,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37,363][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 1.0202281475067139, acc: 0.7014925479888916)
[2025-01-06 01:07:37,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37,756][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 0.9254411458969116, acc: 0.704081654548645)
[2025-01-06 01:07:37,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38,185][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 1.4014099836349487, acc: 0.563829779624939)
[2025-01-06 01:07:38,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38,542][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 1.0264511108398438, acc: 0.699999988079071)
[2025-01-06 01:07:38,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38,909][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 1.5003079175949097, acc: 0.5357142686843872)
[2025-01-06 01:07:39,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39,267][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 1.1794655323028564, acc: 0.695652186870575)
[2025-01-06 01:07:39,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39,601][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 0.9401271939277649, acc: 0.6896551847457886)
[2025-01-06 01:07:39,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40,002][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 1.2284879684448242, acc: 0.695652186870575)
[2025-01-06 01:07:40,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40,394][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 0.939081609249115, acc: 0.7118644118309021)
[2025-01-06 01:07:40,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40,756][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 1.2069635391235352, acc: 0.7017543911933899)
[2025-01-06 01:07:40,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41,115][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 0.9406977295875549, acc: 0.7162162065505981)
[2025-01-06 01:07:41,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41,510][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 0.6051913499832153, acc: 0.7857142686843872)
[2025-01-06 01:07:41,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41,909][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 0.8038878440856934, acc: 0.739130437374115)
[2025-01-06 01:07:42,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42,261][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 2.4875731468200684, acc: 0.31578946113586426)
[2025-01-06 01:07:42,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43,827][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 1.408858060836792, acc: 0.6351351141929626)
[2025-01-06 01:07:43,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44,186][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 1.5831395387649536, acc: 0.5185185074806213)
[2025-01-06 01:07:44,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44,615][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 1.6026121377944946, acc: 0.5581395626068115)
[2025-01-06 01:07:44,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45,201][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 1.7080305814743042, acc: 0.47058823704719543)
[2025-01-06 01:07:45,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45,753][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 1.8001832962036133, acc: 0.5730336904525757)
[2025-01-06 01:07:45,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:46,114][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 0.5737327337265015, acc: 0.8863636255264282)
[2025-01-06 01:07:46,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:46,469][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 0.6101858615875244, acc: 0.8095238208770752)
[2025-01-06 01:07:46,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:46,797][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 1.050488829612732, acc: 0.6206896305084229)
[2025-01-06 01:07:46,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47,129][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 0.24955490231513977, acc: 0.918367326259613)
[2025-01-06 01:07:47,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47,511][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 0.17228718101978302, acc: 0.9800000190734863)
[2025-01-06 01:07:47,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47,907][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 0.5935750007629395, acc: 0.8472222089767456)
[2025-01-06 01:07:47,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:48,246][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 1.0493675470352173, acc: 0.7745097875595093)
[2025-01-06 01:07:48,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49,271][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 0.9713513255119324, acc: 0.732876718044281)
[2025-01-06 01:07:49,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49,607][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 0.2291577011346817, acc: 0.9583333134651184)
[2025-01-06 01:07:49,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49,952][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 0.7591484785079956, acc: 0.7777777910232544)
[2025-01-06 01:07:50,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50,317][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 0.7749648094177246, acc: 0.7857142686843872)
[2025-01-06 01:07:50,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50,855][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 1.2081079483032227, acc: 0.7168141603469849)
[2025-01-06 01:07:50,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51,215][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 0.7791454195976257, acc: 0.8115941882133484)
[2025-01-06 01:07:51,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51,565][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 0.619605302810669, acc: 0.7954545617103577)
[2025-01-06 01:07:51,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:52,469][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 1.2161242961883545, acc: 0.6488549709320068)
[2025-01-06 01:07:52,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53,136][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 1.1498712301254272, acc: 0.644444465637207)
[2025-01-06 01:07:53,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53,440][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 0.7274362444877625, acc: 0.7868852615356445)
[2025-01-06 01:07:53,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53,770][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.03494692221283913, acc: 1.0)
[2025-01-06 01:07:53,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54,108][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 0.3803783655166626, acc: 0.9200000166893005)
[2025-01-06 01:07:54,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54,377][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 0.2723160982131958, acc: 0.9285714030265808)
[2025-01-06 01:07:54,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54,759][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 0.27729323506355286, acc: 0.9024389982223511)
[2025-01-06 01:07:54,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55,153][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 0.5812690258026123, acc: 0.861027181148529)
[2025-01-06 01:07:55,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55,463][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 0.48728063702583313, acc: 0.8703169822692871)
[2025-01-06 01:07:55,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55,941][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 0.4478732645511627, acc: 0.859375)
[2025-01-06 01:07:56,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56,465][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 0.5145640969276428, acc: 0.8649155497550964)
[2025-01-06 01:07:56,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56,902][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 0.5792753100395203, acc: 0.8398576378822327)
[2025-01-06 01:07:57,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57,235][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 0.6577731966972351, acc: 0.800000011920929)
[2025-01-06 01:07:57,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57,782][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 0.871265172958374, acc: 0.7325581312179565)
[2025-01-06 01:07:57,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58,580][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 1.3563264608383179, acc: 0.6190476417541504)
[2025-01-06 01:07:58,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:59,497][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 0.9383601546287537, acc: 0.7272727489471436)
[2025-01-06 01:07:59,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:00,239][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 0.8866698145866394, acc: 0.7764706015586853)
[2025-01-06 01:08:00,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:01,311][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 1.0334964990615845, acc: 0.6913580298423767)
[2025-01-06 01:08:01,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02,265][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 0.6327799558639526, acc: 0.7903226017951965)
[2025-01-06 01:08:02,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02,625][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.3205872178077698, acc: 0.9285714030265808)
[2025-01-06 01:08:02,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02,969][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 1.225093126296997, acc: 0.699999988079071)
[2025-01-06 01:08:03,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03,288][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 1.0174331665039062, acc: 0.720588207244873)
[2025-01-06 01:08:03,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03,604][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 0.9789033532142639, acc: 0.7352941036224365)
[2025-01-06 01:08:03,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03,981][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 0.7351197004318237, acc: 0.805084764957428)
[2025-01-06 01:08:04,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:04,338][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 1.0197926759719849, acc: 0.7388059496879578)
[2025-01-06 01:08:04,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:04,735][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 1.0243254899978638, acc: 0.7281553149223328)
[2025-01-06 01:08:04,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05,109][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 0.8751444220542908, acc: 0.7301587462425232)
[2025-01-06 01:08:05,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05,487][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 0.2478695958852768, acc: 0.9230769276618958)
[2025-01-06 01:08:05,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05,891][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 0.36324235796928406, acc: 0.9058296084403992)
[2025-01-06 01:08:06,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06,320][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 0.5173825621604919, acc: 0.8267716765403748)
[2025-01-06 01:08:06,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06,700][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 0.4304673969745636, acc: 0.8793103694915771)
[2025-01-06 01:08:06,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07,059][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 0.4933858811855316, acc: 0.8768116235733032)
[2025-01-06 01:08:07,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07,452][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 0.46488139033317566, acc: 0.8599221706390381)
[2025-01-06 01:08:07,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07,818][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 0.4092496931552887, acc: 0.8586956262588501)
[2025-01-06 01:08:07,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08,201][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 0.37518373131752014, acc: 0.8260869383811951)
[2025-01-06 01:08:08,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08,594][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 0.1353784054517746, acc: 1.0)
[2025-01-06 01:08:08,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08,982][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 0.18662075698375702, acc: 0.914893627166748)
[2025-01-06 01:08:09,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09,658][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 0.2191350907087326, acc: 0.9538461565971375)
[2025-01-06 01:08:09,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09,942][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 0.2050870656967163, acc: 0.9459459185600281)
[2025-01-06 01:08:10,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10,278][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 0.19047954678535461, acc: 0.930232584476471)
[2025-01-06 01:08:10,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10,818][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 0.30463090538978577, acc: 0.8918918967247009)
[2025-01-06 01:08:10,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11,201][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 0.181721493601799, acc: 0.9444444179534912)
[2025-01-06 01:08:11,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11,550][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.25818267464637756, acc: 0.9696969985961914)
[2025-01-06 01:08:11,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11,944][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.17193907499313354, acc: 0.9259259104728699)
[2025-01-06 01:08:12,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12,287][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.1478959321975708, acc: 0.9599999785423279)
[2025-01-06 01:08:12,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12,614][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 0.7627344727516174, acc: 0.7884615659713745)
[2025-01-06 01:08:12,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13,367][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 0.4612210690975189, acc: 0.8695651888847351)
[2025-01-06 01:08:13,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13,907][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 0.6039080619812012, acc: 0.8068181872367859)
[2025-01-06 01:08:14,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14,344][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 0.8780367970466614, acc: 0.7765957713127136)
[2025-01-06 01:08:14,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14,727][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 0.7256584763526917, acc: 0.8113207817077637)
[2025-01-06 01:08:14,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15,114][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 0.5215114951133728, acc: 0.8166666626930237)
[2025-01-06 01:08:15,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15,499][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 0.36300671100616455, acc: 0.8837209343910217)
[2025-01-06 01:08:15,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15,870][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 1.3852877616882324, acc: 0.6000000238418579)
[2025-01-06 01:08:15,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16,252][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 1.8614717721939087, acc: 0.5263158082962036)
[2025-01-06 01:08:16,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16,661][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 1.4663372039794922, acc: 0.6222222447395325)
[2025-01-06 01:08:16,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17,074][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 1.5620490312576294, acc: 0.5888888835906982)
[2025-01-06 01:08:17,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17,559][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 1.897567868232727, acc: 0.5229358077049255)
[2025-01-06 01:08:17,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18,037][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 1.663816213607788, acc: 0.5307692289352417)
[2025-01-06 01:08:18,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18,362][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 0.39869412779808044, acc: 0.8421052694320679)
[2025-01-06 01:08:18,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18,736][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 0.5083500146865845, acc: 0.875)
[2025-01-06 01:08:18,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19,079][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 1.2531882524490356, acc: 0.6818181872367859)
[2025-01-06 01:08:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19,451][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 0.5937809944152832, acc: 0.8148148059844971)
[2025-01-06 01:08:19,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19,836][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 1.0262434482574463, acc: 0.6571428775787354)
[2025-01-06 01:08:19,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20,170][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 1.2184929847717285, acc: 0.7272727489471436)
[2025-01-06 01:08:20,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20,454][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 0.8291811347007751, acc: 0.7045454382896423)
[2025-01-06 01:08:20,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21,027][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 1.214719533920288, acc: 0.5322580933570862)
[2025-01-06 01:08:21,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21,550][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 1.3463047742843628, acc: 0.6136363744735718)
[2025-01-06 01:08:21,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21,897][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.009452697820961475, acc: 1.0)
[2025-01-06 01:08:22,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22,248][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 0.7142009139060974, acc: 0.7692307829856873)
[2025-01-06 01:08:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22,602][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 0.15961939096450806, acc: 0.9354838728904724)
[2025-01-06 01:08:22,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22,957][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.191090926527977, acc: 0.949999988079071)
[2025-01-06 01:08:23,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23,295][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 0.2324734777212143, acc: 0.9189189076423645)
[2025-01-06 01:08:23,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23,633][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 0.4039711654186249, acc: 0.9189189076423645)
[2025-01-06 01:08:23,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23,971][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 0.0574905164539814, acc: 1.0)
[2025-01-06 01:08:24,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:24,359][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 0.35766148567199707, acc: 0.8676470518112183)
[2025-01-06 01:08:24,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:24,729][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.19536042213439941, acc: 0.9268292784690857)
[2025-01-06 01:08:24,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25,052][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.06356854736804962, acc: 1.0)
[2025-01-06 01:08:25,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25,418][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.028839340433478355, acc: 1.0)
[2025-01-06 01:08:25,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25,775][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.21576686203479767, acc: 0.9354838728904724)
[2025-01-06 01:08:25,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26,126][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 0.3628368079662323, acc: 0.8947368264198303)
[2025-01-06 01:08:26,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26,428][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 0.16504628956317902, acc: 0.9571428298950195)
[2025-01-06 01:08:26,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26,729][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 0.21180780231952667, acc: 0.9342105388641357)
[2025-01-06 01:08:26,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27,288][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 0.5231960415840149, acc: 0.8773584961891174)
[2025-01-06 01:08:27,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27,878][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 0.5335209369659424, acc: 0.8666666746139526)
[2025-01-06 01:08:27,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28,199][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 0.24339696764945984, acc: 0.9444444179534912)
[2025-01-06 01:08:28,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28,540][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 0.7238325476646423, acc: 0.8064516186714172)
[2025-01-06 01:08:28,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28,923][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 1.3118358850479126, acc: 0.7200000286102295)
[2025-01-06 01:08:29,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:29,341][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 0.6788750290870667, acc: 0.7708333134651184)
[2025-01-06 01:08:29,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30,223][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 1.4595978260040283, acc: 0.6240000128746033)
[2025-01-06 01:08:30,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30,618][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 1.4433612823486328, acc: 0.584269642829895)
[2025-01-06 01:08:30,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30,968][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 1.0934873819351196, acc: 0.6891891956329346)
[2025-01-06 01:08:31,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31,430][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 0.8022980690002441, acc: 0.7931034564971924)
[2025-01-06 01:08:31,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31,755][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 0.3029016852378845, acc: 0.9545454382896423)
[2025-01-06 01:08:31,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32,140][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 0.2582816183567047, acc: 0.9090909361839294)
[2025-01-06 01:08:32,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32,486][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.17005567252635956, acc: 0.9375)
[2025-01-06 01:08:32,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32,820][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 0.10974802076816559, acc: 0.9666666388511658)
[2025-01-06 01:08:32,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33,200][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 0.440248966217041, acc: 0.9166666865348816)
[2025-01-06 01:08:33,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33,568][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 0.1566820591688156, acc: 0.96875)
[2025-01-06 01:08:33,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33,950][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.4042738378047943, acc: 0.8999999761581421)
[2025-01-06 01:08:34,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34,333][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 0.4323141276836395, acc: 0.931034505367279)
[2025-01-06 01:08:34,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34,709][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 0.32182666659355164, acc: 0.9200000166893005)
[2025-01-06 01:08:34,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35,057][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 0.37258604168891907, acc: 0.8936170339584351)
[2025-01-06 01:08:35,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35,411][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 0.5757883191108704, acc: 0.8958333134651184)
[2025-01-06 01:08:35,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35,787][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 0.1899528056383133, acc: 0.9545454382896423)
[2025-01-06 01:08:35,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36,232][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 0.9998740553855896, acc: 0.6626505851745605)
[2025-01-06 01:08:36,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36,618][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 0.9177838563919067, acc: 0.7592592835426331)
[2025-01-06 01:08:36,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36,976][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 0.12658433616161346, acc: 0.9736841917037964)
[2025-01-06 01:08:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07,810][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7486, device='cuda:0') eval_epoch_loss=tensor(0.5588, device='cuda:0') eval_epoch_acc=tensor(0.8429, device='cuda:0')
[2025-01-06 01:09:07,812][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:09:07,812][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:09:08,032][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5588149428367615/model.pt
[2025-01-06 01:09:08,035][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:09:08,035][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.5588149428367615
[2025-01-06 01:09:08,036][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8429339528083801
[2025-01-06 01:09:08,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08,439][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 0.5410014986991882, acc: 0.7647058963775635)
[2025-01-06 01:09:08,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08,827][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 0.3317835330963135, acc: 0.949999988079071)
[2025-01-06 01:09:08,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09,163][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 0.4919784665107727, acc: 0.8359375)
[2025-01-06 01:09:09,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09,464][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 0.5918710231781006, acc: 0.8560000061988831)
[2025-01-06 01:09:09,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09,823][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 0.38669222593307495, acc: 0.8791208863258362)
[2025-01-06 01:09:09,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10,162][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 0.5131129622459412, acc: 0.8198757767677307)
[2025-01-06 01:09:10,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10,508][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 0.5449120998382568, acc: 0.8711340427398682)
[2025-01-06 01:09:10,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10,869][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.031286004930734634, acc: 1.0)
[2025-01-06 01:09:10,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11,211][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 0.5264348983764648, acc: 0.7857142686843872)
[2025-01-06 01:09:11,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11,550][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 0.1554819494485855, acc: 0.982758641242981)
[2025-01-06 01:09:11,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12,016][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 0.6471142768859863, acc: 0.8363636136054993)
[2025-01-06 01:09:12,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12,569][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 0.5110794901847839, acc: 0.8659793734550476)
[2025-01-06 01:09:12,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12,929][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 0.4683077037334442, acc: 0.8103448152542114)
[2025-01-06 01:09:13,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13,316][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 0.12846320867538452, acc: 0.9629629850387573)
[2025-01-06 01:09:13,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13,654][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 0.6405021548271179, acc: 0.8421052694320679)
[2025-01-06 01:09:13,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13,980][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 0.07647217810153961, acc: 1.0)
[2025-01-06 01:09:14,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14,296][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 0.08431078493595123, acc: 0.96875)
[2025-01-06 01:09:14,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14,598][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 0.45516812801361084, acc: 0.9056603908538818)
[2025-01-06 01:09:14,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14,939][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 0.058179959654808044, acc: 0.9622641801834106)
[2025-01-06 01:09:15,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15,304][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 0.12783603370189667, acc: 0.970588207244873)
[2025-01-06 01:09:15,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15,668][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 0.3417429029941559, acc: 0.90625)
[2025-01-06 01:09:15,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16,039][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 0.49240830540657043, acc: 0.868852436542511)
[2025-01-06 01:09:16,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16,380][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.14319531619548798, acc: 0.9333333373069763)
[2025-01-06 01:09:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16,728][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.11584921926259995, acc: 0.9473684430122375)
[2025-01-06 01:09:16,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17,132][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 0.3329738676548004, acc: 0.8985507488250732)
[2025-01-06 01:09:17,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17,569][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 0.19488213956356049, acc: 0.9583333134651184)
[2025-01-06 01:09:17,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17,953][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 0.1911911964416504, acc: 0.9759036302566528)
[2025-01-06 01:09:18,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18,317][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 0.40208226442337036, acc: 0.8333333134651184)
[2025-01-06 01:09:18,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18,701][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 0.1609107404947281, acc: 0.9489796161651611)
[2025-01-06 01:09:18,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19,021][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.02529752254486084, acc: 1.0)
[2025-01-06 01:09:19,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19,327][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 0.08305060118436813, acc: 0.9583333134651184)
[2025-01-06 01:09:19,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19,687][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.399039089679718, acc: 0.9032257795333862)
[2025-01-06 01:09:19,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20,046][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 0.5309193134307861, acc: 0.8709677457809448)
[2025-01-06 01:09:20,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20,429][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 0.2862647473812103, acc: 0.9253731369972229)
[2025-01-06 01:09:20,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20,783][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 0.123493991792202, acc: 0.9711538553237915)
[2025-01-06 01:09:20,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21,147][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 0.2532704174518585, acc: 0.9333333373069763)
[2025-01-06 01:09:21,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21,502][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 0.1433107554912567, acc: 0.9516128897666931)
[2025-01-06 01:09:21,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21,819][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.058755069971084595, acc: 1.0)
[2025-01-06 01:09:21,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22,167][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 0.8285386562347412, acc: 0.7037037014961243)
[2025-01-06 01:09:22,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22,533][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 1.652667760848999, acc: 0.5714285969734192)
[2025-01-06 01:09:22,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22,862][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 1.2668801546096802, acc: 0.7435897588729858)
[2025-01-06 01:09:22,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23,226][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 1.634549617767334, acc: 0.5121951103210449)
[2025-01-06 01:09:23,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23,587][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 1.1904208660125732, acc: 0.6578947305679321)
[2025-01-06 01:09:23,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23,956][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 0.5665881633758545, acc: 0.8421052694320679)
[2025-01-06 01:09:24,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24,295][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.2439710795879364, acc: 0.9285714030265808)
[2025-01-06 01:09:24,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24,681][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 0.38667964935302734, acc: 0.8888888955116272)
[2025-01-06 01:09:24,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25,066][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.1381019502878189, acc: 0.96875)
[2025-01-06 01:09:25,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25,395][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 0.29083874821662903, acc: 0.9516128897666931)
[2025-01-06 01:09:25,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25,807][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 0.17583853006362915, acc: 0.9824561476707458)
[2025-01-06 01:09:25,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26,176][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 0.1814458668231964, acc: 0.90625)
[2025-01-06 01:09:26,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26,525][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 0.15867792069911957, acc: 0.9666666388511658)
[2025-01-06 01:09:26,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26,858][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 0.4876401722431183, acc: 0.7894737124443054)
[2025-01-06 01:09:26,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27,224][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 1.0329313278198242, acc: 0.7400000095367432)
[2025-01-06 01:09:27,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27,633][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 1.3398667573928833, acc: 0.6436781883239746)
[2025-01-06 01:09:27,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27,994][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 1.4483225345611572, acc: 0.5531914830207825)
[2025-01-06 01:09:28,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28,370][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 1.3676869869232178, acc: 0.6385542154312134)
[2025-01-06 01:09:28,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28,735][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.12341728061437607, acc: 0.95652174949646)
[2025-01-06 01:09:28,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29,062][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 0.5471484661102295, acc: 0.8461538553237915)
[2025-01-06 01:09:29,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29,443][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 0.4893673360347748, acc: 0.8795180916786194)
[2025-01-06 01:09:29,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29,814][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 0.6121851801872253, acc: 0.8113207817077637)
[2025-01-06 01:09:29,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30,201][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 0.22377699613571167, acc: 0.9240506291389465)
[2025-01-06 01:09:30,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30,572][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 0.11987923830747604, acc: 0.9607843160629272)
[2025-01-06 01:09:30,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30,935][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 0.42870548367500305, acc: 0.8805969953536987)
[2025-01-06 01:09:31,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31,299][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.020947087556123734, acc: 1.0)
[2025-01-06 01:09:31,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31,650][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 0.28168216347694397, acc: 0.8799999952316284)
[2025-01-06 01:09:31,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32,049][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 0.8372288346290588, acc: 0.7777777910232544)
[2025-01-06 01:09:32,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32,368][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 0.7765555381774902, acc: 0.6744186282157898)
[2025-01-06 01:09:32,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32,753][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 0.2054610550403595, acc: 0.9743589758872986)
[2025-01-06 01:09:32,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33,115][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 0.9189299941062927, acc: 0.7111111283302307)
[2025-01-06 01:09:33,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33,457][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.03860524669289589, acc: 1.0)
[2025-01-06 01:09:33,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33,736][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 0.5844686627388, acc: 0.807692289352417)
[2025-01-06 01:09:33,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34,134][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 0.8578941226005554, acc: 0.7692307829856873)
[2025-01-06 01:09:34,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34,649][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 0.7219026684761047, acc: 0.791304349899292)
[2025-01-06 01:09:34,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34,963][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 0.6226112246513367, acc: 0.804347813129425)
[2025-01-06 01:09:35,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35,292][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 0.765417218208313, acc: 0.7551020383834839)
[2025-01-06 01:09:35,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35,644][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.009050541557371616, acc: 1.0)
[2025-01-06 01:09:35,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35,999][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.30224791169166565, acc: 0.9230769276618958)
[2025-01-06 01:09:36,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36,330][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 0.4699779152870178, acc: 0.8536585569381714)
[2025-01-06 01:09:36,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36,660][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 0.2768059968948364, acc: 0.9333333373069763)
[2025-01-06 01:09:36,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36,969][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 0.1592438519001007, acc: 0.9736841917037964)
[2025-01-06 01:09:37,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37,340][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 0.24650222063064575, acc: 0.8780487775802612)
[2025-01-06 01:09:37,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37,669][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 0.18801447749137878, acc: 0.939393937587738)
[2025-01-06 01:09:37,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38,028][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.021567748859524727, acc: 1.0)
[2025-01-06 01:09:38,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38,400][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.05651022121310234, acc: 1.0)
[2025-01-06 01:09:38,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38,731][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.22789618372917175, acc: 0.9642857313156128)
[2025-01-06 01:09:38,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39,054][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 0.21620382368564606, acc: 0.90625)
[2025-01-06 01:09:39,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39,660][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 0.45272234082221985, acc: 0.8545454740524292)
[2025-01-06 01:09:39,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40,493][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 0.34736377000808716, acc: 0.8867924809455872)
[2025-01-06 01:09:40,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40,885][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 0.212608203291893, acc: 0.9222221970558167)
[2025-01-06 01:09:40,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41,232][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 0.31225988268852234, acc: 0.9642857313156128)
[2025-01-06 01:09:41,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41,592][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.1646706759929657, acc: 0.9142857193946838)
[2025-01-06 01:09:41,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41,907][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.003307010279968381, acc: 1.0)
[2025-01-06 01:09:41,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42,228][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.01455992367118597, acc: 1.0)
[2025-01-06 01:09:42,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42,583][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 0.21936948597431183, acc: 0.9166666865348816)
[2025-01-06 01:09:42,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:43,007][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 0.04033759608864784, acc: 0.9789473414421082)
[2025-01-06 01:09:43,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:43,633][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 0.2843441665172577, acc: 0.916167676448822)
[2025-01-06 01:09:43,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:44,040][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 0.3678136169910431, acc: 0.9097744226455688)
[2025-01-06 01:09:44,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45,237][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 0.6576180458068848, acc: 0.8342245817184448)
[2025-01-06 01:09:45,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45,811][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 0.11896272748708725, acc: 0.954954981803894)
[2025-01-06 01:09:45,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46,195][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.495131254196167, acc: 0.8928571343421936)
[2025-01-06 01:09:46,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46,557][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.05110403150320053, acc: 0.9642857313156128)
[2025-01-06 01:09:46,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46,914][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.19042661786079407, acc: 0.96875)
[2025-01-06 01:09:47,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47,264][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 0.03064574860036373, acc: 0.9722222089767456)
[2025-01-06 01:09:47,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47,616][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.032798610627651215, acc: 0.9736841917037964)
[2025-01-06 01:09:47,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47,969][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.0324217863380909, acc: 1.0)
[2025-01-06 01:09:48,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:48,342][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.009395003318786621, acc: 1.0)
[2025-01-06 01:09:48,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:48,731][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.2922971248626709, acc: 0.9047619104385376)
[2025-01-06 01:09:48,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49,075][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 1.148522973060608, acc: 0.6666666865348816)
[2025-01-06 01:09:49,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49,449][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 0.9660611748695374, acc: 0.7572815418243408)
[2025-01-06 01:09:49,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49,975][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 1.188594937324524, acc: 0.7720588445663452)
[2025-01-06 01:09:50,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50,354][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 0.8501960635185242, acc: 0.753333330154419)
[2025-01-06 01:09:50,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50,759][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 0.9026190638542175, acc: 0.7430555820465088)
[2025-01-06 01:09:50,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51,144][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 0.6254448294639587, acc: 0.8139534592628479)
[2025-01-06 01:09:51,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51,504][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.22602415084838867, acc: 0.875)
[2025-01-06 01:09:51,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51,895][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 0.3454936146736145, acc: 0.8837209343910217)
[2025-01-06 01:09:51,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52,189][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 0.08710908144712448, acc: 1.0)
[2025-01-06 01:09:52,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52,720][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 0.3659229278564453, acc: 0.8823529481887817)
[2025-01-06 01:09:52,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53,079][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 0.712188184261322, acc: 0.8266666531562805)
[2025-01-06 01:09:53,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53,408][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 0.4309138357639313, acc: 0.8787878751754761)
[2025-01-06 01:09:53,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53,678][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 0.40064895153045654, acc: 0.8484848737716675)
[2025-01-06 01:09:53,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54,053][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.1417180895805359, acc: 0.9677419066429138)
[2025-01-06 01:09:54,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54,416][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 0.2110268473625183, acc: 0.9259259104728699)
[2025-01-06 01:09:54,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54,787][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.17241860926151276, acc: 0.9200000166893005)
[2025-01-06 01:09:54,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55,139][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.11225351691246033, acc: 0.9722222089767456)
[2025-01-06 01:09:55,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55,495][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.21549008786678314, acc: 0.8888888955116272)
[2025-01-06 01:09:55,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55,852][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.10385574400424957, acc: 1.0)
[2025-01-06 01:09:55,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56,211][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 0.11354184150695801, acc: 0.982758641242981)
[2025-01-06 01:09:56,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56,546][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.034977514296770096, acc: 1.0)
[2025-01-06 01:09:56,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56,864][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.06902731955051422, acc: 1.0)
[2025-01-06 01:09:56,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57,199][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.1992989331483841, acc: 0.9696969985961914)
[2025-01-06 01:09:57,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57,542][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 0.04320191219449043, acc: 1.0)
[2025-01-06 01:09:57,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57,892][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 0.41117358207702637, acc: 0.8823529481887817)
[2025-01-06 01:09:58,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58,258][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 0.3048931658267975, acc: 0.8846153616905212)
[2025-01-06 01:09:58,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58,619][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 0.2140517234802246, acc: 0.8888888955116272)
[2025-01-06 01:09:58,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58,979][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 0.29394200444221497, acc: 0.925000011920929)
[2025-01-06 01:09:59,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59,349][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 0.15621694922447205, acc: 0.949999988079071)
[2025-01-06 01:09:59,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59,692][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.12796294689178467, acc: 1.0)
[2025-01-06 01:09:59,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00,001][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 0.20657728612422943, acc: 0.9666666388511658)
[2025-01-06 01:10:00,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00,335][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 0.30431848764419556, acc: 0.875)
[2025-01-06 01:10:00,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00,685][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 0.4275369942188263, acc: 0.8611111044883728)
[2025-01-06 01:10:00,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01,032][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.2572050392627716, acc: 0.9629629850387573)
[2025-01-06 01:10:01,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01,368][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 0.11367017030715942, acc: 0.9696969985961914)
[2025-01-06 01:10:01,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01,662][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.011768035590648651, acc: 1.0)
[2025-01-06 01:10:02,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32,320][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7696, device='cuda:0') eval_epoch_loss=tensor(0.5708, device='cuda:0') eval_epoch_acc=tensor(0.8469, device='cuda:0')
[2025-01-06 01:10:32,321][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:10:32,322][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:10:32,544][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.5707646608352661/model.pt
[2025-01-06 01:10:32,547][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:10:32,548][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8469486832618713
[2025-01-06 01:10:32,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32,962][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.16360192000865936, acc: 0.9189189076423645)
[2025-01-06 01:10:33,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33,324][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.018785851076245308, acc: 1.0)
[2025-01-06 01:10:33,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33,649][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 0.15976406633853912, acc: 0.95652174949646)
[2025-01-06 01:10:33,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34,014][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.004332300275564194, acc: 1.0)
[2025-01-06 01:10:34,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34,400][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.007757706567645073, acc: 1.0)
[2025-01-06 01:10:34,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34,738][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.0825561061501503, acc: 0.95652174949646)
[2025-01-06 01:10:34,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35,129][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 0.28720852732658386, acc: 0.8888888955116272)
[2025-01-06 01:10:35,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35,484][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.0033494671806693077, acc: 1.0)
[2025-01-06 01:10:35,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35,794][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.0494907908141613, acc: 0.9696969985961914)
[2025-01-06 01:10:35,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36,108][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 0.25067150592803955, acc: 0.9166666865348816)
[2025-01-06 01:10:36,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36,416][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 0.02527969889342785, acc: 1.0)
[2025-01-06 01:10:36,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36,749][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.006398588418960571, acc: 1.0)
[2025-01-06 01:10:36,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37,082][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 0.441477507352829, acc: 0.8974359035491943)
[2025-01-06 01:10:37,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37,596][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 0.5450254678726196, acc: 0.8636363744735718)
[2025-01-06 01:10:37,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38,274][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 0.6531332731246948, acc: 0.8080000281333923)
[2025-01-06 01:10:38,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38,646][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 0.80190509557724, acc: 0.7822580933570862)
[2025-01-06 01:10:38,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39,294][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 0.45512062311172485, acc: 0.8706467747688293)
[2025-01-06 01:10:39,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39,622][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 0.13647815585136414, acc: 0.9433962106704712)
[2025-01-06 01:10:39,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40,039][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 0.24489599466323853, acc: 0.9090909361839294)
[2025-01-06 01:10:40,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40,333][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 0.4259825646877289, acc: 0.95652174949646)
[2025-01-06 01:10:40,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40,693][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 0.42165735363960266, acc: 0.9230769276618958)
[2025-01-06 01:10:40,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41,057][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.1739359050989151, acc: 0.9642857313156128)
[2025-01-06 01:10:41,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41,449][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 0.135550394654274, acc: 0.9850746393203735)
[2025-01-06 01:10:41,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41,844][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 0.09972956776618958, acc: 0.9861111044883728)
[2025-01-06 01:10:41,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42,192][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 0.08823937177658081, acc: 0.95652174949646)
[2025-01-06 01:10:42,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42,513][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 0.17861062288284302, acc: 0.9358974099159241)
[2025-01-06 01:10:42,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42,860][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 0.2765738070011139, acc: 0.9078947305679321)
[2025-01-06 01:10:42,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43,247][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 0.1136411651968956, acc: 0.9591836929321289)
[2025-01-06 01:10:43,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43,614][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 0.18896369636058807, acc: 0.939393937587738)
[2025-01-06 01:10:43,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44,001][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 0.6199153065681458, acc: 0.8453608155250549)
[2025-01-06 01:10:44,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44,385][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 0.028197135776281357, acc: 0.9857142567634583)
[2025-01-06 01:10:44,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44,789][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 0.32925671339035034, acc: 0.9069767594337463)
[2025-01-06 01:10:44,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45,148][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 0.04699961096048355, acc: 1.0)
[2025-01-06 01:10:45,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45,480][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 0.22589589655399323, acc: 0.9382715821266174)
[2025-01-06 01:10:45,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45,800][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 0.3075620234012604, acc: 0.8611111044883728)
[2025-01-06 01:10:45,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46,109][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 0.07157162576913834, acc: 1.0)
[2025-01-06 01:10:46,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46,471][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.5584648847579956, acc: 0.8846153616905212)
[2025-01-06 01:10:46,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46,825][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 0.24493679404258728, acc: 0.9347826242446899)
[2025-01-06 01:10:46,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47,202][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 0.3388485312461853, acc: 0.8928571343421936)
[2025-01-06 01:10:47,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47,575][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 0.4714629054069519, acc: 0.8554216623306274)
[2025-01-06 01:10:47,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47,926][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 0.2064892202615738, acc: 0.954954981803894)
[2025-01-06 01:10:48,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48,262][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 0.6410931944847107, acc: 0.8543689250946045)
[2025-01-06 01:10:48,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48,643][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 0.44647130370140076, acc: 0.8943089246749878)
[2025-01-06 01:10:48,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48,980][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.14008691906929016, acc: 0.9583333134651184)
[2025-01-06 01:10:49,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49,273][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 0.34323838353157043, acc: 0.8571428656578064)
[2025-01-06 01:10:49,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49,694][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 0.6213769316673279, acc: 0.813725471496582)
[2025-01-06 01:10:49,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50,097][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 0.74852454662323, acc: 0.7903929948806763)
[2025-01-06 01:10:50,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50,472][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 0.6829695701599121, acc: 0.8333333134651184)
[2025-01-06 01:10:50,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50,815][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 0.45539167523384094, acc: 0.8527607321739197)
[2025-01-06 01:10:50,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51,157][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 0.3955043852329254, acc: 0.8920863270759583)
[2025-01-06 01:10:51,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51,549][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 0.8844289183616638, acc: 0.713567852973938)
[2025-01-06 01:10:51,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51,905][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 0.548839807510376, acc: 0.8055555820465088)
[2025-01-06 01:10:52,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52,208][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 0.5780535936355591, acc: 0.8484848737716675)
[2025-01-06 01:10:52,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52,560][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.2392561137676239, acc: 0.8518518805503845)
[2025-01-06 01:10:52,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52,904][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 0.36581939458847046, acc: 0.8999999761581421)
[2025-01-06 01:10:52,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53,244][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.5579794645309448, acc: 0.800000011920929)
[2025-01-06 01:10:53,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53,620][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 0.6921526193618774, acc: 0.7931034564971924)
[2025-01-06 01:10:53,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53,911][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.30977025628089905, acc: 0.9354838728904724)
[2025-01-06 01:10:54,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54,264][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.34942832589149475, acc: 0.8947368264198303)
[2025-01-06 01:10:54,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54,618][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 0.644447386264801, acc: 0.7407407164573669)
[2025-01-06 01:10:54,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54,988][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 0.43753406405448914, acc: 0.9047619104385376)
[2025-01-06 01:10:55,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55,338][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.634667158126831, acc: 0.7727272510528564)
[2025-01-06 01:10:55,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55,712][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 0.9891632199287415, acc: 0.7384615540504456)
[2025-01-06 01:10:55,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56,047][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 0.2034786343574524, acc: 0.9666666388511658)
[2025-01-06 01:10:56,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56,354][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 0.5632674694061279, acc: 0.7931034564971924)
[2025-01-06 01:10:56,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56,661][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 0.5830202698707581, acc: 0.8039215803146362)
[2025-01-06 01:10:56,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57,026][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 0.35877758264541626, acc: 0.8620689511299133)
[2025-01-06 01:10:57,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57,363][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.35375332832336426, acc: 0.9473684430122375)
[2025-01-06 01:10:57,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57,727][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 0.971849262714386, acc: 0.7894737124443054)
[2025-01-06 01:10:57,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58,132][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 0.607971727848053, acc: 0.8125)
[2025-01-06 01:10:58,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58,507][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 0.36669304966926575, acc: 0.8876404762268066)
[2025-01-06 01:10:58,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58,824][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 0.817624032497406, acc: 0.7528089880943298)
[2025-01-06 01:10:58,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59,150][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 1.1368296146392822, acc: 0.6382978558540344)
[2025-01-06 01:10:59,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59,516][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 0.6681164503097534, acc: 0.8369565010070801)
[2025-01-06 01:10:59,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59,880][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.013707300648093224, acc: 1.0)
[2025-01-06 01:11:00,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00,256][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.08924949914216995, acc: 0.9615384340286255)
[2025-01-06 01:11:00,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00,598][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.13567867875099182, acc: 0.9259259104728699)
[2025-01-06 01:11:00,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00,927][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 0.10835915803909302, acc: 1.0)
[2025-01-06 01:11:01,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01,248][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 0.5379522442817688, acc: 0.8867924809455872)
[2025-01-06 01:11:01,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01,585][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 0.6210573315620422, acc: 0.7931034564971924)
[2025-01-06 01:11:01,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02,195][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 0.9512720108032227, acc: 0.7567567825317383)
[2025-01-06 01:11:02,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02,636][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 0.7340131402015686, acc: 0.7605633735656738)
[2025-01-06 01:11:02,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02,940][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.1591266691684723, acc: 0.949999988079071)
[2025-01-06 01:11:03,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03,293][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.29675599932670593, acc: 0.8999999761581421)
[2025-01-06 01:11:03,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03,633][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.5315117835998535, acc: 0.807692289352417)
[2025-01-06 01:11:05,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:06,313][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 0.990842342376709, acc: 0.699999988079071)
[2025-01-06 01:11:06,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07,079][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 0.22422708570957184, acc: 0.9047619104385376)
[2025-01-06 01:11:07,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07,459][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 0.5630789995193481, acc: 0.8214285969734192)
[2025-01-06 01:11:07,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07,826][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.05975300073623657, acc: 1.0)
[2025-01-06 01:11:08,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:08,521][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 0.5119724273681641, acc: 0.8611111044883728)
[2025-01-06 01:11:08,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:09,061][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.01990935392677784, acc: 1.0)
[2025-01-06 01:11:09,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:09,404][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 0.046315424144268036, acc: 1.0)
[2025-01-06 01:11:09,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:09,768][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 0.2906058430671692, acc: 0.949999988079071)
[2025-01-06 01:11:09,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:10,139][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 0.5301843285560608, acc: 0.8148148059844971)
[2025-01-06 01:11:10,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11,140][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 0.6161277294158936, acc: 0.805084764957428)
[2025-01-06 01:11:11,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11,533][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 0.223541259765625, acc: 0.9477611780166626)
[2025-01-06 01:11:11,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11,899][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 0.3892118036746979, acc: 0.9051094651222229)
[2025-01-06 01:11:12,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12,460][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 0.631954550743103, acc: 0.8550000190734863)
[2025-01-06 01:11:12,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12,807][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.07950883358716965, acc: 0.9629629850387573)
[2025-01-06 01:11:12,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13,157][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.17438122630119324, acc: 0.942307710647583)
[2025-01-06 01:11:13,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13,510][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 0.3513564467430115, acc: 0.9047619104385376)
[2025-01-06 01:11:13,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13,876][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 1.5616508722305298, acc: 0.5737704634666443)
[2025-01-06 01:11:13,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14,224][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 0.3881109952926636, acc: 0.8644067645072937)
[2025-01-06 01:11:14,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14,578][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 1.0549105405807495, acc: 0.6744186282157898)
[2025-01-06 01:11:14,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14,969][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 0.6448480486869812, acc: 0.7954545617103577)
[2025-01-06 01:11:15,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15,363][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 0.8574753999710083, acc: 0.7735849022865295)
[2025-01-06 01:11:15,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15,736][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 0.680187463760376, acc: 0.8409090638160706)
[2025-01-06 01:11:15,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16,056][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 0.34320107102394104, acc: 0.8799999952316284)
[2025-01-06 01:11:16,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16,364][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.472426176071167, acc: 0.8999999761581421)
[2025-01-06 01:11:16,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16,710][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.14588388800621033, acc: 0.9545454382896423)
[2025-01-06 01:11:16,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17,122][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 0.6489800810813904, acc: 0.8307692408561707)
[2025-01-06 01:11:17,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17,500][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 0.40987372398376465, acc: 0.875)
[2025-01-06 01:11:17,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17,917][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.4687007963657379, acc: 0.78125)
[2025-01-06 01:11:18,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18,284][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 0.5609972476959229, acc: 0.7878788113594055)
[2025-01-06 01:11:18,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18,617][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.2510058581829071, acc: 0.9375)
[2025-01-06 01:11:18,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18,969][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.06286849081516266, acc: 0.9677419066429138)
[2025-01-06 01:11:19,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19,310][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.021792108193039894, acc: 1.0)
[2025-01-06 01:11:19,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19,666][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 0.07390976697206497, acc: 0.9666666388511658)
[2025-01-06 01:11:19,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20,005][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.1422467976808548, acc: 0.9512194991111755)
[2025-01-06 01:11:20,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20,363][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.012496347539126873, acc: 1.0)
[2025-01-06 01:11:20,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20,699][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.04954984039068222, acc: 0.9736841917037964)
[2025-01-06 01:11:20,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21,012][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.17968522012233734, acc: 0.9677419066429138)
[2025-01-06 01:11:21,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21,349][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.0020025044213980436, acc: 1.0)
[2025-01-06 01:11:21,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21,718][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.27830201387405396, acc: 0.9090909361839294)
[2025-01-06 01:11:21,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22,089][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.1304420381784439, acc: 0.925000011920929)
[2025-01-06 01:11:22,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22,424][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.16317354142665863, acc: 0.9571428298950195)
[2025-01-06 01:11:22,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22,772][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 0.41737455129623413, acc: 0.8759124279022217)
[2025-01-06 01:11:22,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23,100][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 0.17063908278942108, acc: 0.9448275566101074)
[2025-01-06 01:11:23,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23,459][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 0.2852111756801605, acc: 0.9428571462631226)
[2025-01-06 01:11:23,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23,842][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 0.39311403036117554, acc: 0.9072847962379456)
[2025-01-06 01:11:23,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24,203][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 0.19363617897033691, acc: 0.9316239356994629)
[2025-01-06 01:11:24,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24,517][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.14645683765411377, acc: 0.9599999785423279)
[2025-01-06 01:11:24,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24,912][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 0.12222567200660706, acc: 0.9615384340286255)
[2025-01-06 01:11:25,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25,281][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.02235976979136467, acc: 1.0)
[2025-01-06 01:11:25,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25,598][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 0.16384169459342957, acc: 0.9487179517745972)
[2025-01-06 01:11:25,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25,935][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 0.49335387349128723, acc: 0.8888888955116272)
[2025-01-06 01:11:26,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26,214][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 0.42917200922966003, acc: 0.8961039185523987)
[2025-01-06 01:11:26,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26,537][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 0.20779003202915192, acc: 0.9375)
[2025-01-06 01:11:26,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26,855][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.24889878928661346, acc: 0.931034505367279)
[2025-01-06 01:11:26,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27,198][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 0.2689167559146881, acc: 0.9285714030265808)
[2025-01-06 01:11:27,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27,542][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.02881295420229435, acc: 1.0)
[2025-01-06 01:11:27,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27,911][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.04308350384235382, acc: 1.0)
[2025-01-06 01:11:28,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28,331][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 0.15379218757152557, acc: 0.9625668525695801)
[2025-01-06 01:11:29,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58,482][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8453, device='cuda:0') eval_epoch_loss=tensor(0.6126, device='cuda:0') eval_epoch_acc=tensor(0.8393, device='cuda:0')
[2025-01-06 01:11:58,483][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:11:58,484][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:11:58,729][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.6126416325569153/model.pt
[2025-01-06 01:11:58,733][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:11:58,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59,033][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.012046636082231998, acc: 1.0)
[2025-01-06 01:11:59,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59,408][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 0.14596819877624512, acc: 0.9316239356994629)
[2025-01-06 01:11:59,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59,731][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 0.4620581567287445, acc: 0.8571428656578064)
[2025-01-06 01:11:59,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00,067][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 0.37414711713790894, acc: 0.9182389974594116)
[2025-01-06 01:12:00,474][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.7165, train_epoch_loss=0.5403, epoch time 355.22854625433683s
[2025-01-06 01:12:00,475][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:12:00,475][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:12:00,475][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:12:00,475][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-01-06 01:12:00,475][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:12:00,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01,293][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 0.2060568481683731, acc: 0.8888888955116272)
[2025-01-06 01:12:01,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01,646][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 0.3444031774997711, acc: 0.9200000166893005)
[2025-01-06 01:12:01,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01,977][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 0.893118679523468, acc: 0.7837837934494019)
[2025-01-06 01:12:02,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02,299][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 0.36575546860694885, acc: 0.9210526347160339)
[2025-01-06 01:12:02,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02,625][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 0.44873690605163574, acc: 0.8918918967247009)
[2025-01-06 01:12:02,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02,926][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.14844803512096405, acc: 0.9642857313156128)
[2025-01-06 01:12:03,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03,303][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 0.7344887256622314, acc: 0.7755101919174194)
[2025-01-06 01:12:03,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03,678][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.2743644416332245, acc: 0.8999999761581421)
[2025-01-06 01:12:03,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04,063][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.09698138386011124, acc: 0.9545454382896423)
[2025-01-06 01:12:04,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04,409][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.02853897400200367, acc: 1.0)
[2025-01-06 01:12:04,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04,782][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.16746623814105988, acc: 0.9259259104728699)
[2025-01-06 01:12:04,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05,127][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 0.27293771505355835, acc: 0.8974359035491943)
[2025-01-06 01:12:05,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05,436][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.02517767809331417, acc: 1.0)
[2025-01-06 01:12:05,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05,824][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.19653913378715515, acc: 0.95652174949646)
[2025-01-06 01:12:05,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06,166][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 0.1556161791086197, acc: 0.9607843160629272)
[2025-01-06 01:12:06,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06,485][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 0.42290857434272766, acc: 0.918367326259613)
[2025-01-06 01:12:06,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06,850][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.23793257772922516, acc: 0.8947368264198303)
[2025-01-06 01:12:06,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07,195][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.1575743854045868, acc: 0.9166666865348816)
[2025-01-06 01:12:07,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07,542][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 0.20022082328796387, acc: 0.9166666865348816)
[2025-01-06 01:12:07,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07,916][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.12335749715566635, acc: 0.9473684430122375)
[2025-01-06 01:12:08,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08,230][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.08519307523965836, acc: 0.9615384340286255)
[2025-01-06 01:12:08,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08,571][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 0.18633291125297546, acc: 0.931034505367279)
[2025-01-06 01:12:08,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08,921][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 0.46786361932754517, acc: 0.8799999952316284)
[2025-01-06 01:12:09,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09,265][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.9079558849334717, acc: 0.8095238208770752)
[2025-01-06 01:12:09,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09,646][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.22600293159484863, acc: 0.875)
[2025-01-06 01:12:09,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10,048][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 0.46493202447891235, acc: 0.849056601524353)
[2025-01-06 01:12:10,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10,428][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 0.6114639639854431, acc: 0.8219178318977356)
[2025-01-06 01:12:10,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:11,704][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 0.7543997764587402, acc: 0.7865612506866455)
[2025-01-06 01:12:11,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12,082][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 0.3116782605648041, acc: 0.930232584476471)
[2025-01-06 01:12:12,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12,428][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 0.481818825006485, acc: 0.8313252925872803)
[2025-01-06 01:12:12,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12,797][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 0.4632972478866577, acc: 0.8641975522041321)
[2025-01-06 01:12:12,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13,185][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 0.3388075828552246, acc: 0.8571428656578064)
[2025-01-06 01:12:13,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13,545][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.36489996314048767, acc: 0.8888888955116272)
[2025-01-06 01:12:13,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13,903][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.03720909357070923, acc: 1.0)
[2025-01-06 01:12:14,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14,313][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 0.5407080054283142, acc: 0.831932783126831)
[2025-01-06 01:12:14,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14,645][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.2605383098125458, acc: 0.9016393423080444)
[2025-01-06 01:12:14,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:15,014][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 0.34652838110923767, acc: 0.8888888955116272)
[2025-01-06 01:12:15,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:15,350][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 0.4994460940361023, acc: 0.8983050584793091)
[2025-01-06 01:12:15,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:15,699][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.3342307507991791, acc: 0.8735632300376892)
[2025-01-06 01:12:15,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16,003][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.17400896549224854, acc: 0.9047619104385376)
[2025-01-06 01:12:16,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16,381][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 0.2716866433620453, acc: 0.8846153616905212)
[2025-01-06 01:12:16,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16,823][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 0.23428712785243988, acc: 0.9189189076423645)
[2025-01-06 01:12:16,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17,224][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 0.37503549456596375, acc: 0.8615384697914124)
[2025-01-06 01:12:17,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17,630][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 0.5182538032531738, acc: 0.8787878751754761)
[2025-01-06 01:12:17,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18,042][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 0.36680150032043457, acc: 0.907216489315033)
[2025-01-06 01:12:18,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18,439][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 0.33621150255203247, acc: 0.904411792755127)
[2025-01-06 01:12:18,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18,779][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.2225184589624405, acc: 0.9230769276618958)
[2025-01-06 01:12:18,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19,132][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.32145389914512634, acc: 0.9259259104728699)
[2025-01-06 01:12:19,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19,485][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.18705371022224426, acc: 0.9642857313156128)
[2025-01-06 01:12:19,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19,804][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.024326032027602196, acc: 1.0)
[2025-01-06 01:12:19,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20,171][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 0.43142732977867126, acc: 0.859649121761322)
[2025-01-06 01:12:20,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20,561][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 0.6148939728736877, acc: 0.8253968358039856)
[2025-01-06 01:12:20,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20,882][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 0.7232410311698914, acc: 0.7746478915214539)
[2025-01-06 01:12:21,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21,327][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 1.3271900415420532, acc: 0.5666666626930237)
[2025-01-06 01:12:21,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21,703][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 0.5265825390815735, acc: 0.8648648858070374)
[2025-01-06 01:12:21,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:22,055][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.040178172290325165, acc: 1.0)
[2025-01-06 01:12:23,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:25,104][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 1.2532460689544678, acc: 0.6484641432762146)
[2025-01-06 01:12:25,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26,307][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 1.0612987279891968, acc: 0.6971677541732788)
[2025-01-06 01:12:26,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26,929][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 0.6549302935600281, acc: 0.8068181872367859)
[2025-01-06 01:12:27,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:27,497][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 0.20511168241500854, acc: 0.9558823704719543)
[2025-01-06 01:12:27,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:28,064][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 0.7426593899726868, acc: 0.804347813129425)
[2025-01-06 01:12:28,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:28,494][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 0.4510098397731781, acc: 0.862500011920929)
[2025-01-06 01:12:28,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:28,833][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.3077095150947571, acc: 0.9411764740943909)
[2025-01-06 01:12:28,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:29,236][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.24751795828342438, acc: 0.9444444179534912)
[2025-01-06 01:12:29,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:29,638][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.08065885305404663, acc: 0.96875)
[2025-01-06 01:12:29,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30,023][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.020372670143842697, acc: 1.0)
[2025-01-06 01:12:30,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30,408][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 0.6039611101150513, acc: 0.8214285969734192)
[2025-01-06 01:12:30,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30,743][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 0.4417625069618225, acc: 0.8500000238418579)
[2025-01-06 01:12:30,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31,080][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.05322854965925217, acc: 0.9599999785423279)
[2025-01-06 01:12:31,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31,419][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 0.3476681113243103, acc: 0.8888888955116272)
[2025-01-06 01:12:31,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31,776][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 0.43166399002075195, acc: 0.8787878751754761)
[2025-01-06 01:12:31,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32,167][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 0.819855272769928, acc: 0.7720588445663452)
[2025-01-06 01:12:32,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32,560][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 0.7244407534599304, acc: 0.7857142686843872)
[2025-01-06 01:12:32,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32,921][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 1.1067181825637817, acc: 0.6666666865348816)
[2025-01-06 01:12:33,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33,208][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 0.9553011059761047, acc: 0.7653061151504517)
[2025-01-06 01:12:33,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33,570][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 1.0444265604019165, acc: 0.6940298676490784)
[2025-01-06 01:12:33,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33,982][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 1.2782835960388184, acc: 0.6569343209266663)
[2025-01-06 01:12:34,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34,313][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.017382508143782616, acc: 1.0)
[2025-01-06 01:12:34,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34,680][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.14397187530994415, acc: 0.9166666865348816)
[2025-01-06 01:12:34,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35,072][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.02289547771215439, acc: 1.0)
[2025-01-06 01:12:35,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35,445][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.2280210703611374, acc: 0.9230769276618958)
[2025-01-06 01:12:35,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35,798][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 0.5149118900299072, acc: 0.8461538553237915)
[2025-01-06 01:12:35,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36,089][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 0.47302940487861633, acc: 0.8653846383094788)
[2025-01-06 01:12:36,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36,469][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.2532682418823242, acc: 0.9375)
[2025-01-06 01:12:36,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36,856][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.34420958161354065, acc: 0.8985507488250732)
[2025-01-06 01:12:36,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37,208][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 0.3971780836582184, acc: 0.8799999952316284)
[2025-01-06 01:12:37,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37,565][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.18041092157363892, acc: 0.9130434989929199)
[2025-01-06 01:12:37,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38,037][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 0.5913477540016174, acc: 0.8199999928474426)
[2025-01-06 01:12:38,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38,397][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 0.6253039836883545, acc: 0.8155339956283569)
[2025-01-06 01:12:38,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39,501][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 0.7909851670265198, acc: 0.7864077687263489)
[2025-01-06 01:12:39,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40,320][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 0.9507529735565186, acc: 0.7688171863555908)
[2025-01-06 01:12:40,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41,128][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 0.929197371006012, acc: 0.7715517282485962)
[2025-01-06 01:12:41,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41,869][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 0.5900800824165344, acc: 0.8631578683853149)
[2025-01-06 01:12:42,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:42,855][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 1.0732629299163818, acc: 0.7128713130950928)
[2025-01-06 01:12:42,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:43,204][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 0.8157100081443787, acc: 0.774193525314331)
[2025-01-06 01:12:43,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:43,603][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 0.7720286846160889, acc: 0.7681159377098083)
[2025-01-06 01:12:43,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44,005][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 0.988433301448822, acc: 0.7142857313156128)
[2025-01-06 01:12:44,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44,404][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 0.8809146881103516, acc: 0.7403846383094788)
[2025-01-06 01:12:44,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44,830][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 0.9640091061592102, acc: 0.7153284549713135)
[2025-01-06 01:12:44,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45,166][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 1.1693707704544067, acc: 0.6865671873092651)
[2025-01-06 01:12:45,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45,499][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.2821902334690094, acc: 0.8999999761581421)
[2025-01-06 01:12:45,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45,837][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.010163858532905579, acc: 1.0)
[2025-01-06 01:12:45,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46,184][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.05774132162332535, acc: 1.0)
[2025-01-06 01:12:46,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46,564][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.02178250625729561, acc: 1.0)
[2025-01-06 01:12:46,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46,936][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.3920137584209442, acc: 0.9137930870056152)
[2025-01-06 01:12:47,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47,271][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.1100425273180008, acc: 0.9534883499145508)
[2025-01-06 01:12:47,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47,557][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.1302783340215683, acc: 0.9599999785423279)
[2025-01-06 01:12:47,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47,937][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.007310142740607262, acc: 1.0)
[2025-01-06 01:12:48,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48,291][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.015192924998700619, acc: 1.0)
[2025-01-06 01:12:48,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48,682][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.021089615300297737, acc: 1.0)
[2025-01-06 01:12:48,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49,024][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.10911314934492111, acc: 0.9538461565971375)
[2025-01-06 01:12:49,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49,421][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 0.3213804066181183, acc: 0.8771929740905762)
[2025-01-06 01:12:49,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49,775][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 0.37880903482437134, acc: 0.859649121761322)
[2025-01-06 01:12:49,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50,129][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 0.21338778734207153, acc: 0.9230769276618958)
[2025-01-06 01:12:50,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50,501][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.2835119366645813, acc: 0.918367326259613)
[2025-01-06 01:12:50,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50,844][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.003633946180343628, acc: 1.0)
[2025-01-06 01:12:50,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51,173][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 0.4722462296485901, acc: 0.8730158805847168)
[2025-01-06 01:12:51,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51,479][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 0.33070749044418335, acc: 0.9024389982223511)
[2025-01-06 01:12:51,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51,856][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 0.13621112704277039, acc: 0.9677419066429138)
[2025-01-06 01:12:52,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52,760][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 0.4486697018146515, acc: 0.8821292519569397)
[2025-01-06 01:12:52,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53,176][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.285171777009964, acc: 0.9066666960716248)
[2025-01-06 01:12:53,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53,592][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.4452970325946808, acc: 0.8846153616905212)
[2025-01-06 01:12:53,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53,893][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.11450314521789551, acc: 0.9583333134651184)
[2025-01-06 01:12:53,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54,246][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.2541395127773285, acc: 0.8947368264198303)
[2025-01-06 01:12:54,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54,631][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 0.8013123273849487, acc: 0.7791411280632019)
[2025-01-06 01:12:54,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55,039][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 0.8168716430664062, acc: 0.8125)
[2025-01-06 01:12:55,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55,446][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 1.024716854095459, acc: 0.7166666388511658)
[2025-01-06 01:12:55,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55,803][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 0.5629020929336548, acc: 0.8273809552192688)
[2025-01-06 01:12:55,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56,129][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 0.6331472992897034, acc: 0.8205128312110901)
[2025-01-06 01:12:56,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56,535][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 0.6862581372261047, acc: 0.779411792755127)
[2025-01-06 01:12:56,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56,882][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.688994288444519, acc: 0.8461538553237915)
[2025-01-06 01:12:56,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57,240][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.26796260476112366, acc: 0.9130434989929199)
[2025-01-06 01:12:57,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57,585][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 0.31386759877204895, acc: 0.90625)
[2025-01-06 01:12:57,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57,935][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 0.3105770945549011, acc: 0.9130434989929199)
[2025-01-06 01:12:58,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58,262][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 0.3560868799686432, acc: 0.8285714387893677)
[2025-01-06 01:12:58,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58,632][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.3365468680858612, acc: 0.8461538553237915)
[2025-01-06 01:12:58,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58,976][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 0.4032745957374573, acc: 0.8809523582458496)
[2025-01-06 01:12:59,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59,342][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 0.6619200110435486, acc: 0.7666666507720947)
[2025-01-06 01:12:59,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59,678][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 0.11411780118942261, acc: 1.0)
[2025-01-06 01:13:00,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30,150][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8039, device='cuda:0') eval_epoch_loss=tensor(0.5900, device='cuda:0') eval_epoch_acc=tensor(0.8447, device='cuda:0')
[2025-01-06 01:13:30,151][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:13:30,151][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:13:30,365][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_139_loss_0.5899622440338135/model.pt
[2025-01-06 01:13:30,368][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:13:30,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30,748][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 0.027257319539785385, acc: 1.0)
[2025-01-06 01:13:30,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31,075][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 0.32478487491607666, acc: 0.8461538553237915)
[2025-01-06 01:13:31,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31,398][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 0.22934529185295105, acc: 0.9354838728904724)
[2025-01-06 01:13:31,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31,798][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 0.3634032607078552, acc: 0.8918918967247009)
[2025-01-06 01:13:31,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32,342][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 0.5743769407272339, acc: 0.7719298005104065)
[2025-01-06 01:13:32,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32,671][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 0.7099626660346985, acc: 0.7910447716712952)
[2025-01-06 01:13:32,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33,030][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 0.5044105648994446, acc: 0.8571428656578064)
[2025-01-06 01:13:33,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33,463][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 1.1381199359893799, acc: 0.6276595592498779)
[2025-01-06 01:13:33,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33,777][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 0.37314170598983765, acc: 0.8714285492897034)
[2025-01-06 01:13:33,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34,142][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 0.4448532164096832, acc: 0.8214285969734192)
[2025-01-06 01:13:34,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34,450][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 0.43880534172058105, acc: 0.95652174949646)
[2025-01-06 01:13:34,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34,712][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 0.2008540779352188, acc: 0.9655172228813171)
[2025-01-06 01:13:34,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35,029][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 0.6929510235786438, acc: 0.804347813129425)
[2025-01-06 01:13:35,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35,401][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 0.6946059465408325, acc: 0.7966101765632629)
[2025-01-06 01:13:35,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35,704][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 0.5261437296867371, acc: 0.7894737124443054)
[2025-01-06 01:13:35,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36,019][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 0.6511924862861633, acc: 0.7972972989082336)
[2025-01-06 01:13:36,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36,311][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.11981165409088135, acc: 0.9285714030265808)
[2025-01-06 01:13:36,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36,652][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.4225808382034302, acc: 0.8695651888847351)
[2025-01-06 01:13:36,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37,014][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 1.7317616939544678, acc: 0.42105263471603394)
[2025-01-06 01:13:37,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38,625][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 0.8824605941772461, acc: 0.7432432174682617)
[2025-01-06 01:13:38,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38,904][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 0.9866980910301208, acc: 0.6481481194496155)
[2025-01-06 01:13:39,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39,295][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 1.100540041923523, acc: 0.6860465407371521)
[2025-01-06 01:13:39,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39,880][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 1.1729393005371094, acc: 0.6352941393852234)
[2025-01-06 01:13:40,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40,436][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 1.386168360710144, acc: 0.6516854166984558)
[2025-01-06 01:13:40,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40,801][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.31256037950515747, acc: 0.9090909361839294)
[2025-01-06 01:13:40,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41,075][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.40952378511428833, acc: 0.9047619104385376)
[2025-01-06 01:13:41,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41,418][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 0.5419397950172424, acc: 0.8620689511299133)
[2025-01-06 01:13:41,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41,771][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.10251650214195251, acc: 0.9795918464660645)
[2025-01-06 01:13:41,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42,131][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.13694219291210175, acc: 0.9399999976158142)
[2025-01-06 01:13:42,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42,549][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 0.31216728687286377, acc: 0.8888888955116272)
[2025-01-06 01:13:42,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42,874][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 0.9257858991622925, acc: 0.813725471496582)
[2025-01-06 01:13:43,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:43,896][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 0.6130539774894714, acc: 0.8219178318977356)
[2025-01-06 01:13:43,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44,179][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.09388357400894165, acc: 0.9583333134651184)
[2025-01-06 01:13:44,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44,439][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.5607706308364868, acc: 0.8518518805503845)
[2025-01-06 01:13:44,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44,810][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.2686103284358978, acc: 0.9642857313156128)
[2025-01-06 01:13:44,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45,352][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 0.9218355417251587, acc: 0.76106196641922)
[2025-01-06 01:13:45,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45,697][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 0.5038160681724548, acc: 0.8985507488250732)
[2025-01-06 01:13:45,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46,004][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.5445501804351807, acc: 0.8295454382896423)
[2025-01-06 01:13:46,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46,911][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 1.009447693824768, acc: 0.7404580116271973)
[2025-01-06 01:13:47,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47,578][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 0.7909722924232483, acc: 0.7851851582527161)
[2025-01-06 01:13:47,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47,883][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.3483680784702301, acc: 0.868852436542511)
[2025-01-06 01:13:47,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48,213][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.008543473668396473, acc: 1.0)
[2025-01-06 01:13:48,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48,495][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.09686777740716934, acc: 0.9599999785423279)
[2025-01-06 01:13:48,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48,787][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.08640497177839279, acc: 1.0)
[2025-01-06 01:13:48,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49,132][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 0.19964727759361267, acc: 0.9146341681480408)
[2025-01-06 01:13:49,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49,519][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 0.3667992949485779, acc: 0.9093655347824097)
[2025-01-06 01:13:49,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49,909][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 0.4055154621601105, acc: 0.8962535858154297)
[2025-01-06 01:13:50,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50,399][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 0.34985870122909546, acc: 0.887499988079071)
[2025-01-06 01:13:50,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50,924][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 0.4359094202518463, acc: 0.8724202513694763)
[2025-01-06 01:13:51,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51,349][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 0.4626369774341583, acc: 0.8612099885940552)
[2025-01-06 01:13:51,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51,732][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.11685898900032043, acc: 1.0)
[2025-01-06 01:13:51,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:52,279][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 0.6150932908058167, acc: 0.7790697813034058)
[2025-01-06 01:13:52,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53,071][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 0.9673848748207092, acc: 0.6984127163887024)
[2025-01-06 01:13:53,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53,988][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 0.6975104808807373, acc: 0.7727272510528564)
[2025-01-06 01:13:54,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54,727][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 0.5505044460296631, acc: 0.8705882430076599)
[2025-01-06 01:13:55,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55,799][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 0.8471108078956604, acc: 0.7716049551963806)
[2025-01-06 01:13:56,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:56,748][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 0.43001142144203186, acc: 0.8548387289047241)
[2025-01-06 01:13:56,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57,034][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.11924320459365845, acc: 0.9642857313156128)
[2025-01-06 01:13:57,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57,375][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 0.6108386516571045, acc: 0.8500000238418579)
[2025-01-06 01:13:57,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57,778][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 0.7595453262329102, acc: 0.7941176295280457)
[2025-01-06 01:13:57,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58,176][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 0.7141152620315552, acc: 0.7867646813392639)
[2025-01-06 01:13:58,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58,566][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 0.5076078772544861, acc: 0.8474576473236084)
[2025-01-06 01:13:58,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58,956][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 0.710196852684021, acc: 0.8059701323509216)
[2025-01-06 01:13:59,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:59,348][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 0.7993957996368408, acc: 0.7864077687263489)
[2025-01-06 01:13:59,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:59,746][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 0.6024582386016846, acc: 0.8253968358039856)
[2025-01-06 01:13:59,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00,109][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.17515261471271515, acc: 0.9450549483299255)
[2025-01-06 01:14:00,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00,486][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 0.29426896572113037, acc: 0.9282511472702026)
[2025-01-06 01:14:00,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00,880][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 0.42326661944389343, acc: 0.8582677245140076)
[2025-01-06 01:14:00,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01,212][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 0.2513430118560791, acc: 0.9181034564971924)
[2025-01-06 01:14:01,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01,580][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.3680625259876251, acc: 0.8804348111152649)
[2025-01-06 01:14:01,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01,979][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.30802080035209656, acc: 0.9066147804260254)
[2025-01-06 01:14:02,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:02,370][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 0.1704394370317459, acc: 0.9239130616188049)
[2025-01-06 01:14:02,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:02,711][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.08848095685243607, acc: 1.0)
[2025-01-06 01:14:02,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03,052][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.02174227498471737, acc: 1.0)
[2025-01-06 01:14:03,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03,402][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.07079651206731796, acc: 0.978723406791687)
[2025-01-06 01:14:03,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04,077][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.16536641120910645, acc: 0.9538461565971375)
[2025-01-06 01:14:04,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04,466][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.09391031414270401, acc: 0.9594594836235046)
[2025-01-06 01:14:04,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04,842][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.12265164405107498, acc: 0.9534883499145508)
[2025-01-06 01:14:04,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05,373][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.16672012209892273, acc: 0.954954981803894)
[2025-01-06 01:14:05,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05,766][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.11750569939613342, acc: 0.9555555582046509)
[2025-01-06 01:14:05,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06,103][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.20543494820594788, acc: 0.9696969985961914)
[2025-01-06 01:14:06,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06,438][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.06275846064090729, acc: 1.0)
[2025-01-06 01:14:06,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06,840][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.10802090913057327, acc: 0.9599999785423279)
[2025-01-06 01:14:06,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07,177][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 0.4598197340965271, acc: 0.8846153616905212)
[2025-01-06 01:14:07,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07,933][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.3086632490158081, acc: 0.91847825050354)
[2025-01-06 01:14:08,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08,470][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.4752352833747864, acc: 0.875)
[2025-01-06 01:14:08,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08,903][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 0.760499119758606, acc: 0.7765957713127136)
[2025-01-06 01:14:08,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09,242][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 0.3762703239917755, acc: 0.9056603908538818)
[2025-01-06 01:14:09,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09,573][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.2634456753730774, acc: 0.9166666865348816)
[2025-01-06 01:14:09,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09,929][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.19673588871955872, acc: 0.9534883499145508)
[2025-01-06 01:14:10,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10,284][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 0.5875488519668579, acc: 0.8333333134651184)
[2025-01-06 01:14:10,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10,634][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 1.6497446298599243, acc: 0.5684210658073425)
[2025-01-06 01:14:10,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11,008][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 1.1404857635498047, acc: 0.6777777671813965)
[2025-01-06 01:14:11,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11,420][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 1.2224854230880737, acc: 0.6777777671813965)
[2025-01-06 01:14:11,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11,902][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 1.4972894191741943, acc: 0.5825688242912292)
[2025-01-06 01:14:12,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12,368][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 1.3277275562286377, acc: 0.607692301273346)
[2025-01-06 01:14:12,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12,729][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.049027081578969955, acc: 1.0)
[2025-01-06 01:14:12,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13,061][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 0.19450069963932037, acc: 0.9583333134651184)
[2025-01-06 01:14:13,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13,430][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 0.60166335105896, acc: 0.7727272510528564)
[2025-01-06 01:14:13,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13,819][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 0.3898281455039978, acc: 0.8888888955116272)
[2025-01-06 01:14:13,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14,195][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.23786330223083496, acc: 0.9428571462631226)
[2025-01-06 01:14:14,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14,603][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 0.5951701998710632, acc: 0.8863636255264282)
[2025-01-06 01:14:14,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14,944][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 0.39266616106033325, acc: 0.8636363744735718)
[2025-01-06 01:14:15,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15,547][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 0.8482591509819031, acc: 0.7903226017951965)
[2025-01-06 01:14:15,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16,071][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 0.6510757207870483, acc: 0.8409090638160706)
[2025-01-06 01:14:16,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16,360][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.06620605289936066, acc: 0.9523809552192688)
[2025-01-06 01:14:16,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16,702][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.37400150299072266, acc: 0.8461538553237915)
[2025-01-06 01:14:16,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17,045][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.006816304754465818, acc: 1.0)
[2025-01-06 01:14:17,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17,406][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.040624819695949554, acc: 1.0)
[2025-01-06 01:14:17,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17,793][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.10346191376447678, acc: 0.9729729890823364)
[2025-01-06 01:14:17,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18,119][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.2221061885356903, acc: 0.9189189076423645)
[2025-01-06 01:14:18,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18,474][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.011697323061525822, acc: 1.0)
[2025-01-06 01:14:18,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18,816][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.12795889377593994, acc: 0.9411764740943909)
[2025-01-06 01:14:18,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19,154][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.011366854421794415, acc: 1.0)
[2025-01-06 01:14:19,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19,516][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.01878800429403782, acc: 1.0)
[2025-01-06 01:14:19,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19,842][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.057332128286361694, acc: 0.9599999785423279)
[2025-01-06 01:14:19,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20,173][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.09630005806684494, acc: 0.9677419066429138)
[2025-01-06 01:14:20,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20,507][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.07546228170394897, acc: 0.9824561476707458)
[2025-01-06 01:14:20,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20,830][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.10582731664180756, acc: 0.9428571462631226)
[2025-01-06 01:14:20,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21,223][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.08280593901872635, acc: 0.9605262875556946)
[2025-01-06 01:14:21,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21,783][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.31203410029411316, acc: 0.9245283007621765)
[2025-01-06 01:14:21,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22,361][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.4355441629886627, acc: 0.875)
[2025-01-06 01:14:22,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22,670][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.13076752424240112, acc: 0.9444444179534912)
[2025-01-06 01:14:22,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22,992][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.23131881654262543, acc: 0.9032257795333862)
[2025-01-06 01:14:23,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23,343][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 0.8845037817955017, acc: 0.800000011920929)
[2025-01-06 01:14:23,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23,666][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 0.5214489102363586, acc: 0.7916666865348816)
[2025-01-06 01:14:23,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24,492][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 1.2572312355041504, acc: 0.5759999752044678)
[2025-01-06 01:14:24,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24,818][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 1.3367042541503906, acc: 0.6292135119438171)
[2025-01-06 01:14:24,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25,204][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 0.7945707440376282, acc: 0.7702702879905701)
[2025-01-06 01:14:25,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25,653][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 0.4944681227207184, acc: 0.8793103694915771)
[2025-01-06 01:14:25,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25,978][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.16606615483760834, acc: 0.9545454382896423)
[2025-01-06 01:14:26,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26,297][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.017755301669239998, acc: 1.0)
[2025-01-06 01:14:26,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26,674][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.028288867324590683, acc: 1.0)
[2025-01-06 01:14:26,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27,054][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.05830370634794235, acc: 0.9666666388511658)
[2025-01-06 01:14:27,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27,450][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 0.20270411670207977, acc: 0.9833333492279053)
[2025-01-06 01:14:27,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27,794][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.10046447813510895, acc: 0.96875)
[2025-01-06 01:14:27,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28,175][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.11326465010643005, acc: 0.9666666388511658)
[2025-01-06 01:14:28,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28,548][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.26937195658683777, acc: 0.9655172228813171)
[2025-01-06 01:14:28,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28,912][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.20277535915374756, acc: 0.9599999785423279)
[2025-01-06 01:14:29,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29,250][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 0.16478031873703003, acc: 0.957446813583374)
[2025-01-06 01:14:29,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29,570][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.20071886479854584, acc: 0.9375)
[2025-01-06 01:14:29,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29,932][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.054474592208862305, acc: 1.0)
[2025-01-06 01:14:30,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:30,383][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 0.40017589926719666, acc: 0.8674699068069458)
[2025-01-06 01:14:31,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:37,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:37,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01,318][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8198, device='cuda:0') eval_epoch_loss=tensor(0.5987, device='cuda:0') eval_epoch_acc=tensor(0.8369, device='cuda:0')
[2025-01-06 01:15:01,319][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:15:01,319][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:15:01,534][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_282_loss_0.5987038612365723/model.pt
[2025-01-06 01:15:01,538][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:15:01,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01,939][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 0.5849902033805847, acc: 0.8333333134651184)
[2025-01-06 01:15:02,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02,284][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 0.04763147979974747, acc: 1.0)
[2025-01-06 01:15:02,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02,645][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 0.10913043469190598, acc: 0.970588207244873)
[2025-01-06 01:15:02,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03,009][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.12542232871055603, acc: 0.949999988079071)
[2025-01-06 01:15:03,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03,349][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.3992087244987488, acc: 0.859375)
[2025-01-06 01:15:03,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03,730][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 0.4904775619506836, acc: 0.8799999952316284)
[2025-01-06 01:15:03,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04,101][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.23344992101192474, acc: 0.9120879173278809)
[2025-01-06 01:15:04,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04,452][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 0.31586623191833496, acc: 0.8819875717163086)
[2025-01-06 01:15:04,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04,813][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 0.4198976755142212, acc: 0.907216489315033)
[2025-01-06 01:15:04,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05,145][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.015250337310135365, acc: 1.0)
[2025-01-06 01:15:05,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05,508][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.16720923781394958, acc: 0.9523809552192688)
[2025-01-06 01:15:05,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05,880][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.09556786715984344, acc: 0.982758641242981)
[2025-01-06 01:15:06,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06,357][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.3919914960861206, acc: 0.8727272748947144)
[2025-01-06 01:15:06,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06,905][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 0.44978663325309753, acc: 0.8865979313850403)
[2025-01-06 01:15:06,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07,204][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 0.1893802136182785, acc: 0.9482758641242981)
[2025-01-06 01:15:07,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07,561][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.24321988224983215, acc: 0.9259259104728699)
[2025-01-06 01:15:07,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07,927][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 0.2299431562423706, acc: 0.9210526347160339)
[2025-01-06 01:15:08,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08,288][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.04034150391817093, acc: 1.0)
[2025-01-06 01:15:08,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08,617][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.051214706152677536, acc: 1.0)
[2025-01-06 01:15:08,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08,989][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.171427920460701, acc: 0.9622641801834106)
[2025-01-06 01:15:09,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09,382][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.039233483374118805, acc: 0.9811320900917053)
[2025-01-06 01:15:09,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09,721][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.02715236507356167, acc: 1.0)
[2025-01-06 01:15:09,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10,048][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.07465199381113052, acc: 0.96875)
[2025-01-06 01:15:10,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10,394][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.15426598489284515, acc: 0.9836065769195557)
[2025-01-06 01:15:10,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10,735][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.042684003710746765, acc: 1.0)
[2025-01-06 01:15:10,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11,079][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.005813079420477152, acc: 1.0)
[2025-01-06 01:15:11,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11,401][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.16638267040252686, acc: 0.9710144996643066)
[2025-01-06 01:15:11,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11,839][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.09019270539283752, acc: 0.9861111044883728)
[2025-01-06 01:15:11,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12,229][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.16188284754753113, acc: 0.9638554453849792)
[2025-01-06 01:15:12,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12,604][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.22862762212753296, acc: 0.9230769276618958)
[2025-01-06 01:15:12,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12,965][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.09686516970396042, acc: 0.9795918464660645)
[2025-01-06 01:15:13,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13,292][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.0039032783824950457, acc: 1.0)
[2025-01-06 01:15:13,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13,618][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.00613338453695178, acc: 1.0)
[2025-01-06 01:15:13,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13,962][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.09487301856279373, acc: 0.9677419066429138)
[2025-01-06 01:15:14,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14,286][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.13754500448703766, acc: 0.9354838728904724)
[2025-01-06 01:15:14,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14,643][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.12086532264947891, acc: 0.9552238583564758)
[2025-01-06 01:15:14,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15,026][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.06251540035009384, acc: 0.9711538553237915)
[2025-01-06 01:15:15,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15,351][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.08839233219623566, acc: 0.9555555582046509)
[2025-01-06 01:15:15,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15,640][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.04729226231575012, acc: 0.9838709831237793)
[2025-01-06 01:15:15,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15,984][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.023461876437067986, acc: 1.0)
[2025-01-06 01:15:16,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16,354][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 0.5382065773010254, acc: 0.8518518805503845)
[2025-01-06 01:15:16,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16,721][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 0.8216984272003174, acc: 0.7142857313156128)
[2025-01-06 01:15:16,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17,106][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 0.8082858920097351, acc: 0.7692307829856873)
[2025-01-06 01:15:17,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17,496][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 0.968281090259552, acc: 0.7560975551605225)
[2025-01-06 01:15:17,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17,849][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 0.5891940593719482, acc: 0.7631579041481018)
[2025-01-06 01:15:17,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18,237][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.3267359137535095, acc: 0.8947368264198303)
[2025-01-06 01:15:18,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18,585][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.09440203756093979, acc: 0.9642857313156128)
[2025-01-06 01:15:18,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18,962][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.012528556399047375, acc: 1.0)
[2025-01-06 01:15:19,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19,301][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.007333712186664343, acc: 1.0)
[2025-01-06 01:15:19,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19,634][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.2224707156419754, acc: 0.9677419066429138)
[2025-01-06 01:15:19,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20,018][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.08615348488092422, acc: 0.9473684430122375)
[2025-01-06 01:15:20,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20,377][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.06883963942527771, acc: 1.0)
[2025-01-06 01:15:20,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20,707][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.03371613845229149, acc: 1.0)
[2025-01-06 01:15:20,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21,092][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.033646367490291595, acc: 1.0)
[2025-01-06 01:15:21,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21,490][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 0.7020953297615051, acc: 0.7799999713897705)
[2025-01-06 01:15:21,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21,884][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 0.8780990839004517, acc: 0.6896551847457886)
[2025-01-06 01:15:22,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22,259][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 1.0753487348556519, acc: 0.6489361524581909)
[2025-01-06 01:15:22,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22,648][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 0.9792063236236572, acc: 0.7108433842658997)
[2025-01-06 01:15:22,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22,981][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.006094671320170164, acc: 1.0)
[2025-01-06 01:15:23,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23,288][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 0.07162179797887802, acc: 0.9743589758872986)
[2025-01-06 01:15:23,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23,657][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 0.2895849943161011, acc: 0.9277108311653137)
[2025-01-06 01:15:23,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24,030][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.19607162475585938, acc: 0.9622641801834106)
[2025-01-06 01:15:24,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24,431][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.09779047220945358, acc: 0.9746835231781006)
[2025-01-06 01:15:24,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24,812][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.04851669445633888, acc: 0.9803921580314636)
[2025-01-06 01:15:24,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25,154][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 0.2803933620452881, acc: 0.9253731369972229)
[2025-01-06 01:15:25,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25,430][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.0027965169865638018, acc: 1.0)
[2025-01-06 01:15:25,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25,766][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.038399018347263336, acc: 1.0)
[2025-01-06 01:15:25,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26,148][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 0.5752474665641785, acc: 0.8055555820465088)
[2025-01-06 01:15:26,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26,453][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 0.34700241684913635, acc: 0.8604651093482971)
[2025-01-06 01:15:26,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26,830][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.052772462368011475, acc: 1.0)
[2025-01-06 01:15:26,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27,192][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 0.43719354271888733, acc: 0.8888888955116272)
[2025-01-06 01:15:27,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27,555][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.048340097069740295, acc: 1.0)
[2025-01-06 01:15:27,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27,937][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.0581304207444191, acc: 1.0)
[2025-01-06 01:15:28,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28,345][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 0.5963963270187378, acc: 0.8571428656578064)
[2025-01-06 01:15:28,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28,866][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 0.5707334280014038, acc: 0.834782600402832)
[2025-01-06 01:15:29,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29,248][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.4176997244358063, acc: 0.8804348111152649)
[2025-01-06 01:15:29,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29,633][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 0.4184977412223816, acc: 0.9387755393981934)
[2025-01-06 01:15:29,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29,967][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.005628162994980812, acc: 1.0)
[2025-01-06 01:15:30,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30,318][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.12599259614944458, acc: 0.9230769276618958)
[2025-01-06 01:15:30,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30,656][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.16923558712005615, acc: 0.9756097793579102)
[2025-01-06 01:15:30,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30,980][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.2530866861343384, acc: 0.9111111164093018)
[2025-01-06 01:15:31,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31,343][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.0558510459959507, acc: 0.9868420958518982)
[2025-01-06 01:15:31,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31,708][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.04268299788236618, acc: 1.0)
[2025-01-06 01:15:31,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32,037][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.04027954861521721, acc: 1.0)
[2025-01-06 01:15:32,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32,383][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.013070132583379745, acc: 1.0)
[2025-01-06 01:15:32,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32,725][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.01821347326040268, acc: 1.0)
[2025-01-06 01:15:32,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33,042][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.012743547558784485, acc: 1.0)
[2025-01-06 01:15:33,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33,388][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.009015592746436596, acc: 1.0)
[2025-01-06 01:15:33,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34,003][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 0.33470478653907776, acc: 0.8848484754562378)
[2025-01-06 01:15:34,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34,856][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.14850929379463196, acc: 0.9528301954269409)
[2025-01-06 01:15:34,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35,187][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.1376684457063675, acc: 0.9444444179534912)
[2025-01-06 01:15:35,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35,550][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.26374509930610657, acc: 0.9464285969734192)
[2025-01-06 01:15:35,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35,912][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.0360136404633522, acc: 1.0)
[2025-01-06 01:15:36,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36,233][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.0007284539169631898, acc: 1.0)
[2025-01-06 01:15:36,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36,573][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.004668225534260273, acc: 1.0)
[2025-01-06 01:15:36,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36,910][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.00719635421410203, acc: 1.0)
[2025-01-06 01:15:37,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37,318][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.022464020177721977, acc: 0.9789473414421082)
[2025-01-06 01:15:37,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37,912][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.1925467997789383, acc: 0.940119743347168)
[2025-01-06 01:15:38,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:38,341][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.2844800055027008, acc: 0.9473684430122375)
[2025-01-06 01:15:38,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39,546][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.4615322947502136, acc: 0.8502673506736755)
[2025-01-06 01:15:39,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40,107][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.1298730969429016, acc: 0.9369369149208069)
[2025-01-06 01:15:40,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40,480][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.12133944779634476, acc: 0.9642857313156128)
[2025-01-06 01:15:40,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40,847][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.01266528107225895, acc: 1.0)
[2025-01-06 01:15:40,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41,177][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.02664697729051113, acc: 0.96875)
[2025-01-06 01:15:41,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41,497][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.0046162353828549385, acc: 1.0)
[2025-01-06 01:15:41,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41,849][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.10675887018442154, acc: 0.9736841917037964)
[2025-01-06 01:15:41,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42,205][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.0005668714875355363, acc: 1.0)
[2025-01-06 01:15:42,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42,539][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.0020937523804605007, acc: 1.0)
[2025-01-06 01:15:42,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42,818][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.09144970774650574, acc: 0.9523809552192688)
[2025-01-06 01:15:42,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43,165][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 0.5027034282684326, acc: 0.8148148059844971)
[2025-01-06 01:15:43,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43,530][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 0.604831337928772, acc: 0.8155339956283569)
[2025-01-06 01:15:43,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44,058][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 0.8104667663574219, acc: 0.8382353186607361)
[2025-01-06 01:15:44,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44,430][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 0.4777505397796631, acc: 0.8533333539962769)
[2025-01-06 01:15:44,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44,821][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 0.5876733660697937, acc: 0.8055555820465088)
[2025-01-06 01:15:44,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45,147][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.3278249502182007, acc: 0.9069767594337463)
[2025-01-06 01:15:45,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45,451][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.028165945783257484, acc: 1.0)
[2025-01-06 01:15:45,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45,766][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.09219244122505188, acc: 0.9767441749572754)
[2025-01-06 01:15:45,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:46,075][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.05913819000124931, acc: 0.9599999785423279)
[2025-01-06 01:15:46,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:46,601][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.22549943625926971, acc: 0.9264705777168274)
[2025-01-06 01:15:46,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:46,914][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.21402989327907562, acc: 0.9466666579246521)
[2025-01-06 01:15:47,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47,277][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.11554232984781265, acc: 0.9696969985961914)
[2025-01-06 01:15:47,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47,660][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.16815119981765747, acc: 0.9696969985961914)
[2025-01-06 01:15:47,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48,015][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.031780313700437546, acc: 1.0)
[2025-01-06 01:15:48,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48,335][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.007712688762694597, acc: 1.0)
[2025-01-06 01:15:48,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48,667][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.01619972474873066, acc: 1.0)
[2025-01-06 01:15:48,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49,008][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.0115602295845747, acc: 1.0)
[2025-01-06 01:15:49,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49,352][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.052723340690135956, acc: 1.0)
[2025-01-06 01:15:49,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49,700][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.016912950202822685, acc: 1.0)
[2025-01-06 01:15:49,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50,058][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.07566409558057785, acc: 0.9655172228813171)
[2025-01-06 01:15:50,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50,404][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.007563020568341017, acc: 1.0)
[2025-01-06 01:15:50,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50,754][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.013133048079907894, acc: 1.0)
[2025-01-06 01:15:50,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51,097][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.021012507379055023, acc: 1.0)
[2025-01-06 01:15:51,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51,464][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.003304531332105398, acc: 1.0)
[2025-01-06 01:15:51,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51,779][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 0.21534912288188934, acc: 0.9607843160629272)
[2025-01-06 01:15:51,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52,109][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.099913589656353, acc: 0.9230769276618958)
[2025-01-06 01:15:52,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52,473][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.05598089471459389, acc: 1.0)
[2025-01-06 01:15:52,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52,826][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.05433148890733719, acc: 0.9750000238418579)
[2025-01-06 01:15:52,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53,158][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.024803346022963524, acc: 1.0)
[2025-01-06 01:15:53,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53,460][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.013218384236097336, acc: 1.0)
[2025-01-06 01:15:53,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53,735][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.028104634955525398, acc: 1.0)
[2025-01-06 01:15:53,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54,090][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.06928477436304092, acc: 1.0)
[2025-01-06 01:15:54,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54,466][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.11897345632314682, acc: 0.9722222089767456)
[2025-01-06 01:15:54,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54,783][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.031509000808000565, acc: 1.0)
[2025-01-06 01:15:55,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25,155][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9466, device='cuda:0') eval_epoch_loss=tensor(0.6661, device='cuda:0') eval_epoch_acc=tensor(0.8424, device='cuda:0')
[2025-01-06 01:16:25,157][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:16:25,157][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:16:25,375][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_425_loss_0.6660631895065308/model.pt
[2025-01-06 01:16:25,379][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:16:25,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25,799][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.07569825649261475, acc: 0.939393937587738)
[2025-01-06 01:16:25,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26,176][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.0076217325404286385, acc: 1.0)
[2025-01-06 01:16:26,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26,544][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.12389195710420609, acc: 0.9729729890823364)
[2025-01-06 01:16:26,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26,924][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.007342509459704161, acc: 1.0)
[2025-01-06 01:16:27,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27,257][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.0039525991305708885, acc: 1.0)
[2025-01-06 01:16:27,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27,603][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.0013561664381995797, acc: 1.0)
[2025-01-06 01:16:27,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27,967][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.00633352342993021, acc: 1.0)
[2025-01-06 01:16:28,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28,339][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.003419470740482211, acc: 1.0)
[2025-01-06 01:16:28,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28,704][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.09371353685855865, acc: 0.9722222089767456)
[2025-01-06 01:16:28,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29,003][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.008732862770557404, acc: 1.0)
[2025-01-06 01:16:29,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29,381][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.0013128425925970078, acc: 1.0)
[2025-01-06 01:16:29,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29,721][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.0949074849486351, acc: 0.9722222089767456)
[2025-01-06 01:16:29,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30,085][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.003747012699022889, acc: 1.0)
[2025-01-06 01:16:30,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30,414][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.002026722999289632, acc: 1.0)
[2025-01-06 01:16:30,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30,740][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 0.13658878207206726, acc: 0.9487179517745972)
[2025-01-06 01:16:30,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31,246][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.2189101129770279, acc: 0.9242424368858337)
[2025-01-06 01:16:31,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31,921][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 0.4764975607395172, acc: 0.8159999847412109)
[2025-01-06 01:16:32,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32,320][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 0.6398841738700867, acc: 0.8306451439857483)
[2025-01-06 01:16:32,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32,974][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.35821935534477234, acc: 0.89552241563797)
[2025-01-06 01:16:33,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:33,350][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.08620786666870117, acc: 0.9622641801834106)
[2025-01-06 01:16:33,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:33,748][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.07584700733423233, acc: 0.9772727489471436)
[2025-01-06 01:16:33,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34,051][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.1096968874335289, acc: 0.95652174949646)
[2025-01-06 01:16:34,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34,443][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.11995996534824371, acc: 0.9230769276618958)
[2025-01-06 01:16:34,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34,820][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.009600162506103516, acc: 1.0)
[2025-01-06 01:16:34,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35,199][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.06574354320764542, acc: 0.9850746393203735)
[2025-01-06 01:16:35,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35,585][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.04015098139643669, acc: 0.9861111044883728)
[2025-01-06 01:16:35,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35,917][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.017359687015414238, acc: 0.989130437374115)
[2025-01-06 01:16:36,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36,292][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.051048509776592255, acc: 0.9871794581413269)
[2025-01-06 01:16:36,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36,612][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.10220616310834885, acc: 0.9473684430122375)
[2025-01-06 01:16:36,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36,950][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.03332175686955452, acc: 0.9795918464660645)
[2025-01-06 01:16:37,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37,325][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.022471468895673752, acc: 1.0)
[2025-01-06 01:16:37,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37,678][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 0.44448399543762207, acc: 0.876288652420044)
[2025-01-06 01:16:37,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38,042][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.010251367464661598, acc: 1.0)
[2025-01-06 01:16:38,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38,427][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 0.13272781670093536, acc: 0.9534883499145508)
[2025-01-06 01:16:38,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38,761][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.018666090443730354, acc: 0.9821428656578064)
[2025-01-06 01:16:38,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39,119][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.09585079550743103, acc: 0.9629629850387573)
[2025-01-06 01:16:39,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39,424][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.028732262551784515, acc: 1.0)
[2025-01-06 01:16:39,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39,755][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.036920640617609024, acc: 1.0)
[2025-01-06 01:16:39,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40,125][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.23201298713684082, acc: 0.9230769276618958)
[2025-01-06 01:16:40,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40,457][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.07237564772367477, acc: 0.97826087474823)
[2025-01-06 01:16:40,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40,807][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.1616465151309967, acc: 0.9523809552192688)
[2025-01-06 01:16:40,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41,194][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.3195815980434418, acc: 0.9277108311653137)
[2025-01-06 01:16:41,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41,587][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.0939689353108406, acc: 0.9639639854431152)
[2025-01-06 01:16:41,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41,936][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 0.4457226097583771, acc: 0.8737863898277283)
[2025-01-06 01:16:42,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42,291][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.2890144884586334, acc: 0.9268292784690857)
[2025-01-06 01:16:42,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42,630][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.012133773416280746, acc: 1.0)
[2025-01-06 01:16:42,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42,998][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.0694989487528801, acc: 0.9642857313156128)
[2025-01-06 01:16:43,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43,399][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 0.3320537209510803, acc: 0.8921568393707275)
[2025-01-06 01:16:43,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43,757][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 0.616849958896637, acc: 0.8296943306922913)
[2025-01-06 01:16:43,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44,146][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.3281414806842804, acc: 0.8958333134651184)
[2025-01-06 01:16:44,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44,513][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.34238582849502563, acc: 0.89570552110672)
[2025-01-06 01:16:44,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44,844][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.23639130592346191, acc: 0.9208633303642273)
[2025-01-06 01:16:44,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45,237][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 0.6522883176803589, acc: 0.7989949584007263)
[2025-01-06 01:16:45,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45,618][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.11717290431261063, acc: 0.9722222089767456)
[2025-01-06 01:16:45,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45,987][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.4867539405822754, acc: 0.8787878751754761)
[2025-01-06 01:16:46,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46,345][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.030331678688526154, acc: 1.0)
[2025-01-06 01:16:46,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46,715][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.025777077302336693, acc: 1.0)
[2025-01-06 01:16:46,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47,042][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.35787537693977356, acc: 0.8999999761581421)
[2025-01-06 01:16:47,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47,436][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 0.40501829981803894, acc: 0.8793103694915771)
[2025-01-06 01:16:47,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47,794][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.013036269694566727, acc: 1.0)
[2025-01-06 01:16:47,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48,156][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.060687121003866196, acc: 1.0)
[2025-01-06 01:16:48,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48,498][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 0.10860970616340637, acc: 1.0)
[2025-01-06 01:16:48,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48,922][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.18295781314373016, acc: 0.9047619104385376)
[2025-01-06 01:16:49,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49,294][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.051038432866334915, acc: 1.0)
[2025-01-06 01:16:49,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49,645][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 0.33749786019325256, acc: 0.892307698726654)
[2025-01-06 01:16:49,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49,995][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.04661447927355766, acc: 1.0)
[2025-01-06 01:16:50,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50,315][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.04991982877254486, acc: 1.0)
[2025-01-06 01:16:50,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50,665][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.17734521627426147, acc: 0.9215686321258545)
[2025-01-06 01:16:50,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51,027][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.05872908979654312, acc: 1.0)
[2025-01-06 01:16:51,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51,378][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.05529043450951576, acc: 1.0)
[2025-01-06 01:16:51,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51,682][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 0.5533989667892456, acc: 0.8947368264198303)
[2025-01-06 01:16:51,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52,078][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.5225176215171814, acc: 0.8303571343421936)
[2025-01-06 01:16:52,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52,488][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.17838214337825775, acc: 0.9213483333587646)
[2025-01-06 01:16:52,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52,865][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 0.4714684784412384, acc: 0.8426966071128845)
[2025-01-06 01:16:52,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53,230][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 0.7639073729515076, acc: 0.758865237236023)
[2025-01-06 01:16:53,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53,599][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 0.4224555492401123, acc: 0.8913043737411499)
[2025-01-06 01:16:53,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53,928][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.0024008143227547407, acc: 1.0)
[2025-01-06 01:16:54,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54,251][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.0017218241700902581, acc: 1.0)
[2025-01-06 01:16:54,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54,593][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.01653648167848587, acc: 1.0)
[2025-01-06 01:16:54,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54,964][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.018637215718626976, acc: 1.0)
[2025-01-06 01:16:55,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55,345][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.24954751133918762, acc: 0.9622641801834106)
[2025-01-06 01:16:55,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55,699][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.36385083198547363, acc: 0.8965517282485962)
[2025-01-06 01:16:55,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56,281][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 0.6063101291656494, acc: 0.8558558821678162)
[2025-01-06 01:16:56,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56,711][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.46468621492385864, acc: 0.8169013857841492)
[2025-01-06 01:16:56,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56,993][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.021513337269425392, acc: 1.0)
[2025-01-06 01:16:57,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57,316][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.029847504571080208, acc: 1.0)
[2025-01-06 01:16:57,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57,611][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.2634037137031555, acc: 0.9230769276618958)
[2025-01-06 01:16:59,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:00,301][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 0.6315717697143555, acc: 0.8214285969734192)
[2025-01-06 01:17:00,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01,062][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.14609883725643158, acc: 0.9365079402923584)
[2025-01-06 01:17:01,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01,396][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.04361632093787193, acc: 1.0)
[2025-01-06 01:17:01,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01,722][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.05141688510775566, acc: 0.9833333492279053)
[2025-01-06 01:17:01,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02,427][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.42028889060020447, acc: 0.8888888955116272)
[2025-01-06 01:17:02,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02,764][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.08164071291685104, acc: 0.9615384340286255)
[2025-01-06 01:17:02,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03,105][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.019474603235721588, acc: 1.0)
[2025-01-06 01:17:03,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03,455][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.039830051362514496, acc: 1.0)
[2025-01-06 01:17:03,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03,803][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.04793901368975639, acc: 1.0)
[2025-01-06 01:17:04,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:04,785][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 0.5193353295326233, acc: 0.8389830589294434)
[2025-01-06 01:17:04,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05,186][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.15952762961387634, acc: 0.9477611780166626)
[2025-01-06 01:17:05,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05,576][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.23301583528518677, acc: 0.9270073175430298)
[2025-01-06 01:17:05,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06,134][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 0.4866219460964203, acc: 0.8849999904632568)
[2025-01-06 01:17:06,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06,484][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.027770554646849632, acc: 1.0)
[2025-01-06 01:17:06,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06,825][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.1354628950357437, acc: 0.942307710647583)
[2025-01-06 01:17:06,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:07,152][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.046461690217256546, acc: 1.0)
[2025-01-06 01:17:07,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:07,545][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 0.7796592116355896, acc: 0.7540983557701111)
[2025-01-06 01:17:07,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:07,922][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.15041625499725342, acc: 0.9491525292396545)
[2025-01-06 01:17:08,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08,227][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 0.9894471168518066, acc: 0.7906976938247681)
[2025-01-06 01:17:08,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08,581][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 0.23217949271202087, acc: 0.9090909361839294)
[2025-01-06 01:17:08,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08,946][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 0.36796846985816956, acc: 0.9056603908538818)
[2025-01-06 01:17:09,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09,322][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.21628357470035553, acc: 0.9090909361839294)
[2025-01-06 01:17:09,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09,687][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.11870325356721878, acc: 1.0)
[2025-01-06 01:17:09,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10,042][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.0984092578291893, acc: 0.949999988079071)
[2025-01-06 01:17:10,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10,387][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.03860512003302574, acc: 1.0)
[2025-01-06 01:17:10,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10,832][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.3141135573387146, acc: 0.892307698726654)
[2025-01-06 01:17:10,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11,207][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.3139643669128418, acc: 0.921875)
[2025-01-06 01:17:11,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11,591][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.28092285990715027, acc: 0.875)
[2025-01-06 01:17:11,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11,935][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.2838160991668701, acc: 0.9090909361839294)
[2025-01-06 01:17:12,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12,294][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.14916396141052246, acc: 0.9375)
[2025-01-06 01:17:12,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12,633][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.01344336662441492, acc: 1.0)
[2025-01-06 01:17:12,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12,923][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.003782743588089943, acc: 1.0)
[2025-01-06 01:17:12,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13,210][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.011140932328999043, acc: 1.0)
[2025-01-06 01:17:13,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13,523][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.008075417019426823, acc: 1.0)
[2025-01-06 01:17:13,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13,812][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.005549981724470854, acc: 1.0)
[2025-01-06 01:17:13,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14,180][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.12973278760910034, acc: 0.9736841917037964)
[2025-01-06 01:17:14,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14,532][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.013838226906955242, acc: 1.0)
[2025-01-06 01:17:14,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14,834][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.000684328842908144, acc: 1.0)
[2025-01-06 01:17:14,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15,212][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.07687634229660034, acc: 0.9696969985961914)
[2025-01-06 01:17:15,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15,588][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.012788532301783562, acc: 1.0)
[2025-01-06 01:17:15,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15,979][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.12413334101438522, acc: 0.9428571462631226)
[2025-01-06 01:17:16,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16,355][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.31994009017944336, acc: 0.9270073175430298)
[2025-01-06 01:17:16,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16,735][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.12431198358535767, acc: 0.9586206674575806)
[2025-01-06 01:17:16,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17,070][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.24419236183166504, acc: 0.9357143044471741)
[2025-01-06 01:17:17,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17,452][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.3908594250679016, acc: 0.9139072895050049)
[2025-01-06 01:17:17,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17,827][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.10983113944530487, acc: 0.94017094373703)
[2025-01-06 01:17:17,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18,199][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.031055428087711334, acc: 1.0)
[2025-01-06 01:17:18,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18,517][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.007181466091424227, acc: 1.0)
[2025-01-06 01:17:18,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18,859][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.018485071137547493, acc: 1.0)
[2025-01-06 01:17:18,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19,200][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.0052594575099647045, acc: 1.0)
[2025-01-06 01:17:19,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19,599][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.18675340712070465, acc: 0.9444444179534912)
[2025-01-06 01:17:19,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19,952][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.3075474798679352, acc: 0.9220778942108154)
[2025-01-06 01:17:20,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20,338][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.1598668247461319, acc: 0.9375)
[2025-01-06 01:17:20,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20,668][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.15796233713626862, acc: 0.931034505367279)
[2025-01-06 01:17:20,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21,033][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.18491461873054504, acc: 0.9523809552192688)
[2025-01-06 01:17:21,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21,370][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.004706013947725296, acc: 1.0)
[2025-01-06 01:17:22,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52,210][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9934, device='cuda:0') eval_epoch_loss=tensor(0.6898, device='cuda:0') eval_epoch_acc=tensor(0.8399, device='cuda:0')
[2025-01-06 01:17:52,211][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:17:52,212][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:17:52,418][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_568_loss_0.6898293495178223/model.pt
[2025-01-06 01:17:52,421][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:17:52,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52,808][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.0033794583287090063, acc: 1.0)
[2025-01-06 01:17:52,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53,245][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.16301298141479492, acc: 0.9679144620895386)
[2025-01-06 01:17:53,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53,590][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.006282580550760031, acc: 1.0)
[2025-01-06 01:17:53,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53,920][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.015188757330179214, acc: 1.0)
[2025-01-06 01:17:54,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54,228][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.27378612756729126, acc: 0.9285714030265808)
[2025-01-06 01:17:54,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54,568][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.21220849454402924, acc: 0.9371069073677063)
[2025-01-06 01:17:54,960][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=1.3584, train_epoch_loss=0.3063, epoch time 354.4841407351196s
[2025-01-06 01:17:54,961][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:17:54,961][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2025-01-06 01:17:54,961][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:17:54,961][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 9
[2025-01-06 01:17:54,961][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:17:55,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:55,788][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.1996583789587021, acc: 0.9629629850387573)
[2025-01-06 01:17:55,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56,173][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.031966567039489746, acc: 1.0)
[2025-01-06 01:17:56,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56,549][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.46868789196014404, acc: 0.8648648858070374)
[2025-01-06 01:17:56,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56,926][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.19846117496490479, acc: 0.9473684430122375)
[2025-01-06 01:17:57,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57,313][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.128665491938591, acc: 0.9729729890823364)
[2025-01-06 01:17:57,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57,631][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.040931787341833115, acc: 1.0)
[2025-01-06 01:17:57,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58,013][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 0.17619813978672028, acc: 0.9387755393981934)
[2025-01-06 01:17:58,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58,395][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.028211809694767, acc: 1.0)
[2025-01-06 01:17:58,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58,751][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.020906299352645874, acc: 1.0)
[2025-01-06 01:17:58,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59,101][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.02819697931408882, acc: 1.0)
[2025-01-06 01:17:59,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59,484][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.007154356222599745, acc: 1.0)
[2025-01-06 01:17:59,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59,829][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.08257442712783813, acc: 0.9743589758872986)
[2025-01-06 01:17:59,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00,208][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.015089014545083046, acc: 1.0)
[2025-01-06 01:18:00,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00,604][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.12039637565612793, acc: 0.9347826242446899)
[2025-01-06 01:18:00,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00,986][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.023094967007637024, acc: 1.0)
[2025-01-06 01:18:01,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01,364][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.15798325836658478, acc: 0.9591836929321289)
[2025-01-06 01:18:01,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01,730][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.08776254206895828, acc: 0.9473684430122375)
[2025-01-06 01:18:01,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02,088][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.12109509855508804, acc: 0.9583333134651184)
[2025-01-06 01:18:02,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02,424][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 0.11636222898960114, acc: 0.9444444179534912)
[2025-01-06 01:18:02,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02,805][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.014693553559482098, acc: 1.0)
[2025-01-06 01:18:02,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:03,141][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.15888606011867523, acc: 0.9615384340286255)
[2025-01-06 01:18:03,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:03,501][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.011280067265033722, acc: 1.0)
[2025-01-06 01:18:03,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:03,857][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.10566629469394684, acc: 0.9200000166893005)
[2025-01-06 01:18:03,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04,190][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.1673641949892044, acc: 0.9523809552192688)
[2025-01-06 01:18:04,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04,554][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.10730976611375809, acc: 0.9375)
[2025-01-06 01:18:04,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04,905][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.19530561566352844, acc: 0.9433962106704712)
[2025-01-06 01:18:04,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05,235][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.27266740798950195, acc: 0.9041095972061157)
[2025-01-06 01:18:05,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06,517][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 0.6642839908599854, acc: 0.8023715615272522)
[2025-01-06 01:18:06,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06,865][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.1770678162574768, acc: 0.9534883499145508)
[2025-01-06 01:18:06,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07,207][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.2717043459415436, acc: 0.9036144614219666)
[2025-01-06 01:18:07,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07,564][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.23775623738765717, acc: 0.9259259104728699)
[2025-01-06 01:18:07,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07,893][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.10505043715238571, acc: 0.9642857313156128)
[2025-01-06 01:18:07,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08,247][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.05852092057466507, acc: 0.9629629850387573)
[2025-01-06 01:18:08,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08,648][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.0024667850229889154, acc: 1.0)
[2025-01-06 01:18:08,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09,044][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.45038875937461853, acc: 0.8739495873451233)
[2025-01-06 01:18:09,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09,389][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.09871965646743774, acc: 0.9672130942344666)
[2025-01-06 01:18:09,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09,775][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.23373009264469147, acc: 0.9047619104385376)
[2025-01-06 01:18:09,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10,125][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.2649860382080078, acc: 0.9322034120559692)
[2025-01-06 01:18:10,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10,473][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.21694745123386383, acc: 0.931034505367279)
[2025-01-06 01:18:10,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10,873][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.3777976930141449, acc: 0.9523809552192688)
[2025-01-06 01:18:10,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11,254][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.18918849527835846, acc: 0.9615384340286255)
[2025-01-06 01:18:11,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11,605][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.16448856890201569, acc: 0.9324324131011963)
[2025-01-06 01:18:11,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11,939][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.2602345943450928, acc: 0.9076923131942749)
[2025-01-06 01:18:12,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12,330][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.4288994371891022, acc: 0.8787878751754761)
[2025-01-06 01:18:12,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12,723][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.22599273920059204, acc: 0.9278350472450256)
[2025-01-06 01:18:12,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13,148][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.26924586296081543, acc: 0.9264705777168274)
[2025-01-06 01:18:13,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13,513][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.046013299375772476, acc: 1.0)
[2025-01-06 01:18:13,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13,876][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.06798674166202545, acc: 1.0)
[2025-01-06 01:18:13,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14,262][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.09822166711091995, acc: 0.9642857313156128)
[2025-01-06 01:18:14,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14,601][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.009844095446169376, acc: 1.0)
[2025-01-06 01:18:14,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14,929][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.2993212938308716, acc: 0.9122806787490845)
[2025-01-06 01:18:15,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15,250][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.28032490611076355, acc: 0.9047619104385376)
[2025-01-06 01:18:15,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15,604][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 0.3943198025226593, acc: 0.8591549396514893)
[2025-01-06 01:18:15,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16,054][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 0.9674098491668701, acc: 0.7133333086967468)
[2025-01-06 01:18:16,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16,428][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.1560957133769989, acc: 0.9729729890823364)
[2025-01-06 01:18:16,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16,757][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.055411938577890396, acc: 0.9615384340286255)
[2025-01-06 01:18:18,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19,780][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 0.769967257976532, acc: 0.7849829196929932)
[2025-01-06 01:18:20,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:20,962][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 0.9412669539451599, acc: 0.7298474907875061)
[2025-01-06 01:18:21,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:21,582][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 0.5850971937179565, acc: 0.8068181872367859)
[2025-01-06 01:18:21,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22,148][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.2051413655281067, acc: 0.9485294222831726)
[2025-01-06 01:18:22,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22,717][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.5459245443344116, acc: 0.8260869383811951)
[2025-01-06 01:18:22,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23,145][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.46979865431785583, acc: 0.875)
[2025-01-06 01:18:23,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23,479][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.06686180084943771, acc: 1.0)
[2025-01-06 01:18:23,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23,869][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.3438034653663635, acc: 0.9444444179534912)
[2025-01-06 01:18:23,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24,222][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.03671806678175926, acc: 1.0)
[2025-01-06 01:18:24,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24,578][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.031099168583750725, acc: 1.0)
[2025-01-06 01:18:24,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24,948][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.32624199986457825, acc: 0.9107142686843872)
[2025-01-06 01:18:25,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25,348][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.11142483353614807, acc: 0.949999988079071)
[2025-01-06 01:18:25,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25,698][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.0027449852786958218, acc: 1.0)
[2025-01-06 01:18:25,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:26,052][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.1115742102265358, acc: 1.0)
[2025-01-06 01:18:26,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:26,471][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.1482153683900833, acc: 1.0)
[2025-01-06 01:18:26,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:26,891][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 0.6315156817436218, acc: 0.8088235259056091)
[2025-01-06 01:18:26,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:27,254][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.5054418444633484, acc: 0.8809523582458496)
[2025-01-06 01:18:27,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:27,635][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 0.8679264187812805, acc: 0.7384615540504456)
[2025-01-06 01:18:27,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:27,955][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 0.5858290791511536, acc: 0.8571428656578064)
[2025-01-06 01:18:28,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:28,323][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 0.8181937336921692, acc: 0.7761194109916687)
[2025-01-06 01:18:28,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:28,735][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 1.1479378938674927, acc: 0.6751824617385864)
[2025-01-06 01:18:28,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:29,083][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.007191786542534828, acc: 1.0)
[2025-01-06 01:18:29,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:29,437][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.03326563164591789, acc: 1.0)
[2025-01-06 01:18:29,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:29,803][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.010837143287062645, acc: 1.0)
[2025-01-06 01:18:29,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30,157][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.007790414150804281, acc: 1.0)
[2025-01-06 01:18:30,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30,515][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.23555408418178558, acc: 0.9230769276618958)
[2025-01-06 01:18:30,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30,882][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.28636425733566284, acc: 0.9230769276618958)
[2025-01-06 01:18:30,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31,223][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.09524978697299957, acc: 0.96875)
[2025-01-06 01:18:31,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31,580][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.2277638167142868, acc: 0.9275362491607666)
[2025-01-06 01:18:31,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31,951][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.1274494230747223, acc: 0.9399999976158142)
[2025-01-06 01:18:32,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32,334][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.040653493255376816, acc: 1.0)
[2025-01-06 01:18:32,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32,824][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 0.24080118536949158, acc: 0.9399999976158142)
[2025-01-06 01:18:32,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:33,185][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 0.494332879781723, acc: 0.8640776872634888)
[2025-01-06 01:18:33,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34,282][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 0.6845685839653015, acc: 0.8155339956283569)
[2025-01-06 01:18:34,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35,101][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 0.6752414107322693, acc: 0.8225806355476379)
[2025-01-06 01:18:35,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35,899][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 0.6852740049362183, acc: 0.8275862336158752)
[2025-01-06 01:18:36,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:36,639][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.43559303879737854, acc: 0.8842105269432068)
[2025-01-06 01:18:36,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37,625][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 0.7083930373191833, acc: 0.8118811845779419)
[2025-01-06 01:18:37,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37,900][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 0.5048126578330994, acc: 0.8548387289047241)
[2025-01-06 01:18:38,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38,289][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 0.45513883233070374, acc: 0.8550724387168884)
[2025-01-06 01:18:38,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38,661][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 0.7449186444282532, acc: 0.7478991746902466)
[2025-01-06 01:18:38,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39,051][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 0.4880842864513397, acc: 0.8942307829856873)
[2025-01-06 01:18:39,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39,437][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 0.7362006902694702, acc: 0.7664233446121216)
[2025-01-06 01:18:39,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39,764][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 0.5741999745368958, acc: 0.7761194109916687)
[2025-01-06 01:18:39,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40,109][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.06283441185951233, acc: 1.0)
[2025-01-06 01:18:40,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40,487][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.005626875441521406, acc: 1.0)
[2025-01-06 01:18:40,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40,877][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.010245045647025108, acc: 1.0)
[2025-01-06 01:18:40,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41,238][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.03339057043194771, acc: 0.9772727489471436)
[2025-01-06 01:18:41,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41,615][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.2992328405380249, acc: 0.9137930870056152)
[2025-01-06 01:18:41,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41,936][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.03414946794509888, acc: 1.0)
[2025-01-06 01:18:42,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42,298][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.03786788880825043, acc: 1.0)
[2025-01-06 01:18:42,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42,679][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.09966220706701279, acc: 0.9411764740943909)
[2025-01-06 01:18:42,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43,027][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.00731790903955698, acc: 1.0)
[2025-01-06 01:18:43,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43,346][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.012675918638706207, acc: 1.0)
[2025-01-06 01:18:43,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43,648][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.04630538076162338, acc: 0.9846153855323792)
[2025-01-06 01:18:43,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44,093][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.21631698310375214, acc: 0.9473684430122375)
[2025-01-06 01:18:44,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44,453][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 0.18383556604385376, acc: 0.9122806787490845)
[2025-01-06 01:18:44,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44,825][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.11149438470602036, acc: 0.9230769276618958)
[2025-01-06 01:18:44,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45,230][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.06753245741128922, acc: 1.0)
[2025-01-06 01:18:45,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45,617][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.002489113248884678, acc: 1.0)
[2025-01-06 01:18:45,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46,028][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.3020956516265869, acc: 0.920634925365448)
[2025-01-06 01:18:46,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46,389][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.2549746334552765, acc: 0.9186992049217224)
[2025-01-06 01:18:46,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46,775][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.044575080275535583, acc: 1.0)
[2025-01-06 01:18:47,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47,668][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 0.341438889503479, acc: 0.9201520681381226)
[2025-01-06 01:18:47,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47,989][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.16173511743545532, acc: 0.9466666579246521)
[2025-01-06 01:18:48,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48,395][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.18625925481319427, acc: 0.942307710647583)
[2025-01-06 01:18:48,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48,747][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.10568661242723465, acc: 0.9583333134651184)
[2025-01-06 01:18:48,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49,114][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.4254663288593292, acc: 0.8947368264198303)
[2025-01-06 01:18:49,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49,520][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 0.6176135540008545, acc: 0.8404908180236816)
[2025-01-06 01:18:49,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49,935][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 0.738135576248169, acc: 0.7916666865348816)
[2025-01-06 01:18:50,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50,327][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 0.7560954689979553, acc: 0.7666666507720947)
[2025-01-06 01:18:50,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50,733][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 0.36501097679138184, acc: 0.8571428656578064)
[2025-01-06 01:18:50,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51,084][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 0.5052440762519836, acc: 0.8512820601463318)
[2025-01-06 01:18:51,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51,497][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 0.506032407283783, acc: 0.8602941036224365)
[2025-01-06 01:18:51,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51,856][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.11641691625118256, acc: 0.9615384340286255)
[2025-01-06 01:18:51,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52,232][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.0747826099395752, acc: 1.0)
[2025-01-06 01:18:52,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52,563][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.05964542552828789, acc: 1.0)
[2025-01-06 01:18:52,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52,889][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.34560146927833557, acc: 0.8260869383811951)
[2025-01-06 01:18:52,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53,239][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.07007449120283127, acc: 1.0)
[2025-01-06 01:18:53,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53,615][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.07223968952894211, acc: 0.9615384340286255)
[2025-01-06 01:18:53,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53,955][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.11767131835222244, acc: 0.9523809552192688)
[2025-01-06 01:18:54,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24,829][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9666, device='cuda:0') eval_epoch_loss=tensor(0.6763, device='cuda:0') eval_epoch_acc=tensor(0.8433, device='cuda:0')
[2025-01-06 01:19:24,830][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:19:24,831][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:19:25,050][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_137_loss_0.6762973666191101/model.pt
[2025-01-06 01:19:25,053][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:19:25,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25,398][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.19335833191871643, acc: 0.9666666388511658)
[2025-01-06 01:19:25,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25,748][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.013754788786172867, acc: 1.0)
[2025-01-06 01:19:25,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26,113][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.010448718443512917, acc: 1.0)
[2025-01-06 01:19:26,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26,462][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.03812042251229286, acc: 1.0)
[2025-01-06 01:19:26,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26,781][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.032908838242292404, acc: 1.0)
[2025-01-06 01:19:26,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27,158][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.3475523889064789, acc: 0.9729729890823364)
[2025-01-06 01:19:27,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27,697][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 0.48300522565841675, acc: 0.8157894611358643)
[2025-01-06 01:19:27,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28,034][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.5282967686653137, acc: 0.8507462739944458)
[2025-01-06 01:19:28,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28,377][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.2805593013763428, acc: 0.9081632494926453)
[2025-01-06 01:19:28,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28,812][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 0.6448691487312317, acc: 0.7659574747085571)
[2025-01-06 01:19:28,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29,172][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.23081493377685547, acc: 0.9142857193946838)
[2025-01-06 01:19:29,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29,508][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.23006601631641388, acc: 0.9285714030265808)
[2025-01-06 01:19:29,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29,881][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.12275685369968414, acc: 1.0)
[2025-01-06 01:19:30,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30,256][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.0590311698615551, acc: 1.0)
[2025-01-06 01:19:30,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30,607][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.3261245787143707, acc: 0.9130434989929199)
[2025-01-06 01:19:30,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30,978][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.35226866602897644, acc: 0.9152542352676392)
[2025-01-06 01:19:31,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31,337][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 0.16860798001289368, acc: 0.9298245906829834)
[2025-01-06 01:19:31,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31,725][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.46785885095596313, acc: 0.8513513803482056)
[2025-01-06 01:19:31,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32,058][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.015291711315512657, acc: 1.0)
[2025-01-06 01:19:32,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32,445][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.11447949707508087, acc: 0.95652174949646)
[2025-01-06 01:19:32,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32,813][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 0.6473756432533264, acc: 0.7894737124443054)
[2025-01-06 01:19:33,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34,435][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 0.5220425128936768, acc: 0.8918918967247009)
[2025-01-06 01:19:34,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34,836][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 0.5962398052215576, acc: 0.8333333134651184)
[2025-01-06 01:19:34,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35,268][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 0.7051142454147339, acc: 0.8139534592628479)
[2025-01-06 01:19:35,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35,861][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 0.5430803894996643, acc: 0.8352941274642944)
[2025-01-06 01:19:36,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36,421][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 0.7601079940795898, acc: 0.7977527976036072)
[2025-01-06 01:19:36,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36,770][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.17807914316654205, acc: 0.9545454382896423)
[2025-01-06 01:19:36,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37,047][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.08603433519601822, acc: 0.9047619104385376)
[2025-01-06 01:19:37,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37,389][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 0.15653887391090393, acc: 0.931034505367279)
[2025-01-06 01:19:37,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37,763][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.03910769522190094, acc: 0.9795918464660645)
[2025-01-06 01:19:37,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38,162][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.09027586132287979, acc: 1.0)
[2025-01-06 01:19:38,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38,580][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.2134813666343689, acc: 0.9166666865348816)
[2025-01-06 01:19:38,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38,938][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 0.5606650710105896, acc: 0.8333333134651184)
[2025-01-06 01:19:39,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39,960][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 0.3983875811100006, acc: 0.8561643958091736)
[2025-01-06 01:19:40,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40,305][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.03010927326977253, acc: 1.0)
[2025-01-06 01:19:40,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40,675][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.6197677850723267, acc: 0.8518518805503845)
[2025-01-06 01:19:40,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41,012][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.1473601758480072, acc: 0.8928571343421936)
[2025-01-06 01:19:41,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41,550][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 0.790982186794281, acc: 0.7876105904579163)
[2025-01-06 01:19:41,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41,913][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.5092169642448425, acc: 0.8695651888847351)
[2025-01-06 01:19:42,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:42,319][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.2964051067829132, acc: 0.8863636255264282)
[2025-01-06 01:19:42,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43,230][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 0.695491373538971, acc: 0.8320610523223877)
[2025-01-06 01:19:43,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43,907][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 0.6156279444694519, acc: 0.8296296000480652)
[2025-01-06 01:19:44,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44,281][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.13614118099212646, acc: 0.9508196711540222)
[2025-01-06 01:19:44,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44,632][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.0213368758559227, acc: 1.0)
[2025-01-06 01:19:44,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44,964][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.0078887315467, acc: 1.0)
[2025-01-06 01:19:45,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45,284][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.009907637722790241, acc: 1.0)
[2025-01-06 01:19:45,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45,606][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.06267630308866501, acc: 0.9634146094322205)
[2025-01-06 01:19:45,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46,018][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.3253243863582611, acc: 0.9244713187217712)
[2025-01-06 01:19:46,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46,383][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 0.35275405645370483, acc: 0.9135446548461914)
[2025-01-06 01:19:46,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46,859][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.3491693139076233, acc: 0.890625)
[2025-01-06 01:19:46,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47,384][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 0.41729649901390076, acc: 0.8874296545982361)
[2025-01-06 01:19:47,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47,798][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.37732914090156555, acc: 0.8896797299385071)
[2025-01-06 01:19:47,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:48,170][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.029762396588921547, acc: 1.0)
[2025-01-06 01:19:48,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:48,718][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 0.5553146004676819, acc: 0.8372092843055725)
[2025-01-06 01:19:48,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49,511][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 0.9134002327919006, acc: 0.7301587462425232)
[2025-01-06 01:19:49,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:50,429][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 0.5309584736824036, acc: 0.8560606241226196)
[2025-01-06 01:19:50,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:51,171][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.37287989258766174, acc: 0.8588235378265381)
[2025-01-06 01:19:51,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:52,242][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 0.6767848134040833, acc: 0.7962962985038757)
[2025-01-06 01:19:52,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53,192][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.22115448117256165, acc: 0.9193548560142517)
[2025-01-06 01:19:53,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53,526][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.14103302359580994, acc: 0.9642857313156128)
[2025-01-06 01:19:53,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53,877][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.28404727578163147, acc: 0.8999999761581421)
[2025-01-06 01:19:53,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54,257][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.29965922236442566, acc: 0.8970588445663452)
[2025-01-06 01:19:54,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54,603][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 0.5650362372398376, acc: 0.8382353186607361)
[2025-01-06 01:19:54,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54,938][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.3718961179256439, acc: 0.8813559412956238)
[2025-01-06 01:19:55,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55,272][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 0.5114176869392395, acc: 0.8507462739944458)
[2025-01-06 01:19:55,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55,636][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.5312240123748779, acc: 0.844660222530365)
[2025-01-06 01:19:55,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56,001][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.24105329811573029, acc: 0.9047619104385376)
[2025-01-06 01:19:56,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56,330][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.07672405987977982, acc: 0.9670329689979553)
[2025-01-06 01:19:56,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56,731][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.17883118987083435, acc: 0.9551569223403931)
[2025-01-06 01:19:56,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57,140][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.2879548966884613, acc: 0.8976377844810486)
[2025-01-06 01:19:57,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57,520][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.19498759508132935, acc: 0.943965494632721)
[2025-01-06 01:19:57,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57,929][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.30975306034088135, acc: 0.9202898740768433)
[2025-01-06 01:19:58,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58,309][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.2773122489452362, acc: 0.9105058312416077)
[2025-01-06 01:19:58,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58,646][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.057757433503866196, acc: 0.97826087474823)
[2025-01-06 01:19:58,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58,959][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.018127724528312683, acc: 1.0)
[2025-01-06 01:19:59,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59,303][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.023736190050840378, acc: 1.0)
[2025-01-06 01:19:59,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59,631][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.13652071356773376, acc: 0.936170220375061)
[2025-01-06 01:19:59,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:00,304][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.10573767125606537, acc: 0.9538461565971375)
[2025-01-06 01:20:00,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:00,676][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.023275073617696762, acc: 1.0)
[2025-01-06 01:20:00,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01,068][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.0729002133011818, acc: 0.9883720874786377)
[2025-01-06 01:20:01,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01,605][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.10350189357995987, acc: 0.954954981803894)
[2025-01-06 01:20:01,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02,007][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.08804280310869217, acc: 0.9777777791023254)
[2025-01-06 01:20:02,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02,356][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.10539007186889648, acc: 0.9696969985961914)
[2025-01-06 01:20:02,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02,720][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.009984738193452358, acc: 1.0)
[2025-01-06 01:20:02,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03,081][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.006580662913620472, acc: 1.0)
[2025-01-06 01:20:03,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03,442][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.1589205414056778, acc: 0.942307710647583)
[2025-01-06 01:20:03,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04,195][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.21787035465240479, acc: 0.9347826242446899)
[2025-01-06 01:20:04,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04,733][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.36841753125190735, acc: 0.8977272510528564)
[2025-01-06 01:20:04,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05,162][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.4484587013721466, acc: 0.8723404407501221)
[2025-01-06 01:20:05,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05,542][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.11999712884426117, acc: 0.9622641801834106)
[2025-01-06 01:20:05,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05,838][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.09331166744232178, acc: 0.9666666388511658)
[2025-01-06 01:20:05,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06,171][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.33474934101104736, acc: 0.8604651093482971)
[2025-01-06 01:20:06,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06,472][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.3631497621536255, acc: 0.8999999761581421)
[2025-01-06 01:20:06,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06,802][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 1.0786508321762085, acc: 0.6736842393875122)
[2025-01-06 01:20:06,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07,149][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 0.9280809760093689, acc: 0.7888888716697693)
[2025-01-06 01:20:07,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07,561][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 0.9008857607841492, acc: 0.7333333492279053)
[2025-01-06 01:20:07,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08,045][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 1.1860235929489136, acc: 0.6651375889778137)
[2025-01-06 01:20:08,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08,512][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 1.0217955112457275, acc: 0.699999988079071)
[2025-01-06 01:20:08,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08,803][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.014772162772715092, acc: 1.0)
[2025-01-06 01:20:08,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09,108][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.044786859303712845, acc: 1.0)
[2025-01-06 01:20:09,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09,395][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.31855860352516174, acc: 0.9545454382896423)
[2025-01-06 01:20:09,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09,733][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.05018668621778488, acc: 1.0)
[2025-01-06 01:20:09,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10,103][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.1604418307542801, acc: 0.9428571462631226)
[2025-01-06 01:20:10,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10,460][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.488224595785141, acc: 0.8636363744735718)
[2025-01-06 01:20:10,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10,817][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.2150600403547287, acc: 0.9318181872367859)
[2025-01-06 01:20:10,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11,394][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 0.5343244075775146, acc: 0.8387096524238586)
[2025-01-06 01:20:11,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11,919][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 0.4476442337036133, acc: 0.8636363744735718)
[2025-01-06 01:20:12,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12,254][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.000706761609762907, acc: 1.0)
[2025-01-06 01:20:12,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12,616][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.28415000438690186, acc: 0.9615384340286255)
[2025-01-06 01:20:12,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12,989][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.01701405830681324, acc: 1.0)
[2025-01-06 01:20:13,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13,379][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.06622256338596344, acc: 1.0)
[2025-01-06 01:20:13,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13,731][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.025388378649950027, acc: 1.0)
[2025-01-06 01:20:13,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14,037][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.12222645431756973, acc: 0.9459459185600281)
[2025-01-06 01:20:14,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14,365][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.011724747717380524, acc: 1.0)
[2025-01-06 01:20:14,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14,747][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.07344089448451996, acc: 0.9558823704719543)
[2025-01-06 01:20:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15,136][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.0928940698504448, acc: 0.9756097793579102)
[2025-01-06 01:20:15,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15,484][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.0029715292621403933, acc: 1.0)
[2025-01-06 01:20:15,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15,797][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.0005471071926876903, acc: 1.0)
[2025-01-06 01:20:15,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16,096][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.004114776849746704, acc: 1.0)
[2025-01-06 01:20:16,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16,413][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.055701468139886856, acc: 0.9824561476707458)
[2025-01-06 01:20:16,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16,772][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.049240998923778534, acc: 0.9714285731315613)
[2025-01-06 01:20:16,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17,138][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.010690310038626194, acc: 1.0)
[2025-01-06 01:20:17,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17,699][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.15397939085960388, acc: 0.9528301954269409)
[2025-01-06 01:20:17,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18,279][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.2562921345233917, acc: 0.9166666865348816)
[2025-01-06 01:20:18,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18,588][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.022193659096956253, acc: 1.0)
[2025-01-06 01:20:18,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18,926][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.06701496243476868, acc: 1.0)
[2025-01-06 01:20:19,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19,320][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 0.415061354637146, acc: 0.8933333158493042)
[2025-01-06 01:20:19,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19,624][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.2678440809249878, acc: 0.9375)
[2025-01-06 01:20:19,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20,473][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 0.9478247165679932, acc: 0.7519999742507935)
[2025-01-06 01:20:20,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20,832][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 0.7138039469718933, acc: 0.7977527976036072)
[2025-01-06 01:20:20,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21,213][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.27815160155296326, acc: 0.8918918967247009)
[2025-01-06 01:20:21,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21,665][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.25760889053344727, acc: 0.9137930870056152)
[2025-01-06 01:20:21,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21,969][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.001274994807317853, acc: 1.0)
[2025-01-06 01:20:22,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22,268][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.01111900806427002, acc: 1.0)
[2025-01-06 01:20:22,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22,568][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.016288835555315018, acc: 1.0)
[2025-01-06 01:20:22,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22,929][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.027740539982914925, acc: 1.0)
[2025-01-06 01:20:23,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23,356][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.1466224491596222, acc: 0.9833333492279053)
[2025-01-06 01:20:23,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23,711][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.12989088892936707, acc: 0.96875)
[2025-01-06 01:20:23,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24,091][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.07105603069067001, acc: 0.9666666388511658)
[2025-01-06 01:20:24,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24,460][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.13538137078285217, acc: 0.9655172228813171)
[2025-01-06 01:20:24,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24,808][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.15672433376312256, acc: 0.9599999785423279)
[2025-01-06 01:20:24,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25,155][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.14721083641052246, acc: 0.936170220375061)
[2025-01-06 01:20:25,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25,511][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.11829423904418945, acc: 0.9583333134651184)
[2025-01-06 01:20:26,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:29,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:29,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56,247][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0572, device='cuda:0') eval_epoch_loss=tensor(0.7213, device='cuda:0') eval_epoch_acc=tensor(0.8390, device='cuda:0')
[2025-01-06 01:20:56,248][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:20:56,248][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:20:56,473][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_280_loss_0.721332848072052/model.pt
[2025-01-06 01:20:56,476][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:20:56,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56,917][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.00740846386179328, acc: 1.0)
[2025-01-06 01:20:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57,347][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 0.29194176197052, acc: 0.891566276550293)
[2025-01-06 01:20:57,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57,717][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 0.3892766535282135, acc: 0.8703703880310059)
[2025-01-06 01:20:57,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58,088][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.0700598731637001, acc: 0.9736841917037964)
[2025-01-06 01:20:58,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58,456][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.007130909711122513, acc: 1.0)
[2025-01-06 01:20:58,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58,819][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.0738244578242302, acc: 0.9750000238418579)
[2025-01-06 01:20:58,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59,174][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.2058815211057663, acc: 0.9296875)
[2025-01-06 01:20:59,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59,535][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.3682396113872528, acc: 0.9120000004768372)
[2025-01-06 01:20:59,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59,904][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.16879306733608246, acc: 0.9230769276618958)
[2025-01-06 01:21:00,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00,264][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.15577231347560883, acc: 0.9503105878829956)
[2025-01-06 01:21:00,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00,605][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.3222205638885498, acc: 0.9278350472450256)
[2025-01-06 01:21:00,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00,902][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.013152099214494228, acc: 1.0)
[2025-01-06 01:21:00,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01,223][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.08908611536026001, acc: 0.976190447807312)
[2025-01-06 01:21:01,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01,627][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.07614148408174515, acc: 0.982758641242981)
[2025-01-06 01:21:01,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02,120][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.24034729599952698, acc: 0.8727272748947144)
[2025-01-06 01:21:02,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02,666][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.30892425775527954, acc: 0.9123711585998535)
[2025-01-06 01:21:02,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02,967][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.1340673416852951, acc: 0.9482758641242981)
[2025-01-06 01:21:03,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03,324][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.03114534169435501, acc: 1.0)
[2025-01-06 01:21:03,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03,691][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.06865998357534409, acc: 1.0)
[2025-01-06 01:21:03,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04,021][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.020669149234890938, acc: 1.0)
[2025-01-06 01:21:04,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04,380][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.005711985751986504, acc: 1.0)
[2025-01-06 01:21:04,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04,752][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.018021542578935623, acc: 1.0)
[2025-01-06 01:21:04,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05,127][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.013624560087919235, acc: 1.0)
[2025-01-06 01:21:05,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05,499][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.010862561874091625, acc: 1.0)
[2025-01-06 01:21:05,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05,848][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.07621946930885315, acc: 0.96875)
[2025-01-06 01:21:05,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06,184][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.21732056140899658, acc: 0.9508196711540222)
[2025-01-06 01:21:06,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06,542][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.4243510663509369, acc: 0.9333333373069763)
[2025-01-06 01:21:06,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06,900][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.004817747510969639, acc: 1.0)
[2025-01-06 01:21:07,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07,289][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.10888669639825821, acc: 0.9710144996643066)
[2025-01-06 01:21:07,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07,737][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.05746915563941002, acc: 0.9722222089767456)
[2025-01-06 01:21:07,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08,109][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.11900890618562698, acc: 0.9518072009086609)
[2025-01-06 01:21:08,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08,471][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.06638161092996597, acc: 1.0)
[2025-01-06 01:21:08,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08,842][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.04855719953775406, acc: 0.9897959232330322)
[2025-01-06 01:21:08,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09,132][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.007906812243163586, acc: 1.0)
[2025-01-06 01:21:09,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09,426][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.005065165460109711, acc: 1.0)
[2025-01-06 01:21:09,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09,801][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.010358543135225773, acc: 1.0)
[2025-01-06 01:21:09,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10,159][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.299386590719223, acc: 0.9354838728904724)
[2025-01-06 01:21:10,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10,550][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.020042071118950844, acc: 1.0)
[2025-01-06 01:21:10,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10,885][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.0516270250082016, acc: 0.9807692170143127)
[2025-01-06 01:21:10,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11,186][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.0417313352227211, acc: 0.9777777791023254)
[2025-01-06 01:21:11,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11,493][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.06623106449842453, acc: 0.9677419066429138)
[2025-01-06 01:21:11,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11,853][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.03208789601922035, acc: 1.0)
[2025-01-06 01:21:11,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:12,198][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.15202274918556213, acc: 0.9629629850387573)
[2025-01-06 01:21:12,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:12,559][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 0.36114487051963806, acc: 0.9142857193946838)
[2025-01-06 01:21:12,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:12,912][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.35305142402648926, acc: 0.8974359035491943)
[2025-01-06 01:21:12,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13,175][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 0.4209345877170563, acc: 0.8536585569381714)
[2025-01-06 01:21:13,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13,491][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 0.19474835693836212, acc: 0.9210526347160339)
[2025-01-06 01:21:13,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13,856][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.08072039484977722, acc: 1.0)
[2025-01-06 01:21:13,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14,177][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.02496255561709404, acc: 1.0)
[2025-01-06 01:21:14,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14,547][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.0037943569477647543, acc: 1.0)
[2025-01-06 01:21:14,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14,886][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.0033474972005933523, acc: 1.0)
[2025-01-06 01:21:15,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15,265][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.16280174255371094, acc: 0.9516128897666931)
[2025-01-06 01:21:15,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15,639][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.0107421251013875, acc: 1.0)
[2025-01-06 01:21:15,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16,008][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.013906477950513363, acc: 1.0)
[2025-01-06 01:21:16,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16,346][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.277995765209198, acc: 0.9666666388511658)
[2025-01-06 01:21:16,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16,745][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.021290406584739685, acc: 1.0)
[2025-01-06 01:21:16,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17,146][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 0.1993684619665146, acc: 0.9599999785423279)
[2025-01-06 01:21:17,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17,513][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 0.3363301157951355, acc: 0.9080459475517273)
[2025-01-06 01:21:17,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17,849][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 0.6794771552085876, acc: 0.8085106611251831)
[2025-01-06 01:21:17,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18,193][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 0.5225037932395935, acc: 0.8554216623306274)
[2025-01-06 01:21:18,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18,549][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.01412777416408062, acc: 1.0)
[2025-01-06 01:21:18,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18,877][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.005343560129404068, acc: 1.0)
[2025-01-06 01:21:18,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19,248][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.1399042308330536, acc: 0.9518072009086609)
[2025-01-06 01:21:19,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19,589][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.21000999212265015, acc: 0.9811320900917053)
[2025-01-06 01:21:19,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19,900][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.07330196350812912, acc: 0.9746835231781006)
[2025-01-06 01:21:19,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20,195][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.04051848128437996, acc: 0.9803921580314636)
[2025-01-06 01:21:20,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20,498][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.3600466251373291, acc: 0.9253731369972229)
[2025-01-06 01:21:20,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20,796][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.0029655699618160725, acc: 1.0)
[2025-01-06 01:21:20,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21,096][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.013696527108550072, acc: 1.0)
[2025-01-06 01:21:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21,496][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.29312586784362793, acc: 0.9444444179534912)
[2025-01-06 01:21:21,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21,874][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.17770496010780334, acc: 0.9534883499145508)
[2025-01-06 01:21:21,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22,252][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.03986469656229019, acc: 1.0)
[2025-01-06 01:21:22,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22,624][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 0.3593731224536896, acc: 0.8888888955116272)
[2025-01-06 01:21:22,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22,946][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.00811754073947668, acc: 1.0)
[2025-01-06 01:21:23,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23,324][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.012227805331349373, acc: 1.0)
[2025-01-06 01:21:23,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23,722][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 0.3306786119937897, acc: 0.8901098966598511)
[2025-01-06 01:21:23,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24,230][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.3244589865207672, acc: 0.886956512928009)
[2025-01-06 01:21:24,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24,616][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.3191465139389038, acc: 0.9239130616188049)
[2025-01-06 01:21:24,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25,014][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.10449858754873276, acc: 0.9795918464660645)
[2025-01-06 01:21:25,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25,336][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.0005740172346122563, acc: 1.0)
[2025-01-06 01:21:25,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25,676][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.016085252165794373, acc: 1.0)
[2025-01-06 01:21:25,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26,025][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.18652649223804474, acc: 0.9512194991111755)
[2025-01-06 01:21:26,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26,380][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.03771911934018135, acc: 0.9777777791023254)
[2025-01-06 01:21:26,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26,730][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.05526840686798096, acc: 0.9868420958518982)
[2025-01-06 01:21:26,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27,059][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.046126268804073334, acc: 0.9756097793579102)
[2025-01-06 01:21:27,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27,360][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.009784293361008167, acc: 1.0)
[2025-01-06 01:21:27,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27,653][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.0003980745968874544, acc: 1.0)
[2025-01-06 01:21:27,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27,948][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.012680765241384506, acc: 1.0)
[2025-01-06 01:21:28,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28,341][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.06115783005952835, acc: 0.9642857313156128)
[2025-01-06 01:21:28,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28,711][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.012596307322382927, acc: 1.0)
[2025-01-06 01:21:28,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29,308][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.28408458828926086, acc: 0.9151515364646912)
[2025-01-06 01:21:29,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30,148][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.1402890682220459, acc: 0.9433962106704712)
[2025-01-06 01:21:30,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30,556][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.061340633779764175, acc: 0.9777777791023254)
[2025-01-06 01:21:30,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30,937][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.06429436802864075, acc: 0.9821428656578064)
[2025-01-06 01:21:31,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31,286][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.010478234849870205, acc: 1.0)
[2025-01-06 01:21:31,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31,631][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.00047074840404093266, acc: 1.0)
[2025-01-06 01:21:31,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32,007][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.027451099827885628, acc: 1.0)
[2025-01-06 01:21:32,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32,360][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.010986440815031528, acc: 1.0)
[2025-01-06 01:21:32,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32,689][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.11530527472496033, acc: 0.9789473414421082)
[2025-01-06 01:21:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33,262][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.1476617008447647, acc: 0.9580838084220886)
[2025-01-06 01:21:33,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33,658][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.20943285524845123, acc: 0.9473684430122375)
[2025-01-06 01:21:34,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34,858][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.3453192114830017, acc: 0.8823529481887817)
[2025-01-06 01:21:35,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35,419][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.054483845829963684, acc: 0.9909909963607788)
[2025-01-06 01:21:35,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35,764][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.009421895258128643, acc: 1.0)
[2025-01-06 01:21:35,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36,084][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.010200655087828636, acc: 1.0)
[2025-01-06 01:21:36,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36,430][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.0034471654798835516, acc: 1.0)
[2025-01-06 01:21:36,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36,761][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.000851326622068882, acc: 1.0)
[2025-01-06 01:21:36,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37,077][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.001087688491679728, acc: 1.0)
[2025-01-06 01:21:37,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37,412][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.0017992559587582946, acc: 1.0)
[2025-01-06 01:21:37,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37,769][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.0007218344835564494, acc: 1.0)
[2025-01-06 01:21:37,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38,104][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.41191309690475464, acc: 0.9523809552192688)
[2025-01-06 01:21:38,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38,473][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 0.2660942077636719, acc: 0.9259259104728699)
[2025-01-06 01:21:38,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38,786][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 0.2842894494533539, acc: 0.9029126167297363)
[2025-01-06 01:21:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39,306][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 0.610426127910614, acc: 0.875)
[2025-01-06 01:21:39,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39,672][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 0.3259909152984619, acc: 0.9066666960716248)
[2025-01-06 01:21:39,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40,077][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.4688858389854431, acc: 0.8680555820465088)
[2025-01-06 01:21:40,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40,450][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.20748834311962128, acc: 0.9767441749572754)
[2025-01-06 01:21:40,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40,772][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.17515723407268524, acc: 0.9583333134651184)
[2025-01-06 01:21:40,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41,080][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.14571231603622437, acc: 0.9534883499145508)
[2025-01-06 01:21:41,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41,434][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.003676481544971466, acc: 1.0)
[2025-01-06 01:21:41,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41,969][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.11514461785554886, acc: 0.9852941036224365)
[2025-01-06 01:21:42,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42,339][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.1949332356452942, acc: 0.9066666960716248)
[2025-01-06 01:21:42,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42,713][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.14203685522079468, acc: 0.9696969985961914)
[2025-01-06 01:21:42,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43,051][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.05036935955286026, acc: 0.9696969985961914)
[2025-01-06 01:21:43,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43,428][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.091546930372715, acc: 0.9354838728904724)
[2025-01-06 01:21:43,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43,791][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.0026808450929820538, acc: 1.0)
[2025-01-06 01:21:43,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44,161][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.04564937576651573, acc: 1.0)
[2025-01-06 01:21:44,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44,552][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.005246406886726618, acc: 1.0)
[2025-01-06 01:21:44,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44,896][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.01153505314141512, acc: 1.0)
[2025-01-06 01:21:44,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45,170][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.016666311770677567, acc: 1.0)
[2025-01-06 01:21:45,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45,481][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.009132477454841137, acc: 1.0)
[2025-01-06 01:21:45,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45,834][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.015530929900705814, acc: 1.0)
[2025-01-06 01:21:45,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46,179][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.0019625327549874783, acc: 1.0)
[2025-01-06 01:21:46,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46,489][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.019039439037442207, acc: 1.0)
[2025-01-06 01:21:46,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46,815][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.005133335944265127, acc: 1.0)
[2025-01-06 01:21:46,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47,155][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.10333807021379471, acc: 0.9607843160629272)
[2025-01-06 01:21:47,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47,427][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.012249200604856014, acc: 1.0)
[2025-01-06 01:21:47,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47,736][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.09022817760705948, acc: 0.9444444179534912)
[2025-01-06 01:21:47,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48,091][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.10309545695781708, acc: 0.9750000238418579)
[2025-01-06 01:21:48,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48,402][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.011480451561510563, acc: 1.0)
[2025-01-06 01:21:48,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48,722][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.008644884452223778, acc: 1.0)
[2025-01-06 01:21:48,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49,045][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.016911858692765236, acc: 1.0)
[2025-01-06 01:21:49,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49,343][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.005110962316393852, acc: 1.0)
[2025-01-06 01:21:50,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18,996][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0576, device='cuda:0') eval_epoch_loss=tensor(0.7215, device='cuda:0') eval_epoch_acc=tensor(0.8381, device='cuda:0')
[2025-01-06 01:22:18,997][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:22:18,997][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:22:19,222][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_423_loss_0.7215169072151184/model.pt
[2025-01-06 01:22:19,225][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:22:19,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19,639][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.16329821944236755, acc: 0.9722222089767456)
[2025-01-06 01:22:19,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20,015][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.1979086846113205, acc: 0.9259259104728699)
[2025-01-06 01:22:20,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20,373][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.034730784595012665, acc: 0.9696969985961914)
[2025-01-06 01:22:20,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20,736][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.004117846023291349, acc: 1.0)
[2025-01-06 01:22:20,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21,085][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.0820070430636406, acc: 0.9729729890823364)
[2025-01-06 01:22:21,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21,487][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.04452384263277054, acc: 1.0)
[2025-01-06 01:22:21,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21,826][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.0012545526260510087, acc: 1.0)
[2025-01-06 01:22:21,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22,183][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.0007624722202308476, acc: 1.0)
[2025-01-06 01:22:22,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22,583][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.008278435096144676, acc: 1.0)
[2025-01-06 01:22:22,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22,948][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.0016370725352317095, acc: 1.0)
[2025-01-06 01:22:23,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23,353][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.12210524082183838, acc: 0.9444444179534912)
[2025-01-06 01:22:23,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23,725][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.03471769019961357, acc: 0.9599999785423279)
[2025-01-06 01:22:23,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24,102][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.001189547241665423, acc: 1.0)
[2025-01-06 01:22:24,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24,432][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.03693551570177078, acc: 1.0)
[2025-01-06 01:22:24,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24,785][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.009694485925137997, acc: 1.0)
[2025-01-06 01:22:24,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25,122][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.003900888841599226, acc: 1.0)
[2025-01-06 01:22:25,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25,478][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.004400054924190044, acc: 1.0)
[2025-01-06 01:22:25,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25,982][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.10509684681892395, acc: 0.9545454382896423)
[2025-01-06 01:22:26,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26,668][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.31373170018196106, acc: 0.9039999842643738)
[2025-01-06 01:22:26,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27,087][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.4243440330028534, acc: 0.8870967626571655)
[2025-01-06 01:22:27,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27,744][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.30083709955215454, acc: 0.9104477763175964)
[2025-01-06 01:22:27,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28,080][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.0617084875702858, acc: 0.9811320900917053)
[2025-01-06 01:22:28,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28,504][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.033575043082237244, acc: 1.0)
[2025-01-06 01:22:28,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28,819][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.01429785043001175, acc: 1.0)
[2025-01-06 01:22:28,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29,203][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.16796912252902985, acc: 0.9615384340286255)
[2025-01-06 01:22:29,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29,582][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.012341344729065895, acc: 1.0)
[2025-01-06 01:22:29,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29,962][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.02284196950495243, acc: 0.9850746393203735)
[2025-01-06 01:22:30,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30,280][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.008604801259934902, acc: 1.0)
[2025-01-06 01:22:30,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30,575][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.017460795119404793, acc: 1.0)
[2025-01-06 01:22:30,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30,934][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.05789008364081383, acc: 0.9871794581413269)
[2025-01-06 01:22:31,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31,282][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.06603879481554031, acc: 0.9736841917037964)
[2025-01-06 01:22:31,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31,638][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.006815850734710693, acc: 1.0)
[2025-01-06 01:22:31,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32,017][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.018000002950429916, acc: 1.0)
[2025-01-06 01:22:32,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32,387][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.3220456540584564, acc: 0.9278350472450256)
[2025-01-06 01:22:32,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32,738][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.007041491102427244, acc: 1.0)
[2025-01-06 01:22:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33,107][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.13796277344226837, acc: 0.9534883499145508)
[2025-01-06 01:22:33,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33,472][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.005752589087933302, acc: 1.0)
[2025-01-06 01:22:33,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33,846][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.03381248563528061, acc: 1.0)
[2025-01-06 01:22:33,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34,163][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.01969578117132187, acc: 1.0)
[2025-01-06 01:22:34,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34,497][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.00931653380393982, acc: 1.0)
[2025-01-06 01:22:34,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34,879][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.05200057476758957, acc: 1.0)
[2025-01-06 01:22:34,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35,239][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.02119695581495762, acc: 1.0)
[2025-01-06 01:22:35,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35,572][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.08805594593286514, acc: 0.9642857313156128)
[2025-01-06 01:22:35,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35,916][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.1953592449426651, acc: 0.9397590160369873)
[2025-01-06 01:22:36,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36,300][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.031378500163555145, acc: 0.9909909963607788)
[2025-01-06 01:22:36,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36,624][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.29467377066612244, acc: 0.9320388436317444)
[2025-01-06 01:22:36,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37,007][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.13373172283172607, acc: 0.9674796462059021)
[2025-01-06 01:22:37,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37,339][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.006791144143790007, acc: 1.0)
[2025-01-06 01:22:37,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37,662][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.008584840223193169, acc: 1.0)
[2025-01-06 01:22:37,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38,060][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.3062222898006439, acc: 0.9313725233078003)
[2025-01-06 01:22:38,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38,470][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.5566161274909973, acc: 0.8515284061431885)
[2025-01-06 01:22:38,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38,857][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.14783868193626404, acc: 0.9270833134651184)
[2025-01-06 01:22:38,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39,228][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.18544448912143707, acc: 0.9447852969169617)
[2025-01-06 01:22:39,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39,596][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.14111366868019104, acc: 0.9424460530281067)
[2025-01-06 01:22:39,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39,945][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.45057782530784607, acc: 0.8743718862533569)
[2025-01-06 01:22:40,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40,320][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.032938163727521896, acc: 1.0)
[2025-01-06 01:22:40,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40,697][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.037901587784290314, acc: 1.0)
[2025-01-06 01:22:40,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41,055][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.014518028125166893, acc: 1.0)
[2025-01-06 01:22:41,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41,396][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.14020796120166779, acc: 0.949999988079071)
[2025-01-06 01:22:41,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41,743][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.2719058692455292, acc: 0.8500000238418579)
[2025-01-06 01:22:41,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42,109][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.4504775404930115, acc: 0.9137930870056152)
[2025-01-06 01:22:42,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42,461][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.005185466259717941, acc: 1.0)
[2025-01-06 01:22:42,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42,829][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.02675282023847103, acc: 1.0)
[2025-01-06 01:22:42,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43,166][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.12263811379671097, acc: 0.9259259104728699)
[2025-01-06 01:22:43,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43,512][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.023165490478277206, acc: 1.0)
[2025-01-06 01:22:43,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43,889][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.03776389732956886, acc: 1.0)
[2025-01-06 01:22:44,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44,260][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.269597589969635, acc: 0.9076923131942749)
[2025-01-06 01:22:44,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44,637][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.017697079107165337, acc: 1.0)
[2025-01-06 01:22:44,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44,985][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.04707453399896622, acc: 1.0)
[2025-01-06 01:22:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45,355][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.10991669446229935, acc: 0.9607843160629272)
[2025-01-06 01:22:45,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45,747][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.011859224177896976, acc: 1.0)
[2025-01-06 01:22:45,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46,092][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.1416277438402176, acc: 0.9473684430122375)
[2025-01-06 01:22:46,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46,471][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.015942543745040894, acc: 1.0)
[2025-01-06 01:22:46,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46,897][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.3009811043739319, acc: 0.9017857313156128)
[2025-01-06 01:22:47,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47,292][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.10648035258054733, acc: 0.966292142868042)
[2025-01-06 01:22:47,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47,667][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.24879655241966248, acc: 0.8876404762268066)
[2025-01-06 01:22:47,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48,009][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 0.4990527033805847, acc: 0.8226950168609619)
[2025-01-06 01:22:48,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48,342][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 0.360302597284317, acc: 0.9021739363670349)
[2025-01-06 01:22:48,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48,719][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.0364045612514019, acc: 0.9599999785423279)
[2025-01-06 01:22:48,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49,062][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.000955507974140346, acc: 1.0)
[2025-01-06 01:22:49,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49,384][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.007397472392767668, acc: 1.0)
[2025-01-06 01:22:49,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49,731][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.009489822201430798, acc: 1.0)
[2025-01-06 01:22:49,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50,076][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.20408767461776733, acc: 0.9245283007621765)
[2025-01-06 01:22:50,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50,460][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.8078685998916626, acc: 0.8275862336158752)
[2025-01-06 01:22:50,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51,056][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 0.6194598078727722, acc: 0.8468468189239502)
[2025-01-06 01:22:51,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51,498][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.22152899205684662, acc: 0.9436619877815247)
[2025-01-06 01:22:51,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51,841][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.008567312732338905, acc: 1.0)
[2025-01-06 01:22:51,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52,232][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.007783496752381325, acc: 1.0)
[2025-01-06 01:22:52,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52,566][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.047158755362033844, acc: 0.9615384340286255)
[2025-01-06 01:22:53,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55,243][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 0.4341450035572052, acc: 0.8785714507102966)
[2025-01-06 01:22:55,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56,007][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.06299154460430145, acc: 0.976190447807312)
[2025-01-06 01:22:56,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56,319][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.16584445536136627, acc: 0.9285714030265808)
[2025-01-06 01:22:56,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56,701][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.016427787020802498, acc: 1.0)
[2025-01-06 01:22:56,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57,389][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.19140392541885376, acc: 0.9444444179534912)
[2025-01-06 01:22:57,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57,696][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.000823112903162837, acc: 1.0)
[2025-01-06 01:22:57,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57,985][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.009221833199262619, acc: 1.0)
[2025-01-06 01:22:58,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58,275][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.05177469179034233, acc: 1.0)
[2025-01-06 01:22:58,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58,575][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.018502771854400635, acc: 1.0)
[2025-01-06 01:22:58,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:59,576][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.4555322825908661, acc: 0.8516949415206909)
[2025-01-06 01:22:59,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:59,926][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.049714453518390656, acc: 0.9925373196601868)
[2025-01-06 01:23:00,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:00,331][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.1651802361011505, acc: 0.9343065619468689)
[2025-01-06 01:23:00,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:00,890][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.39213359355926514, acc: 0.8949999809265137)
[2025-01-06 01:23:00,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01,238][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.005283591337502003, acc: 1.0)
[2025-01-06 01:23:01,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01,584][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.038947947323322296, acc: 1.0)
[2025-01-06 01:23:01,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01,951][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.02767491340637207, acc: 1.0)
[2025-01-06 01:23:02,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02,307][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 0.4452095031738281, acc: 0.8524590134620667)
[2025-01-06 01:23:02,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02,667][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.08805210143327713, acc: 0.9830508232116699)
[2025-01-06 01:23:02,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03,010][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 0.4729470908641815, acc: 0.8837209343910217)
[2025-01-06 01:23:03,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03,375][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.15396995842456818, acc: 0.9772727489471436)
[2025-01-06 01:23:03,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03,738][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 0.19061918556690216, acc: 0.9056603908538818)
[2025-01-06 01:23:03,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04,118][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.33842456340789795, acc: 0.9545454382896423)
[2025-01-06 01:23:04,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04,480][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.04975563660264015, acc: 1.0)
[2025-01-06 01:23:04,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04,868][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.028610264882445335, acc: 1.0)
[2025-01-06 01:23:04,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05,215][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.1006564199924469, acc: 0.9545454382896423)
[2025-01-06 01:23:05,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05,621][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.16098365187644958, acc: 0.9692307710647583)
[2025-01-06 01:23:05,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05,968][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.23977245390415192, acc: 0.90625)
[2025-01-06 01:23:06,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06,358][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.03805811330676079, acc: 1.0)
[2025-01-06 01:23:06,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06,693][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.05822901800274849, acc: 0.9696969985961914)
[2025-01-06 01:23:06,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07,053][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.003570006461814046, acc: 1.0)
[2025-01-06 01:23:07,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07,391][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.0018334167543798685, acc: 1.0)
[2025-01-06 01:23:07,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07,697][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.0015173867577686906, acc: 1.0)
[2025-01-06 01:23:07,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08,034][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.018886912614107132, acc: 1.0)
[2025-01-06 01:23:08,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08,375][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.011263348162174225, acc: 1.0)
[2025-01-06 01:23:08,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08,713][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.013193645514547825, acc: 1.0)
[2025-01-06 01:23:08,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09,049][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.01037314347922802, acc: 1.0)
[2025-01-06 01:23:09,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09,411][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.045091498643159866, acc: 0.9677419066429138)
[2025-01-06 01:23:09,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09,763][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.04582832753658295, acc: 0.9599999785423279)
[2025-01-06 01:23:09,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10,141][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.021392088383436203, acc: 1.0)
[2025-01-06 01:23:10,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10,488][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.0412035807967186, acc: 0.9750000238418579)
[2025-01-06 01:23:10,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10,818][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.043356262147426605, acc: 0.9714285731315613)
[2025-01-06 01:23:10,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11,215][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.16153211891651154, acc: 0.956204354763031)
[2025-01-06 01:23:11,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11,597][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.05388270691037178, acc: 0.9862068891525269)
[2025-01-06 01:23:11,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11,976][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.1497790813446045, acc: 0.9857142567634583)
[2025-01-06 01:23:12,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12,330][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.24491523206233978, acc: 0.9337748289108276)
[2025-01-06 01:23:12,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12,684][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.0627792552113533, acc: 0.9743589758872986)
[2025-01-06 01:23:12,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13,021][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.025572722777724266, acc: 1.0)
[2025-01-06 01:23:13,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13,336][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.05325213819742203, acc: 0.9615384340286255)
[2025-01-06 01:23:13,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13,696][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.0017364324303343892, acc: 1.0)
[2025-01-06 01:23:13,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14,052][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.2858171761035919, acc: 0.9743589758872986)
[2025-01-06 01:23:14,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14,425][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.14748142659664154, acc: 0.9555555582046509)
[2025-01-06 01:23:14,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14,792][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.13769471645355225, acc: 0.9740259647369385)
[2025-01-06 01:23:14,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15,175][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.011711892671883106, acc: 1.0)
[2025-01-06 01:23:15,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15,546][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.06642382591962814, acc: 0.9655172228813171)
[2025-01-06 01:23:16,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:20,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:20,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45,804][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1154, device='cuda:0') eval_epoch_loss=tensor(0.7492, device='cuda:0') eval_epoch_acc=tensor(0.8448, device='cuda:0')
[2025-01-06 01:23:45,805][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:23:45,807][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:23:46,021][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_566_loss_0.7492378950119019/model.pt
[2025-01-06 01:23:46,027][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:23:46,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46,388][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.10738175362348557, acc: 0.9642857313156128)
[2025-01-06 01:23:46,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46,725][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.0026657767593860626, acc: 1.0)
[2025-01-06 01:23:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47,029][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.004107438959181309, acc: 1.0)
[2025-01-06 01:23:47,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47,413][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.11200448870658875, acc: 0.9786096215248108)
[2025-01-06 01:23:47,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47,751][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.003864176804199815, acc: 1.0)
[2025-01-06 01:23:47,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48,126][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.01451493427157402, acc: 0.9914529919624329)
[2025-01-06 01:23:48,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48,455][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.2148391604423523, acc: 0.918367326259613)
[2025-01-06 01:23:48,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48,799][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.11717749387025833, acc: 0.9811320900917053)
[2025-01-06 01:23:49,206][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.2091, train_epoch_loss=0.1899, epoch time 354.24308686703444s
[2025-01-06 01:23:49,206][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:23:49,206][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2025-01-06 01:23:49,206][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:23:49,206][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-01-06 01:23:49,206][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:23:49,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49,993][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.01660514622926712, acc: 1.0)
[2025-01-06 01:23:50,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:50,365][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.03390007093548775, acc: 1.0)
[2025-01-06 01:23:50,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:50,755][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.5597116947174072, acc: 0.9189189076423645)
[2025-01-06 01:23:50,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51,126][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.0100776432082057, acc: 1.0)
[2025-01-06 01:23:51,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51,457][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.07594954967498779, acc: 0.9729729890823364)
[2025-01-06 01:23:51,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51,794][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.018001452088356018, acc: 1.0)
[2025-01-06 01:23:51,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52,134][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.17811495065689087, acc: 0.9387755393981934)
[2025-01-06 01:23:52,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52,503][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.018380049616098404, acc: 1.0)
[2025-01-06 01:23:52,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52,860][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.05484870821237564, acc: 0.9545454382896423)
[2025-01-06 01:23:52,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53,171][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.001398535561747849, acc: 1.0)
[2025-01-06 01:23:53,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53,480][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.03515641391277313, acc: 0.9629629850387573)
[2025-01-06 01:23:53,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53,838][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.09382610023021698, acc: 0.9487179517745972)
[2025-01-06 01:23:53,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54,233][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.011907785199582577, acc: 1.0)
[2025-01-06 01:23:54,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54,618][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.08189965039491653, acc: 0.97826087474823)
[2025-01-06 01:23:54,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54,947][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.013241001404821873, acc: 1.0)
[2025-01-06 01:23:55,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55,290][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.05698870122432709, acc: 0.9795918464660645)
[2025-01-06 01:23:55,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55,660][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.0018444565357640386, acc: 1.0)
[2025-01-06 01:23:55,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55,995][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.025469234213232994, acc: 1.0)
[2025-01-06 01:23:56,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56,359][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.08021073043346405, acc: 0.9722222089767456)
[2025-01-06 01:23:56,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56,737][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.05322107672691345, acc: 1.0)
[2025-01-06 01:23:56,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57,119][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.009410485625267029, acc: 1.0)
[2025-01-06 01:23:57,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57,463][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.001719304476864636, acc: 1.0)
[2025-01-06 01:23:57,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57,779][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.0069836825132369995, acc: 1.0)
[2025-01-06 01:23:57,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58,128][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.04341956973075867, acc: 1.0)
[2025-01-06 01:23:58,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58,513][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.03395194932818413, acc: 1.0)
[2025-01-06 01:23:58,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58,819][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.106379434466362, acc: 0.9622641801834106)
[2025-01-06 01:23:58,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59,211][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.11081089079380035, acc: 0.9863013625144958)
[2025-01-06 01:23:59,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:00,523][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 0.46106505393981934, acc: 0.8537549376487732)
[2025-01-06 01:24:00,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:00,817][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.022539444267749786, acc: 1.0)
[2025-01-06 01:24:00,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01,146][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.13871921598911285, acc: 0.9638554453849792)
[2025-01-06 01:24:01,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01,520][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.09180906414985657, acc: 0.9753086566925049)
[2025-01-06 01:24:01,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01,868][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.04703620821237564, acc: 1.0)
[2025-01-06 01:24:01,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02,182][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.052823565900325775, acc: 0.9629629850387573)
[2025-01-06 01:24:02,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02,496][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.008719949051737785, acc: 1.0)
[2025-01-06 01:24:02,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02,839][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.1467677652835846, acc: 0.9495798349380493)
[2025-01-06 01:24:02,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03,128][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.029669221490621567, acc: 0.9836065769195557)
[2025-01-06 01:24:03,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03,487][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.14925691485404968, acc: 0.9365079402923584)
[2025-01-06 01:24:03,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03,820][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.16651470959186554, acc: 0.9661017060279846)
[2025-01-06 01:24:03,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04,192][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.046023473143577576, acc: 1.0)
[2025-01-06 01:24:04,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04,483][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.010108904913067818, acc: 1.0)
[2025-01-06 01:24:04,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04,797][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.10061465203762054, acc: 0.9615384340286255)
[2025-01-06 01:24:04,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05,177][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.06594770401716232, acc: 0.9729729890823364)
[2025-01-06 01:24:05,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05,602][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.19569559395313263, acc: 0.9538461565971375)
[2025-01-06 01:24:05,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06,037][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.22517862915992737, acc: 0.9494949579238892)
[2025-01-06 01:24:06,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06,451][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.23028552532196045, acc: 0.938144326210022)
[2025-01-06 01:24:06,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06,824][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.14046388864517212, acc: 0.9558823704719543)
[2025-01-06 01:24:06,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07,175][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.06071082130074501, acc: 1.0)
[2025-01-06 01:24:07,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07,540][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.03040466271340847, acc: 1.0)
[2025-01-06 01:24:07,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07,902][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.1283913105726242, acc: 0.9285714030265808)
[2025-01-06 01:24:08,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08,275][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.0057349856942892075, acc: 1.0)
[2025-01-06 01:24:08,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08,668][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.3581145405769348, acc: 0.8771929740905762)
[2025-01-06 01:24:08,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09,070][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.2533422112464905, acc: 0.9365079402923584)
[2025-01-06 01:24:09,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09,424][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.22011516988277435, acc: 0.9154929518699646)
[2025-01-06 01:24:09,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09,876][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 0.8960555195808411, acc: 0.746666669845581)
[2025-01-06 01:24:09,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10,233][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.22673049569129944, acc: 0.9189189076423645)
[2025-01-06 01:24:10,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10,565][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.00512132840231061, acc: 1.0)
[2025-01-06 01:24:12,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:13,687][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 0.9546047449111938, acc: 0.6962457299232483)
[2025-01-06 01:24:14,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:14,926][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 0.9924182891845703, acc: 0.7254902124404907)
[2025-01-06 01:24:15,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:15,554][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.5381298065185547, acc: 0.8238636255264282)
[2025-01-06 01:24:15,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16,123][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.07952892780303955, acc: 0.9779411554336548)
[2025-01-06 01:24:16,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16,682][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.4547921121120453, acc: 0.8550724387168884)
[2025-01-06 01:24:16,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17,096][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.2392096072435379, acc: 0.9375)
[2025-01-06 01:24:17,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17,434][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.04950325936079025, acc: 1.0)
[2025-01-06 01:24:17,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17,824][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.04775679111480713, acc: 0.9722222089767456)
[2025-01-06 01:24:17,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18,239][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.021696731448173523, acc: 1.0)
[2025-01-06 01:24:18,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18,621][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.011471165344119072, acc: 1.0)
[2025-01-06 01:24:18,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19,019][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.24379651248455048, acc: 0.9464285969734192)
[2025-01-06 01:24:19,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19,382][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.05440446361899376, acc: 1.0)
[2025-01-06 01:24:19,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19,712][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.0008991159265860915, acc: 1.0)
[2025-01-06 01:24:19,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20,061][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.04048463702201843, acc: 1.0)
[2025-01-06 01:24:20,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20,407][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.04926832765340805, acc: 1.0)
[2025-01-06 01:24:20,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20,706][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 0.3917418420314789, acc: 0.875)
[2025-01-06 01:24:20,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21,069][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.28192993998527527, acc: 0.89682537317276)
[2025-01-06 01:24:21,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21,491][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 0.6798601746559143, acc: 0.7846153974533081)
[2025-01-06 01:24:21,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21,895][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.40063580870628357, acc: 0.8775510191917419)
[2025-01-06 01:24:22,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22,283][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 0.5527570247650146, acc: 0.8358209133148193)
[2025-01-06 01:24:22,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22,682][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 0.8195779323577881, acc: 0.7518247961997986)
[2025-01-06 01:24:22,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22,992][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.0027252216823399067, acc: 1.0)
[2025-01-06 01:24:23,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23,338][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.02336827665567398, acc: 1.0)
[2025-01-06 01:24:23,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23,693][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.143771693110466, acc: 0.9696969985961914)
[2025-01-06 01:24:23,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24,030][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.013255228288471699, acc: 1.0)
[2025-01-06 01:24:24,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24,416][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.11759453266859055, acc: 0.942307710647583)
[2025-01-06 01:24:24,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24,763][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.11471159011125565, acc: 0.9807692170143127)
[2025-01-06 01:24:24,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25,161][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.034202732145786285, acc: 1.0)
[2025-01-06 01:24:25,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25,515][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.14895890653133392, acc: 0.9130434989929199)
[2025-01-06 01:24:25,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25,851][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.07382180541753769, acc: 0.9800000190734863)
[2025-01-06 01:24:25,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26,188][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.04767230898141861, acc: 0.95652174949646)
[2025-01-06 01:24:26,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26,653][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.16451789438724518, acc: 0.9399999976158142)
[2025-01-06 01:24:26,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26,999][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.23603905737400055, acc: 0.9417475461959839)
[2025-01-06 01:24:27,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28,075][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.5144832730293274, acc: 0.844660222530365)
[2025-01-06 01:24:28,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28,894][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 0.5783466100692749, acc: 0.8225806355476379)
[2025-01-06 01:24:29,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:29,700][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.6620256304740906, acc: 0.818965494632721)
[2025-01-06 01:24:29,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:30,443][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.28326651453971863, acc: 0.9052631855010986)
[2025-01-06 01:24:30,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31,433][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 0.5513370037078857, acc: 0.8514851331710815)
[2025-01-06 01:24:31,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31,785][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.20921345055103302, acc: 0.9516128897666931)
[2025-01-06 01:24:31,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32,172][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.25921958684921265, acc: 0.8840579986572266)
[2025-01-06 01:24:32,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32,528][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 0.4633651077747345, acc: 0.8571428656578064)
[2025-01-06 01:24:32,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32,905][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 0.40754276514053345, acc: 0.8942307829856873)
[2025-01-06 01:24:33,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33,288][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 0.4905973970890045, acc: 0.8540145754814148)
[2025-01-06 01:24:33,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33,611][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.4076608121395111, acc: 0.8656716346740723)
[2025-01-06 01:24:33,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33,935][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.037580158561468124, acc: 1.0)
[2025-01-06 01:24:34,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34,320][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.0009425808093510568, acc: 1.0)
[2025-01-06 01:24:34,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34,713][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.007348355837166309, acc: 1.0)
[2025-01-06 01:24:34,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35,049][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.0026503638364374638, acc: 1.0)
[2025-01-06 01:24:35,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35,391][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.1464981734752655, acc: 0.9482758641242981)
[2025-01-06 01:24:35,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35,744][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.008757298812270164, acc: 1.0)
[2025-01-06 01:24:35,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36,091][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.043796490877866745, acc: 1.0)
[2025-01-06 01:24:36,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36,474][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.01013028621673584, acc: 1.0)
[2025-01-06 01:24:36,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36,843][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.01028294488787651, acc: 1.0)
[2025-01-06 01:24:36,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37,167][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.008463176898658276, acc: 1.0)
[2025-01-06 01:24:37,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37,463][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.04265333712100983, acc: 0.9692307710647583)
[2025-01-06 01:24:37,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37,874][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.17384043335914612, acc: 0.9473684430122375)
[2025-01-06 01:24:37,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38,217][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.26819559931755066, acc: 0.9122806787490845)
[2025-01-06 01:24:38,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38,532][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.05779882147908211, acc: 0.9743589758872986)
[2025-01-06 01:24:38,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38,911][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.04020087420940399, acc: 1.0)
[2025-01-06 01:24:39,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39,270][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.0017902744002640247, acc: 1.0)
[2025-01-06 01:24:39,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39,651][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.07865625619888306, acc: 0.9682539701461792)
[2025-01-06 01:24:39,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39,980][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.14130929112434387, acc: 0.9430894255638123)
[2025-01-06 01:24:40,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:40,310][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.03947267681360245, acc: 0.9838709831237793)
[2025-01-06 01:24:40,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41,190][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.3250516653060913, acc: 0.9049429893493652)
[2025-01-06 01:24:41,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41,565][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.05054334178566933, acc: 1.0)
[2025-01-06 01:24:41,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41,978][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.08193360269069672, acc: 0.9807692170143127)
[2025-01-06 01:24:42,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42,344][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.0031406215857714415, acc: 1.0)
[2025-01-06 01:24:42,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42,695][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.023872749879956245, acc: 1.0)
[2025-01-06 01:24:42,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43,072][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 0.48608702421188354, acc: 0.8895705342292786)
[2025-01-06 01:24:43,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43,441][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 0.3414955735206604, acc: 0.9166666865348816)
[2025-01-06 01:24:43,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43,802][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 0.5375179052352905, acc: 0.8583333492279053)
[2025-01-06 01:24:43,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44,169][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 0.2365550547838211, acc: 0.9166666865348816)
[2025-01-06 01:24:44,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44,578][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.29582005739212036, acc: 0.9230769276618958)
[2025-01-06 01:24:44,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45,007][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.45058080554008484, acc: 0.904411792755127)
[2025-01-06 01:24:45,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45,373][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.041716769337654114, acc: 1.0)
[2025-01-06 01:24:45,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45,697][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.04225165396928787, acc: 1.0)
[2025-01-06 01:24:45,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46,001][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.026739949360489845, acc: 1.0)
[2025-01-06 01:24:46,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46,259][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.013790574856102467, acc: 1.0)
[2025-01-06 01:24:46,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46,603][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.031652726233005524, acc: 1.0)
[2025-01-06 01:24:47,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17,259][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0317, device='cuda:0') eval_epoch_loss=tensor(0.7089, device='cuda:0') eval_epoch_acc=tensor(0.8498, device='cuda:0')
[2025-01-06 01:25:17,260][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:25:17,260][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:25:17,510][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_135_loss_0.7088739275932312/model.pt
[2025-01-06 01:25:17,513][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:25:17,514][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.8498251438140869
[2025-01-06 01:25:17,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17,906][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.007859908044338226, acc: 1.0)
[2025-01-06 01:25:18,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18,263][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.3374413847923279, acc: 0.9285714030265808)
[2025-01-06 01:25:18,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18,596][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.0424632653594017, acc: 1.0)
[2025-01-06 01:25:18,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18,882][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.25670281052589417, acc: 0.9130434989929199)
[2025-01-06 01:25:18,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19,232][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.01272617932409048, acc: 1.0)
[2025-01-06 01:25:19,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19,608][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.036792878061532974, acc: 1.0)
[2025-01-06 01:25:19,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19,976][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.09292290359735489, acc: 0.9677419066429138)
[2025-01-06 01:25:20,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20,316][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.12656164169311523, acc: 0.9729729890823364)
[2025-01-06 01:25:20,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20,860][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.2681470811367035, acc: 0.8947368264198303)
[2025-01-06 01:25:20,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21,271][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.27588269114494324, acc: 0.9029850959777832)
[2025-01-06 01:25:21,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21,669][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.4843291938304901, acc: 0.9081632494926453)
[2025-01-06 01:25:21,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22,099][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 0.3556733727455139, acc: 0.8617021441459656)
[2025-01-06 01:25:22,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22,453][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.19375401735305786, acc: 0.9571428298950195)
[2025-01-06 01:25:22,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22,814][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.024217883124947548, acc: 1.0)
[2025-01-06 01:25:22,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:23,150][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.21219143271446228, acc: 0.95652174949646)
[2025-01-06 01:25:23,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:23,465][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.043290600180625916, acc: 0.9655172228813171)
[2025-01-06 01:25:23,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:23,841][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.33255451917648315, acc: 0.9347826242446899)
[2025-01-06 01:25:23,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24,227][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.09728389233350754, acc: 0.9661017060279846)
[2025-01-06 01:25:24,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24,583][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.0854366272687912, acc: 0.9824561476707458)
[2025-01-06 01:25:24,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24,936][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.20196668803691864, acc: 0.9054054021835327)
[2025-01-06 01:25:25,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25,290][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.2726328670978546, acc: 0.9642857313156128)
[2025-01-06 01:25:25,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25,591][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.004490775987505913, acc: 1.0)
[2025-01-06 01:25:25,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25,892][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 0.26216766238212585, acc: 0.8947368264198303)
[2025-01-06 01:25:26,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27,458][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.47215601801872253, acc: 0.837837815284729)
[2025-01-06 01:25:27,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27,752][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 0.373520165681839, acc: 0.8888888955116272)
[2025-01-06 01:25:27,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28,151][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 0.4445546269416809, acc: 0.8488371968269348)
[2025-01-06 01:25:28,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28,735][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 0.33882996439933777, acc: 0.9176470637321472)
[2025-01-06 01:25:28,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29,287][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 0.4056873023509979, acc: 0.9101123809814453)
[2025-01-06 01:25:29,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29,594][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.16270987689495087, acc: 0.9545454382896423)
[2025-01-06 01:25:29,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29,885][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.004262459930032492, acc: 1.0)
[2025-01-06 01:25:29,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30,196][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.29046565294265747, acc: 0.8965517282485962)
[2025-01-06 01:25:30,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30,518][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.2609015703201294, acc: 0.918367326259613)
[2025-01-06 01:25:30,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30,856][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.16097165644168854, acc: 0.9800000190734863)
[2025-01-06 01:25:30,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31,268][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 0.1391068547964096, acc: 0.9583333134651184)
[2025-01-06 01:25:31,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31,645][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 0.5360598564147949, acc: 0.8627451062202454)
[2025-01-06 01:25:31,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32,671][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 0.3781038522720337, acc: 0.9109588861465454)
[2025-01-06 01:25:32,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32,961][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.0659874826669693, acc: 0.9583333134651184)
[2025-01-06 01:25:33,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33,236][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.021246878430247307, acc: 1.0)
[2025-01-06 01:25:33,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33,537][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.19279655814170837, acc: 0.9642857313156128)
[2025-01-06 01:25:33,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34,074][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 0.49962785840034485, acc: 0.8230088353157043)
[2025-01-06 01:25:34,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34,406][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.35172539949417114, acc: 0.8840579986572266)
[2025-01-06 01:25:34,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34,760][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.15167908370494843, acc: 0.9431818127632141)
[2025-01-06 01:25:35,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:35,666][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 0.5061529278755188, acc: 0.8625954389572144)
[2025-01-06 01:25:35,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36,332][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 0.5370325446128845, acc: 0.8518518805503845)
[2025-01-06 01:25:36,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36,671][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.12936542928218842, acc: 0.9672130942344666)
[2025-01-06 01:25:36,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36,969][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.02056131698191166, acc: 1.0)
[2025-01-06 01:25:37,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37,272][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.003101334208622575, acc: 1.0)
[2025-01-06 01:25:37,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37,571][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.07103672623634338, acc: 0.9642857313156128)
[2025-01-06 01:25:37,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37,948][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.060200102627277374, acc: 0.9878048896789551)
[2025-01-06 01:25:38,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:38,352][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.25940585136413574, acc: 0.9456193447113037)
[2025-01-06 01:25:38,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:38,695][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.32192036509513855, acc: 0.9308357238769531)
[2025-01-06 01:25:38,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:39,171][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.23560908436775208, acc: 0.9375)
[2025-01-06 01:25:39,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:39,701][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 0.35336530208587646, acc: 0.904315173625946)
[2025-01-06 01:25:39,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:40,153][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.2704228162765503, acc: 0.9145907759666443)
[2025-01-06 01:25:40,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:40,526][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.02998211607336998, acc: 1.0)
[2025-01-06 01:25:40,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:41,073][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.30883923172950745, acc: 0.8837209343910217)
[2025-01-06 01:25:41,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:41,864][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 0.5856488347053528, acc: 0.8253968358039856)
[2025-01-06 01:25:42,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42,778][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 0.44558092951774597, acc: 0.8333333134651184)
[2025-01-06 01:25:42,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43,518][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.23792169988155365, acc: 0.9411764740943909)
[2025-01-06 01:25:43,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44,592][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.48076948523521423, acc: 0.8580247163772583)
[2025-01-06 01:25:44,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45,543][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.17351564764976501, acc: 0.9677419066429138)
[2025-01-06 01:25:45,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45,789][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.0014337111497297883, acc: 1.0)
[2025-01-06 01:25:45,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46,158][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.04727910831570625, acc: 1.0)
[2025-01-06 01:25:46,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46,536][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.15846773982048035, acc: 0.9264705777168274)
[2025-01-06 01:25:46,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46,918][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.42729368805885315, acc: 0.8676470518112183)
[2025-01-06 01:25:47,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47,246][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.24065454304218292, acc: 0.9322034120559692)
[2025-01-06 01:25:47,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47,539][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.24413318932056427, acc: 0.9029850959777832)
[2025-01-06 01:25:47,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47,901][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.2862491309642792, acc: 0.893203854560852)
[2025-01-06 01:25:48,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48,281][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.1091911569237709, acc: 0.9841269850730896)
[2025-01-06 01:25:48,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48,662][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.022766204550862312, acc: 1.0)
[2025-01-06 01:25:48,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49,056][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.0859808400273323, acc: 0.9820627570152283)
[2025-01-06 01:25:49,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49,454][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.20507581532001495, acc: 0.9212598204612732)
[2025-01-06 01:25:49,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49,817][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.12494514137506485, acc: 0.9655172228813171)
[2025-01-06 01:25:49,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50,258][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.2490309327840805, acc: 0.9239130616188049)
[2025-01-06 01:25:50,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50,617][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.14441224932670593, acc: 0.9649805426597595)
[2025-01-06 01:25:50,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50,995][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.031778253614902496, acc: 0.989130437374115)
[2025-01-06 01:25:51,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51,394][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.008585343137383461, acc: 1.0)
[2025-01-06 01:25:51,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51,749][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.01520487479865551, acc: 1.0)
[2025-01-06 01:25:51,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:52,119][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.10187780112028122, acc: 0.957446813583374)
[2025-01-06 01:25:52,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:52,812][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.06416916847229004, acc: 0.9769230484962463)
[2025-01-06 01:25:52,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53,192][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.019784657284617424, acc: 1.0)
[2025-01-06 01:25:53,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53,569][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.04695798456668854, acc: 0.9767441749572754)
[2025-01-06 01:25:53,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54,099][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.07656590640544891, acc: 0.9729729890823364)
[2025-01-06 01:25:54,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54,489][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.07302849739789963, acc: 0.9555555582046509)
[2025-01-06 01:25:54,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54,850][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.10227382183074951, acc: 0.9696969985961914)
[2025-01-06 01:25:54,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55,173][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.0014644465409219265, acc: 1.0)
[2025-01-06 01:25:55,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55,555][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.0030209391843527555, acc: 1.0)
[2025-01-06 01:25:55,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55,921][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.26853618025779724, acc: 0.9230769276618958)
[2025-01-06 01:25:56,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:56,723][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.1675121784210205, acc: 0.9619565010070801)
[2025-01-06 01:25:56,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57,262][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.27963316440582275, acc: 0.9090909361839294)
[2025-01-06 01:25:57,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57,716][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.37277400493621826, acc: 0.8829787373542786)
[2025-01-06 01:25:57,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58,091][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.05453601852059364, acc: 1.0)
[2025-01-06 01:25:58,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58,441][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.2670617997646332, acc: 0.9666666388511658)
[2025-01-06 01:25:58,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58,802][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.30241692066192627, acc: 0.9534883499145508)
[2025-01-06 01:25:58,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59,152][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.32384830713272095, acc: 0.8999999761581421)
[2025-01-06 01:25:59,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59,558][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 0.7010190486907959, acc: 0.821052610874176)
[2025-01-06 01:25:59,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59,879][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 0.8114289045333862, acc: 0.7666666507720947)
[2025-01-06 01:25:59,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00,295][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 0.8005346655845642, acc: 0.7722222208976746)
[2025-01-06 01:26:00,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00,792][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 0.9961376190185547, acc: 0.7018348574638367)
[2025-01-06 01:26:00,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01,264][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 0.6755216121673584, acc: 0.7923076748847961)
[2025-01-06 01:26:01,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01,556][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.020527567714452744, acc: 1.0)
[2025-01-06 01:26:01,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01,854][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.029804738238453865, acc: 1.0)
[2025-01-06 01:26:01,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02,191][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.19802136719226837, acc: 0.9090909361839294)
[2025-01-06 01:26:02,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02,603][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.4209010601043701, acc: 0.8888888955116272)
[2025-01-06 01:26:02,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02,990][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.12851358950138092, acc: 0.9428571462631226)
[2025-01-06 01:26:03,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03,404][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.2802499532699585, acc: 0.9318181872367859)
[2025-01-06 01:26:03,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03,767][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.11004795879125595, acc: 0.9772727489471436)
[2025-01-06 01:26:03,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04,361][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 0.46786198019981384, acc: 0.8709677457809448)
[2025-01-06 01:26:04,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04,884][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.2871505618095398, acc: 0.9318181872367859)
[2025-01-06 01:26:04,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05,187][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.0003037721908185631, acc: 1.0)
[2025-01-06 01:26:05,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05,546][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.10667803883552551, acc: 0.9615384340286255)
[2025-01-06 01:26:05,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05,857][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.0012507331557571888, acc: 1.0)
[2025-01-06 01:26:05,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06,161][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.028841596096754074, acc: 1.0)
[2025-01-06 01:26:06,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06,502][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.08121296018362045, acc: 0.9729729890823364)
[2025-01-06 01:26:06,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06,768][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.02353774383664131, acc: 1.0)
[2025-01-06 01:26:06,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07,064][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.0028469280805438757, acc: 1.0)
[2025-01-06 01:26:07,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07,408][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.15782222151756287, acc: 0.970588207244873)
[2025-01-06 01:26:07,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07,781][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.004371787887066603, acc: 1.0)
[2025-01-06 01:26:07,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08,173][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.05143548548221588, acc: 0.9599999785423279)
[2025-01-06 01:26:08,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08,503][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.01112521905452013, acc: 1.0)
[2025-01-06 01:26:08,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08,875][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.04430421069264412, acc: 0.9677419066429138)
[2025-01-06 01:26:08,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09,188][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.1615438163280487, acc: 0.9473684430122375)
[2025-01-06 01:26:09,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09,529][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.08468752354383469, acc: 0.9857142567634583)
[2025-01-06 01:26:09,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09,882][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.12818659842014313, acc: 0.9605262875556946)
[2025-01-06 01:26:10,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10,443][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.14081159234046936, acc: 0.9433962106704712)
[2025-01-06 01:26:10,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11,022][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.11950688809156418, acc: 0.9666666388511658)
[2025-01-06 01:26:11,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11,385][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.021977722644805908, acc: 1.0)
[2025-01-06 01:26:11,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11,732][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.01852884329855442, acc: 1.0)
[2025-01-06 01:26:11,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12,087][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.21894940733909607, acc: 0.9733333587646484)
[2025-01-06 01:26:12,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12,405][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.08694851398468018, acc: 0.9583333134651184)
[2025-01-06 01:26:12,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13,233][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 0.7172345519065857, acc: 0.7919999957084656)
[2025-01-06 01:26:13,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13,601][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.4181017577648163, acc: 0.9101123809814453)
[2025-01-06 01:26:13,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13,989][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.28304198384284973, acc: 0.9324324131011963)
[2025-01-06 01:26:14,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14,444][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.07747028023004532, acc: 0.982758641242981)
[2025-01-06 01:26:14,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14,810][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.008657287806272507, acc: 1.0)
[2025-01-06 01:26:14,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15,181][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.01782812923192978, acc: 1.0)
[2025-01-06 01:26:15,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15,521][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.10387726873159409, acc: 0.96875)
[2025-01-06 01:26:15,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15,852][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.007469704374670982, acc: 1.0)
[2025-01-06 01:26:15,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16,256][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.17929385602474213, acc: 0.949999988079071)
[2025-01-06 01:26:16,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16,615][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.00902919564396143, acc: 1.0)
[2025-01-06 01:26:16,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16,993][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.040432192385196686, acc: 1.0)
[2025-01-06 01:26:17,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17,381][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.020371921360492706, acc: 1.0)
[2025-01-06 01:26:17,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17,757][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.0027712166775017977, acc: 1.0)
[2025-01-06 01:26:18,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48,542][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1413, device='cuda:0') eval_epoch_loss=tensor(0.7614, device='cuda:0') eval_epoch_acc=tensor(0.8405, device='cuda:0')
[2025-01-06 01:26:48,543][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:26:48,543][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:26:48,846][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_278_loss_0.7614323496818542/model.pt
[2025-01-06 01:26:48,849][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:26:48,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49,215][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.09446119517087936, acc: 0.978723406791687)
[2025-01-06 01:26:49,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49,600][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.1000872477889061, acc: 0.9791666865348816)
[2025-01-06 01:26:49,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49,969][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.02282707579433918, acc: 1.0)
[2025-01-06 01:26:50,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50,398][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.18277877569198608, acc: 0.9397590160369873)
[2025-01-06 01:26:50,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50,783][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.19619005918502808, acc: 0.9444444179534912)
[2025-01-06 01:26:50,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51,158][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.0338355228304863, acc: 0.9736841917037964)
[2025-01-06 01:26:51,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51,457][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.019937559962272644, acc: 1.0)
[2025-01-06 01:26:51,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51,787][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.002314798766747117, acc: 1.0)
[2025-01-06 01:26:51,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52,144][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.15163522958755493, acc: 0.9453125)
[2025-01-06 01:26:52,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52,537][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.2741069197654724, acc: 0.9120000004768372)
[2025-01-06 01:26:52,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52,912][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.05927163362503052, acc: 0.9780219793319702)
[2025-01-06 01:26:53,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53,285][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.10194288939237595, acc: 0.9627329111099243)
[2025-01-06 01:26:53,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53,653][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.2736797034740448, acc: 0.9175257682800293)
[2025-01-06 01:26:53,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54,024][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.028385134413838387, acc: 1.0)
[2025-01-06 01:26:54,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54,404][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.02146364375948906, acc: 1.0)
[2025-01-06 01:26:54,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54,756][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.03666216507554054, acc: 0.982758641242981)
[2025-01-06 01:26:54,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55,237][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.10942082107067108, acc: 0.9272727370262146)
[2025-01-06 01:26:55,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55,791][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.26366832852363586, acc: 0.8969072103500366)
[2025-01-06 01:26:55,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56,113][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.04374275356531143, acc: 0.982758641242981)
[2025-01-06 01:26:56,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56,432][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.006078848149627447, acc: 1.0)
[2025-01-06 01:26:56,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56,795][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.023590795695781708, acc: 1.0)
[2025-01-06 01:26:56,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57,165][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.02168828621506691, acc: 0.9821428656578064)
[2025-01-06 01:26:57,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57,530][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.0028441089671105146, acc: 1.0)
[2025-01-06 01:26:57,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57,894][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.03537972643971443, acc: 1.0)
[2025-01-06 01:26:58,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58,271][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.05564550682902336, acc: 0.9811320900917053)
[2025-01-06 01:26:58,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58,655][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.0020473836921155453, acc: 1.0)
[2025-01-06 01:26:58,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58,997][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.0013468454126268625, acc: 1.0)
[2025-01-06 01:26:59,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59,337][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.08951439708471298, acc: 0.9508196711540222)
[2025-01-06 01:26:59,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59,680][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.0700717493891716, acc: 0.9666666388511658)
[2025-01-06 01:26:59,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00,010][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.00030118582071736455, acc: 1.0)
[2025-01-06 01:27:00,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00,361][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.029163412749767303, acc: 1.0)
[2025-01-06 01:27:00,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00,800][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.028318140655755997, acc: 1.0)
[2025-01-06 01:27:00,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01,146][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.03489932790398598, acc: 0.9879518151283264)
[2025-01-06 01:27:01,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01,474][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.05316273868083954, acc: 0.9743589758872986)
[2025-01-06 01:27:01,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01,809][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.020654898136854172, acc: 1.0)
[2025-01-06 01:27:01,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02,151][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.0013623674167320132, acc: 1.0)
[2025-01-06 01:27:02,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02,468][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.0025221758987754583, acc: 1.0)
[2025-01-06 01:27:02,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02,825][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.029099240899086, acc: 1.0)
[2025-01-06 01:27:02,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03,162][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.12304261326789856, acc: 0.9677419066429138)
[2025-01-06 01:27:03,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03,507][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.02577589824795723, acc: 0.9850746393203735)
[2025-01-06 01:27:03,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03,904][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.01253843866288662, acc: 1.0)
[2025-01-06 01:27:04,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04,239][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.006728836335241795, acc: 1.0)
[2025-01-06 01:27:04,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04,580][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.00852762907743454, acc: 1.0)
[2025-01-06 01:27:04,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04,953][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.005945785436779261, acc: 1.0)
[2025-01-06 01:27:05,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05,316][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.2586607336997986, acc: 0.8888888955116272)
[2025-01-06 01:27:05,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05,664][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.17567719519138336, acc: 0.9428571462631226)
[2025-01-06 01:27:05,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06,068][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.16564202308654785, acc: 0.9743589758872986)
[2025-01-06 01:27:06,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06,388][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 0.22119805216789246, acc: 0.9756097793579102)
[2025-01-06 01:27:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06,740][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.25683867931365967, acc: 0.8947368264198303)
[2025-01-06 01:27:06,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07,114][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.007524657994508743, acc: 1.0)
[2025-01-06 01:27:07,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07,447][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.004108463879674673, acc: 1.0)
[2025-01-06 01:27:07,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07,816][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.06992152333259583, acc: 0.9629629850387573)
[2025-01-06 01:27:07,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08,189][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.006346498150378466, acc: 1.0)
[2025-01-06 01:27:08,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08,564][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.10947614163160324, acc: 0.9677419066429138)
[2025-01-06 01:27:08,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08,946][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.08551617711782455, acc: 0.9824561476707458)
[2025-01-06 01:27:09,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09,287][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.0021991438698023558, acc: 1.0)
[2025-01-06 01:27:09,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09,682][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.020380409434437752, acc: 1.0)
[2025-01-06 01:27:09,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10,051][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.002545527881011367, acc: 1.0)
[2025-01-06 01:27:10,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10,448][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.0440489687025547, acc: 1.0)
[2025-01-06 01:27:10,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10,831][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 0.2829391062259674, acc: 0.9195402264595032)
[2025-01-06 01:27:10,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11,231][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 0.4618622660636902, acc: 0.8297872543334961)
[2025-01-06 01:27:11,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11,610][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 0.30443599820137024, acc: 0.891566276550293)
[2025-01-06 01:27:11,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11,982][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.00040581266512162983, acc: 1.0)
[2025-01-06 01:27:12,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12,336][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.007873283699154854, acc: 1.0)
[2025-01-06 01:27:12,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12,629][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.09778737276792526, acc: 0.9518072009086609)
[2025-01-06 01:27:12,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12,947][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.20232631266117096, acc: 0.9433962106704712)
[2025-01-06 01:27:13,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13,286][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.06882838159799576, acc: 0.9746835231781006)
[2025-01-06 01:27:13,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13,622][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.014903794042766094, acc: 1.0)
[2025-01-06 01:27:13,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13,920][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.024662338197231293, acc: 1.0)
[2025-01-06 01:27:13,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14,225][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.1598929464817047, acc: 0.949999988079071)
[2025-01-06 01:27:14,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14,606][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.0062452200800180435, acc: 1.0)
[2025-01-06 01:27:14,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14,976][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.15310314297676086, acc: 0.9722222089767456)
[2025-01-06 01:27:15,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15,354][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.09164083749055862, acc: 0.9767441749572754)
[2025-01-06 01:27:15,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15,685][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.004340869374573231, acc: 1.0)
[2025-01-06 01:27:15,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16,071][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.12306564301252365, acc: 0.9555555582046509)
[2025-01-06 01:27:16,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16,398][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.1826259344816208, acc: 0.95652174949646)
[2025-01-06 01:27:16,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16,717][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.011533782817423344, acc: 1.0)
[2025-01-06 01:27:16,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17,104][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.30068182945251465, acc: 0.901098906993866)
[2025-01-06 01:27:17,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17,613][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.2712782025337219, acc: 0.9304347634315491)
[2025-01-06 01:27:17,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17,964][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.17081762850284576, acc: 0.9130434989929199)
[2025-01-06 01:27:18,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18,314][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.22060920298099518, acc: 0.9591836929321289)
[2025-01-06 01:27:18,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18,666][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.0013140140799805522, acc: 1.0)
[2025-01-06 01:27:18,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,000][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.055173177272081375, acc: 0.9615384340286255)
[2025-01-06 01:27:19,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,331][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.1326172947883606, acc: 0.9756097793579102)
[2025-01-06 01:27:19,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,662][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.019437653943896294, acc: 1.0)
[2025-01-06 01:27:19,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19,978][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.006996853742748499, acc: 1.0)
[2025-01-06 01:27:20,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20,261][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.10821381211280823, acc: 0.9512194991111755)
[2025-01-06 01:27:20,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20,553][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.009212777018547058, acc: 1.0)
[2025-01-06 01:27:20,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20,883][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.00030203265487216413, acc: 1.0)
[2025-01-06 01:27:20,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21,225][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.0007721219444647431, acc: 1.0)
[2025-01-06 01:27:21,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21,549][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.05201243981719017, acc: 0.9642857313156128)
[2025-01-06 01:27:21,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21,921][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.5281215906143188, acc: 0.90625)
[2025-01-06 01:27:22,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22,537][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.2180103361606598, acc: 0.9151515364646912)
[2025-01-06 01:27:22,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23,378][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.09272973984479904, acc: 0.9716981053352356)
[2025-01-06 01:27:23,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23,744][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.0451531782746315, acc: 1.0)
[2025-01-06 01:27:23,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24,115][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.06956909596920013, acc: 0.9642857313156128)
[2025-01-06 01:27:24,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24,460][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.07852821797132492, acc: 0.9714285731315613)
[2025-01-06 01:27:24,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24,777][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.01037752628326416, acc: 1.0)
[2025-01-06 01:27:24,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25,108][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.027696343138813972, acc: 1.0)
[2025-01-06 01:27:25,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25,421][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.007215121295303106, acc: 1.0)
[2025-01-06 01:27:25,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25,744][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.004399087745696306, acc: 1.0)
[2025-01-06 01:27:25,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26,331][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.11784379929304123, acc: 0.9580838084220886)
[2025-01-06 01:27:26,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26,726][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.23974987864494324, acc: 0.932330846786499)
[2025-01-06 01:27:27,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27,760][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.3169262111186981, acc: 0.903743326663971)
[2025-01-06 01:27:27,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28,324][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.02841075137257576, acc: 0.9909909963607788)
[2025-01-06 01:27:28,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28,673][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.05896434932947159, acc: 0.9642857313156128)
[2025-01-06 01:27:28,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29,025][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.004937656223773956, acc: 1.0)
[2025-01-06 01:27:29,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29,368][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.003989015705883503, acc: 1.0)
[2025-01-06 01:27:29,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29,731][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.002729849424213171, acc: 1.0)
[2025-01-06 01:27:29,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30,094][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.0024785855785012245, acc: 1.0)
[2025-01-06 01:27:30,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30,431][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.024568496271967888, acc: 1.0)
[2025-01-06 01:27:30,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30,782][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.1497591882944107, acc: 0.949999988079071)
[2025-01-06 01:27:30,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31,173][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.03892127051949501, acc: 1.0)
[2025-01-06 01:27:31,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31,562][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.19336281716823578, acc: 0.9444444179534912)
[2025-01-06 01:27:31,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31,931][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.3023039698600769, acc: 0.9029126167297363)
[2025-01-06 01:27:32,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32,451][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.6074501276016235, acc: 0.8235294222831726)
[2025-01-06 01:27:32,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32,852][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.305763304233551, acc: 0.8799999952316284)
[2025-01-06 01:27:32,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33,230][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.30826830863952637, acc: 0.9027777910232544)
[2025-01-06 01:27:33,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33,600][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.10212334990501404, acc: 0.9767441749572754)
[2025-01-06 01:27:33,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33,960][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.005621037911623716, acc: 1.0)
[2025-01-06 01:27:34,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34,337][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.07207099348306656, acc: 0.9767441749572754)
[2025-01-06 01:27:34,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34,674][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.021836290135979652, acc: 1.0)
[2025-01-06 01:27:34,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35,204][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.12026084214448929, acc: 0.9558823704719543)
[2025-01-06 01:27:35,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35,606][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.09340032190084457, acc: 0.9599999785423279)
[2025-01-06 01:27:35,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35,956][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.00695204408839345, acc: 1.0)
[2025-01-06 01:27:36,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36,289][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.03928856924176216, acc: 1.0)
[2025-01-06 01:27:36,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36,656][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.007080798037350178, acc: 1.0)
[2025-01-06 01:27:36,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36,968][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.0016479798359796405, acc: 1.0)
[2025-01-06 01:27:37,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37,323][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.010044828988611698, acc: 1.0)
[2025-01-06 01:27:37,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37,643][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.003189267124980688, acc: 1.0)
[2025-01-06 01:27:37,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37,998][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.0371885821223259, acc: 0.9629629850387573)
[2025-01-06 01:27:38,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38,357][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.004197196569293737, acc: 1.0)
[2025-01-06 01:27:38,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38,668][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.015606967732310295, acc: 1.0)
[2025-01-06 01:27:38,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38,971][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.008727753534913063, acc: 1.0)
[2025-01-06 01:27:39,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39,324][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.011051137931644917, acc: 1.0)
[2025-01-06 01:27:39,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39,665][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.05002026632428169, acc: 0.9696969985961914)
[2025-01-06 01:27:39,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40,011][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.004536377731710672, acc: 1.0)
[2025-01-06 01:27:40,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40,336][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.08348539471626282, acc: 0.9607843160629272)
[2025-01-06 01:27:40,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40,648][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.009596670046448708, acc: 1.0)
[2025-01-06 01:27:40,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41,032][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.01742870733141899, acc: 1.0)
[2025-01-06 01:27:41,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41,383][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.04620359092950821, acc: 0.9750000238418579)
[2025-01-06 01:27:41,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41,748][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.011067907325923443, acc: 1.0)
[2025-01-06 01:27:41,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42,107][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.004527214914560318, acc: 1.0)
[2025-01-06 01:27:42,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13,047][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1171, device='cuda:0') eval_epoch_loss=tensor(0.7500, device='cuda:0') eval_epoch_acc=tensor(0.8393, device='cuda:0')
[2025-01-06 01:28:13,048][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:28:13,048][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:28:13,311][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_421_loss_0.7500268816947937/model.pt
[2025-01-06 01:28:13,316][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:28:13,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13,662][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.10452965646982193, acc: 0.9666666388511658)
[2025-01-06 01:28:13,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14,006][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.018454141914844513, acc: 1.0)
[2025-01-06 01:28:14,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14,403][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.03676685690879822, acc: 1.0)
[2025-01-06 01:28:14,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14,740][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.24243271350860596, acc: 0.9629629850387573)
[2025-01-06 01:28:14,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15,037][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.014624170958995819, acc: 1.0)
[2025-01-06 01:28:15,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15,375][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 0.017646925523877144, acc: 1.0)
[2025-01-06 01:28:15,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15,739][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.06657545268535614, acc: 0.9729729890823364)
[2025-01-06 01:28:15,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16,094][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.0016812423709779978, acc: 1.0)
[2025-01-06 01:28:16,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16,422][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.0011645958293229342, acc: 1.0)
[2025-01-06 01:28:16,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16,787][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.00035198446130380034, acc: 1.0)
[2025-01-06 01:28:16,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17,101][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.012645240873098373, acc: 1.0)
[2025-01-06 01:28:17,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17,461][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.0059364126063883305, acc: 1.0)
[2025-01-06 01:28:17,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17,870][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.040030986070632935, acc: 0.9722222089767456)
[2025-01-06 01:28:17,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18,235][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.010070081800222397, acc: 1.0)
[2025-01-06 01:28:18,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18,575][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.004772393964231014, acc: 1.0)
[2025-01-06 01:28:18,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18,926][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.20980127155780792, acc: 0.9722222089767456)
[2025-01-06 01:28:19,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19,265][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.1260678470134735, acc: 0.9772727489471436)
[2025-01-06 01:28:19,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19,636][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.2717028558254242, acc: 0.9523809552192688)
[2025-01-06 01:28:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19,970][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.01512401644140482, acc: 1.0)
[2025-01-06 01:28:20,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20,466][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.07508666813373566, acc: 0.9696969985961914)
[2025-01-06 01:28:20,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21,146][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.44525375962257385, acc: 0.8399999737739563)
[2025-01-06 01:28:21,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21,551][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.27424052357673645, acc: 0.9274193644523621)
[2025-01-06 01:28:21,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22,198][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.2262672632932663, acc: 0.9402984976768494)
[2025-01-06 01:28:22,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22,533][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.027545830234885216, acc: 1.0)
[2025-01-06 01:28:22,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22,960][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.04043794050812721, acc: 0.9772727489471436)
[2025-01-06 01:28:23,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23,329][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.01881680265069008, acc: 1.0)
[2025-01-06 01:28:23,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23,693][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.03372962772846222, acc: 1.0)
[2025-01-06 01:28:23,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24,056][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.00234905444085598, acc: 1.0)
[2025-01-06 01:28:24,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24,386][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.052459243685007095, acc: 0.9850746393203735)
[2025-01-06 01:28:24,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24,788][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.019190283492207527, acc: 0.9861111044883728)
[2025-01-06 01:28:24,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25,171][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.02142396941781044, acc: 0.989130437374115)
[2025-01-06 01:28:25,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25,519][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.029013773426413536, acc: 0.9871794581413269)
[2025-01-06 01:28:25,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25,876][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.21008527278900146, acc: 0.9473684430122375)
[2025-01-06 01:28:26,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26,292][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.1563054621219635, acc: 0.9795918464660645)
[2025-01-06 01:28:26,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26,657][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.06612078845500946, acc: 0.9696969985961914)
[2025-01-06 01:28:26,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27,008][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.18034416437149048, acc: 0.9484536051750183)
[2025-01-06 01:28:27,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27,306][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.0019379350123926997, acc: 1.0)
[2025-01-06 01:28:27,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27,678][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.08632851392030716, acc: 0.9593023061752319)
[2025-01-06 01:28:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28,031][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.0069190082140266895, acc: 1.0)
[2025-01-06 01:28:28,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28,384][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.10769517719745636, acc: 0.9753086566925049)
[2025-01-06 01:28:28,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28,743][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.0522071048617363, acc: 0.9722222089767456)
[2025-01-06 01:28:28,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:29,114][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.00971379317343235, acc: 1.0)
[2025-01-06 01:28:29,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:29,453][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.004807473160326481, acc: 1.0)
[2025-01-06 01:28:29,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:29,782][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.033539820462465286, acc: 1.0)
[2025-01-06 01:28:29,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30,143][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.023321649059653282, acc: 1.0)
[2025-01-06 01:28:30,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30,511][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.4438745379447937, acc: 0.8554216623306274)
[2025-01-06 01:28:30,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30,866][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.047820378094911575, acc: 0.9819819927215576)
[2025-01-06 01:28:30,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31,215][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.22840796411037445, acc: 0.9514563083648682)
[2025-01-06 01:28:31,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31,561][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.12421952188014984, acc: 0.9674796462059021)
[2025-01-06 01:28:31,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31,858][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.04841454699635506, acc: 1.0)
[2025-01-06 01:28:31,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32,158][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.06086162105202675, acc: 0.9642857313156128)
[2025-01-06 01:28:32,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32,580][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.2937150299549103, acc: 0.9313725233078003)
[2025-01-06 01:28:32,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32,954][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.3485242426395416, acc: 0.8864628672599792)
[2025-01-06 01:28:33,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33,248][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.11789926141500473, acc: 0.9583333134651184)
[2025-01-06 01:28:33,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33,567][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.11121195554733276, acc: 0.9570552110671997)
[2025-01-06 01:28:33,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33,909][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.12485118210315704, acc: 0.9640287756919861)
[2025-01-06 01:28:34,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34,264][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.27059218287467957, acc: 0.9045225977897644)
[2025-01-06 01:28:34,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34,628][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.09250347316265106, acc: 0.9722222089767456)
[2025-01-06 01:28:34,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34,974][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.420729398727417, acc: 0.9090909361839294)
[2025-01-06 01:28:35,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35,308][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.07489307969808578, acc: 0.9629629850387573)
[2025-01-06 01:28:35,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35,626][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.4295447766780853, acc: 0.949999988079071)
[2025-01-06 01:28:35,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35,962][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.07226525992155075, acc: 0.949999988079071)
[2025-01-06 01:28:36,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36,331][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.09842870384454727, acc: 0.982758641242981)
[2025-01-06 01:28:36,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36,687][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.0071039083413779736, acc: 1.0)
[2025-01-06 01:28:36,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37,008][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.19251787662506104, acc: 0.9473684430122375)
[2025-01-06 01:28:37,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37,350][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.24987651407718658, acc: 0.8888888955116272)
[2025-01-06 01:28:37,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37,736][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.2659749686717987, acc: 0.9047619104385376)
[2025-01-06 01:28:37,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38,111][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.0461856871843338, acc: 1.0)
[2025-01-06 01:28:38,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38,444][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.18177473545074463, acc: 0.9384615421295166)
[2025-01-06 01:28:38,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38,789][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.0033078519627451897, acc: 1.0)
[2025-01-06 01:28:38,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39,131][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.25311359763145447, acc: 0.9655172228813171)
[2025-01-06 01:28:39,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39,419][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.14560841023921967, acc: 0.9607843160629272)
[2025-01-06 01:28:39,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39,778][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.013998077251017094, acc: 1.0)
[2025-01-06 01:28:39,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40,071][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.4144653379917145, acc: 0.8947368264198303)
[2025-01-06 01:28:40,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40,385][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.06976091116666794, acc: 0.9473684430122375)
[2025-01-06 01:28:40,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40,750][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.15440905094146729, acc: 0.9553571343421936)
[2025-01-06 01:28:40,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41,116][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.1851135641336441, acc: 0.9550561904907227)
[2025-01-06 01:28:41,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41,486][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.35232192277908325, acc: 0.898876428604126)
[2025-01-06 01:28:41,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41,837][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 0.47656288743019104, acc: 0.8510638475418091)
[2025-01-06 01:28:41,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42,222][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.169070765376091, acc: 0.945652186870575)
[2025-01-06 01:28:42,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42,577][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.025912852957844734, acc: 1.0)
[2025-01-06 01:28:42,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42,929][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.0398080088198185, acc: 0.9615384340286255)
[2025-01-06 01:28:43,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43,288][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.08954206109046936, acc: 0.9629629850387573)
[2025-01-06 01:28:43,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43,635][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.024503683671355247, acc: 1.0)
[2025-01-06 01:28:43,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43,999][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.13710598647594452, acc: 0.9622641801834106)
[2025-01-06 01:28:44,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44,335][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.3394685685634613, acc: 0.931034505367279)
[2025-01-06 01:28:44,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44,917][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 0.41750025749206543, acc: 0.8918918967247009)
[2025-01-06 01:28:45,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45,357][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.216444730758667, acc: 0.9718309640884399)
[2025-01-06 01:28:45,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45,612][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.3286808431148529, acc: 0.8999999761581421)
[2025-01-06 01:28:45,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45,948][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.13996492326259613, acc: 0.8999999761581421)
[2025-01-06 01:28:46,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46,227][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.28194722533226013, acc: 0.8846153616905212)
[2025-01-06 01:28:47,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:48,888][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 0.43594223260879517, acc: 0.8714285492897034)
[2025-01-06 01:28:49,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49,649][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.08139365911483765, acc: 0.9920634627342224)
[2025-01-06 01:28:49,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49,927][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.2305796593427658, acc: 0.9285714030265808)
[2025-01-06 01:28:50,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50,294][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.011579722166061401, acc: 1.0)
[2025-01-06 01:28:50,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50,991][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.09883275628089905, acc: 0.9444444179534912)
[2025-01-06 01:28:51,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51,349][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.001978945219889283, acc: 1.0)
[2025-01-06 01:28:51,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51,697][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.008585695177316666, acc: 1.0)
[2025-01-06 01:28:51,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52,064][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.02068377658724785, acc: 1.0)
[2025-01-06 01:28:52,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52,376][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.04240727052092552, acc: 1.0)
[2025-01-06 01:28:52,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53,357][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.4430408477783203, acc: 0.8813559412956238)
[2025-01-06 01:28:53,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53,766][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.05589281767606735, acc: 0.9925373196601868)
[2025-01-06 01:28:53,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54,176][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.11298269033432007, acc: 0.970802903175354)
[2025-01-06 01:28:54,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54,736][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.3337634205818176, acc: 0.8799999952316284)
[2025-01-06 01:28:54,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55,079][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.004224235191941261, acc: 1.0)
[2025-01-06 01:28:55,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55,406][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.0761677473783493, acc: 0.9807692170143127)
[2025-01-06 01:28:55,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55,795][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.034623391926288605, acc: 1.0)
[2025-01-06 01:28:55,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56,170][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 0.24272117018699646, acc: 0.9344262480735779)
[2025-01-06 01:28:56,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56,563][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.05331287905573845, acc: 1.0)
[2025-01-06 01:28:56,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56,983][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.1711321920156479, acc: 0.930232584476471)
[2025-01-06 01:28:57,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57,338][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.5134637355804443, acc: 0.8636363744735718)
[2025-01-06 01:28:57,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57,637][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.2144843190908432, acc: 0.8867924809455872)
[2025-01-06 01:28:57,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57,948][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.12504634261131287, acc: 0.9318181872367859)
[2025-01-06 01:28:58,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58,252][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.023393232375383377, acc: 1.0)
[2025-01-06 01:28:58,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58,645][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.05179436877369881, acc: 1.0)
[2025-01-06 01:28:58,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58,981][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.034872088581323624, acc: 1.0)
[2025-01-06 01:28:59,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59,401][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.1572466641664505, acc: 0.9230769276618958)
[2025-01-06 01:28:59,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59,761][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.2063066065311432, acc: 0.953125)
[2025-01-06 01:28:59,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00,124][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.1663832813501358, acc: 0.90625)
[2025-01-06 01:29:00,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00,477][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.037075985223054886, acc: 1.0)
[2025-01-06 01:29:00,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00,833][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.02561674267053604, acc: 1.0)
[2025-01-06 01:29:00,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01,156][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.002386718988418579, acc: 1.0)
[2025-01-06 01:29:01,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01,479][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.043421436101198196, acc: 0.95652174949646)
[2025-01-06 01:29:01,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01,845][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.0202823244035244, acc: 1.0)
[2025-01-06 01:29:01,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02,239][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.031188564375042915, acc: 1.0)
[2025-01-06 01:29:02,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02,569][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.011009562760591507, acc: 1.0)
[2025-01-06 01:29:02,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02,927][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.0007066137040965259, acc: 1.0)
[2025-01-06 01:29:03,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03,265][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.05549639090895653, acc: 0.9677419066429138)
[2025-01-06 01:29:03,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03,608][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.009497375227510929, acc: 1.0)
[2025-01-06 01:29:03,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03,991][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.23069506883621216, acc: 0.8787878751754761)
[2025-01-06 01:29:04,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04,357][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.12355102598667145, acc: 0.949999988079071)
[2025-01-06 01:29:04,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04,707][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.02707132138311863, acc: 0.9857142567634583)
[2025-01-06 01:29:04,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05,078][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.06976743787527084, acc: 0.970802903175354)
[2025-01-06 01:29:05,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05,434][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.07701417058706284, acc: 0.9655172228813171)
[2025-01-06 01:29:05,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05,840][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.15124979615211487, acc: 0.9357143044471741)
[2025-01-06 01:29:05,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06,186][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.16770590841770172, acc: 0.9536423683166504)
[2025-01-06 01:29:06,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06,504][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.04416178539395332, acc: 0.9914529919624329)
[2025-01-06 01:29:06,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06,866][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.027139313519001007, acc: 1.0)
[2025-01-06 01:29:06,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07,214][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.024734126403927803, acc: 1.0)
[2025-01-06 01:29:07,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07,593][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.0007507042610086501, acc: 1.0)
[2025-01-06 01:29:07,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07,931][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.19279740750789642, acc: 0.9487179517745972)
[2025-01-06 01:29:08,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08,270][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.06538917124271393, acc: 0.9666666388511658)
[2025-01-06 01:29:08,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08,645][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.13165071606636047, acc: 0.9740259647369385)
[2025-01-06 01:29:09,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:30,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:30,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:30,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38,306][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1991, device='cuda:0') eval_epoch_loss=tensor(0.7880, device='cuda:0') eval_epoch_acc=tensor(0.8423, device='cuda:0')
[2025-01-06 01:29:38,307][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:29:38,308][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:29:38,600][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_564_loss_0.7880285978317261/model.pt
[2025-01-06 01:29:38,605][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:29:38,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38,900][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.025892918929457664, acc: 1.0)
[2025-01-06 01:29:39,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39,266][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.03370610624551773, acc: 1.0)
[2025-01-06 01:29:39,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39,596][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.09954212605953217, acc: 0.9642857313156128)
[2025-01-06 01:29:39,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39,928][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.013459471054375172, acc: 1.0)
[2025-01-06 01:29:40,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40,253][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.04657291620969772, acc: 0.9629629850387573)
[2025-01-06 01:29:40,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40,668][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.15703339874744415, acc: 0.9572192430496216)
[2025-01-06 01:29:40,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40,992][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.015539342537522316, acc: 1.0)
[2025-01-06 01:29:41,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41,283][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.03763257712125778, acc: 0.9914529919624329)
[2025-01-06 01:29:41,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41,585][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.13224737346172333, acc: 0.9591836929321289)
[2025-01-06 01:29:41,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41,910][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.10142943263053894, acc: 0.9748427867889404)
[2025-01-06 01:29:42,443][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.1540, train_epoch_loss=0.1433, epoch time 353.23452988639474s
[2025-01-06 01:29:42,443][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:29:42,443][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:29:42,444][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:29:42,444][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 14
[2025-01-06 01:29:42,444][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:29:43,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:43,314][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.03650952875614166, acc: 1.0)
[2025-01-06 01:29:43,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:43,662][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.1182468980550766, acc: 0.9599999785423279)
[2025-01-06 01:29:43,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44,027][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.3303297460079193, acc: 0.9459459185600281)
[2025-01-06 01:29:44,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44,431][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.05277125537395477, acc: 0.9736841917037964)
[2025-01-06 01:29:44,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44,731][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.1685672253370285, acc: 0.9459459185600281)
[2025-01-06 01:29:44,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45,035][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.00391375133767724, acc: 1.0)
[2025-01-06 01:29:45,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45,417][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.1358402967453003, acc: 0.918367326259613)
[2025-01-06 01:29:45,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45,798][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.1875970959663391, acc: 0.9666666388511658)
[2025-01-06 01:29:45,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46,197][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.012026667594909668, acc: 1.0)
[2025-01-06 01:29:46,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46,529][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.07162021100521088, acc: 0.9615384340286255)
[2025-01-06 01:29:46,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46,904][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.0032022190280258656, acc: 1.0)
[2025-01-06 01:29:47,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47,257][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.09005831182003021, acc: 0.9743589758872986)
[2025-01-06 01:29:47,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47,648][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.01929617114365101, acc: 1.0)
[2025-01-06 01:29:47,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48,033][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.03383411094546318, acc: 1.0)
[2025-01-06 01:29:48,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48,420][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.020708736032247543, acc: 1.0)
[2025-01-06 01:29:48,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48,776][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.05101849511265755, acc: 1.0)
[2025-01-06 01:29:48,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49,150][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.25824078917503357, acc: 0.9473684430122375)
[2025-01-06 01:29:49,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49,468][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.02589741162955761, acc: 1.0)
[2025-01-06 01:29:49,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49,825][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.03409735858440399, acc: 1.0)
[2025-01-06 01:29:49,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50,200][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.07857441157102585, acc: 0.9473684430122375)
[2025-01-06 01:29:50,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50,560][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.0071403151378035545, acc: 1.0)
[2025-01-06 01:29:50,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50,940][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.04018312320113182, acc: 1.0)
[2025-01-06 01:29:51,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51,283][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.019435223191976547, acc: 1.0)
[2025-01-06 01:29:51,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51,644][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.252669095993042, acc: 0.9523809552192688)
[2025-01-06 01:29:51,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52,046][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.009375734254717827, acc: 1.0)
[2025-01-06 01:29:52,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52,406][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.08388864994049072, acc: 0.9811320900917053)
[2025-01-06 01:29:52,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52,755][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.24925537407398224, acc: 0.9178082346916199)
[2025-01-06 01:29:53,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54,014][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 0.37279650568962097, acc: 0.8972331881523132)
[2025-01-06 01:29:54,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54,316][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.054241012781858444, acc: 0.9767441749572754)
[2025-01-06 01:29:54,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54,730][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.11230344325304031, acc: 0.9397590160369873)
[2025-01-06 01:29:54,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55,128][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.09408427774906158, acc: 0.9629629850387573)
[2025-01-06 01:29:55,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55,469][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.08853346109390259, acc: 0.9642857313156128)
[2025-01-06 01:29:55,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55,833][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.006982472725212574, acc: 1.0)
[2025-01-06 01:29:55,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56,229][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.011103869415819645, acc: 1.0)
[2025-01-06 01:29:56,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56,624][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.10677780956029892, acc: 0.9831932783126831)
[2025-01-06 01:29:56,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56,973][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.06652642786502838, acc: 0.9672130942344666)
[2025-01-06 01:29:57,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:57,378][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.1930590569972992, acc: 0.9047619104385376)
[2025-01-06 01:29:57,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:57,738][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.18122322857379913, acc: 0.9661017060279846)
[2025-01-06 01:29:57,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58,090][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.04578394070267677, acc: 0.9885057210922241)
[2025-01-06 01:29:58,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58,432][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.056825216859579086, acc: 1.0)
[2025-01-06 01:29:58,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58,818][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.037654001265764236, acc: 1.0)
[2025-01-06 01:29:58,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59,201][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.18566887080669403, acc: 0.9594594836235046)
[2025-01-06 01:29:59,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59,525][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.2692771553993225, acc: 0.9384615421295166)
[2025-01-06 01:29:59,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59,939][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.3755378723144531, acc: 0.9090909361839294)
[2025-01-06 01:30:00,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:00,363][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.14549823105335236, acc: 0.9484536051750183)
[2025-01-06 01:30:00,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:00,777][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.12707878649234772, acc: 0.9632353186607361)
[2025-01-06 01:30:00,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01,126][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.005124996416270733, acc: 1.0)
[2025-01-06 01:30:01,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01,445][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.17473629117012024, acc: 0.9629629850387573)
[2025-01-06 01:30:01,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01,804][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.017331983894109726, acc: 1.0)
[2025-01-06 01:30:01,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02,186][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.003418693784624338, acc: 1.0)
[2025-01-06 01:30:02,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02,581][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.2126428335905075, acc: 0.9298245906829834)
[2025-01-06 01:30:02,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02,928][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.10820740461349487, acc: 0.9365079402923584)
[2025-01-06 01:30:03,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03,299][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.22845740616321564, acc: 0.9295774698257446)
[2025-01-06 01:30:03,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03,752][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 0.5182448625564575, acc: 0.8199999928474426)
[2025-01-06 01:30:03,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04,098][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.09214067459106445, acc: 0.9729729890823364)
[2025-01-06 01:30:04,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04,453][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.10808281600475311, acc: 0.9230769276618958)
[2025-01-06 01:30:06,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:07,573][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 0.662148118019104, acc: 0.7747440338134766)
[2025-01-06 01:30:07,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08,788][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 0.8399643301963806, acc: 0.7603485584259033)
[2025-01-06 01:30:08,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09,410][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.47580936551094055, acc: 0.8465909361839294)
[2025-01-06 01:30:09,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09,985][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.12737159430980682, acc: 0.9632353186607361)
[2025-01-06 01:30:10,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10,547][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.3534499704837799, acc: 0.8913043737411499)
[2025-01-06 01:30:10,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10,962][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.13555563986301422, acc: 0.9624999761581421)
[2025-01-06 01:30:11,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11,298][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.043638262897729874, acc: 1.0)
[2025-01-06 01:30:11,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11,633][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.2540118098258972, acc: 0.9166666865348816)
[2025-01-06 01:30:11,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12,036][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.055409256368875504, acc: 0.96875)
[2025-01-06 01:30:12,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12,418][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.028882376849651337, acc: 1.0)
[2025-01-06 01:30:12,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12,799][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.14609725773334503, acc: 0.9642857313156128)
[2025-01-06 01:30:12,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13,184][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.02271166257560253, acc: 1.0)
[2025-01-06 01:30:13,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13,537][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.005287297535687685, acc: 1.0)
[2025-01-06 01:30:13,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13,914][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.02710566483438015, acc: 1.0)
[2025-01-06 01:30:14,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14,296][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.047220051288604736, acc: 1.0)
[2025-01-06 01:30:14,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14,657][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.2042800933122635, acc: 0.9558823704719543)
[2025-01-06 01:30:14,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14,938][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.21783210337162018, acc: 0.920634925365448)
[2025-01-06 01:30:15,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15,324][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 0.5042523741722107, acc: 0.8410256505012512)
[2025-01-06 01:30:15,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15,720][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.22981661558151245, acc: 0.9387755393981934)
[2025-01-06 01:30:15,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16,092][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.4059169590473175, acc: 0.888059675693512)
[2025-01-06 01:30:16,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16,498][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 0.697751522064209, acc: 0.7992700934410095)
[2025-01-06 01:30:16,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16,843][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.004244180396199226, acc: 1.0)
[2025-01-06 01:30:16,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17,227][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.0056671216152608395, acc: 1.0)
[2025-01-06 01:30:17,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17,581][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.07033957540988922, acc: 0.9696969985961914)
[2025-01-06 01:30:17,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17,963][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.012716993689537048, acc: 1.0)
[2025-01-06 01:30:18,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18,348][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.08677180856466293, acc: 0.9615384340286255)
[2025-01-06 01:30:18,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18,686][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.07504735887050629, acc: 0.9807692170143127)
[2025-01-06 01:30:18,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19,013][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.022191425785422325, acc: 1.0)
[2025-01-06 01:30:19,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19,387][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.046143539249897, acc: 1.0)
[2025-01-06 01:30:19,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19,783][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.0841284841299057, acc: 0.9800000190734863)
[2025-01-06 01:30:19,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20,151][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.05804302543401718, acc: 0.95652174949646)
[2025-01-06 01:30:20,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20,605][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.06592926383018494, acc: 0.9800000190734863)
[2025-01-06 01:30:20,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20,966][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.3357716500759125, acc: 0.9029126167297363)
[2025-01-06 01:30:21,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22,109][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.5254918932914734, acc: 0.8495145440101624)
[2025-01-06 01:30:22,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22,935][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 0.4647981822490692, acc: 0.8870967626571655)
[2025-01-06 01:30:23,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23,736][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.44534915685653687, acc: 0.875)
[2025-01-06 01:30:23,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24,476][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.2542400658130646, acc: 0.9157894849777222)
[2025-01-06 01:30:24,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25,476][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 0.4932478964328766, acc: 0.8811880946159363)
[2025-01-06 01:30:25,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25,873][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.21016378700733185, acc: 0.9516128897666931)
[2025-01-06 01:30:25,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26,236][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.11403991281986237, acc: 0.9710144996643066)
[2025-01-06 01:30:26,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26,632][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.37388578057289124, acc: 0.8823529481887817)
[2025-01-06 01:30:26,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27,040][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.2606305480003357, acc: 0.9134615659713745)
[2025-01-06 01:30:27,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27,458][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 0.3025721311569214, acc: 0.9343065619468689)
[2025-01-06 01:30:27,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27,825][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.19858239591121674, acc: 0.9253731369972229)
[2025-01-06 01:30:27,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28,191][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.054068613797426224, acc: 1.0)
[2025-01-06 01:30:28,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28,566][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.0025099795311689377, acc: 1.0)
[2025-01-06 01:30:28,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28,921][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.00740353437140584, acc: 1.0)
[2025-01-06 01:30:28,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29,277][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.003968818578869104, acc: 1.0)
[2025-01-06 01:30:29,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29,672][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.055359840393066406, acc: 0.982758641242981)
[2025-01-06 01:30:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,024][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.013414776884019375, acc: 1.0)
[2025-01-06 01:30:30,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,356][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.002970757195726037, acc: 1.0)
[2025-01-06 01:30:30,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,614][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.008628655225038528, acc: 1.0)
[2025-01-06 01:30:30,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30,905][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.018435893580317497, acc: 1.0)
[2025-01-06 01:30:30,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31,186][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.009877899661660194, acc: 1.0)
[2025-01-06 01:30:31,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31,567][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.015659578144550323, acc: 1.0)
[2025-01-06 01:30:31,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31,973][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.07383082062005997, acc: 0.9649122953414917)
[2025-01-06 01:30:32,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32,362][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.3472055494785309, acc: 0.9649122953414917)
[2025-01-06 01:30:32,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32,730][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.019955093041062355, acc: 1.0)
[2025-01-06 01:30:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33,142][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.0352177694439888, acc: 1.0)
[2025-01-06 01:30:33,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33,538][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.09160766005516052, acc: 0.9545454382896423)
[2025-01-06 01:30:33,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33,886][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.043869465589523315, acc: 0.9841269850730896)
[2025-01-06 01:30:33,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34,174][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.10488398373126984, acc: 0.9593495726585388)
[2025-01-06 01:30:34,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34,480][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.036726076155900955, acc: 0.9838709831237793)
[2025-01-06 01:30:34,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35,326][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.33518892526626587, acc: 0.9049429893493652)
[2025-01-06 01:30:35,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35,658][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.023998497053980827, acc: 0.9866666793823242)
[2025-01-06 01:30:35,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36,074][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.06980709731578827, acc: 0.942307710647583)
[2025-01-06 01:30:36,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36,370][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.028036082163453102, acc: 1.0)
[2025-01-06 01:30:36,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36,741][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.0038307940121740103, acc: 1.0)
[2025-01-06 01:30:36,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37,125][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.3228626549243927, acc: 0.9141104221343994)
[2025-01-06 01:30:37,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37,494][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 0.2725054621696472, acc: 0.9305555820465088)
[2025-01-06 01:30:37,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37,833][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.3578115403652191, acc: 0.9166666865348816)
[2025-01-06 01:30:37,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38,145][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.20011213421821594, acc: 0.9523809552192688)
[2025-01-06 01:30:38,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38,536][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.2053147554397583, acc: 0.9487179517745972)
[2025-01-06 01:30:38,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38,946][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.2274349480867386, acc: 0.9191176295280457)
[2025-01-06 01:30:39,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39,302][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.10724695026874542, acc: 0.9615384340286255)
[2025-01-06 01:30:39,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39,625][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.08985039591789246, acc: 0.95652174949646)
[2025-01-06 01:30:39,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39,966][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.050413746386766434, acc: 1.0)
[2025-01-06 01:30:40,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:10,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:10,888][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0603, device='cuda:0') eval_epoch_loss=tensor(0.7228, device='cuda:0') eval_epoch_acc=tensor(0.8469, device='cuda:0')
[2025-01-06 01:31:10,889][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:31:10,889][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:31:11,203][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_133_loss_0.7228496074676514/model.pt
[2025-01-06 01:31:11,206][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:31:11,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11,596][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.03444460779428482, acc: 1.0)
[2025-01-06 01:31:11,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11,950][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.046977922320365906, acc: 1.0)
[2025-01-06 01:31:12,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12,298][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.12710198760032654, acc: 0.9615384340286255)
[2025-01-06 01:31:12,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12,638][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.04089578986167908, acc: 1.0)
[2025-01-06 01:31:12,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13,404][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.06025998666882515, acc: 1.0)
[2025-01-06 01:31:13,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13,745][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.06474440544843674, acc: 0.95652174949646)
[2025-01-06 01:31:13,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14,052][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.0029855328612029552, acc: 1.0)
[2025-01-06 01:31:14,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14,415][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.021389760076999664, acc: 1.0)
[2025-01-06 01:31:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14,790][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.059531450271606445, acc: 0.9677419066429138)
[2025-01-06 01:31:14,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15,141][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.15124273300170898, acc: 0.9729729890823364)
[2025-01-06 01:31:15,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15,670][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.14486879110336304, acc: 0.9561403393745422)
[2025-01-06 01:31:15,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16,080][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.13679252564907074, acc: 0.9402984976768494)
[2025-01-06 01:31:16,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16,490][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.10097069293260574, acc: 0.9795918464660645)
[2025-01-06 01:31:16,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16,937][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.21924401819705963, acc: 0.936170220375061)
[2025-01-06 01:31:17,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17,280][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.09883362799882889, acc: 0.9714285731315613)
[2025-01-06 01:31:17,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17,645][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.009242304600775242, acc: 1.0)
[2025-01-06 01:31:17,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18,015][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.18873775005340576, acc: 0.95652174949646)
[2025-01-06 01:31:18,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18,367][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.010923308320343494, acc: 1.0)
[2025-01-06 01:31:18,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18,772][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.1651451736688614, acc: 0.9347826242446899)
[2025-01-06 01:31:18,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19,119][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.1476735770702362, acc: 0.9491525292396545)
[2025-01-06 01:31:19,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19,471][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.23755231499671936, acc: 0.9649122953414917)
[2025-01-06 01:31:19,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19,826][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.14391855895519257, acc: 0.9189189076423645)
[2025-01-06 01:31:19,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20,220][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.08606299012899399, acc: 0.9642857313156128)
[2025-01-06 01:31:20,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20,610][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.255880743265152, acc: 0.9130434989929199)
[2025-01-06 01:31:20,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20,958][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.346884548664093, acc: 0.8947368264198303)
[2025-01-06 01:31:21,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22,577][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.25656408071517944, acc: 0.9054054021835327)
[2025-01-06 01:31:22,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22,879][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.5718026757240295, acc: 0.8333333134651184)
[2025-01-06 01:31:23,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23,303][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 0.3385399580001831, acc: 0.8720930218696594)
[2025-01-06 01:31:23,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23,899][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 0.358117014169693, acc: 0.8941176533699036)
[2025-01-06 01:31:24,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24,456][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 0.2719663381576538, acc: 0.932584285736084)
[2025-01-06 01:31:24,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24,811][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.07393158227205276, acc: 0.9772727489471436)
[2025-01-06 01:31:24,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25,175][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.11941118538379669, acc: 0.9523809552192688)
[2025-01-06 01:31:25,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25,516][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.22014465928077698, acc: 0.931034505367279)
[2025-01-06 01:31:25,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25,876][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.042566221207380295, acc: 0.9795918464660645)
[2025-01-06 01:31:25,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26,252][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.014325905591249466, acc: 1.0)
[2025-01-06 01:31:26,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26,646][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.10290935635566711, acc: 0.9722222089767456)
[2025-01-06 01:31:26,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26,962][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.381963849067688, acc: 0.8823529481887817)
[2025-01-06 01:31:27,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28,021][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.36752015352249146, acc: 0.9041095972061157)
[2025-01-06 01:31:28,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28,340][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.153365820646286, acc: 0.9583333134651184)
[2025-01-06 01:31:28,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28,679][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.21721000969409943, acc: 0.8888888955116272)
[2025-01-06 01:31:28,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29,044][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.0165774617344141, acc: 1.0)
[2025-01-06 01:31:29,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29,582][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 0.5674290060997009, acc: 0.8407079577445984)
[2025-01-06 01:31:29,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29,906][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.1275276243686676, acc: 0.9420289993286133)
[2025-01-06 01:31:30,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30,315][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.1307777464389801, acc: 0.9318181872367859)
[2025-01-06 01:31:30,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31,230][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 0.3723434805870056, acc: 0.8702290058135986)
[2025-01-06 01:31:31,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31,899][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.32359614968299866, acc: 0.9185185432434082)
[2025-01-06 01:31:31,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32,199][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.03549972549080849, acc: 0.9836065769195557)
[2025-01-06 01:31:32,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32,486][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.0020635323598980904, acc: 1.0)
[2025-01-06 01:31:32,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32,810][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.2606298625469208, acc: 0.9599999785423279)
[2025-01-06 01:31:32,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33,144][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.01518244482576847, acc: 1.0)
[2025-01-06 01:31:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33,455][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.02291400544345379, acc: 0.9878048896789551)
[2025-01-06 01:31:33,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33,795][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.23321567475795746, acc: 0.9425981640815735)
[2025-01-06 01:31:33,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34,165][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.23617488145828247, acc: 0.9365994334220886)
[2025-01-06 01:31:34,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34,648][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.23318776488304138, acc: 0.921875)
[2025-01-06 01:31:34,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35,177][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.26366111636161804, acc: 0.9212007522583008)
[2025-01-06 01:31:35,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35,582][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.24416446685791016, acc: 0.935943067073822)
[2025-01-06 01:31:35,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35,961][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.14010868966579437, acc: 0.9599999785423279)
[2025-01-06 01:31:36,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36,517][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.13047197461128235, acc: 0.9651162624359131)
[2025-01-06 01:31:36,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:37,311][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 0.5491508841514587, acc: 0.8730158805847168)
[2025-01-06 01:31:37,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38,223][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.19701680541038513, acc: 0.9469696879386902)
[2025-01-06 01:31:38,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38,970][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.14766769111156464, acc: 0.9764705896377563)
[2025-01-06 01:31:39,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40,048][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.3210596740245819, acc: 0.895061731338501)
[2025-01-06 01:31:40,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40,998][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.05605609714984894, acc: 1.0)
[2025-01-06 01:31:41,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41,280][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.01982317492365837, acc: 1.0)
[2025-01-06 01:31:41,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41,627][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.04062856733798981, acc: 1.0)
[2025-01-06 01:31:41,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41,956][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.1742628663778305, acc: 0.970588207244873)
[2025-01-06 01:31:42,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42,323][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.26767390966415405, acc: 0.9264705777168274)
[2025-01-06 01:31:42,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42,673][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.22173435986042023, acc: 0.9237288236618042)
[2025-01-06 01:31:42,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42,987][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.2635229825973511, acc: 0.8805969953536987)
[2025-01-06 01:31:43,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43,322][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.18126805126667023, acc: 0.9320388436317444)
[2025-01-06 01:31:43,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43,714][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.13232649862766266, acc: 0.9682539701461792)
[2025-01-06 01:31:43,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44,103][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.015369324944913387, acc: 1.0)
[2025-01-06 01:31:44,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44,496][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.07968297600746155, acc: 0.9775784611701965)
[2025-01-06 01:31:44,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44,898][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.16003850102424622, acc: 0.9527559280395508)
[2025-01-06 01:31:45,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45,260][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.08632442355155945, acc: 0.9525862336158752)
[2025-01-06 01:31:45,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45,610][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.15566182136535645, acc: 0.967391312122345)
[2025-01-06 01:31:45,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45,965][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.12713995575904846, acc: 0.9649805426597595)
[2025-01-06 01:31:46,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46,356][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.0707005187869072, acc: 0.97826087474823)
[2025-01-06 01:31:46,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46,707][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.008575263433158398, acc: 1.0)
[2025-01-06 01:31:46,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47,056][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.007990442216396332, acc: 1.0)
[2025-01-06 01:31:47,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47,411][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.04738704860210419, acc: 0.978723406791687)
[2025-01-06 01:31:47,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48,089][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.026809087023139, acc: 0.9923076629638672)
[2025-01-06 01:31:48,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48,406][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.11006941646337509, acc: 0.9594594836235046)
[2025-01-06 01:31:48,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48,715][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.021342534571886063, acc: 0.9883720874786377)
[2025-01-06 01:31:48,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49,256][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.07269305735826492, acc: 0.9819819927215576)
[2025-01-06 01:31:49,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49,666][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.040745366364717484, acc: 0.9888888597488403)
[2025-01-06 01:31:49,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50,027][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.02504030056297779, acc: 1.0)
[2025-01-06 01:31:50,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50,339][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.03342768922448158, acc: 1.0)
[2025-01-06 01:31:50,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50,658][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.03327949345111847, acc: 1.0)
[2025-01-06 01:31:50,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51,060][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.22141963243484497, acc: 0.9038461446762085)
[2025-01-06 01:31:51,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51,819][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.11132761836051941, acc: 0.9619565010070801)
[2025-01-06 01:31:51,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52,358][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.21553954482078552, acc: 0.9375)
[2025-01-06 01:31:52,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52,797][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.18663443624973297, acc: 0.9042553305625916)
[2025-01-06 01:31:52,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53,172][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.09352244436740875, acc: 0.9622641801834106)
[2025-01-06 01:31:53,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53,577][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.04877977445721626, acc: 0.9666666388511658)
[2025-01-06 01:31:53,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53,937][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.09304754436016083, acc: 0.9767441749572754)
[2025-01-06 01:31:54,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54,211][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.30190640687942505, acc: 0.9333333373069763)
[2025-01-06 01:31:54,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54,625][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 0.7360243201255798, acc: 0.75789475440979)
[2025-01-06 01:31:54,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54,978][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 0.42897307872772217, acc: 0.8888888955116272)
[2025-01-06 01:31:55,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55,394][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 0.634574294090271, acc: 0.800000011920929)
[2025-01-06 01:31:55,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55,879][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 0.9545538425445557, acc: 0.7247706651687622)
[2025-01-06 01:31:56,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56,346][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 0.601556658744812, acc: 0.8153846263885498)
[2025-01-06 01:31:56,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56,650][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.024033622816205025, acc: 1.0)
[2025-01-06 01:31:56,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57,000][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.009581254795193672, acc: 1.0)
[2025-01-06 01:31:57,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57,394][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.4288305342197418, acc: 0.9090909361839294)
[2025-01-06 01:31:57,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57,779][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.03607817366719246, acc: 1.0)
[2025-01-06 01:31:57,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58,144][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.2136107087135315, acc: 0.9428571462631226)
[2025-01-06 01:31:58,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58,501][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.10690510272979736, acc: 0.9772727489471436)
[2025-01-06 01:31:58,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58,890][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.06620409339666367, acc: 1.0)
[2025-01-06 01:31:59,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59,470][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.2694966197013855, acc: 0.9193548560142517)
[2025-01-06 01:31:59,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59,996][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.39043551683425903, acc: 0.8636363744735718)
[2025-01-06 01:32:00,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00,317][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.00025455429567955434, acc: 1.0)
[2025-01-06 01:32:00,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00,659][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.22359831631183624, acc: 0.9615384340286255)
[2025-01-06 01:32:00,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00,948][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.0027582915499806404, acc: 1.0)
[2025-01-06 01:32:01,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01,243][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.008493723347783089, acc: 1.0)
[2025-01-06 01:32:01,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01,620][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.01944374106824398, acc: 1.0)
[2025-01-06 01:32:01,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01,958][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.025205660611391068, acc: 1.0)
[2025-01-06 01:32:02,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02,300][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.00389865436591208, acc: 1.0)
[2025-01-06 01:32:02,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02,685][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.020787781104445457, acc: 1.0)
[2025-01-06 01:32:02,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03,061][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.005202674772590399, acc: 1.0)
[2025-01-06 01:32:03,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03,427][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.017133379355072975, acc: 1.0)
[2025-01-06 01:32:03,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03,766][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.0001236781827174127, acc: 1.0)
[2025-01-06 01:32:03,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04,148][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.1035512313246727, acc: 0.9354838728904724)
[2025-01-06 01:32:04,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04,555][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.010251899249851704, acc: 1.0)
[2025-01-06 01:32:04,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04,946][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.04868520423769951, acc: 1.0)
[2025-01-06 01:32:05,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05,334][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.017298752442002296, acc: 0.9868420958518982)
[2025-01-06 01:32:05,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05,906][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.05634129419922829, acc: 0.9905660152435303)
[2025-01-06 01:32:06,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06,488][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.0911596342921257, acc: 0.9666666388511658)
[2025-01-06 01:32:06,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06,803][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.0040955254808068275, acc: 1.0)
[2025-01-06 01:32:06,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07,184][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.024853233247995377, acc: 1.0)
[2025-01-06 01:32:07,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07,534][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.3255406320095062, acc: 0.9066666960716248)
[2025-01-06 01:32:07,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07,835][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.11774390935897827, acc: 0.9583333134651184)
[2025-01-06 01:32:08,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08,663][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.6238560676574707, acc: 0.8240000009536743)
[2025-01-06 01:32:08,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08,981][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.22616499662399292, acc: 0.9213483333587646)
[2025-01-06 01:32:09,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09,364][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.19348643720149994, acc: 0.9324324131011963)
[2025-01-06 01:32:09,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09,819][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.16994260251522064, acc: 0.931034505367279)
[2025-01-06 01:32:09,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10,189][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.07196524739265442, acc: 0.9545454382896423)
[2025-01-06 01:32:10,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10,575][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.01747133582830429, acc: 1.0)
[2025-01-06 01:32:10,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10,923][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.08751257508993149, acc: 0.96875)
[2025-01-06 01:32:11,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11,267][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.0018845272716134787, acc: 1.0)
[2025-01-06 01:32:11,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11,648][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.2075362205505371, acc: 0.9833333492279053)
[2025-01-06 01:32:11,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11,933][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.28238430619239807, acc: 0.9375)
[2025-01-06 01:32:12,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12,278][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.006143494509160519, acc: 1.0)
[2025-01-06 01:32:12,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41,994][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1688, device='cuda:0') eval_epoch_loss=tensor(0.7742, device='cuda:0') eval_epoch_acc=tensor(0.8434, device='cuda:0')
[2025-01-06 01:32:41,995][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:32:41,995][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:32:42,309][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_276_loss_0.7741731405258179/model.pt
[2025-01-06 01:32:42,313][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:32:42,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:42,702][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.10278045386075974, acc: 0.9655172228813171)
[2025-01-06 01:32:42,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43,055][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.006748638115823269, acc: 1.0)
[2025-01-06 01:32:43,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43,435][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.02653028815984726, acc: 1.0)
[2025-01-06 01:32:43,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43,784][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.012382294982671738, acc: 1.0)
[2025-01-06 01:32:43,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44,177][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.0070374878123402596, acc: 1.0)
[2025-01-06 01:32:44,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44,607][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.11910182237625122, acc: 0.9397590160369873)
[2025-01-06 01:32:44,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44,982][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.30774420499801636, acc: 0.9074074029922485)
[2025-01-06 01:32:45,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45,309][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.07193896919488907, acc: 0.9736841917037964)
[2025-01-06 01:32:45,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45,641][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.037578992545604706, acc: 1.0)
[2025-01-06 01:32:45,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45,980][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.11379307508468628, acc: 0.949999988079071)
[2025-01-06 01:32:46,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:46,321][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.10909318923950195, acc: 0.9609375)
[2025-01-06 01:32:46,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:46,704][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.16687768697738647, acc: 0.9520000219345093)
[2025-01-06 01:32:46,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47,060][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.1068543791770935, acc: 0.9560439586639404)
[2025-01-06 01:32:47,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47,454][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.05441426485776901, acc: 0.9813664555549622)
[2025-01-06 01:32:47,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47,820][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.23541338741779327, acc: 0.9329897165298462)
[2025-01-06 01:32:47,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48,129][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.004299132153391838, acc: 1.0)
[2025-01-06 01:32:48,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48,474][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.05241621285676956, acc: 0.976190447807312)
[2025-01-06 01:32:48,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48,903][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.045889295637607574, acc: 0.982758641242981)
[2025-01-06 01:32:49,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49,401][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.0370635911822319, acc: 0.9818181991577148)
[2025-01-06 01:32:49,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49,947][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.18212302029132843, acc: 0.9484536051750183)
[2025-01-06 01:32:50,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50,322][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.16754846274852753, acc: 0.9482758641242981)
[2025-01-06 01:32:50,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50,739][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.009146523661911488, acc: 1.0)
[2025-01-06 01:32:50,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51,123][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.08572299778461456, acc: 0.9736841917037964)
[2025-01-06 01:32:51,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51,483][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.16222499310970306, acc: 0.9821428656578064)
[2025-01-06 01:32:51,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51,824][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.005702645052224398, acc: 1.0)
[2025-01-06 01:32:51,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52,195][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.07783173769712448, acc: 0.9811320900917053)
[2025-01-06 01:32:52,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52,532][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.011260507628321648, acc: 1.0)
[2025-01-06 01:32:52,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52,901][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.0067502595484256744, acc: 1.0)
[2025-01-06 01:32:53,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53,242][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.01666737161576748, acc: 1.0)
[2025-01-06 01:32:53,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53,568][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.04518572613596916, acc: 0.9672130942344666)
[2025-01-06 01:32:53,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53,891][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.00464399391785264, acc: 1.0)
[2025-01-06 01:32:54,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54,249][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.0007529830327257514, acc: 1.0)
[2025-01-06 01:32:54,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54,623][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.024814048781991005, acc: 1.0)
[2025-01-06 01:32:54,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55,075][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.044500455260276794, acc: 0.9583333134651184)
[2025-01-06 01:32:55,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55,399][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.016330182552337646, acc: 1.0)
[2025-01-06 01:32:55,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55,715][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.08074148744344711, acc: 0.9615384340286255)
[2025-01-06 01:32:55,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56,097][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.03496692702174187, acc: 0.9795918464660645)
[2025-01-06 01:32:56,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56,446][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.008213474415242672, acc: 1.0)
[2025-01-06 01:32:56,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56,825][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.0016600271919742227, acc: 1.0)
[2025-01-06 01:32:56,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57,173][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.006424207240343094, acc: 1.0)
[2025-01-06 01:32:57,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57,515][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.029801633208990097, acc: 1.0)
[2025-01-06 01:32:57,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57,890][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.007052839267998934, acc: 1.0)
[2025-01-06 01:32:58,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58,280][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.017113305628299713, acc: 0.9903846383094788)
[2025-01-06 01:32:58,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58,616][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.03795073181390762, acc: 0.9777777791023254)
[2025-01-06 01:32:58,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58,965][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.0034215592313557863, acc: 1.0)
[2025-01-06 01:32:59,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59,342][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.0016244164435192943, acc: 1.0)
[2025-01-06 01:32:59,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59,664][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.10584460943937302, acc: 0.9629629850387573)
[2025-01-06 01:32:59,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00,029][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.09594405442476273, acc: 0.9714285731315613)
[2025-01-06 01:33:00,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00,364][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.12302635610103607, acc: 0.9487179517745972)
[2025-01-06 01:33:00,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00,712][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.21803715825080872, acc: 0.9268292784690857)
[2025-01-06 01:33:00,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01,076][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.049975864589214325, acc: 1.0)
[2025-01-06 01:33:01,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01,443][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.16882078349590302, acc: 0.9473684430122375)
[2025-01-06 01:33:01,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01,808][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.019254982471466064, acc: 1.0)
[2025-01-06 01:33:01,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02,203][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.007463687099516392, acc: 1.0)
[2025-01-06 01:33:02,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02,578][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.0011243380140513182, acc: 1.0)
[2025-01-06 01:33:02,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02,950][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.032213110476732254, acc: 0.9838709831237793)
[2025-01-06 01:33:03,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03,324][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.024699347093701363, acc: 0.9824561476707458)
[2025-01-06 01:33:03,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03,652][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.0793459564447403, acc: 0.96875)
[2025-01-06 01:33:03,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04,021][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.004616448190063238, acc: 1.0)
[2025-01-06 01:33:04,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04,388][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.005080194678157568, acc: 1.0)
[2025-01-06 01:33:04,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04,739][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.15831510722637177, acc: 0.9399999976158142)
[2025-01-06 01:33:04,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05,122][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.08464670926332474, acc: 0.977011501789093)
[2025-01-06 01:33:05,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05,494][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.40119507908821106, acc: 0.8617021441459656)
[2025-01-06 01:33:05,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05,852][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.10754992067813873, acc: 0.9879518151283264)
[2025-01-06 01:33:05,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06,213][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.011337706819176674, acc: 1.0)
[2025-01-06 01:33:06,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06,538][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.10827485471963882, acc: 0.9487179517745972)
[2025-01-06 01:33:06,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06,915][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.2397916167974472, acc: 0.9638554453849792)
[2025-01-06 01:33:07,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07,269][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.05721766874194145, acc: 0.9811320900917053)
[2025-01-06 01:33:07,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07,595][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.00411476194858551, acc: 1.0)
[2025-01-06 01:33:07,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07,955][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.02766335755586624, acc: 0.9803921580314636)
[2025-01-06 01:33:08,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08,294][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.014090297743678093, acc: 1.0)
[2025-01-06 01:33:08,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08,607][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.0032794601283967495, acc: 1.0)
[2025-01-06 01:33:08,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08,844][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.008913015015423298, acc: 1.0)
[2025-01-06 01:33:08,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09,221][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.3275739848613739, acc: 0.8888888955116272)
[2025-01-06 01:33:09,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09,594][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.321812242269516, acc: 0.9069767594337463)
[2025-01-06 01:33:09,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09,979][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.07366607338190079, acc: 0.9487179517745972)
[2025-01-06 01:33:10,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10,314][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.03825896233320236, acc: 1.0)
[2025-01-06 01:33:10,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10,589][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.016304904595017433, acc: 1.0)
[2025-01-06 01:33:10,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10,917][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.011945128440856934, acc: 1.0)
[2025-01-06 01:33:11,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11,322][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.18585726618766785, acc: 0.9560439586639404)
[2025-01-06 01:33:11,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11,822][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.14753605425357819, acc: 0.947826087474823)
[2025-01-06 01:33:11,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12,214][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.08494336158037186, acc: 0.967391312122345)
[2025-01-06 01:33:12,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12,601][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.033474043011665344, acc: 1.0)
[2025-01-06 01:33:12,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12,927][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.0014425860717892647, acc: 1.0)
[2025-01-06 01:33:13,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13,285][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.0031132192816585302, acc: 1.0)
[2025-01-06 01:33:13,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13,612][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.0136609161272645, acc: 1.0)
[2025-01-06 01:33:13,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13,991][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.007079548202455044, acc: 1.0)
[2025-01-06 01:33:14,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14,369][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.015617243945598602, acc: 1.0)
[2025-01-06 01:33:14,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14,708][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.01963130570948124, acc: 1.0)
[2025-01-06 01:33:14,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15,035][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.017830319702625275, acc: 1.0)
[2025-01-06 01:33:15,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15,333][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.00047169686877168715, acc: 1.0)
[2025-01-06 01:33:15,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15,632][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.036772605031728745, acc: 0.95652174949646)
[2025-01-06 01:33:15,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15,905][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.0074937790632247925, acc: 1.0)
[2025-01-06 01:33:15,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16,231][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.5571556091308594, acc: 0.9375)
[2025-01-06 01:33:16,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16,850][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.1721702218055725, acc: 0.9575757384300232)
[2025-01-06 01:33:17,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17,691][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.09283622354269028, acc: 0.9811320900917053)
[2025-01-06 01:33:17,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17,998][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.021678809076547623, acc: 1.0)
[2025-01-06 01:33:18,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18,371][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.05487345904111862, acc: 0.9821428656578064)
[2025-01-06 01:33:18,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18,751][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.005277516786009073, acc: 1.0)
[2025-01-06 01:33:18,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19,144][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 5.768975097453222e-05, acc: 1.0)
[2025-01-06 01:33:19,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19,519][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.0018339331727474928, acc: 1.0)
[2025-01-06 01:33:19,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19,852][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.03645722195506096, acc: 0.9791666865348816)
[2025-01-06 01:33:19,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20,249][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.04974592477083206, acc: 0.9789473414421082)
[2025-01-06 01:33:20,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20,847][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.06765305995941162, acc: 0.9820359349250793)
[2025-01-06 01:33:20,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21,265][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.1481207311153412, acc: 0.9624060392379761)
[2025-01-06 01:33:21,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22,485][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.23079340159893036, acc: 0.9197860956192017)
[2025-01-06 01:33:22,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23,045][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.024394946172833443, acc: 0.9909909963607788)
[2025-01-06 01:33:23,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23,341][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.015704449266195297, acc: 1.0)
[2025-01-06 01:33:23,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23,722][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.003938536159694195, acc: 1.0)
[2025-01-06 01:33:23,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24,087][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.07293940335512161, acc: 0.96875)
[2025-01-06 01:33:24,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24,443][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.00027825275901705027, acc: 1.0)
[2025-01-06 01:33:24,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24,771][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.0003093126288149506, acc: 1.0)
[2025-01-06 01:33:24,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25,080][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.00030087196500971913, acc: 1.0)
[2025-01-06 01:33:25,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25,443][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.0003158801409881562, acc: 1.0)
[2025-01-06 01:33:25,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25,777][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.038304537534713745, acc: 1.0)
[2025-01-06 01:33:25,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26,121][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.0765577182173729, acc: 0.9629629850387573)
[2025-01-06 01:33:26,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26,445][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.26807278394699097, acc: 0.8737863898277283)
[2025-01-06 01:33:26,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26,963][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.31856846809387207, acc: 0.9264705777168274)
[2025-01-06 01:33:27,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27,321][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.19633714854717255, acc: 0.9200000166893005)
[2025-01-06 01:33:27,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27,694][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.15157100558280945, acc: 0.9513888955116272)
[2025-01-06 01:33:27,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28,066][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.05287773162126541, acc: 0.9767441749572754)
[2025-01-06 01:33:28,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28,398][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.019059354439377785, acc: 1.0)
[2025-01-06 01:33:28,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28,731][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.03688112273812294, acc: 0.9767441749572754)
[2025-01-06 01:33:28,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29,122][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.0520535372197628, acc: 0.9599999785423279)
[2025-01-06 01:33:29,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29,657][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.07973217964172363, acc: 0.970588207244873)
[2025-01-06 01:33:29,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29,971][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.0974743664264679, acc: 0.9466666579246521)
[2025-01-06 01:33:30,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30,333][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.06665624678134918, acc: 0.9696969985961914)
[2025-01-06 01:33:30,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30,632][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.2049250453710556, acc: 0.9696969985961914)
[2025-01-06 01:33:30,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30,928][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.013647007755935192, acc: 1.0)
[2025-01-06 01:33:31,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31,276][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.06309269368648529, acc: 0.9629629850387573)
[2025-01-06 01:33:31,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31,616][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.0005689347162842751, acc: 1.0)
[2025-01-06 01:33:31,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31,943][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.0024057577829807997, acc: 1.0)
[2025-01-06 01:33:32,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32,287][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.28494465351104736, acc: 0.9629629850387573)
[2025-01-06 01:33:32,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32,671][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.006250203587114811, acc: 1.0)
[2025-01-06 01:33:32,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33,057][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.00683211162686348, acc: 1.0)
[2025-01-06 01:33:33,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33,392][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.012379731051623821, acc: 1.0)
[2025-01-06 01:33:33,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33,767][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.0051358952187001705, acc: 1.0)
[2025-01-06 01:33:33,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34,104][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.010620251297950745, acc: 1.0)
[2025-01-06 01:33:34,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34,469][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.002715068869292736, acc: 1.0)
[2025-01-06 01:33:34,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34,777][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.1901063770055771, acc: 0.9607843160629272)
[2025-01-06 01:33:34,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35,068][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.06578825414180756, acc: 0.9615384340286255)
[2025-01-06 01:33:35,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35,439][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.33157864212989807, acc: 0.8888888955116272)
[2025-01-06 01:33:35,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35,797][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.10794515907764435, acc: 0.949999988079071)
[2025-01-06 01:33:36,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:02,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:02,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:02,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06,370][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3787, device='cuda:0') eval_epoch_loss=tensor(0.8666, device='cuda:0') eval_epoch_acc=tensor(0.8354, device='cuda:0')
[2025-01-06 01:34:06,371][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:34:06,371][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:34:06,652][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_419_loss_0.8665545582771301/model.pt
[2025-01-06 01:34:06,656][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:34:06,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07,070][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.0403631255030632, acc: 0.949999988079071)
[2025-01-06 01:34:07,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07,423][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.0008651833049952984, acc: 1.0)
[2025-01-06 01:34:07,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07,782][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.06004168093204498, acc: 0.9666666388511658)
[2025-01-06 01:34:07,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08,152][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.03393124043941498, acc: 1.0)
[2025-01-06 01:34:08,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08,549][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.0268156286329031, acc: 1.0)
[2025-01-06 01:34:08,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08,926][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.5041995048522949, acc: 0.8888888955116272)
[2025-01-06 01:34:09,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09,252][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.0038165340665727854, acc: 1.0)
[2025-01-06 01:34:09,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09,555][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.002311698393896222, acc: 1.0)
[2025-01-06 01:34:09,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09,929][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.019584326073527336, acc: 1.0)
[2025-01-06 01:34:10,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10,296][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.07819552719593048, acc: 0.9629629850387573)
[2025-01-06 01:34:10,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10,658][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.06525228917598724, acc: 0.95652174949646)
[2025-01-06 01:34:10,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10,990][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.002940624486654997, acc: 1.0)
[2025-01-06 01:34:11,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11,341][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.010417391546070576, acc: 1.0)
[2025-01-06 01:34:11,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11,645][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.0014634733088314533, acc: 1.0)
[2025-01-06 01:34:11,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11,939][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.2998598515987396, acc: 0.9722222089767456)
[2025-01-06 01:34:12,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12,199][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.0012305256677791476, acc: 1.0)
[2025-01-06 01:34:12,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12,484][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.31058669090270996, acc: 0.9696969985961914)
[2025-01-06 01:34:12,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12,789][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.0741790384054184, acc: 0.9444444179534912)
[2025-01-06 01:34:12,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13,153][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.004655696451663971, acc: 1.0)
[2025-01-06 01:34:13,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13,537][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.00584357650950551, acc: 1.0)
[2025-01-06 01:34:13,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13,922][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.09041236340999603, acc: 0.9487179517745972)
[2025-01-06 01:34:14,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14,428][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.06544823944568634, acc: 0.9696969985961914)
[2025-01-06 01:34:14,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15,105][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.3161803185939789, acc: 0.8960000276565552)
[2025-01-06 01:34:15,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15,505][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.21029715240001678, acc: 0.9516128897666931)
[2025-01-06 01:34:15,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16,156][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.3050900995731354, acc: 0.9054726362228394)
[2025-01-06 01:34:16,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16,520][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.01969217136502266, acc: 1.0)
[2025-01-06 01:34:16,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16,932][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.02409030869603157, acc: 1.0)
[2025-01-06 01:34:17,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17,264][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.0667891576886177, acc: 1.0)
[2025-01-06 01:34:17,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17,621][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.011525972746312618, acc: 1.0)
[2025-01-06 01:34:17,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17,987][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.005001655779778957, acc: 1.0)
[2025-01-06 01:34:18,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18,387][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.04772862792015076, acc: 0.9850746393203735)
[2025-01-06 01:34:18,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18,768][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.008020654320716858, acc: 1.0)
[2025-01-06 01:34:18,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19,111][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.03955338895320892, acc: 0.989130437374115)
[2025-01-06 01:34:19,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19,452][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.05055515095591545, acc: 0.9871794581413269)
[2025-01-06 01:34:19,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19,808][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.10913335531949997, acc: 0.9605262875556946)
[2025-01-06 01:34:19,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20,126][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.07201651483774185, acc: 0.9795918464660645)
[2025-01-06 01:34:20,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20,485][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.01937311328947544, acc: 1.0)
[2025-01-06 01:34:20,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20,855][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.05822399631142616, acc: 0.9896907210350037)
[2025-01-06 01:34:20,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21,161][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.004780067130923271, acc: 1.0)
[2025-01-06 01:34:21,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21,563][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.09583868831396103, acc: 0.9709302186965942)
[2025-01-06 01:34:21,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21,940][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.028777381405234337, acc: 0.9821428656578064)
[2025-01-06 01:34:22,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:22,323][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.06011819839477539, acc: 0.9876543283462524)
[2025-01-06 01:34:22,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:22,647][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.18969778716564178, acc: 0.9444444179534912)
[2025-01-06 01:34:22,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23,004][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.0016833062982186675, acc: 1.0)
[2025-01-06 01:34:23,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23,322][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.01620902493596077, acc: 1.0)
[2025-01-06 01:34:23,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23,669][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.1164911761879921, acc: 0.95652174949646)
[2025-01-06 01:34:23,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24,048][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.048767607659101486, acc: 0.988095223903656)
[2025-01-06 01:34:24,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24,396][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.17328746616840363, acc: 0.9397590160369873)
[2025-01-06 01:34:24,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24,802][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.022092659026384354, acc: 1.0)
[2025-01-06 01:34:24,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25,130][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.1390576809644699, acc: 0.9611650705337524)
[2025-01-06 01:34:25,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25,464][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.05604574456810951, acc: 0.9918699264526367)
[2025-01-06 01:34:25,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25,807][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.002740591997280717, acc: 1.0)
[2025-01-06 01:34:25,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26,178][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.033907923847436905, acc: 1.0)
[2025-01-06 01:34:26,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26,579][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.19647091627120972, acc: 0.9215686321258545)
[2025-01-06 01:34:26,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26,949][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.2770191431045532, acc: 0.9213973879814148)
[2025-01-06 01:34:27,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27,335][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.06410545110702515, acc: 0.9895833134651184)
[2025-01-06 01:34:27,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27,676][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.12280554324388504, acc: 0.9570552110671997)
[2025-01-06 01:34:27,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28,030][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.09822164475917816, acc: 0.9784172773361206)
[2025-01-06 01:34:28,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28,429][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.22111378610134125, acc: 0.929648220539093)
[2025-01-06 01:34:28,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28,791][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.03624119982123375, acc: 1.0)
[2025-01-06 01:34:28,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29,134][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.10281414538621902, acc: 0.9696969985961914)
[2025-01-06 01:34:29,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29,447][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.07865174859762192, acc: 0.9629629850387573)
[2025-01-06 01:34:29,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29,739][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.04255694895982742, acc: 1.0)
[2025-01-06 01:34:29,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30,111][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.01955919899046421, acc: 1.0)
[2025-01-06 01:34:30,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30,498][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.20777660608291626, acc: 0.9482758641242981)
[2025-01-06 01:34:30,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30,813][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.009448567405343056, acc: 1.0)
[2025-01-06 01:34:30,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31,127][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.048354994505643845, acc: 1.0)
[2025-01-06 01:34:31,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31,500][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.052473898977041245, acc: 1.0)
[2025-01-06 01:34:31,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31,885][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.0580231174826622, acc: 1.0)
[2025-01-06 01:34:31,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32,232][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.005302578676491976, acc: 1.0)
[2025-01-06 01:34:32,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32,575][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.09929175674915314, acc: 0.9384615421295166)
[2025-01-06 01:34:32,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32,925][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.007131831720471382, acc: 1.0)
[2025-01-06 01:34:33,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33,260][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.11710484325885773, acc: 0.9655172228813171)
[2025-01-06 01:34:33,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33,575][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.07066133618354797, acc: 0.9607843160629272)
[2025-01-06 01:34:33,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33,934][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.21575890481472015, acc: 0.931034505367279)
[2025-01-06 01:34:34,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34,305][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.010928419418632984, acc: 1.0)
[2025-01-06 01:34:34,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34,649][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.018533017486333847, acc: 1.0)
[2025-01-06 01:34:34,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34,985][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.1753067672252655, acc: 0.9553571343421936)
[2025-01-06 01:34:35,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35,389][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.07102975249290466, acc: 0.9887640476226807)
[2025-01-06 01:34:35,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35,747][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.14242906868457794, acc: 0.932584285736084)
[2025-01-06 01:34:35,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36,113][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.30605366826057434, acc: 0.9078013896942139)
[2025-01-06 01:34:36,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36,445][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.10286932438611984, acc: 0.95652174949646)
[2025-01-06 01:34:36,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36,795][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.3207860589027405, acc: 0.9200000166893005)
[2025-01-06 01:34:36,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37,169][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.0019147115526720881, acc: 1.0)
[2025-01-06 01:34:37,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37,517][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.005275324452668428, acc: 1.0)
[2025-01-06 01:34:37,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37,906][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.019997937604784966, acc: 1.0)
[2025-01-06 01:34:38,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38,264][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.14181473851203918, acc: 0.9433962106704712)
[2025-01-06 01:34:38,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38,568][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.1292939931154251, acc: 0.9655172228813171)
[2025-01-06 01:34:38,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39,148][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 0.26838618516921997, acc: 0.8918918967247009)
[2025-01-06 01:34:39,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39,594][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.18714389204978943, acc: 0.9577465057373047)
[2025-01-06 01:34:39,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39,948][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.000955109135247767, acc: 1.0)
[2025-01-06 01:34:40,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40,293][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.0064267488196492195, acc: 1.0)
[2025-01-06 01:34:40,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40,613][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.010189591906964779, acc: 1.0)
[2025-01-06 01:34:42,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43,295][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.5551205277442932, acc: 0.8571428656578064)
[2025-01-06 01:34:43,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44,057][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.08642581105232239, acc: 0.9682539701461792)
[2025-01-06 01:34:44,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44,353][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.20126888155937195, acc: 0.8928571343421936)
[2025-01-06 01:34:44,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44,697][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.022330408915877342, acc: 0.9833333492279053)
[2025-01-06 01:34:44,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45,387][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.0345921590924263, acc: 0.9722222089767456)
[2025-01-06 01:34:45,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45,757][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.0004664086445700377, acc: 1.0)
[2025-01-06 01:34:45,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46,105][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.030905498191714287, acc: 1.0)
[2025-01-06 01:34:46,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46,468][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.05054134130477905, acc: 1.0)
[2025-01-06 01:34:46,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46,850][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.5095783472061157, acc: 0.9259259104728699)
[2025-01-06 01:34:47,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47,873][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.3542916774749756, acc: 0.8813559412956238)
[2025-01-06 01:34:47,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48,218][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.06299909949302673, acc: 0.9850746393203735)
[2025-01-06 01:34:48,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48,615][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.17304140329360962, acc: 0.956204354763031)
[2025-01-06 01:34:48,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49,186][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.4118534028530121, acc: 0.875)
[2025-01-06 01:34:49,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49,556][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.02921699546277523, acc: 0.9814814925193787)
[2025-01-06 01:34:49,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49,863][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.016085751354694366, acc: 1.0)
[2025-01-06 01:34:49,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50,218][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.0896524041891098, acc: 0.9523809552192688)
[2025-01-06 01:34:50,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50,612][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.1842847317457199, acc: 0.9180327653884888)
[2025-01-06 01:34:50,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50,975][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.035196974873542786, acc: 1.0)
[2025-01-06 01:34:51,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51,301][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.13062314689159393, acc: 0.930232584476471)
[2025-01-06 01:34:51,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51,627][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.19665144383907318, acc: 0.9545454382896423)
[2025-01-06 01:34:51,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51,971][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.07910272479057312, acc: 0.9811320900917053)
[2025-01-06 01:34:52,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52,347][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.030659638345241547, acc: 1.0)
[2025-01-06 01:34:52,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52,676][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.023824870586395264, acc: 1.0)
[2025-01-06 01:34:52,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52,996][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.003496424062177539, acc: 1.0)
[2025-01-06 01:34:53,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53,371][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.0031364017631858587, acc: 1.0)
[2025-01-06 01:34:53,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53,764][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.06455197930335999, acc: 0.9846153855323792)
[2025-01-06 01:34:53,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:54,118][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.0736374482512474, acc: 0.984375)
[2025-01-06 01:34:54,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:54,513][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.23044487833976746, acc: 0.9375)
[2025-01-06 01:34:54,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:54,846][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.028455840423703194, acc: 1.0)
[2025-01-06 01:34:54,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55,173][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.00232579349540174, acc: 1.0)
[2025-01-06 01:34:55,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55,486][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.04843852296471596, acc: 0.9677419066429138)
[2025-01-06 01:34:55,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55,844][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.008078722283244133, acc: 1.0)
[2025-01-06 01:34:55,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56,209][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.020670881494879723, acc: 1.0)
[2025-01-06 01:34:56,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56,553][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.03377458080649376, acc: 0.9756097793579102)
[2025-01-06 01:34:56,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56,882][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.0012175824958831072, acc: 1.0)
[2025-01-06 01:34:56,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57,235][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.0034773023799061775, acc: 1.0)
[2025-01-06 01:34:57,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57,613][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.30864179134368896, acc: 0.9354838728904724)
[2025-01-06 01:34:57,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57,983][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.0006257833447307348, acc: 1.0)
[2025-01-06 01:34:58,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58,369][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.03985165059566498, acc: 1.0)
[2025-01-06 01:34:58,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58,704][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.0026926775462925434, acc: 1.0)
[2025-01-06 01:34:58,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59,046][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.07615789771080017, acc: 0.9714285731315613)
[2025-01-06 01:34:59,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59,391][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.05246060714125633, acc: 0.9927007555961609)
[2025-01-06 01:34:59,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59,761][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.03351413831114769, acc: 0.9862068891525269)
[2025-01-06 01:34:59,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00,107][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.09798713773488998, acc: 0.9785714149475098)
[2025-01-06 01:35:00,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00,481][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.1596389263868332, acc: 0.9470198750495911)
[2025-01-06 01:35:00,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00,836][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.019674403592944145, acc: 0.9914529919624329)
[2025-01-06 01:35:00,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01,202][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.06741105765104294, acc: 0.9599999785423279)
[2025-01-06 01:35:01,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01,494][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.006015316117554903, acc: 1.0)
[2025-01-06 01:35:01,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01,770][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.013220962136983871, acc: 1.0)
[2025-01-06 01:35:01,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02,069][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.28635546565055847, acc: 0.9743589758872986)
[2025-01-06 01:35:02,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:15,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:15,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:15,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:16,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:16,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:16,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:33,081][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1280, device='cuda:0') eval_epoch_loss=tensor(0.7552, device='cuda:0') eval_epoch_acc=tensor(0.8425, device='cuda:0')
[2025-01-06 01:35:33,083][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:35:33,083][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:35:33,323][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_562_loss_0.7552012205123901/model.pt
[2025-01-06 01:35:33,327][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:35:33,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:33,725][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.08381180465221405, acc: 0.9666666388511658)
[2025-01-06 01:35:33,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34,097][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.037585366517305374, acc: 0.9740259647369385)
[2025-01-06 01:35:34,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34,485][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.0033771756570786238, acc: 1.0)
[2025-01-06 01:35:34,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34,851][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.026690417900681496, acc: 1.0)
[2025-01-06 01:35:34,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35,194][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.039883796125650406, acc: 0.976190447807312)
[2025-01-06 01:35:35,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35,548][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.006542564835399389, acc: 1.0)
[2025-01-06 01:35:35,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35,859][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.008308377116918564, acc: 1.0)
[2025-01-06 01:35:35,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36,271][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.09307005256414413, acc: 0.9839572310447693)
[2025-01-06 01:35:36,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36,617][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.003872403409332037, acc: 1.0)
[2025-01-06 01:35:36,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36,931][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.04260778799653053, acc: 0.9829059839248657)
[2025-01-06 01:35:37,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37,234][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.09259326756000519, acc: 0.9642857313156128)
[2025-01-06 01:35:37,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37,580][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.11526312679052353, acc: 0.9622641801834106)
[2025-01-06 01:35:37,939][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.1228, train_epoch_loss=0.1158, epoch time 355.4939832240343s
[2025-01-06 01:35:37,939][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:35:37,940][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:35:37,940][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:35:37,940][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 17
[2025-01-06 01:35:37,940][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:35:38,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:38,773][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.019515499472618103, acc: 1.0)
[2025-01-06 01:35:38,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39,119][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.04647856578230858, acc: 1.0)
[2025-01-06 01:35:39,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39,467][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.21153946220874786, acc: 0.9459459185600281)
[2025-01-06 01:35:39,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39,790][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.045258570462465286, acc: 1.0)
[2025-01-06 01:35:39,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40,119][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.028284665197134018, acc: 1.0)
[2025-01-06 01:35:40,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40,468][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.010175665840506554, acc: 1.0)
[2025-01-06 01:35:40,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40,825][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.035106111317873, acc: 1.0)
[2025-01-06 01:35:40,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41,232][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.30322885513305664, acc: 0.9333333373069763)
[2025-01-06 01:35:41,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41,598][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.0032159248366951942, acc: 1.0)
[2025-01-06 01:35:41,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41,929][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.0017803417285904288, acc: 1.0)
[2025-01-06 01:35:42,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42,314][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.012927062809467316, acc: 1.0)
[2025-01-06 01:35:42,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42,693][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.05215383321046829, acc: 0.9743589758872986)
[2025-01-06 01:35:42,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43,083][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.0399191789329052, acc: 0.9696969985961914)
[2025-01-06 01:35:43,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43,479][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.024987690150737762, acc: 1.0)
[2025-01-06 01:35:43,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43,834][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.06991589069366455, acc: 0.9803921580314636)
[2025-01-06 01:35:43,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44,207][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.02045130915939808, acc: 1.0)
[2025-01-06 01:35:44,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44,565][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.0073695434257388115, acc: 1.0)
[2025-01-06 01:35:44,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44,886][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.011028748005628586, acc: 1.0)
[2025-01-06 01:35:44,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45,207][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.04542434215545654, acc: 1.0)
[2025-01-06 01:35:45,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45,525][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.008897869847714901, acc: 1.0)
[2025-01-06 01:35:45,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45,892][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.06100032478570938, acc: 0.9615384340286255)
[2025-01-06 01:35:45,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46,245][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.005601371638476849, acc: 1.0)
[2025-01-06 01:35:46,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46,594][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.17027989029884338, acc: 0.9599999785423279)
[2025-01-06 01:35:46,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46,915][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.010779076255857944, acc: 1.0)
[2025-01-06 01:35:46,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47,278][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.003698104526847601, acc: 1.0)
[2025-01-06 01:35:47,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47,620][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.048016294836997986, acc: 0.9811320900917053)
[2025-01-06 01:35:47,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47,973][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.18685896694660187, acc: 0.931506872177124)
[2025-01-06 01:35:48,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49,239][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.26866891980171204, acc: 0.9011857509613037)
[2025-01-06 01:35:49,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49,606][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.06531868129968643, acc: 0.9767441749572754)
[2025-01-06 01:35:49,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49,973][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.05077093839645386, acc: 0.9759036302566528)
[2025-01-06 01:35:50,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50,316][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.1600164771080017, acc: 0.9753086566925049)
[2025-01-06 01:35:50,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50,659][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.034518592059612274, acc: 1.0)
[2025-01-06 01:35:50,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51,057][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.1485964059829712, acc: 0.9629629850387573)
[2025-01-06 01:35:51,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51,451][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.0010247942991554737, acc: 1.0)
[2025-01-06 01:35:51,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51,828][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.08794452995061874, acc: 0.9663865566253662)
[2025-01-06 01:35:51,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52,228][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.045450810343027115, acc: 1.0)
[2025-01-06 01:35:52,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52,567][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.025321882218122482, acc: 1.0)
[2025-01-06 01:35:52,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52,919][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.038049716502428055, acc: 0.9830508232116699)
[2025-01-06 01:35:53,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53,317][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.031463395804166794, acc: 0.9885057210922241)
[2025-01-06 01:35:53,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53,699][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.0031307849567383528, acc: 1.0)
[2025-01-06 01:35:53,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54,036][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.003833635011687875, acc: 1.0)
[2025-01-06 01:35:54,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54,416][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.29298773407936096, acc: 0.9594594836235046)
[2025-01-06 01:35:54,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54,797][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.15815187990665436, acc: 0.9538461565971375)
[2025-01-06 01:35:54,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55,246][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.1264532506465912, acc: 0.9595959782600403)
[2025-01-06 01:35:55,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55,659][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.06448759138584137, acc: 0.9793814420700073)
[2025-01-06 01:35:55,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56,051][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.07313098013401031, acc: 0.9779411554336548)
[2025-01-06 01:35:56,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56,419][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.17896977066993713, acc: 0.9615384340286255)
[2025-01-06 01:35:56,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56,761][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.0008371869917027652, acc: 1.0)
[2025-01-06 01:35:56,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57,102][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.12013112753629684, acc: 0.9642857313156128)
[2025-01-06 01:35:57,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57,464][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.0028592152521014214, acc: 1.0)
[2025-01-06 01:35:57,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57,856][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.042181357741355896, acc: 0.9824561476707458)
[2025-01-06 01:35:57,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58,248][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.08721275627613068, acc: 0.9841269850730896)
[2025-01-06 01:35:58,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58,644][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.28296929597854614, acc: 0.9154929518699646)
[2025-01-06 01:35:58,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59,109][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 0.5314255952835083, acc: 0.846666693687439)
[2025-01-06 01:35:59,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59,389][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.11573171615600586, acc: 0.9459459185600281)
[2025-01-06 01:35:59,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59,766][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.003823786973953247, acc: 1.0)
[2025-01-06 01:36:01,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02,770][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.48197436332702637, acc: 0.8464163541793823)
[2025-01-06 01:36:03,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03,985][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 0.8407737016677856, acc: 0.7625272274017334)
[2025-01-06 01:36:04,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04,607][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.284329891204834, acc: 0.9034090638160706)
[2025-01-06 01:36:04,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05,180][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.10783999413251877, acc: 0.9338235259056091)
[2025-01-06 01:36:05,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05,745][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.31568643450737, acc: 0.9057971239089966)
[2025-01-06 01:36:05,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06,165][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.11168378591537476, acc: 0.9624999761581421)
[2025-01-06 01:36:06,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06,516][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.030324630439281464, acc: 0.970588207244873)
[2025-01-06 01:36:06,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06,892][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.04671858251094818, acc: 1.0)
[2025-01-06 01:36:07,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07,325][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.012237422168254852, acc: 1.0)
[2025-01-06 01:36:07,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07,675][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.015476793050765991, acc: 1.0)
[2025-01-06 01:36:07,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08,045][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.08010502904653549, acc: 0.9821428656578064)
[2025-01-06 01:36:08,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08,439][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.05469227954745293, acc: 0.9833333492279053)
[2025-01-06 01:36:08,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08,827][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.005668175406754017, acc: 1.0)
[2025-01-06 01:36:08,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09,236][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.15863163769245148, acc: 0.9444444179534912)
[2025-01-06 01:36:09,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09,627][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.11931376904249191, acc: 0.9696969985961914)
[2025-01-06 01:36:09,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09,994][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.2926276624202728, acc: 0.8970588445663452)
[2025-01-06 01:36:10,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10,352][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.10711457580327988, acc: 0.9682539701461792)
[2025-01-06 01:36:10,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10,790][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 0.39264601469039917, acc: 0.8564102649688721)
[2025-01-06 01:36:10,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11,137][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.22265025973320007, acc: 0.9387755393981934)
[2025-01-06 01:36:11,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11,526][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.2618279457092285, acc: 0.9477611780166626)
[2025-01-06 01:36:11,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11,942][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 0.5701901316642761, acc: 0.8248175382614136)
[2025-01-06 01:36:12,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12,251][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.019943654537200928, acc: 1.0)
[2025-01-06 01:36:12,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12,603][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.007899765856564045, acc: 1.0)
[2025-01-06 01:36:12,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12,933][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.0038891241420060396, acc: 1.0)
[2025-01-06 01:36:13,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13,306][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.0801958367228508, acc: 0.9615384340286255)
[2025-01-06 01:36:13,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13,689][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.07830814272165298, acc: 0.9807692170143127)
[2025-01-06 01:36:13,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14,090][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.04552099481225014, acc: 1.0)
[2025-01-06 01:36:14,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14,482][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.04169739782810211, acc: 0.96875)
[2025-01-06 01:36:14,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14,867][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.044817935675382614, acc: 0.9855072498321533)
[2025-01-06 01:36:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:15,251][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.008650748059153557, acc: 1.0)
[2025-01-06 01:36:15,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:15,592][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.03442057594656944, acc: 1.0)
[2025-01-06 01:36:15,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16,051][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.19386161863803864, acc: 0.8999999761581421)
[2025-01-06 01:36:16,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16,388][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.18593253195285797, acc: 0.9320388436317444)
[2025-01-06 01:36:16,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17,508][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.3557494878768921, acc: 0.917475700378418)
[2025-01-06 01:36:17,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18,326][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.36923348903656006, acc: 0.9139785170555115)
[2025-01-06 01:36:18,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19,126][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.32213711738586426, acc: 0.9094827771186829)
[2025-01-06 01:36:19,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19,872][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.2323738932609558, acc: 0.9263157844543457)
[2025-01-06 01:36:20,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:20,865][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.32330283522605896, acc: 0.9108911156654358)
[2025-01-06 01:36:20,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21,238][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.3511098027229309, acc: 0.9032257795333862)
[2025-01-06 01:36:21,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21,642][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.1996980905532837, acc: 0.9275362491607666)
[2025-01-06 01:36:21,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21,988][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.20772044360637665, acc: 0.9327731132507324)
[2025-01-06 01:36:22,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22,379][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.16586217284202576, acc: 0.942307710647583)
[2025-01-06 01:36:22,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22,773][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.2147744745016098, acc: 0.9270073175430298)
[2025-01-06 01:36:22,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23,099][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.17267727851867676, acc: 0.9402984976768494)
[2025-01-06 01:36:23,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23,448][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.005457508377730846, acc: 1.0)
[2025-01-06 01:36:23,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23,787][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.003983341157436371, acc: 1.0)
[2025-01-06 01:36:23,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24,170][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.013032638467848301, acc: 1.0)
[2025-01-06 01:36:24,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24,547][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.004256175830960274, acc: 1.0)
[2025-01-06 01:36:24,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24,930][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.016622208058834076, acc: 1.0)
[2025-01-06 01:36:25,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:25,307][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.0228901207447052, acc: 1.0)
[2025-01-06 01:36:25,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:25,643][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.007345764432102442, acc: 1.0)
[2025-01-06 01:36:25,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:25,933][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.02421589009463787, acc: 1.0)
[2025-01-06 01:36:26,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26,309][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.0005762280197814107, acc: 1.0)
[2025-01-06 01:36:26,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26,674][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.004784358665347099, acc: 1.0)
[2025-01-06 01:36:26,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27,045][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.018768111243844032, acc: 0.9846153855323792)
[2025-01-06 01:36:27,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27,444][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.01053573563694954, acc: 1.0)
[2025-01-06 01:36:27,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27,827][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.1883438527584076, acc: 0.9649122953414917)
[2025-01-06 01:36:27,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28,225][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.04039511829614639, acc: 0.9743589758872986)
[2025-01-06 01:36:28,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28,602][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.01578638330101967, acc: 1.0)
[2025-01-06 01:36:28,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28,916][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.0002520004636608064, acc: 1.0)
[2025-01-06 01:36:29,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29,263][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.018119558691978455, acc: 1.0)
[2025-01-06 01:36:29,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29,618][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.1154458224773407, acc: 0.9593495726585388)
[2025-01-06 01:36:29,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30,002][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.011962966062128544, acc: 1.0)
[2025-01-06 01:36:30,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30,857][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.19788327813148499, acc: 0.9391635060310364)
[2025-01-06 01:36:30,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31,241][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.024258136749267578, acc: 0.9866666793823242)
[2025-01-06 01:36:31,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31,649][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.014914309605956078, acc: 1.0)
[2025-01-06 01:36:31,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31,968][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.011586061678826809, acc: 1.0)
[2025-01-06 01:36:32,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32,316][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.09579954296350479, acc: 0.9473684430122375)
[2025-01-06 01:36:32,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32,713][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.2590107023715973, acc: 0.9325153231620789)
[2025-01-06 01:36:32,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33,127][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.33705395460128784, acc: 0.9166666865348816)
[2025-01-06 01:36:33,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33,520][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.23220199346542358, acc: 0.9166666865348816)
[2025-01-06 01:36:33,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33,903][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.21145617961883545, acc: 0.9464285969734192)
[2025-01-06 01:36:34,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:34,279][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.24534496665000916, acc: 0.9487179517745972)
[2025-01-06 01:36:34,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:34,735][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.27293628454208374, acc: 0.9191176295280457)
[2025-01-06 01:36:34,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:35,114][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.015179576352238655, acc: 1.0)
[2025-01-06 01:36:35,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06,023][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1767, device='cuda:0') eval_epoch_loss=tensor(0.7778, device='cuda:0') eval_epoch_acc=tensor(0.8451, device='cuda:0')
[2025-01-06 01:37:06,024][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:37:06,025][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:37:06,315][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_131_loss_0.7778241634368896/model.pt
[2025-01-06 01:37:06,321][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:37:06,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06,645][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.07742335647344589, acc: 0.95652174949646)
[2025-01-06 01:37:06,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07,006][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.14617107808589935, acc: 0.96875)
[2025-01-06 01:37:07,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07,323][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.0530773289501667, acc: 1.0)
[2025-01-06 01:37:07,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07,695][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.030222268775105476, acc: 1.0)
[2025-01-06 01:37:07,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08,057][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.030847657471895218, acc: 1.0)
[2025-01-06 01:37:08,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08,386][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.00973086804151535, acc: 1.0)
[2025-01-06 01:37:08,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08,750][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.01786564476788044, acc: 1.0)
[2025-01-06 01:37:08,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09,088][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.11791455000638962, acc: 0.95652174949646)
[2025-01-06 01:37:09,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09,443][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.04110027849674225, acc: 0.9523809552192688)
[2025-01-06 01:37:09,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09,840][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.01826714351773262, acc: 1.0)
[2025-01-06 01:37:09,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10,221][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.16874730587005615, acc: 0.9677419066429138)
[2025-01-06 01:37:10,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10,586][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.2016930878162384, acc: 0.9729729890823364)
[2025-01-06 01:37:10,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11,128][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.11278266459703445, acc: 0.9736841917037964)
[2025-01-06 01:37:11,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11,528][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.15360550582408905, acc: 0.9402984976768494)
[2025-01-06 01:37:11,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11,891][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.08095937222242355, acc: 0.9591836929321289)
[2025-01-06 01:37:12,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12,337][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.10597966611385345, acc: 0.957446813583374)
[2025-01-06 01:37:12,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12,707][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.08626621216535568, acc: 0.9714285731315613)
[2025-01-06 01:37:12,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13,041][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.09369028359651566, acc: 0.9642857313156128)
[2025-01-06 01:37:13,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13,403][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.09072756767272949, acc: 0.95652174949646)
[2025-01-06 01:37:13,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13,761][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.005090627819299698, acc: 1.0)
[2025-01-06 01:37:13,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14,139][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.051774267107248306, acc: 0.97826087474823)
[2025-01-06 01:37:14,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14,539][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.3816600739955902, acc: 0.9152542352676392)
[2025-01-06 01:37:14,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14,871][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.041435226798057556, acc: 1.0)
[2025-01-06 01:37:14,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15,193][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.08646588027477264, acc: 0.9459459185600281)
[2025-01-06 01:37:15,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15,555][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.024678515270352364, acc: 1.0)
[2025-01-06 01:37:15,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15,870][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.05780632048845291, acc: 0.95652174949646)
[2025-01-06 01:37:15,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16,186][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.3181530237197876, acc: 0.9473684430122375)
[2025-01-06 01:37:16,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17,782][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.21444706618785858, acc: 0.9324324131011963)
[2025-01-06 01:37:17,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18,127][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.19078131020069122, acc: 0.9629629850387573)
[2025-01-06 01:37:18,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18,527][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.2542167007923126, acc: 0.9534883499145508)
[2025-01-06 01:37:18,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19,112][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.18901580572128296, acc: 0.9411764740943909)
[2025-01-06 01:37:19,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19,667][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.16152489185333252, acc: 0.9438202381134033)
[2025-01-06 01:37:19,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20,049][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.05294065177440643, acc: 0.9772727489471436)
[2025-01-06 01:37:20,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20,434][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.005611402913928032, acc: 1.0)
[2025-01-06 01:37:20,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20,756][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.035801660269498825, acc: 0.9655172228813171)
[2025-01-06 01:37:20,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21,126][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.043214213103055954, acc: 0.9795918464660645)
[2025-01-06 01:37:21,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21,492][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.026251569390296936, acc: 1.0)
[2025-01-06 01:37:21,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21,936][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.13923178613185883, acc: 0.9444444179534912)
[2025-01-06 01:37:22,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22,332][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.2610861659049988, acc: 0.9313725233078003)
[2025-01-06 01:37:22,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23,360][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.1428515911102295, acc: 0.9452054500579834)
[2025-01-06 01:37:23,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23,663][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.0026973795611411333, acc: 1.0)
[2025-01-06 01:37:23,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23,989][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.010039901360869408, acc: 1.0)
[2025-01-06 01:37:24,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24,334][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.005784688983112574, acc: 1.0)
[2025-01-06 01:37:24,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24,873][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.4223771095275879, acc: 0.8584070801734924)
[2025-01-06 01:37:24,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25,220][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.1650850474834442, acc: 0.9855072498321533)
[2025-01-06 01:37:25,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25,609][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.09358447045087814, acc: 0.9659090638160706)
[2025-01-06 01:37:25,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26,544][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.26675617694854736, acc: 0.9007633328437805)
[2025-01-06 01:37:26,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27,214][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.284026563167572, acc: 0.8962963223457336)
[2025-01-06 01:37:27,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27,554][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.02904292568564415, acc: 1.0)
[2025-01-06 01:37:27,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27,910][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.0029601778369396925, acc: 1.0)
[2025-01-06 01:37:28,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28,237][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.0026843203231692314, acc: 1.0)
[2025-01-06 01:37:28,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28,549][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.0008277992019429803, acc: 1.0)
[2025-01-06 01:37:28,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28,899][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.005078284069895744, acc: 1.0)
[2025-01-06 01:37:29,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29,273][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.15214022994041443, acc: 0.9546827673912048)
[2025-01-06 01:37:29,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29,661][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.19917899370193481, acc: 0.9365994334220886)
[2025-01-06 01:37:29,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30,158][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.1823299676179886, acc: 0.956250011920929)
[2025-01-06 01:37:30,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30,683][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.30774596333503723, acc: 0.9155722260475159)
[2025-01-06 01:37:30,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31,077][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.20227022469043732, acc: 0.9501779079437256)
[2025-01-06 01:37:31,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31,369][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.030864829197525978, acc: 1.0)
[2025-01-06 01:37:31,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31,916][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.15693140029907227, acc: 0.9651162624359131)
[2025-01-06 01:37:32,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32,710][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.43806275725364685, acc: 0.8650793433189392)
[2025-01-06 01:37:32,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:33,622][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.17778313159942627, acc: 0.9318181872367859)
[2025-01-06 01:37:33,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34,360][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.08494293689727783, acc: 0.9764705896377563)
[2025-01-06 01:37:34,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:35,435][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.2639086842536926, acc: 0.9197530746459961)
[2025-01-06 01:37:35,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36,386][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.02639143355190754, acc: 1.0)
[2025-01-06 01:37:36,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36,733][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.009360876865684986, acc: 1.0)
[2025-01-06 01:37:36,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37,081][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.05492456629872322, acc: 1.0)
[2025-01-06 01:37:37,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37,421][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.1248420998454094, acc: 0.970588207244873)
[2025-01-06 01:37:37,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37,805][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.13751114904880524, acc: 0.9411764740943909)
[2025-01-06 01:37:37,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38,208][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.06879187375307083, acc: 0.9745762944221497)
[2025-01-06 01:37:38,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38,544][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.14114916324615479, acc: 0.9701492786407471)
[2025-01-06 01:37:38,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38,862][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.0833720713853836, acc: 0.9805825352668762)
[2025-01-06 01:37:38,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39,220][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.030761128291487694, acc: 1.0)
[2025-01-06 01:37:39,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39,550][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.03001752868294716, acc: 0.9890109896659851)
[2025-01-06 01:37:39,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39,863][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.05016353353857994, acc: 0.9865471124649048)
[2025-01-06 01:37:39,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40,252][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.13473190367221832, acc: 0.9566929340362549)
[2025-01-06 01:37:40,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40,587][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.058386724442243576, acc: 0.9741379022598267)
[2025-01-06 01:37:40,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40,949][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.14840631186962128, acc: 0.97826087474823)
[2025-01-06 01:37:41,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41,332][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.08886583894491196, acc: 0.9727626442909241)
[2025-01-06 01:37:41,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41,669][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.011005682870745659, acc: 1.0)
[2025-01-06 01:37:41,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41,983][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.00320043065585196, acc: 1.0)
[2025-01-06 01:37:42,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42,314][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.0012535888236016035, acc: 1.0)
[2025-01-06 01:37:42,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42,649][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.06108666583895683, acc: 0.978723406791687)
[2025-01-06 01:37:42,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43,322][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.17009757459163666, acc: 0.9538461565971375)
[2025-01-06 01:37:43,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43,635][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.015086052939295769, acc: 1.0)
[2025-01-06 01:37:43,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43,996][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.2107488214969635, acc: 0.9651162624359131)
[2025-01-06 01:37:44,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44,529][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.031748220324516296, acc: 0.9909909963607788)
[2025-01-06 01:37:44,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44,928][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.05476197600364685, acc: 0.9777777791023254)
[2025-01-06 01:37:45,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45,251][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.010502456687390804, acc: 1.0)
[2025-01-06 01:37:45,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45,543][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.0006591911078430712, acc: 1.0)
[2025-01-06 01:37:45,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45,848][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.49174433946609497, acc: 0.9599999785423279)
[2025-01-06 01:37:45,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46,228][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.0636175349354744, acc: 0.9615384340286255)
[2025-01-06 01:37:46,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47,014][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.1269647628068924, acc: 0.9619565010070801)
[2025-01-06 01:37:47,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47,552][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.1997871845960617, acc: 0.9261363744735718)
[2025-01-06 01:37:47,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47,984][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.09205380827188492, acc: 0.978723406791687)
[2025-01-06 01:37:48,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48,352][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.1837429702281952, acc: 0.9622641801834106)
[2025-01-06 01:37:48,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48,746][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.08198990672826767, acc: 0.9666666388511658)
[2025-01-06 01:37:48,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49,092][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.0664806216955185, acc: 0.9534883499145508)
[2025-01-06 01:37:49,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49,481][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.04274637624621391, acc: 1.0)
[2025-01-06 01:37:49,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49,847][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 0.2636665999889374, acc: 0.8947368264198303)
[2025-01-06 01:37:49,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50,225][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.13618801534175873, acc: 0.9666666388511658)
[2025-01-06 01:37:50,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50,648][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 0.3892672657966614, acc: 0.8888888955116272)
[2025-01-06 01:37:50,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51,135][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 0.6560089588165283, acc: 0.8073394298553467)
[2025-01-06 01:37:51,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51,612][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.32222577929496765, acc: 0.8999999761581421)
[2025-01-06 01:37:51,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51,957][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.010298676788806915, acc: 1.0)
[2025-01-06 01:37:52,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52,343][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.009245045483112335, acc: 1.0)
[2025-01-06 01:37:52,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52,725][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.16573375463485718, acc: 0.9545454382896423)
[2025-01-06 01:37:52,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53,058][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.04256931319832802, acc: 1.0)
[2025-01-06 01:37:53,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53,397][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.19421543180942535, acc: 0.9428571462631226)
[2025-01-06 01:37:53,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53,782][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.033989761024713516, acc: 0.9772727489471436)
[2025-01-06 01:37:53,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54,118][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.03233114257454872, acc: 1.0)
[2025-01-06 01:37:54,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54,695][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.15497852861881256, acc: 0.9354838728904724)
[2025-01-06 01:37:54,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55,215][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.15622510015964508, acc: 0.9545454382896423)
[2025-01-06 01:37:55,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55,505][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.0001724840112728998, acc: 1.0)
[2025-01-06 01:37:55,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55,829][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.043016429990530014, acc: 0.9615384340286255)
[2025-01-06 01:37:55,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56,164][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.004387064836919308, acc: 1.0)
[2025-01-06 01:37:56,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56,460][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.002580296481028199, acc: 1.0)
[2025-01-06 01:37:56,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56,799][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.017680147662758827, acc: 1.0)
[2025-01-06 01:37:56,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57,111][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.06872845441102982, acc: 0.9729729890823364)
[2025-01-06 01:37:57,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57,440][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.07741031795740128, acc: 0.9729729890823364)
[2025-01-06 01:37:57,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57,776][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.17646491527557373, acc: 0.9558823704719543)
[2025-01-06 01:37:57,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58,132][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.012346658855676651, acc: 1.0)
[2025-01-06 01:37:58,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58,475][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.09001408517360687, acc: 0.9599999785423279)
[2025-01-06 01:37:58,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58,814][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.0001622780109755695, acc: 1.0)
[2025-01-06 01:37:58,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59,107][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.057955436408519745, acc: 0.9677419066429138)
[2025-01-06 01:37:59,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59,483][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.0070353914052248, acc: 1.0)
[2025-01-06 01:37:59,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59,827][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.011362412944436073, acc: 1.0)
[2025-01-06 01:37:59,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00,216][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.010955623351037502, acc: 1.0)
[2025-01-06 01:38:00,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00,791][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.06684543192386627, acc: 0.9622641801834106)
[2025-01-06 01:38:00,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01,372][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.1468321532011032, acc: 0.9333333373069763)
[2025-01-06 01:38:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01,658][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.014718770980834961, acc: 1.0)
[2025-01-06 01:38:01,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01,972][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.014731014147400856, acc: 1.0)
[2025-01-06 01:38:02,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02,354][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.1923944056034088, acc: 0.9200000166893005)
[2025-01-06 01:38:02,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02,699][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.11709165573120117, acc: 0.9583333134651184)
[2025-01-06 01:38:02,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03,540][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.4827558100223541, acc: 0.8880000114440918)
[2025-01-06 01:38:03,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03,888][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.19757358729839325, acc: 0.932584285736084)
[2025-01-06 01:38:04,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04,258][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.12554271519184113, acc: 0.9594594836235046)
[2025-01-06 01:38:04,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04,709][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.09033602476119995, acc: 0.982758641242981)
[2025-01-06 01:38:04,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05,067][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.006208099890500307, acc: 1.0)
[2025-01-06 01:38:05,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05,430][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.0007797161233611405, acc: 1.0)
[2025-01-06 01:38:05,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05,794][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.08434763550758362, acc: 0.96875)
[2025-01-06 01:38:05,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06,151][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.001203657011501491, acc: 1.0)
[2025-01-06 01:38:06,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06,534][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.1635536104440689, acc: 0.9666666388511658)
[2025-01-06 01:38:07,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37,518][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2601, device='cuda:0') eval_epoch_loss=tensor(0.8154, device='cuda:0') eval_epoch_acc=tensor(0.8363, device='cuda:0')
[2025-01-06 01:38:37,519][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:38:37,520][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:38:37,748][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_274_loss_0.8154280781745911/model.pt
[2025-01-06 01:38:37,751][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:38:37,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38,157][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.026400936767458916, acc: 0.96875)
[2025-01-06 01:38:38,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38,555][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.0469999685883522, acc: 0.9666666388511658)
[2025-01-06 01:38:38,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38,944][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.10622889548540115, acc: 0.9655172228813171)
[2025-01-06 01:38:39,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39,341][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.0033378119114786386, acc: 1.0)
[2025-01-06 01:38:39,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39,713][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.0265080276876688, acc: 0.978723406791687)
[2025-01-06 01:38:39,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40,105][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.1574205905199051, acc: 0.9375)
[2025-01-06 01:38:40,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40,394][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.006453373935073614, acc: 1.0)
[2025-01-06 01:38:40,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40,817][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.08450701087713242, acc: 0.9759036302566528)
[2025-01-06 01:38:40,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41,191][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.2902686893939972, acc: 0.9444444179534912)
[2025-01-06 01:38:41,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41,530][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.06753000617027283, acc: 0.9736841917037964)
[2025-01-06 01:38:41,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41,923][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.15338435769081116, acc: 0.970588207244873)
[2025-01-06 01:38:42,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42,253][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.03156743198633194, acc: 0.9750000238418579)
[2025-01-06 01:38:42,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42,614][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.08213499933481216, acc: 0.9765625)
[2025-01-06 01:38:42,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43,022][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.13301940262317657, acc: 0.9440000057220459)
[2025-01-06 01:38:43,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43,367][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.04176202788949013, acc: 0.9890109896659851)
[2025-01-06 01:38:43,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43,688][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.11447305977344513, acc: 0.9503105878829956)
[2025-01-06 01:38:43,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44,091][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.1241694763302803, acc: 0.9536082744598389)
[2025-01-06 01:38:44,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44,428][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.005828123074024916, acc: 1.0)
[2025-01-06 01:38:44,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44,763][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.12030690908432007, acc: 0.976190447807312)
[2025-01-06 01:38:44,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45,138][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.02125271037220955, acc: 0.982758641242981)
[2025-01-06 01:38:45,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45,625][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.03578713908791542, acc: 0.9818181991577148)
[2025-01-06 01:38:45,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46,172][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.15123897790908813, acc: 0.9536082744598389)
[2025-01-06 01:38:46,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46,544][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.07427627593278885, acc: 0.982758641242981)
[2025-01-06 01:38:46,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46,844][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.013074018992483616, acc: 1.0)
[2025-01-06 01:38:46,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47,219][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.18187205493450165, acc: 0.9473684430122375)
[2025-01-06 01:38:47,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47,585][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.025965729728341103, acc: 0.9821428656578064)
[2025-01-06 01:38:47,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47,922][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.0010529060382395983, acc: 1.0)
[2025-01-06 01:38:48,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48,286][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.017541123554110527, acc: 1.0)
[2025-01-06 01:38:48,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48,648][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.05892422795295715, acc: 0.9811320900917053)
[2025-01-06 01:38:48,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48,995][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.011053591035306454, acc: 1.0)
[2025-01-06 01:38:49,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49,322][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.000926839595194906, acc: 1.0)
[2025-01-06 01:38:49,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49,699][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.04565110430121422, acc: 0.9836065769195557)
[2025-01-06 01:38:49,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50,026][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.006886618677526712, acc: 1.0)
[2025-01-06 01:38:50,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50,356][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.00533638708293438, acc: 1.0)
[2025-01-06 01:38:50,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50,708][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.010679432190954685, acc: 1.0)
[2025-01-06 01:38:50,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51,123][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.017344733700156212, acc: 1.0)
[2025-01-06 01:38:51,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51,437][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.052681416273117065, acc: 0.9759036302566528)
[2025-01-06 01:38:51,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51,752][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.09433547407388687, acc: 0.9615384340286255)
[2025-01-06 01:38:51,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52,087][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.011476748622953892, acc: 1.0)
[2025-01-06 01:38:52,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52,431][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.0007533243042416871, acc: 1.0)
[2025-01-06 01:38:52,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52,798][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.022413725033402443, acc: 1.0)
[2025-01-06 01:38:52,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53,177][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.08833938837051392, acc: 0.9677419066429138)
[2025-01-06 01:38:53,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53,543][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.01752205565571785, acc: 1.0)
[2025-01-06 01:38:53,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53,928][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.010176915675401688, acc: 1.0)
[2025-01-06 01:38:54,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54,283][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.015773454681038857, acc: 0.9903846383094788)
[2025-01-06 01:38:54,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54,626][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.04290485754609108, acc: 0.9777777791023254)
[2025-01-06 01:38:54,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54,997][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.022302469238638878, acc: 0.9838709831237793)
[2025-01-06 01:38:55,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55,366][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.0028391508385539055, acc: 1.0)
[2025-01-06 01:38:55,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55,755][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.07311786711215973, acc: 0.9629629850387573)
[2025-01-06 01:38:55,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56,124][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.0611710399389267, acc: 0.9714285731315613)
[2025-01-06 01:38:56,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56,494][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.06970439851284027, acc: 0.9743589758872986)
[2025-01-06 01:38:56,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56,857][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.19354526698589325, acc: 0.9512194991111755)
[2025-01-06 01:38:56,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57,210][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.11536350101232529, acc: 0.9736841917037964)
[2025-01-06 01:38:57,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57,549][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.009510700590908527, acc: 1.0)
[2025-01-06 01:38:57,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57,854][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.0021870818454772234, acc: 1.0)
[2025-01-06 01:38:57,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58,221][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.0074240900576114655, acc: 1.0)
[2025-01-06 01:38:58,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58,572][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.00390580203384161, acc: 1.0)
[2025-01-06 01:38:58,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58,905][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.023772476240992546, acc: 0.9838709831237793)
[2025-01-06 01:38:59,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59,263][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.003704830538481474, acc: 1.0)
[2025-01-06 01:38:59,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59,622][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.018664969131350517, acc: 1.0)
[2025-01-06 01:38:59,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00,029][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.0031985712703317404, acc: 1.0)
[2025-01-06 01:39:00,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00,404][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.004075898323208094, acc: 1.0)
[2025-01-06 01:39:00,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00,801][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.08674630522727966, acc: 0.9800000190734863)
[2025-01-06 01:39:00,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01,162][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.10853679478168488, acc: 0.9655172228813171)
[2025-01-06 01:39:01,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01,499][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.3190551698207855, acc: 0.9255319237709045)
[2025-01-06 01:39:01,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01,816][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.14779354631900787, acc: 0.9638554453849792)
[2025-01-06 01:39:01,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02,121][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.0019452780252322555, acc: 1.0)
[2025-01-06 01:39:02,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02,434][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.008283154107630253, acc: 1.0)
[2025-01-06 01:39:02,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02,763][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.06293142586946487, acc: 0.9638554453849792)
[2025-01-06 01:39:02,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03,115][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.04235818609595299, acc: 1.0)
[2025-01-06 01:39:03,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03,454][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.03413727134466171, acc: 0.9620253443717957)
[2025-01-06 01:39:03,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03,779][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.009280181489884853, acc: 1.0)
[2025-01-06 01:39:03,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04,150][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.022553108632564545, acc: 0.9850746393203735)
[2025-01-06 01:39:04,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04,530][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.471735417842865, acc: 0.949999988079071)
[2025-01-06 01:39:04,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04,925][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.04508783668279648, acc: 0.9599999785423279)
[2025-01-06 01:39:05,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05,327][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.3549598157405853, acc: 0.8888888955116272)
[2025-01-06 01:39:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05,638][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.09857074916362762, acc: 0.9534883499145508)
[2025-01-06 01:39:05,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06,012][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.0018379153916612267, acc: 1.0)
[2025-01-06 01:39:06,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06,389][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.054468415677547455, acc: 0.9777777791023254)
[2025-01-06 01:39:06,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06,673][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.05275563523173332, acc: 0.95652174949646)
[2025-01-06 01:39:06,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07,039][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.01785902865231037, acc: 1.0)
[2025-01-06 01:39:07,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07,431][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.1279093474149704, acc: 0.9230769276618958)
[2025-01-06 01:39:07,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07,947][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.1837042272090912, acc: 0.947826087474823)
[2025-01-06 01:39:08,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08,290][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.055721256881952286, acc: 0.97826087474823)
[2025-01-06 01:39:08,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08,618][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.05625090003013611, acc: 0.9591836929321289)
[2025-01-06 01:39:08,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08,982][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.0005571426590904593, acc: 1.0)
[2025-01-06 01:39:09,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09,338][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.005219758488237858, acc: 1.0)
[2025-01-06 01:39:09,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09,643][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.017200864851474762, acc: 1.0)
[2025-01-06 01:39:09,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09,960][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.007620353251695633, acc: 1.0)
[2025-01-06 01:39:10,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10,268][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.007254365365952253, acc: 1.0)
[2025-01-06 01:39:10,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10,597][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.012015004642307758, acc: 1.0)
[2025-01-06 01:39:10,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10,957][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.008884460665285587, acc: 1.0)
[2025-01-06 01:39:11,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11,300][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.0002334020537091419, acc: 1.0)
[2025-01-06 01:39:11,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11,607][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.0016073889564722776, acc: 1.0)
[2025-01-06 01:39:11,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11,951][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.010247534140944481, acc: 1.0)
[2025-01-06 01:39:12,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12,253][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.024130798876285553, acc: 1.0)
[2025-01-06 01:39:12,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12,858][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.16233505308628082, acc: 0.9515151381492615)
[2025-01-06 01:39:13,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13,752][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.041520487517118454, acc: 0.9905660152435303)
[2025-01-06 01:39:13,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14,094][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.03347424790263176, acc: 1.0)
[2025-01-06 01:39:14,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14,462][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.020574605092406273, acc: 1.0)
[2025-01-06 01:39:14,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14,813][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.002661748556420207, acc: 1.0)
[2025-01-06 01:39:14,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15,155][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.00012478102871682495, acc: 1.0)
[2025-01-06 01:39:15,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15,506][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.0001411221455782652, acc: 1.0)
[2025-01-06 01:39:15,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15,911][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.004346113186329603, acc: 1.0)
[2025-01-06 01:39:16,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16,263][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.001367542427033186, acc: 1.0)
[2025-01-06 01:39:16,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16,851][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.07205692678689957, acc: 0.9640718698501587)
[2025-01-06 01:39:16,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:17,252][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.061434343457221985, acc: 0.9774436354637146)
[2025-01-06 01:39:17,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18,286][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.20552727580070496, acc: 0.9304812550544739)
[2025-01-06 01:39:18,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18,848][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.05466679111123085, acc: 0.9819819927215576)
[2025-01-06 01:39:18,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19,189][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.008230520412325859, acc: 1.0)
[2025-01-06 01:39:19,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19,548][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.0008319303160533309, acc: 1.0)
[2025-01-06 01:39:19,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19,886][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.00252925674431026, acc: 1.0)
[2025-01-06 01:39:19,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20,223][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.0004702662117779255, acc: 1.0)
[2025-01-06 01:39:20,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20,560][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.0002993001544382423, acc: 1.0)
[2025-01-06 01:39:20,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20,870][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.0003045813355129212, acc: 1.0)
[2025-01-06 01:39:20,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21,175][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.0007363929180428386, acc: 1.0)
[2025-01-06 01:39:21,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21,547][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.01739899069070816, acc: 1.0)
[2025-01-06 01:39:21,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21,924][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.05434027314186096, acc: 0.9814814925193787)
[2025-01-06 01:39:22,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22,230][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.18040433526039124, acc: 0.9417475461959839)
[2025-01-06 01:39:22,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22,749][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.23734420537948608, acc: 0.9191176295280457)
[2025-01-06 01:39:22,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23,119][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.09434735029935837, acc: 0.9733333587646484)
[2025-01-06 01:39:23,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23,514][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.11566837877035141, acc: 0.9583333134651184)
[2025-01-06 01:39:23,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23,846][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.05374637246131897, acc: 0.9767441749572754)
[2025-01-06 01:39:23,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24,166][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.0029464776162058115, acc: 1.0)
[2025-01-06 01:39:24,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24,560][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.019684825092554092, acc: 1.0)
[2025-01-06 01:39:24,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24,878][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.006554256193339825, acc: 1.0)
[2025-01-06 01:39:25,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25,407][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.027875587344169617, acc: 0.9852941036224365)
[2025-01-06 01:39:25,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25,760][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.07542995363473892, acc: 0.9733333587646484)
[2025-01-06 01:39:25,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26,103][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.06707337498664856, acc: 0.9696969985961914)
[2025-01-06 01:39:26,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26,410][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.18123899400234222, acc: 0.9696969985961914)
[2025-01-06 01:39:26,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26,727][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.003408104181289673, acc: 1.0)
[2025-01-06 01:39:26,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27,034][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.009408817626535892, acc: 1.0)
[2025-01-06 01:39:27,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27,374][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.0006336006335914135, acc: 1.0)
[2025-01-06 01:39:27,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27,755][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.02898101694881916, acc: 0.9722222089767456)
[2025-01-06 01:39:27,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28,125][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.0007976027554832399, acc: 1.0)
[2025-01-06 01:39:28,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28,479][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.00313750677742064, acc: 1.0)
[2025-01-06 01:39:28,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28,827][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.011249745264649391, acc: 1.0)
[2025-01-06 01:39:28,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29,194][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.0029088675510138273, acc: 1.0)
[2025-01-06 01:39:29,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29,568][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.0003526675282046199, acc: 1.0)
[2025-01-06 01:39:29,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29,944][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.007526574190706015, acc: 1.0)
[2025-01-06 01:39:30,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30,324][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.04291164502501488, acc: 0.9545454382896423)
[2025-01-06 01:39:30,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30,647][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.1018039882183075, acc: 0.9607843160629272)
[2025-01-06 01:39:30,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30,944][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.005555709823966026, acc: 1.0)
[2025-01-06 01:39:31,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:42,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:42,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:42,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:43,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:43,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:43,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01,358][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2787, device='cuda:0') eval_epoch_loss=tensor(0.8236, device='cuda:0') eval_epoch_acc=tensor(0.8426, device='cuda:0')
[2025-01-06 01:40:01,360][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:40:01,360][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:40:01,609][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_417_loss_0.8236241936683655/model.pt
[2025-01-06 01:40:01,614][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:40:01,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02,024][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.011305004358291626, acc: 1.0)
[2025-01-06 01:40:02,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02,399][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.011938373558223248, acc: 1.0)
[2025-01-06 01:40:02,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02,750][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.22693702578544617, acc: 0.949999988079071)
[2025-01-06 01:40:02,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03,120][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.0007845332729630172, acc: 1.0)
[2025-01-06 01:40:03,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03,490][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.008661055937409401, acc: 1.0)
[2025-01-06 01:40:03,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03,808][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.11600328236818314, acc: 0.96875)
[2025-01-06 01:40:03,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04,162][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.014451139606535435, acc: 1.0)
[2025-01-06 01:40:04,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04,500][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.0009275208576582372, acc: 1.0)
[2025-01-06 01:40:04,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04,797][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.01735064759850502, acc: 1.0)
[2025-01-06 01:40:04,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05,161][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.13675351440906525, acc: 0.95652174949646)
[2025-01-06 01:40:05,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05,507][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.01909846067428589, acc: 1.0)
[2025-01-06 01:40:05,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05,856][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.02365647442638874, acc: 1.0)
[2025-01-06 01:40:05,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06,223][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.26695042848587036, acc: 0.95652174949646)
[2025-01-06 01:40:06,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06,582][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.003588673658668995, acc: 1.0)
[2025-01-06 01:40:06,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06,913][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.00012633552250918, acc: 1.0)
[2025-01-06 01:40:07,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07,264][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.00030957473791204393, acc: 1.0)
[2025-01-06 01:40:07,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07,645][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.015981055796146393, acc: 1.0)
[2025-01-06 01:40:07,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07,928][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.0002923218999058008, acc: 1.0)
[2025-01-06 01:40:08,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08,235][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.0006086572539061308, acc: 1.0)
[2025-01-06 01:40:08,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08,544][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.02176552265882492, acc: 1.0)
[2025-01-06 01:40:08,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08,916][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.001143458066508174, acc: 1.0)
[2025-01-06 01:40:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09,251][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.00024546580971218646, acc: 1.0)
[2025-01-06 01:40:09,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09,627][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.11644061654806137, acc: 0.9743589758872986)
[2025-01-06 01:40:09,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10,132][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.014279845170676708, acc: 1.0)
[2025-01-06 01:40:10,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10,815][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.1713217794895172, acc: 0.9279999732971191)
[2025-01-06 01:40:10,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11,235][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.14502660930156708, acc: 0.9677419066429138)
[2025-01-06 01:40:11,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11,892][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.13813497126102448, acc: 0.9452736377716064)
[2025-01-06 01:40:11,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12,265][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.10317393392324448, acc: 0.9811320900917053)
[2025-01-06 01:40:12,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12,680][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.014047667384147644, acc: 1.0)
[2025-01-06 01:40:12,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13,059][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.001991748809814453, acc: 1.0)
[2025-01-06 01:40:13,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13,449][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.006641075015068054, acc: 1.0)
[2025-01-06 01:40:13,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13,801][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.0047341203317046165, acc: 1.0)
[2025-01-06 01:40:13,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14,190][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.0036334169562906027, acc: 1.0)
[2025-01-06 01:40:14,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14,536][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.019349228590726852, acc: 1.0)
[2025-01-06 01:40:14,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14,849][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.04561459645628929, acc: 0.989130437374115)
[2025-01-06 01:40:14,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15,179][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.02704150229692459, acc: 0.9871794581413269)
[2025-01-06 01:40:15,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15,519][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.17364175617694855, acc: 0.9473684430122375)
[2025-01-06 01:40:15,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15,907][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.014118644408881664, acc: 1.0)
[2025-01-06 01:40:16,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16,236][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.18505771458148956, acc: 0.939393937587738)
[2025-01-06 01:40:16,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16,612][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.044343866407871246, acc: 0.9896907210350037)
[2025-01-06 01:40:16,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16,980][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.0029291652608662844, acc: 1.0)
[2025-01-06 01:40:17,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17,379][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.0882648453116417, acc: 0.9709302186965942)
[2025-01-06 01:40:17,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17,724][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.008007684722542763, acc: 1.0)
[2025-01-06 01:40:17,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18,105][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.023689113557338715, acc: 0.9876543283462524)
[2025-01-06 01:40:18,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18,473][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.027645962312817574, acc: 1.0)
[2025-01-06 01:40:18,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18,837][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.0017984033329412341, acc: 1.0)
[2025-01-06 01:40:18,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19,174][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.027823276817798615, acc: 1.0)
[2025-01-06 01:40:19,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19,558][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.0019584600813686848, acc: 1.0)
[2025-01-06 01:40:19,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19,896][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.021847853437066078, acc: 1.0)
[2025-01-06 01:40:20,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20,307][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.07459460943937302, acc: 0.9638554453849792)
[2025-01-06 01:40:20,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20,670][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.06703509390354156, acc: 0.9819819927215576)
[2025-01-06 01:40:20,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20,999][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.08362433314323425, acc: 0.9611650705337524)
[2025-01-06 01:40:21,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21,362][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.07358501106500626, acc: 0.9837398529052734)
[2025-01-06 01:40:21,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21,735][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.0061713457107543945, acc: 1.0)
[2025-01-06 01:40:21,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22,103][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.002168989507481456, acc: 1.0)
[2025-01-06 01:40:22,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22,514][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.14380857348442078, acc: 0.9607843160629272)
[2025-01-06 01:40:22,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22,900][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.21393685042858124, acc: 0.9388646483421326)
[2025-01-06 01:40:23,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23,282][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.07840154320001602, acc: 0.96875)
[2025-01-06 01:40:23,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23,639][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.1060294434428215, acc: 0.9447852969169617)
[2025-01-06 01:40:23,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23,965][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.126651793718338, acc: 0.9568345546722412)
[2025-01-06 01:40:24,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24,322][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.1352698653936386, acc: 0.9497487545013428)
[2025-01-06 01:40:24,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24,648][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.03016582503914833, acc: 1.0)
[2025-01-06 01:40:24,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25,012][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.013277355581521988, acc: 1.0)
[2025-01-06 01:40:25,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25,325][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.0037172299344092607, acc: 1.0)
[2025-01-06 01:40:25,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25,694][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.009587913751602173, acc: 1.0)
[2025-01-06 01:40:25,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26,055][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.0567130371928215, acc: 0.949999988079071)
[2025-01-06 01:40:26,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26,446][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.03556423634290695, acc: 1.0)
[2025-01-06 01:40:26,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26,776][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.005540528334677219, acc: 1.0)
[2025-01-06 01:40:26,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27,123][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.013481169939041138, acc: 1.0)
[2025-01-06 01:40:27,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27,429][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.04530573636293411, acc: 1.0)
[2025-01-06 01:40:27,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27,753][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.02456699125468731, acc: 1.0)
[2025-01-06 01:40:27,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28,106][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.14755471050739288, acc: 0.9090909361839294)
[2025-01-06 01:40:28,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28,447][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.09162013232707977, acc: 0.9538461565971375)
[2025-01-06 01:40:28,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28,808][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.027152955532073975, acc: 1.0)
[2025-01-06 01:40:28,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29,163][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.015701932832598686, acc: 1.0)
[2025-01-06 01:40:29,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29,487][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.017256610095500946, acc: 1.0)
[2025-01-06 01:40:29,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29,812][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.328881174325943, acc: 0.9655172228813171)
[2025-01-06 01:40:29,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30,109][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.0017246680799871683, acc: 1.0)
[2025-01-06 01:40:30,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30,485][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.0017056249780580401, acc: 1.0)
[2025-01-06 01:40:30,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30,832][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.1001960039138794, acc: 0.9821428656578064)
[2025-01-06 01:40:30,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31,212][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.04855649545788765, acc: 0.9775280952453613)
[2025-01-06 01:40:31,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31,496][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.10136664658784866, acc: 0.9775280952453613)
[2025-01-06 01:40:31,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31,860][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.3792698085308075, acc: 0.9078013896942139)
[2025-01-06 01:40:31,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32,238][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.08042880147695541, acc: 0.989130437374115)
[2025-01-06 01:40:32,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32,598][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.04273270070552826, acc: 0.9599999785423279)
[2025-01-06 01:40:32,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32,937][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.00016825793136376888, acc: 1.0)
[2025-01-06 01:40:33,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33,254][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.05811520293354988, acc: 0.9629629850387573)
[2025-01-06 01:40:33,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33,603][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.013323948718607426, acc: 1.0)
[2025-01-06 01:40:33,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33,944][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.12966549396514893, acc: 0.9433962106704712)
[2025-01-06 01:40:34,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34,288][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.06296263635158539, acc: 0.9655172228813171)
[2025-01-06 01:40:34,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34,870][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.17899222671985626, acc: 0.9369369149208069)
[2025-01-06 01:40:35,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35,314][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.06759771704673767, acc: 0.9718309640884399)
[2025-01-06 01:40:35,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35,672][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.000433055916801095, acc: 1.0)
[2025-01-06 01:40:35,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36,041][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.004792148247361183, acc: 1.0)
[2025-01-06 01:40:36,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36,358][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.01494678296148777, acc: 1.0)
[2025-01-06 01:40:37,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39,090][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.16956357657909393, acc: 0.9714285731315613)
[2025-01-06 01:40:39,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39,849][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.07422038167715073, acc: 0.976190447807312)
[2025-01-06 01:40:39,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40,179][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.11884276568889618, acc: 0.9642857313156128)
[2025-01-06 01:40:40,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40,517][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.003118910826742649, acc: 1.0)
[2025-01-06 01:40:40,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41,226][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.18929354846477509, acc: 0.9583333134651184)
[2025-01-06 01:40:41,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41,539][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.0013070199638605118, acc: 1.0)
[2025-01-06 01:40:41,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41,828][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.09675164520740509, acc: 0.9677419066429138)
[2025-01-06 01:40:41,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42,188][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.00855074729770422, acc: 1.0)
[2025-01-06 01:40:42,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42,570][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.0005940741975791752, acc: 1.0)
[2025-01-06 01:40:42,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43,562][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.3439781665802002, acc: 0.8983050584793091)
[2025-01-06 01:40:43,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43,904][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.04255610331892967, acc: 0.9776119589805603)
[2025-01-06 01:40:44,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44,271][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.059504494071006775, acc: 0.9781022071838379)
[2025-01-06 01:40:44,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44,834][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.31291335821151733, acc: 0.9150000214576721)
[2025-01-06 01:40:44,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45,118][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.0013502262299880385, acc: 1.0)
[2025-01-06 01:40:45,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45,427][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.007851474918425083, acc: 1.0)
[2025-01-06 01:40:45,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45,779][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.013267145492136478, acc: 1.0)
[2025-01-06 01:40:45,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46,178][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.06137589365243912, acc: 0.9508196711540222)
[2025-01-06 01:40:46,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46,525][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.018823204562067986, acc: 1.0)
[2025-01-06 01:40:46,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46,858][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.0761757642030716, acc: 0.9767441749572754)
[2025-01-06 01:40:46,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47,202][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.04342443495988846, acc: 1.0)
[2025-01-06 01:40:47,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47,582][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.02448495849967003, acc: 1.0)
[2025-01-06 01:40:47,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47,947][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.05448979139328003, acc: 0.9772727489471436)
[2025-01-06 01:40:48,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48,278][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.007318529300391674, acc: 1.0)
[2025-01-06 01:40:48,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48,569][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.004004199989140034, acc: 1.0)
[2025-01-06 01:40:48,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48,866][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.0011148892808705568, acc: 1.0)
[2025-01-06 01:40:48,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49,255][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.04522997885942459, acc: 0.9846153855323792)
[2025-01-06 01:40:49,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49,538][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.03751780837774277, acc: 0.984375)
[2025-01-06 01:40:49,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49,910][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.011906512081623077, acc: 1.0)
[2025-01-06 01:40:50,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50,258][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.10676797479391098, acc: 0.939393937587738)
[2025-01-06 01:40:50,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50,562][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.20841281116008759, acc: 0.9375)
[2025-01-06 01:40:50,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50,850][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.0015115150017663836, acc: 1.0)
[2025-01-06 01:40:50,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51,196][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.00764613738283515, acc: 1.0)
[2025-01-06 01:40:51,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51,518][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.0019644161220639944, acc: 1.0)
[2025-01-06 01:40:51,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51,875][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.001206541433930397, acc: 1.0)
[2025-01-06 01:40:51,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52,209][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.0007388510275632143, acc: 1.0)
[2025-01-06 01:40:52,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52,522][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.0004928445559926331, acc: 1.0)
[2025-01-06 01:40:52,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52,837][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.018118122592568398, acc: 1.0)
[2025-01-06 01:40:52,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53,192][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 9.62316116783768e-05, acc: 1.0)
[2025-01-06 01:40:53,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53,555][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.006722953636199236, acc: 1.0)
[2025-01-06 01:40:53,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53,958][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.07035758346319199, acc: 0.9750000238418579)
[2025-01-06 01:40:54,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54,313][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.002982927020639181, acc: 1.0)
[2025-01-06 01:40:54,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54,693][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.024643460288643837, acc: 1.0)
[2025-01-06 01:40:54,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55,051][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.05663010850548744, acc: 0.9862068891525269)
[2025-01-06 01:40:55,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55,339][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.02546153962612152, acc: 0.9928571581840515)
[2025-01-06 01:40:55,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55,691][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.03475892171263695, acc: 0.9867549538612366)
[2025-01-06 01:40:55,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55,999][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.01496187038719654, acc: 1.0)
[2025-01-06 01:40:56,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56,344][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.002112473826855421, acc: 1.0)
[2025-01-06 01:40:56,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56,695][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.0022105632815510035, acc: 1.0)
[2025-01-06 01:40:57,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:57,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:59,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:59,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:00,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:00,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:00,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27,383][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2633, device='cuda:0') eval_epoch_loss=tensor(0.8168, device='cuda:0') eval_epoch_acc=tensor(0.8458, device='cuda:0')
[2025-01-06 01:41:27,385][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:41:27,385][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:41:27,608][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_560_loss_0.81681889295578/model.pt
[2025-01-06 01:41:27,611][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:41:27,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27,995][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.0028806044720113277, acc: 1.0)
[2025-01-06 01:41:28,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28,363][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.0039250836707651615, acc: 1.0)
[2025-01-06 01:41:28,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28,705][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.10928919166326523, acc: 0.9555555582046509)
[2025-01-06 01:41:28,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29,036][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.015708016231656075, acc: 1.0)
[2025-01-06 01:41:29,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29,409][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.054684653878211975, acc: 0.9791666865348816)
[2025-01-06 01:41:29,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29,740][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.013247818686068058, acc: 1.0)
[2025-01-06 01:41:29,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30,059][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.014055706560611725, acc: 1.0)
[2025-01-06 01:41:30,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30,382][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.022829309105873108, acc: 0.9736841917037964)
[2025-01-06 01:41:30,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30,717][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.0016166218556463718, acc: 1.0)
[2025-01-06 01:41:30,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31,109][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.03127351775765419, acc: 0.9839572310447693)
[2025-01-06 01:41:31,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31,494][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.011792424134910107, acc: 1.0)
[2025-01-06 01:41:31,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31,836][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.010877089574933052, acc: 1.0)
[2025-01-06 01:41:31,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32,129][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.05540525168180466, acc: 0.9693877696990967)
[2025-01-06 01:41:32,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32,467][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.0437561497092247, acc: 0.9874213933944702)
[2025-01-06 01:41:32,865][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.0855, train_epoch_loss=0.0820, epoch time 354.92431953176856s
[2025-01-06 01:41:32,865][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:41:32,866][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 18 GB
[2025-01-06 01:41:32,866][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:41:32,866][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 19
[2025-01-06 01:41:32,866][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:41:33,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33,728][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.043390993028879166, acc: 0.9629629850387573)
[2025-01-06 01:41:33,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34,113][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.0054265172220766544, acc: 1.0)
[2025-01-06 01:41:34,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34,481][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.26747551560401917, acc: 0.9189189076423645)
[2025-01-06 01:41:34,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34,815][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.009364563971757889, acc: 1.0)
[2025-01-06 01:41:34,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35,130][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.04240557178854942, acc: 0.9729729890823364)
[2025-01-06 01:41:35,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35,491][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.0032665752805769444, acc: 1.0)
[2025-01-06 01:41:35,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35,870][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.010323858819901943, acc: 1.0)
[2025-01-06 01:41:35,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36,219][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.009973255917429924, acc: 1.0)
[2025-01-06 01:41:36,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36,594][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.0006703244871459901, acc: 1.0)
[2025-01-06 01:41:36,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36,920][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.0016043982468545437, acc: 1.0)
[2025-01-06 01:41:37,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37,241][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.002638993551954627, acc: 1.0)
[2025-01-06 01:41:37,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37,551][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.0168659258633852, acc: 1.0)
[2025-01-06 01:41:37,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37,866][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.008061721920967102, acc: 1.0)
[2025-01-06 01:41:37,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38,274][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.024637706577777863, acc: 0.97826087474823)
[2025-01-06 01:41:38,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38,670][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.032832495868206024, acc: 0.9803921580314636)
[2025-01-06 01:41:38,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39,020][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.019686821848154068, acc: 1.0)
[2025-01-06 01:41:39,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39,392][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.005507681518793106, acc: 1.0)
[2025-01-06 01:41:39,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39,756][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.0021003286819905043, acc: 1.0)
[2025-01-06 01:41:39,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40,075][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.011952072381973267, acc: 1.0)
[2025-01-06 01:41:40,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40,448][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.023721693083643913, acc: 1.0)
[2025-01-06 01:41:40,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40,828][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.000608331523835659, acc: 1.0)
[2025-01-06 01:41:40,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41,186][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.004296568222343922, acc: 1.0)
[2025-01-06 01:41:41,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41,488][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.1540120542049408, acc: 0.9599999785423279)
[2025-01-06 01:41:41,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41,888][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.0012211956782266498, acc: 1.0)
[2025-01-06 01:41:42,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42,276][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.00044344281195662916, acc: 1.0)
[2025-01-06 01:41:42,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42,643][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.01664971560239792, acc: 1.0)
[2025-01-06 01:41:42,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42,998][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.13766832649707794, acc: 0.9726027250289917)
[2025-01-06 01:41:43,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44,245][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.17245765030384064, acc: 0.9446640610694885)
[2025-01-06 01:41:44,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44,541][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.045227278023958206, acc: 0.9767441749572754)
[2025-01-06 01:41:44,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44,874][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.03810454159975052, acc: 1.0)
[2025-01-06 01:41:44,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45,240][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.05018419772386551, acc: 0.9876543283462524)
[2025-01-06 01:41:45,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45,569][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.005041311029344797, acc: 1.0)
[2025-01-06 01:41:45,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45,910][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.00539772491902113, acc: 1.0)
[2025-01-06 01:41:45,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46,226][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.0004215584194753319, acc: 1.0)
[2025-01-06 01:41:46,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46,567][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.03391440212726593, acc: 0.9831932783126831)
[2025-01-06 01:41:46,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46,911][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.046405620872974396, acc: 0.9836065769195557)
[2025-01-06 01:41:47,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47,244][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.04728071019053459, acc: 0.9841269850730896)
[2025-01-06 01:41:47,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47,595][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.007857056334614754, acc: 1.0)
[2025-01-06 01:41:47,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47,951][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.02038412168622017, acc: 1.0)
[2025-01-06 01:41:48,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48,336][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.001065222080796957, acc: 1.0)
[2025-01-06 01:41:48,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48,694][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.02945811301469803, acc: 1.0)
[2025-01-06 01:41:48,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49,075][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.0048699649050831795, acc: 1.0)
[2025-01-06 01:41:49,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49,469][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.14198745787143707, acc: 0.9692307710647583)
[2025-01-06 01:41:49,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49,899][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.036221474409103394, acc: 1.0)
[2025-01-06 01:41:50,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50,314][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.08765605837106705, acc: 0.969072163105011)
[2025-01-06 01:41:50,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50,714][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.06714996695518494, acc: 0.9852941036224365)
[2025-01-06 01:41:50,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51,064][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.057978589087724686, acc: 0.9615384340286255)
[2025-01-06 01:41:51,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51,426][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.0045273457653820515, acc: 1.0)
[2025-01-06 01:41:51,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51,787][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.01638660579919815, acc: 1.0)
[2025-01-06 01:41:51,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52,143][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.016902746632695198, acc: 1.0)
[2025-01-06 01:41:52,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52,484][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.022677762433886528, acc: 1.0)
[2025-01-06 01:41:52,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52,771][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.11542688310146332, acc: 0.9523809552192688)
[2025-01-06 01:41:52,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53,124][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.058952104300260544, acc: 0.98591548204422)
[2025-01-06 01:41:53,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53,587][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 0.25670337677001953, acc: 0.9266666769981384)
[2025-01-06 01:41:53,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53,908][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.5198224782943726, acc: 0.9189189076423645)
[2025-01-06 01:41:53,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54,194][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.00019811184029094875, acc: 1.0)
[2025-01-06 01:41:55,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57,167][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.3926876187324524, acc: 0.8464163541793823)
[2025-01-06 01:41:57,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58,384][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 0.578173041343689, acc: 0.843137264251709)
[2025-01-06 01:41:58,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59,013][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.21694308519363403, acc: 0.9545454382896423)
[2025-01-06 01:41:59,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59,582][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.011733283288776875, acc: 1.0)
[2025-01-06 01:41:59,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00,141][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.12141184508800507, acc: 0.9492753744125366)
[2025-01-06 01:42:00,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00,539][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.08540375530719757, acc: 0.9750000238418579)
[2025-01-06 01:42:00,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00,891][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.011612392961978912, acc: 1.0)
[2025-01-06 01:42:01,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01,260][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.01684807427227497, acc: 1.0)
[2025-01-06 01:42:01,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01,659][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.018200155347585678, acc: 0.984375)
[2025-01-06 01:42:01,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02,016][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.004731911700218916, acc: 1.0)
[2025-01-06 01:42:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02,359][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.10871506482362747, acc: 0.9642857313156128)
[2025-01-06 01:42:02,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02,734][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.04812009632587433, acc: 0.9833333492279053)
[2025-01-06 01:42:02,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03,061][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.00017563867731951177, acc: 1.0)
[2025-01-06 01:42:03,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03,432][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.1260225772857666, acc: 0.9722222089767456)
[2025-01-06 01:42:03,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03,803][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.007271077949553728, acc: 1.0)
[2025-01-06 01:42:03,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04,142][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.160374715924263, acc: 0.9411764740943909)
[2025-01-06 01:42:04,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04,468][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.09265090525150299, acc: 0.9603174328804016)
[2025-01-06 01:42:04,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04,841][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 0.2446167916059494, acc: 0.9230769276618958)
[2025-01-06 01:42:04,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05,204][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.1894208937883377, acc: 0.9693877696990967)
[2025-01-06 01:42:05,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05,559][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.13069015741348267, acc: 0.9626865386962891)
[2025-01-06 01:42:05,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05,982][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 0.3630726933479309, acc: 0.8832116723060608)
[2025-01-06 01:42:06,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06,325][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.0014312678249552846, acc: 1.0)
[2025-01-06 01:42:06,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06,667][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.00385264097712934, acc: 1.0)
[2025-01-06 01:42:06,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06,941][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.0333683043718338, acc: 1.0)
[2025-01-06 01:42:07,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07,238][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.2381707727909088, acc: 0.9230769276618958)
[2025-01-06 01:42:07,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07,606][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.003195317229256034, acc: 1.0)
[2025-01-06 01:42:07,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07,933][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.008028930984437466, acc: 1.0)
[2025-01-06 01:42:08,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08,298][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.030828090384602547, acc: 1.0)
[2025-01-06 01:42:08,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08,677][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.02525128610432148, acc: 0.9855072498321533)
[2025-01-06 01:42:08,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09,045][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.0028687329031527042, acc: 1.0)
[2025-01-06 01:42:09,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09,430][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.024707607924938202, acc: 1.0)
[2025-01-06 01:42:09,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09,915][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.09044806659221649, acc: 0.9599999785423279)
[2025-01-06 01:42:10,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10,282][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.06080701947212219, acc: 0.9902912378311157)
[2025-01-06 01:42:10,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:11,434][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.3091241717338562, acc: 0.917475700378418)
[2025-01-06 01:42:11,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:12,256][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.23992745578289032, acc: 0.9193548560142517)
[2025-01-06 01:42:12,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13,058][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.2603229880332947, acc: 0.9267241358757019)
[2025-01-06 01:42:13,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13,803][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.09700430929660797, acc: 0.9789473414421082)
[2025-01-06 01:42:14,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14,796][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.24291467666625977, acc: 0.9009901285171509)
[2025-01-06 01:42:14,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15,163][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.13662023842334747, acc: 0.9677419066429138)
[2025-01-06 01:42:15,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15,544][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.030753903090953827, acc: 0.9855072498321533)
[2025-01-06 01:42:15,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15,918][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.19440858066082, acc: 0.9327731132507324)
[2025-01-06 01:42:16,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16,302][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.09957598894834518, acc: 0.9711538553237915)
[2025-01-06 01:42:16,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16,720][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.1355164796113968, acc: 0.9343065619468689)
[2025-01-06 01:42:16,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17,084][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.19772082567214966, acc: 0.9552238583564758)
[2025-01-06 01:42:17,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17,466][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.01258201152086258, acc: 1.0)
[2025-01-06 01:42:17,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17,823][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.0010328078642487526, acc: 1.0)
[2025-01-06 01:42:17,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18,112][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.07597982883453369, acc: 0.95652174949646)
[2025-01-06 01:42:18,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18,481][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.009660604409873486, acc: 1.0)
[2025-01-06 01:42:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18,838][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.015140121802687645, acc: 1.0)
[2025-01-06 01:42:18,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19,200][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.004259107168763876, acc: 1.0)
[2025-01-06 01:42:19,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19,545][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.0077884020283818245, acc: 1.0)
[2025-01-06 01:42:19,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19,902][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.0009580337791703641, acc: 1.0)
[2025-01-06 01:42:20,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20,246][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.0015903905732557178, acc: 1.0)
[2025-01-06 01:42:20,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20,576][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.006712037138640881, acc: 1.0)
[2025-01-06 01:42:20,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20,956][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.01990404538810253, acc: 1.0)
[2025-01-06 01:42:21,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21,360][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.02370285615324974, acc: 0.9824561476707458)
[2025-01-06 01:42:21,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21,753][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.12101709097623825, acc: 0.9473684430122375)
[2025-01-06 01:42:21,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22,097][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.04595359414815903, acc: 0.9743589758872986)
[2025-01-06 01:42:22,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22,463][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.003998809959739447, acc: 1.0)
[2025-01-06 01:42:22,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22,818][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.00021152160479687154, acc: 1.0)
[2025-01-06 01:42:22,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23,224][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.02271444723010063, acc: 1.0)
[2025-01-06 01:42:23,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23,583][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.12886200845241547, acc: 0.9674796462059021)
[2025-01-06 01:42:23,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23,961][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.007862296886742115, acc: 1.0)
[2025-01-06 01:42:24,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:24,814][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.10864797979593277, acc: 0.9619771838188171)
[2025-01-06 01:42:24,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25,172][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.00418383814394474, acc: 1.0)
[2025-01-06 01:42:25,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25,582][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.006821786519140005, acc: 1.0)
[2025-01-06 01:42:25,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25,906][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.000995021895505488, acc: 1.0)
[2025-01-06 01:42:26,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26,278][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.00890713557600975, acc: 1.0)
[2025-01-06 01:42:26,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26,623][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.09174796938896179, acc: 0.9754601120948792)
[2025-01-06 01:42:26,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27,013][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.1622065156698227, acc: 0.9513888955116272)
[2025-01-06 01:42:27,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27,448][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.12856006622314453, acc: 0.9666666388511658)
[2025-01-06 01:42:27,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27,836][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.059990376234054565, acc: 0.9821428656578064)
[2025-01-06 01:42:27,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28,175][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.1615035980939865, acc: 0.9692307710647583)
[2025-01-06 01:42:28,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58,251][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2409, device='cuda:0') eval_epoch_loss=tensor(0.8069, device='cuda:0') eval_epoch_acc=tensor(0.8502, device='cuda:0')
[2025-01-06 01:42:58,252][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:42:58,252][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:42:58,474][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_129_loss_0.8068996071815491/model.pt
[2025-01-06 01:42:58,478][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:42:58,478][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.8502339124679565
[2025-01-06 01:42:58,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58,922][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.1721368432044983, acc: 0.9485294222831726)
[2025-01-06 01:42:59,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59,284][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.013749880716204643, acc: 1.0)
[2025-01-06 01:42:59,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59,613][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.014283097349107265, acc: 1.0)
[2025-01-06 01:42:59,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59,930][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.013399003073573112, acc: 1.0)
[2025-01-06 01:43:00,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00,279][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.01650106906890869, acc: 1.0)
[2025-01-06 01:43:00,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00,630][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.19536925852298737, acc: 0.9428571462631226)
[2025-01-06 01:43:00,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00,982][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.02954651042819023, acc: 1.0)
[2025-01-06 01:43:01,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01,299][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.07998014986515045, acc: 0.9523809552192688)
[2025-01-06 01:43:01,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01,583][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.04369180276989937, acc: 0.9666666388511658)
[2025-01-06 01:43:01,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01,933][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.015774602070450783, acc: 1.0)
[2025-01-06 01:43:02,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02,271][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.005721895955502987, acc: 1.0)
[2025-01-06 01:43:02,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02,651][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.024972757324576378, acc: 1.0)
[2025-01-06 01:43:02,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03,039][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.028289692476391792, acc: 1.0)
[2025-01-06 01:43:03,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03,376][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.012252876535058022, acc: 1.0)
[2025-01-06 01:43:03,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03,900][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.09305419772863388, acc: 0.9649122953414917)
[2025-01-06 01:43:03,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04,283][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.2741970717906952, acc: 0.9253731369972229)
[2025-01-06 01:43:04,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04,676][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.037315063178539276, acc: 0.9897959232330322)
[2025-01-06 01:43:04,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05,115][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.10925205051898956, acc: 0.9468085169792175)
[2025-01-06 01:43:05,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05,432][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.02479853853583336, acc: 1.0)
[2025-01-06 01:43:05,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05,743][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.004123592749238014, acc: 1.0)
[2025-01-06 01:43:05,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06,093][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.11105087399482727, acc: 0.9130434989929199)
[2025-01-06 01:43:06,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06,424][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.004461185075342655, acc: 1.0)
[2025-01-06 01:43:06,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06,771][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.010681225918233395, acc: 1.0)
[2025-01-06 01:43:06,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07,129][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.044864799827337265, acc: 1.0)
[2025-01-06 01:43:07,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07,500][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.030348608270287514, acc: 0.9824561476707458)
[2025-01-06 01:43:07,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07,879][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.19787706434726715, acc: 0.9729729890823364)
[2025-01-06 01:43:07,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08,235][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.0013124661054462194, acc: 1.0)
[2025-01-06 01:43:08,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08,559][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.007650356739759445, acc: 1.0)
[2025-01-06 01:43:08,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08,887][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.022548120468854904, acc: 1.0)
[2025-01-06 01:43:09,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10,446][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.18191900849342346, acc: 0.9459459185600281)
[2025-01-06 01:43:10,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10,772][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.18390896916389465, acc: 0.9629629850387573)
[2025-01-06 01:43:10,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11,186][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.24769611656665802, acc: 0.9418604373931885)
[2025-01-06 01:43:11,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11,770][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.19201651215553284, acc: 0.9411764740943909)
[2025-01-06 01:43:11,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12,322][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.15667614340782166, acc: 0.9775280952453613)
[2025-01-06 01:43:12,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12,619][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.03357897326350212, acc: 1.0)
[2025-01-06 01:43:12,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12,972][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.004700514022260904, acc: 1.0)
[2025-01-06 01:43:13,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13,283][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.0367036834359169, acc: 0.9655172228813171)
[2025-01-06 01:43:13,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13,635][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.01289879996329546, acc: 1.0)
[2025-01-06 01:43:13,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13,964][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.04775461554527283, acc: 0.9800000190734863)
[2025-01-06 01:43:14,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14,365][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.13837699592113495, acc: 0.9583333134651184)
[2025-01-06 01:43:14,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14,676][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.04722657799720764, acc: 1.0)
[2025-01-06 01:43:14,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:15,704][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.09393414109945297, acc: 0.965753436088562)
[2025-01-06 01:43:15,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16,032][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.010525349527597427, acc: 1.0)
[2025-01-06 01:43:16,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16,376][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.0014293730491772294, acc: 1.0)
[2025-01-06 01:43:16,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16,664][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.01672198437154293, acc: 1.0)
[2025-01-06 01:43:16,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17,210][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.21028535068035126, acc: 0.9026548862457275)
[2025-01-06 01:43:17,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17,528][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.07163790613412857, acc: 0.9710144996643066)
[2025-01-06 01:43:17,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17,896][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.10272400081157684, acc: 0.9772727489471436)
[2025-01-06 01:43:18,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18,803][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.2168138325214386, acc: 0.9465649127960205)
[2025-01-06 01:43:18,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19,472][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.13199381530284882, acc: 0.9629629850387573)
[2025-01-06 01:43:19,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19,837][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.02486003190279007, acc: 1.0)
[2025-01-06 01:43:19,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20,159][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.00029916767380200326, acc: 1.0)
[2025-01-06 01:43:20,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20,464][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.031624529510736465, acc: 0.9599999785423279)
[2025-01-06 01:43:20,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20,826][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.0036022700369358063, acc: 1.0)
[2025-01-06 01:43:20,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21,213][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.003989300224930048, acc: 1.0)
[2025-01-06 01:43:21,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21,582][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.11188622564077377, acc: 0.9637462496757507)
[2025-01-06 01:43:21,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21,972][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.13665907084941864, acc: 0.9567723274230957)
[2025-01-06 01:43:22,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22,458][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.11329196393489838, acc: 0.956250011920929)
[2025-01-06 01:43:22,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22,982][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.20227603614330292, acc: 0.9380863308906555)
[2025-01-06 01:43:23,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23,394][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.12308697402477264, acc: 0.9644128084182739)
[2025-01-06 01:43:23,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23,751][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.00248716096393764, acc: 1.0)
[2025-01-06 01:43:23,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:24,300][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.06892111897468567, acc: 0.9883720874786377)
[2025-01-06 01:43:24,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25,108][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.3304632902145386, acc: 0.8888888955116272)
[2025-01-06 01:43:25,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26,023][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.18296122550964355, acc: 0.9242424368858337)
[2025-01-06 01:43:26,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26,771][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.05544279143214226, acc: 0.9882352948188782)
[2025-01-06 01:43:27,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27,849][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.2154521495103836, acc: 0.9629629850387573)
[2025-01-06 01:43:28,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28,805][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.03838811814785004, acc: 1.0)
[2025-01-06 01:43:28,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29,146][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.02341805398464203, acc: 1.0)
[2025-01-06 01:43:29,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29,547][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.0026823836378753185, acc: 1.0)
[2025-01-06 01:43:29,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29,891][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.023016221821308136, acc: 1.0)
[2025-01-06 01:43:29,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30,241][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.05634801462292671, acc: 0.9852941036224365)
[2025-01-06 01:43:30,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30,603][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.04658733308315277, acc: 0.9745762944221497)
[2025-01-06 01:43:30,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30,997][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.08863367885351181, acc: 0.9776119589805603)
[2025-01-06 01:43:31,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31,391][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.07083098590373993, acc: 0.9805825352668762)
[2025-01-06 01:43:31,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31,672][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.06979997456073761, acc: 0.9682539701461792)
[2025-01-06 01:43:31,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31,980][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.02117823250591755, acc: 0.9890109896659851)
[2025-01-06 01:43:32,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32,330][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.03481800854206085, acc: 0.9865471124649048)
[2025-01-06 01:43:32,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32,778][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.07081490755081177, acc: 0.9763779640197754)
[2025-01-06 01:43:32,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33,139][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.0492389053106308, acc: 0.9741379022598267)
[2025-01-06 01:43:33,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33,513][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.11497239023447037, acc: 0.9746376872062683)
[2025-01-06 01:43:33,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33,880][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.03575313463807106, acc: 0.9844357967376709)
[2025-01-06 01:43:34,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34,260][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.01270385179668665, acc: 1.0)
[2025-01-06 01:43:34,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34,599][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.0050841742195189, acc: 1.0)
[2025-01-06 01:43:34,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34,976][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.0010332464007660747, acc: 1.0)
[2025-01-06 01:43:35,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35,376][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.0014974031364545226, acc: 1.0)
[2025-01-06 01:43:35,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36,075][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.029934056103229523, acc: 0.9923076629638672)
[2025-01-06 01:43:36,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36,395][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.005458451807498932, acc: 1.0)
[2025-01-06 01:43:36,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36,714][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.027686651796102524, acc: 0.9883720874786377)
[2025-01-06 01:43:36,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37,246][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.009527793154120445, acc: 1.0)
[2025-01-06 01:43:37,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37,630][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.01066676340997219, acc: 1.0)
[2025-01-06 01:43:37,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37,929][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.005289873108267784, acc: 1.0)
[2025-01-06 01:43:38,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38,273][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.0012336497893556952, acc: 1.0)
[2025-01-06 01:43:38,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38,659][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.001154061988927424, acc: 1.0)
[2025-01-06 01:43:38,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39,058][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.020510368049144745, acc: 1.0)
[2025-01-06 01:43:39,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39,861][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.09751839190721512, acc: 0.95652174949646)
[2025-01-06 01:43:39,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40,399][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.17816825211048126, acc: 0.9488636255264282)
[2025-01-06 01:43:40,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40,845][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.042482633143663406, acc: 0.9893617033958435)
[2025-01-06 01:43:40,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41,220][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.055178556591272354, acc: 0.9811320900917053)
[2025-01-06 01:43:41,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41,592][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.037829115986824036, acc: 0.9833333492279053)
[2025-01-06 01:43:41,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41,988][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.1276955008506775, acc: 0.9534883499145508)
[2025-01-06 01:43:42,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42,360][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.030166734009981155, acc: 0.9666666388511658)
[2025-01-06 01:43:42,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42,773][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.22491252422332764, acc: 0.8947368264198303)
[2025-01-06 01:43:42,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43,170][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.08001605421304703, acc: 0.9777777791023254)
[2025-01-06 01:43:43,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43,593][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.32611793279647827, acc: 0.8888888955116272)
[2025-01-06 01:43:43,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44,083][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 0.42038464546203613, acc: 0.8394495248794556)
[2025-01-06 01:43:44,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44,547][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.24124518036842346, acc: 0.8999999761581421)
[2025-01-06 01:43:44,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44,857][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.0014713252894580364, acc: 1.0)
[2025-01-06 01:43:44,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45,212][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.0011513630161061883, acc: 1.0)
[2025-01-06 01:43:45,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45,582][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.22735753655433655, acc: 0.9090909361839294)
[2025-01-06 01:43:45,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45,931][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.005882642697542906, acc: 1.0)
[2025-01-06 01:43:46,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46,253][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.046743445098400116, acc: 0.9714285731315613)
[2025-01-06 01:43:46,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46,642][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.028126852586865425, acc: 1.0)
[2025-01-06 01:43:46,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46,979][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.004234789405018091, acc: 1.0)
[2025-01-06 01:43:47,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47,557][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.09814141690731049, acc: 0.9838709831237793)
[2025-01-06 01:43:47,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48,085][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.11036942154169083, acc: 0.9545454382896423)
[2025-01-06 01:43:48,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48,391][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 9.484888141741976e-05, acc: 1.0)
[2025-01-06 01:43:48,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48,694][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.002437618561089039, acc: 1.0)
[2025-01-06 01:43:48,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49,041][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.0004830717807635665, acc: 1.0)
[2025-01-06 01:43:49,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49,415][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.0002856479841284454, acc: 1.0)
[2025-01-06 01:43:49,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49,759][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.0031580247450619936, acc: 1.0)
[2025-01-06 01:43:49,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50,055][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.028028562664985657, acc: 0.9729729890823364)
[2025-01-06 01:43:50,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50,420][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.023819511756300926, acc: 0.9729729890823364)
[2025-01-06 01:43:50,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50,793][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.056745536625385284, acc: 0.9852941036224365)
[2025-01-06 01:43:50,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51,157][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.015333786606788635, acc: 1.0)
[2025-01-06 01:43:51,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51,454][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.0030838754028081894, acc: 1.0)
[2025-01-06 01:43:51,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51,810][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 8.778781921137124e-05, acc: 1.0)
[2025-01-06 01:43:51,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52,196][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.0013387516373768449, acc: 1.0)
[2025-01-06 01:43:52,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52,548][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.0012067030183970928, acc: 1.0)
[2025-01-06 01:43:52,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52,849][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.027582457289099693, acc: 0.9857142567634583)
[2025-01-06 01:43:52,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53,192][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.04371999576687813, acc: 0.9868420958518982)
[2025-01-06 01:43:53,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53,766][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.017924342304468155, acc: 1.0)
[2025-01-06 01:43:53,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54,359][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.049282193183898926, acc: 0.9750000238418579)
[2025-01-06 01:43:54,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54,732][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.004254703875631094, acc: 1.0)
[2025-01-06 01:43:54,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55,116][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.018152007833123207, acc: 1.0)
[2025-01-06 01:43:55,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55,472][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.1761491745710373, acc: 0.9599999785423279)
[2025-01-06 01:43:55,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55,807][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.14980857074260712, acc: 0.9583333134651184)
[2025-01-06 01:43:56,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56,640][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.4584125876426697, acc: 0.9039999842643738)
[2025-01-06 01:43:56,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57,000][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.06252673268318176, acc: 0.9775280952453613)
[2025-01-06 01:43:57,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57,386][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.1591499149799347, acc: 0.9729729890823364)
[2025-01-06 01:43:57,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57,846][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.029770879074931145, acc: 1.0)
[2025-01-06 01:43:57,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58,182][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.001152311684563756, acc: 1.0)
[2025-01-06 01:43:58,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58,548][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.00476619740948081, acc: 1.0)
[2025-01-06 01:43:58,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58,928][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.022899359464645386, acc: 1.0)
[2025-01-06 01:43:59,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:16,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:16,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29,206][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2899, device='cuda:0') eval_epoch_loss=tensor(0.8285, device='cuda:0') eval_epoch_acc=tensor(0.8504, device='cuda:0')
[2025-01-06 01:44:29,207][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:44:29,207][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:44:29,419][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_272_loss_0.8285099864006042/model.pt
[2025-01-06 01:44:29,423][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:44:29,423][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.8504191637039185
[2025-01-06 01:44:29,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29,846][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.09231914579868317, acc: 0.9666666388511658)
[2025-01-06 01:44:29,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30,250][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.12398984283208847, acc: 0.949999988079071)
[2025-01-06 01:44:30,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30,588][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.001351614249870181, acc: 1.0)
[2025-01-06 01:44:30,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30,912][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.009633520618081093, acc: 1.0)
[2025-01-06 01:44:30,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31,268][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.09038740396499634, acc: 0.9655172228813171)
[2025-01-06 01:44:31,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31,656][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.008086038753390312, acc: 1.0)
[2025-01-06 01:44:31,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31,990][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.002916793804615736, acc: 1.0)
[2025-01-06 01:44:32,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32,359][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.06287730485200882, acc: 0.9583333134651184)
[2025-01-06 01:44:32,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32,770][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.0019864668138325214, acc: 1.0)
[2025-01-06 01:44:32,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33,216][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.05900232493877411, acc: 0.9638554453849792)
[2025-01-06 01:44:33,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33,562][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.051142994314432144, acc: 0.9814814925193787)
[2025-01-06 01:44:33,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33,897][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.012315365485846996, acc: 1.0)
[2025-01-06 01:44:33,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34,266][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.02489110641181469, acc: 1.0)
[2025-01-06 01:44:34,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34,615][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.007633143104612827, acc: 1.0)
[2025-01-06 01:44:34,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34,930][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.0348881296813488, acc: 0.9921875)
[2025-01-06 01:44:35,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35,276][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.16770420968532562, acc: 0.9599999785423279)
[2025-01-06 01:44:35,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35,603][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.015618466772139072, acc: 1.0)
[2025-01-06 01:44:35,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35,948][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.04566014185547829, acc: 0.9937888383865356)
[2025-01-06 01:44:36,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36,323][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.11025065928697586, acc: 0.9587628841400146)
[2025-01-06 01:44:36,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36,656][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.00194785266648978, acc: 1.0)
[2025-01-06 01:44:36,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36,984][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.00570607790723443, acc: 1.0)
[2025-01-06 01:44:37,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37,353][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.011305335909128189, acc: 1.0)
[2025-01-06 01:44:37,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37,844][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.013254846446216106, acc: 1.0)
[2025-01-06 01:44:37,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38,403][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.08984319120645523, acc: 0.9639175534248352)
[2025-01-06 01:44:38,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38,784][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.023063628003001213, acc: 1.0)
[2025-01-06 01:44:38,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39,147][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.06620880216360092, acc: 0.9629629850387573)
[2025-01-06 01:44:39,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39,455][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.04256582632660866, acc: 1.0)
[2025-01-06 01:44:39,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39,791][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.002712056739255786, acc: 1.0)
[2025-01-06 01:44:39,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40,123][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.0031110134441405535, acc: 1.0)
[2025-01-06 01:44:40,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40,495][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.0021946902852505445, acc: 1.0)
[2025-01-06 01:44:40,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40,843][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.0050110300071537495, acc: 1.0)
[2025-01-06 01:44:40,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41,191][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.002736029215157032, acc: 1.0)
[2025-01-06 01:44:41,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41,515][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.006831202656030655, acc: 1.0)
[2025-01-06 01:44:41,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41,865][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.03250986337661743, acc: 0.9836065769195557)
[2025-01-06 01:44:41,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42,218][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.00497967517003417, acc: 1.0)
[2025-01-06 01:44:42,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42,595][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.006930535659193993, acc: 1.0)
[2025-01-06 01:44:42,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42,950][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.05892959609627724, acc: 0.9855072498321533)
[2025-01-06 01:44:43,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43,389][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.004647634457796812, acc: 1.0)
[2025-01-06 01:44:43,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43,769][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.08548454940319061, acc: 0.9759036302566528)
[2025-01-06 01:44:43,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44,147][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.013090885244309902, acc: 1.0)
[2025-01-06 01:44:44,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44,503][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.0015165313379839063, acc: 1.0)
[2025-01-06 01:44:44,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44,835][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.00032548047602176666, acc: 1.0)
[2025-01-06 01:44:44,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45,140][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.00048085497110150754, acc: 1.0)
[2025-01-06 01:44:45,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45,453][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.0036376069765537977, acc: 1.0)
[2025-01-06 01:44:45,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45,775][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.04531170427799225, acc: 0.9677419066429138)
[2025-01-06 01:44:45,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46,129][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.009984046220779419, acc: 1.0)
[2025-01-06 01:44:46,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46,446][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.005495448596775532, acc: 1.0)
[2025-01-06 01:44:46,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46,743][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.004198300652205944, acc: 1.0)
[2025-01-06 01:44:46,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47,107][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.0017310979310423136, acc: 1.0)
[2025-01-06 01:44:47,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47,428][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.001722340239211917, acc: 1.0)
[2025-01-06 01:44:47,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47,788][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.07980827242136002, acc: 0.9629629850387573)
[2025-01-06 01:44:47,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48,164][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.016374165192246437, acc: 1.0)
[2025-01-06 01:44:48,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48,504][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.08874022215604782, acc: 0.9487179517745972)
[2025-01-06 01:44:48,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48,865][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.03513818979263306, acc: 1.0)
[2025-01-06 01:44:48,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49,228][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.011911657638847828, acc: 1.0)
[2025-01-06 01:44:49,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49,587][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.00593662029132247, acc: 1.0)
[2025-01-06 01:44:49,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49,931][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.0038817881140857935, acc: 1.0)
[2025-01-06 01:44:50,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50,251][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.001116527826525271, acc: 1.0)
[2025-01-06 01:44:50,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50,602][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.0017506645526736975, acc: 1.0)
[2025-01-06 01:44:50,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50,992][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.016029030084609985, acc: 1.0)
[2025-01-06 01:44:51,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51,361][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.028486300259828568, acc: 0.9824561476707458)
[2025-01-06 01:44:51,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51,694][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.001500685466453433, acc: 1.0)
[2025-01-06 01:44:51,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52,019][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.029353361576795578, acc: 0.9666666388511658)
[2025-01-06 01:44:52,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52,344][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.0051716407760977745, acc: 1.0)
[2025-01-06 01:44:52,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52,707][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.07128539681434631, acc: 0.9800000190734863)
[2025-01-06 01:44:52,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53,100][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.11651840060949326, acc: 0.9425287246704102)
[2025-01-06 01:44:53,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53,454][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.11303127557039261, acc: 0.957446813583374)
[2025-01-06 01:44:53,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53,820][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.05647262930870056, acc: 0.9879518151283264)
[2025-01-06 01:44:53,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54,192][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.00013484068040270358, acc: 1.0)
[2025-01-06 01:44:54,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54,517][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.004770258441567421, acc: 1.0)
[2025-01-06 01:44:54,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54,867][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.018052298575639725, acc: 1.0)
[2025-01-06 01:44:54,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55,218][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.028430594131350517, acc: 0.9811320900917053)
[2025-01-06 01:44:55,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55,533][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.05063510686159134, acc: 0.9873417615890503)
[2025-01-06 01:44:55,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55,873][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.003530298126861453, acc: 1.0)
[2025-01-06 01:44:55,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56,233][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.012549939565360546, acc: 1.0)
[2025-01-06 01:44:56,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56,586][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.06050931289792061, acc: 0.949999988079071)
[2025-01-06 01:44:56,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56,952][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.04024483636021614, acc: 0.9599999785423279)
[2025-01-06 01:44:57,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57,382][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.010799579322338104, acc: 1.0)
[2025-01-06 01:44:57,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57,732][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.01872173137962818, acc: 1.0)
[2025-01-06 01:44:57,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58,100][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.0074407015927135944, acc: 1.0)
[2025-01-06 01:44:58,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58,463][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.06926774233579636, acc: 0.9555555582046509)
[2025-01-06 01:44:58,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58,747][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.000156004200107418, acc: 1.0)
[2025-01-06 01:44:58,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59,065][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.005388950929045677, acc: 1.0)
[2025-01-06 01:44:59,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59,413][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.06670922785997391, acc: 0.9780219793319702)
[2025-01-06 01:44:59,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59,909][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.12388886511325836, acc: 0.9652174115180969)
[2025-01-06 01:45:00,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00,262][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.058413390070199966, acc: 0.989130437374115)
[2025-01-06 01:45:00,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00,646][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.00951690599322319, acc: 1.0)
[2025-01-06 01:45:00,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00,999][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.00022035282745491713, acc: 1.0)
[2025-01-06 01:45:01,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01,360][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.0030891289934515953, acc: 1.0)
[2025-01-06 01:45:01,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01,686][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.00435449555516243, acc: 1.0)
[2025-01-06 01:45:01,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02,035][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.08285774290561676, acc: 0.9777777791023254)
[2025-01-06 01:45:02,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02,376][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.008391983807086945, acc: 1.0)
[2025-01-06 01:45:02,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02,741][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.021231291815638542, acc: 1.0)
[2025-01-06 01:45:02,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03,074][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.04587321728467941, acc: 0.9696969985961914)
[2025-01-06 01:45:03,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03,441][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.00020197458798065782, acc: 1.0)
[2025-01-06 01:45:03,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03,797][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.0008795014582574368, acc: 1.0)
[2025-01-06 01:45:03,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04,134][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.002267546486109495, acc: 1.0)
[2025-01-06 01:45:04,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04,519][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.018079886212944984, acc: 1.0)
[2025-01-06 01:45:04,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05,116][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.11376544088125229, acc: 0.9575757384300232)
[2025-01-06 01:45:05,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05,941][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.0790441706776619, acc: 0.9811320900917053)
[2025-01-06 01:45:06,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06,261][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.019787680357694626, acc: 0.9888888597488403)
[2025-01-06 01:45:06,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06,602][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.007926201447844505, acc: 1.0)
[2025-01-06 01:45:06,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06,943][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.002207444980740547, acc: 1.0)
[2025-01-06 01:45:07,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07,257][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 8.163628808688372e-05, acc: 1.0)
[2025-01-06 01:45:07,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07,564][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.0009022592566907406, acc: 1.0)
[2025-01-06 01:45:07,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07,947][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.004908423870801926, acc: 1.0)
[2025-01-06 01:45:08,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08,342][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.0006012468365952373, acc: 1.0)
[2025-01-06 01:45:08,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08,917][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.02885940484702587, acc: 0.9880239367485046)
[2025-01-06 01:45:09,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09,324][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.04327383264899254, acc: 0.9849624037742615)
[2025-01-06 01:45:09,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:10,539][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.2118217498064041, acc: 0.9465240836143494)
[2025-01-06 01:45:10,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11,101][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.012565302662551403, acc: 1.0)
[2025-01-06 01:45:11,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11,462][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.00407779635861516, acc: 1.0)
[2025-01-06 01:45:11,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11,765][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.0021512294188141823, acc: 1.0)
[2025-01-06 01:45:11,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12,065][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.00044492442975752056, acc: 1.0)
[2025-01-06 01:45:12,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12,371][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.0012531790416687727, acc: 1.0)
[2025-01-06 01:45:12,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12,681][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.0004974150797352195, acc: 1.0)
[2025-01-06 01:45:12,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13,023][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.0008173391688615084, acc: 1.0)
[2025-01-06 01:45:13,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13,400][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 8.72557720867917e-05, acc: 1.0)
[2025-01-06 01:45:13,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13,747][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.22258183360099792, acc: 0.9523809552192688)
[2025-01-06 01:45:13,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14,082][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.17329546809196472, acc: 0.9444444179534912)
[2025-01-06 01:45:14,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14,396][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.16826827824115753, acc: 0.9417475461959839)
[2025-01-06 01:45:14,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14,920][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.12916608154773712, acc: 0.9632353186607361)
[2025-01-06 01:45:15,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15,326][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.11226365715265274, acc: 0.9599999785423279)
[2025-01-06 01:45:15,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15,708][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.07255228608846664, acc: 0.9722222089767456)
[2025-01-06 01:45:15,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16,033][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.10370869934558868, acc: 0.9767441749572754)
[2025-01-06 01:45:16,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16,357][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.048118382692337036, acc: 0.9583333134651184)
[2025-01-06 01:45:16,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16,729][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.0044067357666790485, acc: 1.0)
[2025-01-06 01:45:16,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17,073][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.0022365720942616463, acc: 1.0)
[2025-01-06 01:45:17,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17,612][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.023350393399596214, acc: 0.9852941036224365)
[2025-01-06 01:45:17,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17,970][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.09354156255722046, acc: 0.9599999785423279)
[2025-01-06 01:45:18,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18,329][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.0013483210932463408, acc: 1.0)
[2025-01-06 01:45:18,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18,667][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.044899385422468185, acc: 0.9696969985961914)
[2025-01-06 01:45:18,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18,993][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.00983953196555376, acc: 1.0)
[2025-01-06 01:45:19,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19,369][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.00041160976979881525, acc: 1.0)
[2025-01-06 01:45:19,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19,645][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.0007595508359372616, acc: 1.0)
[2025-01-06 01:45:19,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19,953][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.005715389735996723, acc: 1.0)
[2025-01-06 01:45:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20,267][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.004127028398215771, acc: 1.0)
[2025-01-06 01:45:20,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20,615][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.0014401033986359835, acc: 1.0)
[2025-01-06 01:45:20,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20,994][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.0038717181887477636, acc: 1.0)
[2025-01-06 01:45:21,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:21,323][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.0029499230440706015, acc: 1.0)
[2025-01-06 01:45:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:21,678][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.0035849723499268293, acc: 1.0)
[2025-01-06 01:45:21,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:22,020][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.19151246547698975, acc: 0.9696969985961914)
[2025-01-06 01:45:22,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:22,369][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.00040571793215349317, acc: 1.0)
[2025-01-06 01:45:23,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:27,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:27,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52,681][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3589, device='cuda:0') eval_epoch_loss=tensor(0.8582, device='cuda:0') eval_epoch_acc=tensor(0.8407, device='cuda:0')
[2025-01-06 01:45:52,682][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:45:52,683][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:45:53,138][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_415_loss_0.8582107424736023/model.pt
[2025-01-06 01:45:53,142][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:45:53,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53,484][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.05138351395726204, acc: 0.9803921580314636)
[2025-01-06 01:45:53,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53,844][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.026830347254872322, acc: 1.0)
[2025-01-06 01:45:53,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54,235][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.3249455392360687, acc: 0.9444444179534912)
[2025-01-06 01:45:54,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54,594][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.0008444407139904797, acc: 1.0)
[2025-01-06 01:45:54,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54,912][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.12530259788036346, acc: 0.949999988079071)
[2025-01-06 01:45:54,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55,241][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.0018188580870628357, acc: 1.0)
[2025-01-06 01:45:55,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55,601][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.0024456302635371685, acc: 1.0)
[2025-01-06 01:45:55,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55,995][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.18965457379817963, acc: 0.96875)
[2025-01-06 01:45:56,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56,364][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.01689954847097397, acc: 1.0)
[2025-01-06 01:45:56,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56,673][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.0017887079156935215, acc: 1.0)
[2025-01-06 01:45:56,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56,992][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.05111249163746834, acc: 0.9696969985961914)
[2025-01-06 01:45:57,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57,345][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.0010927643161267042, acc: 1.0)
[2025-01-06 01:45:57,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57,726][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.01507255807518959, acc: 1.0)
[2025-01-06 01:45:57,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58,089][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.09058918058872223, acc: 0.9629629850387573)
[2025-01-06 01:45:58,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58,417][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.006644159089773893, acc: 1.0)
[2025-01-06 01:45:58,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58,780][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.007725905627012253, acc: 1.0)
[2025-01-06 01:45:58,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59,146][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.0005515171214938164, acc: 1.0)
[2025-01-06 01:45:59,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59,502][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.0034667341969907284, acc: 1.0)
[2025-01-06 01:45:59,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59,890][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.032014038413763046, acc: 1.0)
[2025-01-06 01:46:00,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00,259][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.000495226529892534, acc: 1.0)
[2025-01-06 01:46:00,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00,601][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.03132608160376549, acc: 0.9696969985961914)
[2025-01-06 01:46:00,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00,965][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.009070155210793018, acc: 1.0)
[2025-01-06 01:46:01,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01,336][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.003200518898665905, acc: 1.0)
[2025-01-06 01:46:01,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01,634][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.003992669750005007, acc: 1.0)
[2025-01-06 01:46:01,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01,945][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.0030055001843720675, acc: 1.0)
[2025-01-06 01:46:02,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02,415][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.07971023768186569, acc: 0.9848484992980957)
[2025-01-06 01:46:02,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03,093][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.22413313388824463, acc: 0.9279999732971191)
[2025-01-06 01:46:03,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03,492][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.1596708446741104, acc: 0.9516128897666931)
[2025-01-06 01:46:03,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04,140][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.15184670686721802, acc: 0.9601989984512329)
[2025-01-06 01:46:04,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04,487][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.08549059927463531, acc: 0.9622641801834106)
[2025-01-06 01:46:04,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04,919][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.010271884500980377, acc: 1.0)
[2025-01-06 01:46:05,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05,235][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.0045910184271633625, acc: 1.0)
[2025-01-06 01:46:05,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05,546][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.006881068926304579, acc: 1.0)
[2025-01-06 01:46:05,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05,913][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.012968932278454304, acc: 1.0)
[2025-01-06 01:46:06,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06,265][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.003675731597468257, acc: 1.0)
[2025-01-06 01:46:06,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06,645][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.008564364165067673, acc: 1.0)
[2025-01-06 01:46:06,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07,033][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.026584478095173836, acc: 0.989130437374115)
[2025-01-06 01:46:07,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07,379][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.011211495846509933, acc: 1.0)
[2025-01-06 01:46:07,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07,697][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.10641469806432724, acc: 0.9605262875556946)
[2025-01-06 01:46:07,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08,031][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.006795104593038559, acc: 1.0)
[2025-01-06 01:46:08,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08,359][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.008837677538394928, acc: 1.0)
[2025-01-06 01:46:08,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08,751][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.11739453673362732, acc: 0.969072163105011)
[2025-01-06 01:46:08,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09,106][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.0047323089092969894, acc: 1.0)
[2025-01-06 01:46:09,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09,512][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.08819825947284698, acc: 0.9825581312179565)
[2025-01-06 01:46:09,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09,871][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.0035860652569681406, acc: 1.0)
[2025-01-06 01:46:09,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10,237][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.054296962916851044, acc: 0.9753086566925049)
[2025-01-06 01:46:10,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10,602][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.00314327422529459, acc: 1.0)
[2025-01-06 01:46:10,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10,980][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.0007547722198069096, acc: 1.0)
[2025-01-06 01:46:11,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11,355][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.0074808294884860516, acc: 1.0)
[2025-01-06 01:46:11,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11,714][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.016743941232562065, acc: 1.0)
[2025-01-06 01:46:11,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12,012][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.04646392911672592, acc: 0.976190447807312)
[2025-01-06 01:46:12,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12,331][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.11659540981054306, acc: 0.9759036302566528)
[2025-01-06 01:46:12,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12,704][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.005689102225005627, acc: 1.0)
[2025-01-06 01:46:12,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13,090][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.0857100784778595, acc: 0.9708737730979919)
[2025-01-06 01:46:13,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13,489][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.06169712916016579, acc: 0.9837398529052734)
[2025-01-06 01:46:13,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13,834][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.006054277997463942, acc: 1.0)
[2025-01-06 01:46:13,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14,189][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.015844101086258888, acc: 1.0)
[2025-01-06 01:46:14,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14,588][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.061634279787540436, acc: 0.9901960492134094)
[2025-01-06 01:46:14,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14,939][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.19552268087863922, acc: 0.9563318490982056)
[2025-01-06 01:46:15,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15,241][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.039355721324682236, acc: 0.9895833134651184)
[2025-01-06 01:46:15,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15,601][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.0656237080693245, acc: 0.9754601120948792)
[2025-01-06 01:46:15,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15,921][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.055001672357320786, acc: 0.9784172773361206)
[2025-01-06 01:46:16,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16,307][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.14083673059940338, acc: 0.9447236061096191)
[2025-01-06 01:46:16,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16,675][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.0592939518392086, acc: 0.9722222089767456)
[2025-01-06 01:46:16,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17,004][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.012182543985545635, acc: 1.0)
[2025-01-06 01:46:17,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17,316][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.00346627039834857, acc: 1.0)
[2025-01-06 01:46:17,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17,595][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.007750650402158499, acc: 1.0)
[2025-01-06 01:46:17,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17,898][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.26151520013809204, acc: 0.949999988079071)
[2025-01-06 01:46:18,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18,288][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.02481386810541153, acc: 1.0)
[2025-01-06 01:46:18,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18,643][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.004658353514969349, acc: 1.0)
[2025-01-06 01:46:18,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19,000][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.002805357798933983, acc: 1.0)
[2025-01-06 01:46:19,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19,331][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.1553017646074295, acc: 0.9629629850387573)
[2025-01-06 01:46:19,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19,641][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.0021668586414307356, acc: 1.0)
[2025-01-06 01:46:19,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20,009][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.4201395809650421, acc: 0.9545454382896423)
[2025-01-06 01:46:20,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20,361][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.058668531477451324, acc: 0.9846153855323792)
[2025-01-06 01:46:20,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20,739][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.004702267237007618, acc: 1.0)
[2025-01-06 01:46:20,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21,106][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.012404787354171276, acc: 1.0)
[2025-01-06 01:46:21,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21,441][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.020038718357682228, acc: 0.9803921580314636)
[2025-01-06 01:46:21,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21,797][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.005159914027899504, acc: 1.0)
[2025-01-06 01:46:21,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22,140][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.0019273224752396345, acc: 1.0)
[2025-01-06 01:46:22,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22,509][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.0071848127990961075, acc: 1.0)
[2025-01-06 01:46:22,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22,881][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.07875948399305344, acc: 0.9553571343421936)
[2025-01-06 01:46:23,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23,285][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.04067270830273628, acc: 0.9887640476226807)
[2025-01-06 01:46:23,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23,668][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.17376495897769928, acc: 0.9550561904907227)
[2025-01-06 01:46:23,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24,013][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.21226447820663452, acc: 0.9432623982429504)
[2025-01-06 01:46:24,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24,401][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.19580137729644775, acc: 0.9347826242446899)
[2025-01-06 01:46:24,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24,772][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.0003006323822773993, acc: 1.0)
[2025-01-06 01:46:24,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25,137][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.00018690506112761796, acc: 1.0)
[2025-01-06 01:46:25,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25,499][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.11169376969337463, acc: 0.9629629850387573)
[2025-01-06 01:46:25,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25,825][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.06387251615524292, acc: 0.9629629850387573)
[2025-01-06 01:46:25,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26,116][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.05629126355051994, acc: 0.9811320900917053)
[2025-01-06 01:46:26,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26,424][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.012528615072369576, acc: 1.0)
[2025-01-06 01:46:26,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27,006][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.29093509912490845, acc: 0.9099099040031433)
[2025-01-06 01:46:27,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27,441][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.09906763583421707, acc: 0.9436619877815247)
[2025-01-06 01:46:27,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27,742][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.0013162099057808518, acc: 1.0)
[2025-01-06 01:46:27,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28,075][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.012768294662237167, acc: 1.0)
[2025-01-06 01:46:28,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28,418][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.0038779578171670437, acc: 1.0)
[2025-01-06 01:46:29,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31,121][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.1458173543214798, acc: 0.9428571462631226)
[2025-01-06 01:46:31,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31,880][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.024399401620030403, acc: 0.9920634627342224)
[2025-01-06 01:46:31,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32,239][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.023487064987421036, acc: 1.0)
[2025-01-06 01:46:32,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32,614][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.004872446414083242, acc: 1.0)
[2025-01-06 01:46:32,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33,303][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.05371077358722687, acc: 0.9722222089767456)
[2025-01-06 01:46:33,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33,613][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.00021758103684987873, acc: 1.0)
[2025-01-06 01:46:33,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33,918][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.0030901602003723383, acc: 1.0)
[2025-01-06 01:46:33,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34,219][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.017522919923067093, acc: 1.0)
[2025-01-06 01:46:34,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34,520][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.005920272320508957, acc: 1.0)
[2025-01-06 01:46:34,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:35,524][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.17878864705562592, acc: 0.9406779408454895)
[2025-01-06 01:46:35,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:35,898][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.0045841410756111145, acc: 1.0)
[2025-01-06 01:46:36,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:36,297][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.07738455384969711, acc: 0.970802903175354)
[2025-01-06 01:46:36,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:36,870][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.13130050897598267, acc: 0.9599999785423279)
[2025-01-06 01:46:36,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:37,172][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.002842443995177746, acc: 1.0)
[2025-01-06 01:46:37,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:37,550][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.008282424882054329, acc: 1.0)
[2025-01-06 01:46:37,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:37,917][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.008290235884487629, acc: 1.0)
[2025-01-06 01:46:38,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38,258][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.06961189210414886, acc: 0.9836065769195557)
[2025-01-06 01:46:38,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38,569][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.009712470695376396, acc: 1.0)
[2025-01-06 01:46:38,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38,909][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.07528164982795715, acc: 0.9767441749572754)
[2025-01-06 01:46:39,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39,281][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.09504590183496475, acc: 0.9545454382896423)
[2025-01-06 01:46:39,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39,573][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.0478450208902359, acc: 1.0)
[2025-01-06 01:46:39,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39,860][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.031161339953541756, acc: 1.0)
[2025-01-06 01:46:39,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40,168][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.019078118726611137, acc: 1.0)
[2025-01-06 01:46:40,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40,553][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.0006802105344831944, acc: 1.0)
[2025-01-06 01:46:40,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40,928][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.008543088100850582, acc: 1.0)
[2025-01-06 01:46:41,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41,321][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.10323863476514816, acc: 0.9692307710647583)
[2025-01-06 01:46:41,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41,664][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.021302562206983566, acc: 1.0)
[2025-01-06 01:46:41,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42,052][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.014322265982627869, acc: 1.0)
[2025-01-06 01:46:42,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42,420][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.012161806225776672, acc: 1.0)
[2025-01-06 01:46:42,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42,756][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.048624925315380096, acc: 1.0)
[2025-01-06 01:46:42,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43,066][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.003400871530175209, acc: 1.0)
[2025-01-06 01:46:43,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43,457][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.11292660981416702, acc: 0.95652174949646)
[2025-01-06 01:46:43,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43,803][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.22992680966854095, acc: 0.9333333373069763)
[2025-01-06 01:46:43,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44,171][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.0065188175067305565, acc: 1.0)
[2025-01-06 01:46:44,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44,524][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.0007665461744181812, acc: 1.0)
[2025-01-06 01:46:44,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44,894][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.015606217086315155, acc: 1.0)
[2025-01-06 01:46:45,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45,233][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.001585738966241479, acc: 1.0)
[2025-01-06 01:46:45,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45,591][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.0010610786266624928, acc: 1.0)
[2025-01-06 01:46:45,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45,961][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.005416269414126873, acc: 1.0)
[2025-01-06 01:46:46,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46,328][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.028837066143751144, acc: 0.9750000238418579)
[2025-01-06 01:46:46,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46,682][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.1248411014676094, acc: 0.9714285731315613)
[2025-01-06 01:46:46,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47,045][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.03082343004643917, acc: 0.985401451587677)
[2025-01-06 01:46:47,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47,370][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.014466311782598495, acc: 0.9931034445762634)
[2025-01-06 01:46:47,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47,737][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.025142820551991463, acc: 0.9857142567634583)
[2025-01-06 01:46:47,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48,119][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.052334997802972794, acc: 0.9735099077224731)
[2025-01-06 01:46:48,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48,462][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.015231844037771225, acc: 0.9914529919624329)
[2025-01-06 01:46:49,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18,933][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3389, device='cuda:0') eval_epoch_loss=tensor(0.8497, device='cuda:0') eval_epoch_acc=tensor(0.8424, device='cuda:0')
[2025-01-06 01:47:18,934][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:47:18,934][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:47:19,175][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_558_loss_0.849675178527832/model.pt
[2025-01-06 01:47:19,179][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:47:19,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19,568][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.008727817796170712, acc: 1.0)
[2025-01-06 01:47:19,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19,947][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.022611528635025024, acc: 1.0)
[2025-01-06 01:47:20,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20,322][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.0006678560166619718, acc: 1.0)
[2025-01-06 01:47:20,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20,663][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.14627684652805328, acc: 0.9743589758872986)
[2025-01-06 01:47:20,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21,041][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.22301432490348816, acc: 0.9555555582046509)
[2025-01-06 01:47:21,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21,387][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.012065861374139786, acc: 1.0)
[2025-01-06 01:47:21,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21,756][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.038624998182058334, acc: 0.9791666865348816)
[2025-01-06 01:47:21,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22,094][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.06342020630836487, acc: 0.982758641242981)
[2025-01-06 01:47:22,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22,476][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.01206880621612072, acc: 1.0)
[2025-01-06 01:47:22,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22,849][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.0011823448585346341, acc: 1.0)
[2025-01-06 01:47:22,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23,207][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.004882153123617172, acc: 1.0)
[2025-01-06 01:47:23,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23,597][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.06131460890173912, acc: 0.9893048405647278)
[2025-01-06 01:47:23,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23,961][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.0007885186350904405, acc: 1.0)
[2025-01-06 01:47:24,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24,339][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.007590972352772951, acc: 1.0)
[2025-01-06 01:47:24,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24,672][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.051294758915901184, acc: 0.9897959232330322)
[2025-01-06 01:47:24,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25,015][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.0457347109913826, acc: 0.9811320900917053)
[2025-01-06 01:47:25,436][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.0582, train_epoch_loss=0.0565, epoch time 352.5695807337761s
[2025-01-06 01:47:25,437][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:47:25,437][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:47:25,437][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:47:25,437][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 22
[2025-01-06 01:47:25,437][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:47:25,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26,289][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.0023318917956203222, acc: 1.0)
[2025-01-06 01:47:26,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26,638][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.002202319446951151, acc: 1.0)
[2025-01-06 01:47:26,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26,942][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.02475140057504177, acc: 1.0)
[2025-01-06 01:47:27,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27,297][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.05095674470067024, acc: 0.9736841917037964)
[2025-01-06 01:47:27,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27,696][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.015289321541786194, acc: 1.0)
[2025-01-06 01:47:27,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28,015][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.009136230684816837, acc: 1.0)
[2025-01-06 01:47:28,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28,351][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.04227451980113983, acc: 0.9795918464660645)
[2025-01-06 01:47:28,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28,657][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.00458995345979929, acc: 1.0)
[2025-01-06 01:47:28,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28,994][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.0009745506104081869, acc: 1.0)
[2025-01-06 01:47:29,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29,335][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.0011771071003749967, acc: 1.0)
[2025-01-06 01:47:29,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29,704][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.0011743897339329123, acc: 1.0)
[2025-01-06 01:47:29,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30,082][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.0028595563489943743, acc: 1.0)
[2025-01-06 01:47:30,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30,481][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.0014286440564319491, acc: 1.0)
[2025-01-06 01:47:30,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30,885][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.0041366503573954105, acc: 1.0)
[2025-01-06 01:47:30,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31,262][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.017680643126368523, acc: 1.0)
[2025-01-06 01:47:31,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31,615][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.02607795223593712, acc: 0.9795918464660645)
[2025-01-06 01:47:31,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31,924][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.0013892841525375843, acc: 1.0)
[2025-01-06 01:47:31,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32,280][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.02535981871187687, acc: 1.0)
[2025-01-06 01:47:32,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32,650][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.006156274117529392, acc: 1.0)
[2025-01-06 01:47:32,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32,995][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.046487342566251755, acc: 0.9473684430122375)
[2025-01-06 01:47:33,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33,370][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.0027320210356265306, acc: 1.0)
[2025-01-06 01:47:33,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33,715][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.0037227803841233253, acc: 1.0)
[2025-01-06 01:47:33,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34,103][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.04440680891275406, acc: 0.9599999785423279)
[2025-01-06 01:47:34,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34,429][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.005047729704529047, acc: 1.0)
[2025-01-06 01:47:34,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34,815][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.0006259400397539139, acc: 1.0)
[2025-01-06 01:47:34,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35,189][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.01579407788813114, acc: 1.0)
[2025-01-06 01:47:35,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35,571][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.05575505271553993, acc: 0.9863013625144958)
[2025-01-06 01:47:36,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:36,836][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.2506677508354187, acc: 0.9209486246109009)
[2025-01-06 01:47:36,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37,194][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.010805551894009113, acc: 1.0)
[2025-01-06 01:47:37,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37,543][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.0551212914288044, acc: 0.9759036302566528)
[2025-01-06 01:47:37,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37,881][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.01813134178519249, acc: 1.0)
[2025-01-06 01:47:37,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38,237][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.012551786378026009, acc: 1.0)
[2025-01-06 01:47:38,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38,569][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.0018703421810641885, acc: 1.0)
[2025-01-06 01:47:38,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38,889][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.002599524799734354, acc: 1.0)
[2025-01-06 01:47:38,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39,252][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.02626294270157814, acc: 0.9915966391563416)
[2025-01-06 01:47:39,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39,610][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.004134449642151594, acc: 1.0)
[2025-01-06 01:47:39,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39,944][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.022225165739655495, acc: 1.0)
[2025-01-06 01:47:40,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40,299][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.021762609481811523, acc: 1.0)
[2025-01-06 01:47:40,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40,654][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.053510308265686035, acc: 0.9885057210922241)
[2025-01-06 01:47:40,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41,013][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.0071692923083901405, acc: 1.0)
[2025-01-06 01:47:41,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41,405][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.06265123933553696, acc: 0.9615384340286255)
[2025-01-06 01:47:41,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41,773][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.053542207926511765, acc: 0.9729729890823364)
[2025-01-06 01:47:41,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42,166][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.06297094374895096, acc: 0.9538461565971375)
[2025-01-06 01:47:42,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42,624][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.06488726288080215, acc: 0.9797979593276978)
[2025-01-06 01:47:42,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43,052][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.0701722726225853, acc: 0.9896907210350037)
[2025-01-06 01:47:43,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43,457][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.04113219678401947, acc: 0.9852941036224365)
[2025-01-06 01:47:43,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43,806][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.2855221927165985, acc: 0.9615384340286255)
[2025-01-06 01:47:43,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44,123][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.0021900152787566185, acc: 1.0)
[2025-01-06 01:47:44,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44,466][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.0030058010015636683, acc: 1.0)
[2025-01-06 01:47:44,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44,860][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.00039607807411812246, acc: 1.0)
[2025-01-06 01:47:45,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45,267][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.0360187366604805, acc: 0.9824561476707458)
[2025-01-06 01:47:45,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45,619][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.029832933098077774, acc: 0.9841269850730896)
[2025-01-06 01:47:45,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45,984][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.12556777894496918, acc: 0.9577465057373047)
[2025-01-06 01:47:46,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46,464][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.354116827249527, acc: 0.9066666960716248)
[2025-01-06 01:47:46,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46,851][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.011706686578691006, acc: 1.0)
[2025-01-06 01:47:46,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:47,186][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.0025225868448615074, acc: 1.0)
[2025-01-06 01:47:48,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50,208][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.3456701338291168, acc: 0.8873720169067383)
[2025-01-06 01:47:50,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51,407][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 0.49725449085235596, acc: 0.8540304899215698)
[2025-01-06 01:47:51,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52,029][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.16303877532482147, acc: 0.9488636255264282)
[2025-01-06 01:47:52,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52,603][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.013490524142980576, acc: 1.0)
[2025-01-06 01:47:52,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53,165][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.17361100018024445, acc: 0.9420289993286133)
[2025-01-06 01:47:53,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53,569][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.0459863618016243, acc: 0.987500011920929)
[2025-01-06 01:47:53,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53,904][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.0016326751792803407, acc: 1.0)
[2025-01-06 01:47:54,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54,255][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.0019313833909109235, acc: 1.0)
[2025-01-06 01:47:54,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54,603][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.00495602423325181, acc: 1.0)
[2025-01-06 01:47:54,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54,943][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.002693323651328683, acc: 1.0)
[2025-01-06 01:47:55,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55,297][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.0407169945538044, acc: 0.9821428656578064)
[2025-01-06 01:47:55,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55,672][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.007223390508443117, acc: 1.0)
[2025-01-06 01:47:55,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56,018][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.0004213885113131255, acc: 1.0)
[2025-01-06 01:47:56,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56,372][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.07289334386587143, acc: 0.9722222089767456)
[2025-01-06 01:47:56,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56,708][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.009813360869884491, acc: 1.0)
[2025-01-06 01:47:56,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57,075][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.2325371503829956, acc: 0.9485294222831726)
[2025-01-06 01:47:57,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57,415][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.07152356952428818, acc: 0.9841269850730896)
[2025-01-06 01:47:57,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57,789][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.2565337121486664, acc: 0.9333333373069763)
[2025-01-06 01:47:57,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58,186][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.024523576721549034, acc: 0.9897959232330322)
[2025-01-06 01:47:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58,516][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.09575251489877701, acc: 0.9776119589805603)
[2025-01-06 01:47:58,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58,928][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 0.3300589919090271, acc: 0.8759124279022217)
[2025-01-06 01:47:59,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59,296][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.00029621447902172804, acc: 1.0)
[2025-01-06 01:47:59,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59,666][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.0008827997371554375, acc: 1.0)
[2025-01-06 01:47:59,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00,039][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.001734934514388442, acc: 1.0)
[2025-01-06 01:48:00,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00,428][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.0006043206085450947, acc: 1.0)
[2025-01-06 01:48:00,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00,786][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.006964616011828184, acc: 1.0)
[2025-01-06 01:48:00,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01,131][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.020212896168231964, acc: 1.0)
[2025-01-06 01:48:01,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01,506][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.523918867111206, acc: 0.9375)
[2025-01-06 01:48:01,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01,884][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.035630982369184494, acc: 0.9855072498321533)
[2025-01-06 01:48:02,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02,296][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.0671144500374794, acc: 0.9800000190734863)
[2025-01-06 01:48:02,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02,633][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.000955873285420239, acc: 1.0)
[2025-01-06 01:48:02,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:03,101][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.16895098984241486, acc: 0.9599999785423279)
[2025-01-06 01:48:03,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:03,488][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.09068118780851364, acc: 0.9611650705337524)
[2025-01-06 01:48:03,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:04,605][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.12137433141469955, acc: 0.946601927280426)
[2025-01-06 01:48:04,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:05,420][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.22648905217647552, acc: 0.9247311949729919)
[2025-01-06 01:48:05,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06,218][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.2727415859699249, acc: 0.9396551847457886)
[2025-01-06 01:48:06,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06,960][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.04929584264755249, acc: 0.9894737005233765)
[2025-01-06 01:48:07,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:07,950][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.2288847118616104, acc: 0.9504950642585754)
[2025-01-06 01:48:08,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08,259][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.02764211781322956, acc: 1.0)
[2025-01-06 01:48:08,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08,637][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.06414689868688583, acc: 0.9710144996643066)
[2025-01-06 01:48:08,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08,967][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.15488281846046448, acc: 0.9495798349380493)
[2025-01-06 01:48:09,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09,329][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.08665701001882553, acc: 0.9807692170143127)
[2025-01-06 01:48:09,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09,716][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.059117142111063004, acc: 0.985401451587677)
[2025-01-06 01:48:09,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10,025][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.20091009140014648, acc: 0.9402984976768494)
[2025-01-06 01:48:10,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10,378][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.007883453741669655, acc: 1.0)
[2025-01-06 01:48:10,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10,760][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.00036116529372520745, acc: 1.0)
[2025-01-06 01:48:10,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11,105][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.002710917964577675, acc: 1.0)
[2025-01-06 01:48:11,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11,505][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.052861712872982025, acc: 0.9772727489471436)
[2025-01-06 01:48:11,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11,842][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.02143961936235428, acc: 1.0)
[2025-01-06 01:48:11,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12,196][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.014828967861831188, acc: 1.0)
[2025-01-06 01:48:12,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12,566][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.0068488698452711105, acc: 1.0)
[2025-01-06 01:48:12,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12,939][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.0001314463297603652, acc: 1.0)
[2025-01-06 01:48:13,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13,297][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.00034013419644907117, acc: 1.0)
[2025-01-06 01:48:13,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13,687][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.0020339603070169687, acc: 1.0)
[2025-01-06 01:48:13,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14,069][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.0024445257149636745, acc: 1.0)
[2025-01-06 01:48:14,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14,488][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.01220853254199028, acc: 1.0)
[2025-01-06 01:48:14,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14,896][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.11201956868171692, acc: 0.9649122953414917)
[2025-01-06 01:48:15,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15,270][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.11624018102884293, acc: 0.9487179517745972)
[2025-01-06 01:48:15,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15,649][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.004647546447813511, acc: 1.0)
[2025-01-06 01:48:15,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16,037][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.0013064263621345162, acc: 1.0)
[2025-01-06 01:48:16,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16,423][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.062005944550037384, acc: 0.9841269850730896)
[2025-01-06 01:48:16,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16,840][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.02193031832575798, acc: 0.9918699264526367)
[2025-01-06 01:48:16,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17,172][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.006329917814582586, acc: 1.0)
[2025-01-06 01:48:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18,018][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.12903787195682526, acc: 0.9733840227127075)
[2025-01-06 01:48:18,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18,368][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.04046162590384483, acc: 0.9866666793823242)
[2025-01-06 01:48:18,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18,769][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.0047364868223667145, acc: 1.0)
[2025-01-06 01:48:18,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19,067][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.0010183979757130146, acc: 1.0)
[2025-01-06 01:48:19,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19,359][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.0016087971162050962, acc: 1.0)
[2025-01-06 01:48:19,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19,696][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.09186072647571564, acc: 0.9815950989723206)
[2025-01-06 01:48:19,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20,052][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.05512479320168495, acc: 0.9861111044883728)
[2025-01-06 01:48:20,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20,370][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.17084594070911407, acc: 0.949999988079071)
[2025-01-06 01:48:21,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51,345][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2247, device='cuda:0') eval_epoch_loss=tensor(0.7996, device='cuda:0') eval_epoch_acc=tensor(0.8509, device='cuda:0')
[2025-01-06 01:48:51,346][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:48:51,346][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:48:51,584][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_127_loss_0.7996403574943542/model.pt
[2025-01-06 01:48:51,587][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:48:51,588][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 9 is 0.8509207367897034
[2025-01-06 01:48:51,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51,997][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.06510607898235321, acc: 0.976190447807312)
[2025-01-06 01:48:52,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52,312][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.1529974341392517, acc: 0.9692307710647583)
[2025-01-06 01:48:52,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52,702][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.07696311920881271, acc: 0.9852941036224365)
[2025-01-06 01:48:52,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53,086][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.034159280359745026, acc: 1.0)
[2025-01-06 01:48:53,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53,417][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.002500210190191865, acc: 1.0)
[2025-01-06 01:48:53,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53,744][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.0036162161268293858, acc: 1.0)
[2025-01-06 01:48:53,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54,061][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.0024046937469393015, acc: 1.0)
[2025-01-06 01:48:54,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54,458][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.14647988975048065, acc: 0.9714285731315613)
[2025-01-06 01:48:54,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54,797][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.004260709509253502, acc: 1.0)
[2025-01-06 01:48:54,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55,129][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.005410739220678806, acc: 1.0)
[2025-01-06 01:48:55,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55,447][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.2017696499824524, acc: 0.9666666388511658)
[2025-01-06 01:48:55,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55,750][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.0012679891660809517, acc: 1.0)
[2025-01-06 01:48:55,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56,056][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.0014102112036198378, acc: 1.0)
[2025-01-06 01:48:56,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56,398][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.0011401831870898604, acc: 1.0)
[2025-01-06 01:48:56,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56,731][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.0026681546587496996, acc: 1.0)
[2025-01-06 01:48:56,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57,106][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.015393493697047234, acc: 1.0)
[2025-01-06 01:48:57,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57,649][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.07024964690208435, acc: 0.9649122953414917)
[2025-01-06 01:48:57,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58,031][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.11084144562482834, acc: 0.9626865386962891)
[2025-01-06 01:48:58,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58,419][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.03712652251124382, acc: 0.9897959232330322)
[2025-01-06 01:48:58,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58,861][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.24715681374073029, acc: 0.9042553305625916)
[2025-01-06 01:48:58,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59,183][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.03049507550895214, acc: 0.9857142567634583)
[2025-01-06 01:48:59,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59,542][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.04953545331954956, acc: 1.0)
[2025-01-06 01:48:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59,924][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.0012346070725470781, acc: 1.0)
[2025-01-06 01:49:00,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00,263][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.010959151200950146, acc: 1.0)
[2025-01-06 01:49:00,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00,586][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.01810169219970703, acc: 1.0)
[2025-01-06 01:49:00,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00,862][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.0095845190808177, acc: 1.0)
[2025-01-06 01:49:00,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01,217][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.02366291731595993, acc: 1.0)
[2025-01-06 01:49:01,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01,554][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.06229257583618164, acc: 0.9729729890823364)
[2025-01-06 01:49:01,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01,846][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.02259550429880619, acc: 1.0)
[2025-01-06 01:49:01,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02,137][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.19739973545074463, acc: 0.95652174949646)
[2025-01-06 01:49:02,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02,500][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.5817663073539734, acc: 0.8421052694320679)
[2025-01-06 01:49:03,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04,099][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.19085459411144257, acc: 0.9324324131011963)
[2025-01-06 01:49:04,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04,454][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 0.31036967039108276, acc: 0.9444444179534912)
[2025-01-06 01:49:04,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04,852][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.16525088250637054, acc: 0.9418604373931885)
[2025-01-06 01:49:05,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05,439][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.17217670381069183, acc: 0.9529411792755127)
[2025-01-06 01:49:05,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05,993][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.1313820332288742, acc: 0.966292142868042)
[2025-01-06 01:49:06,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06,335][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.018539678305387497, acc: 1.0)
[2025-01-06 01:49:06,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06,651][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.006765895057469606, acc: 1.0)
[2025-01-06 01:49:06,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06,984][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.11461041122674942, acc: 0.931034505367279)
[2025-01-06 01:49:07,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07,332][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.003803685074672103, acc: 1.0)
[2025-01-06 01:49:07,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07,712][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.02905714325606823, acc: 0.9800000190734863)
[2025-01-06 01:49:07,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08,111][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.01618359051644802, acc: 1.0)
[2025-01-06 01:49:08,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08,491][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.09439052641391754, acc: 0.9607843160629272)
[2025-01-06 01:49:08,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09,525][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.18059618771076202, acc: 0.9452054500579834)
[2025-01-06 01:49:09,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09,845][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.0011626621708273888, acc: 1.0)
[2025-01-06 01:49:09,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10,222][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.09340845048427582, acc: 0.9629629850387573)
[2025-01-06 01:49:10,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10,589][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.020640024915337563, acc: 1.0)
[2025-01-06 01:49:10,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11,147][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.18011268973350525, acc: 0.9380530714988708)
[2025-01-06 01:49:11,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11,445][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.1395214945077896, acc: 0.9420289993286133)
[2025-01-06 01:49:11,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11,790][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.02590312995016575, acc: 0.9886363744735718)
[2025-01-06 01:49:12,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12,707][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.13068102300167084, acc: 0.9389312863349915)
[2025-01-06 01:49:12,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13,375][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.07331464439630508, acc: 0.9777777791023254)
[2025-01-06 01:49:13,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13,731][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.020951732993125916, acc: 1.0)
[2025-01-06 01:49:13,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14,069][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.004875510465353727, acc: 1.0)
[2025-01-06 01:49:14,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14,436][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.0019043725915253162, acc: 1.0)
[2025-01-06 01:49:14,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14,777][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.0015138277085497975, acc: 1.0)
[2025-01-06 01:49:14,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15,065][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.008992145769298077, acc: 1.0)
[2025-01-06 01:49:15,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15,443][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.08566267788410187, acc: 0.9728096723556519)
[2025-01-06 01:49:15,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15,856][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.0731973648071289, acc: 0.9827089309692383)
[2025-01-06 01:49:15,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16,334][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.04633525386452675, acc: 0.984375)
[2025-01-06 01:49:16,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16,858][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.14881165325641632, acc: 0.9474671483039856)
[2025-01-06 01:49:16,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17,262][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.05551288649439812, acc: 0.9857650995254517)
[2025-01-06 01:49:17,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17,643][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.017590614035725594, acc: 1.0)
[2025-01-06 01:49:17,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18,190][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.07463519275188446, acc: 0.9534883499145508)
[2025-01-06 01:49:18,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18,989][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.16336609423160553, acc: 0.9365079402923584)
[2025-01-06 01:49:19,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19,909][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.16615013778209686, acc: 0.939393937587738)
[2025-01-06 01:49:20,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20,652][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.1502813845872879, acc: 0.9647058844566345)
[2025-01-06 01:49:20,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21,727][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.15651196241378784, acc: 0.9691358208656311)
[2025-01-06 01:49:21,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22,676][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.05754515528678894, acc: 0.9838709831237793)
[2025-01-06 01:49:22,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22,948][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.002445041900500655, acc: 1.0)
[2025-01-06 01:49:23,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23,238][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.044423360377550125, acc: 0.9750000238418579)
[2025-01-06 01:49:23,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23,626][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.018534788861870766, acc: 1.0)
[2025-01-06 01:49:23,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24,022][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.09607309848070145, acc: 0.970588207244873)
[2025-01-06 01:49:24,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24,380][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.1273832619190216, acc: 0.9491525292396545)
[2025-01-06 01:49:24,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24,687][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.061061397194862366, acc: 0.9925373196601868)
[2025-01-06 01:49:24,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25,028][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.08518365025520325, acc: 0.9708737730979919)
[2025-01-06 01:49:25,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25,342][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.006756216753274202, acc: 1.0)
[2025-01-06 01:49:25,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25,650][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.0023612945806235075, acc: 1.0)
[2025-01-06 01:49:25,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26,031][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.020346375182271004, acc: 0.9910314083099365)
[2025-01-06 01:49:26,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26,464][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.03587644547224045, acc: 0.9842519760131836)
[2025-01-06 01:49:26,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26,826][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.012935309670865536, acc: 1.0)
[2025-01-06 01:49:26,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27,196][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.06195973604917526, acc: 0.9855072498321533)
[2025-01-06 01:49:27,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27,565][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.03647862374782562, acc: 0.9883268475532532)
[2025-01-06 01:49:27,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27,942][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.039871424436569214, acc: 0.989130437374115)
[2025-01-06 01:49:28,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28,311][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.007325244136154652, acc: 1.0)
[2025-01-06 01:49:28,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28,656][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.002167736878618598, acc: 1.0)
[2025-01-06 01:49:28,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29,059][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.02367938868701458, acc: 0.978723406791687)
[2025-01-06 01:49:29,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29,738][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.05386098474264145, acc: 0.9923076629638672)
[2025-01-06 01:49:29,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30,064][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.006561394315212965, acc: 1.0)
[2025-01-06 01:49:30,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30,385][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.020653843879699707, acc: 0.9883720874786377)
[2025-01-06 01:49:30,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30,927][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.011911244131624699, acc: 1.0)
[2025-01-06 01:49:31,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31,318][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.005106302909553051, acc: 1.0)
[2025-01-06 01:49:31,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31,661][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.0071328068152070045, acc: 1.0)
[2025-01-06 01:49:31,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32,023][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.002067963359877467, acc: 1.0)
[2025-01-06 01:49:32,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32,363][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.0003405326569918543, acc: 1.0)
[2025-01-06 01:49:32,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32,744][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.00680173747241497, acc: 1.0)
[2025-01-06 01:49:32,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33,525][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.07890794426202774, acc: 0.97826087474823)
[2025-01-06 01:49:33,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34,062][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.07669834792613983, acc: 0.9829545617103577)
[2025-01-06 01:49:34,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34,489][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.04139527678489685, acc: 0.9893617033958435)
[2025-01-06 01:49:34,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34,833][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.06303026527166367, acc: 0.9622641801834106)
[2025-01-06 01:49:34,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35,175][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.014667356386780739, acc: 1.0)
[2025-01-06 01:49:35,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35,567][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.098067507147789, acc: 0.9534883499145508)
[2025-01-06 01:49:35,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35,977][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.1669103056192398, acc: 0.8999999761581421)
[2025-01-06 01:49:36,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36,385][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.4913907051086426, acc: 0.8842105269432068)
[2025-01-06 01:49:36,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36,781][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.2734322249889374, acc: 0.8999999761581421)
[2025-01-06 01:49:36,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37,204][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.3862982392311096, acc: 0.894444465637207)
[2025-01-06 01:49:37,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37,690][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 0.32544171810150146, acc: 0.8899082541465759)
[2025-01-06 01:49:37,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38,160][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.19489197432994843, acc: 0.9384615421295166)
[2025-01-06 01:49:38,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38,502][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.06230950355529785, acc: 0.9473684430122375)
[2025-01-06 01:49:38,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38,825][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.026147177442908287, acc: 1.0)
[2025-01-06 01:49:38,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39,156][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.11713993549346924, acc: 0.9545454382896423)
[2025-01-06 01:49:39,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39,486][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.09192696213722229, acc: 0.9629629850387573)
[2025-01-06 01:49:39,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39,798][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.033149056136608124, acc: 0.9714285731315613)
[2025-01-06 01:49:39,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40,137][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.03028736263513565, acc: 1.0)
[2025-01-06 01:49:40,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40,511][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.03870264068245888, acc: 1.0)
[2025-01-06 01:49:40,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:41,089][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.08320695906877518, acc: 0.9838709831237793)
[2025-01-06 01:49:41,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:41,610][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.13543573021888733, acc: 0.9772727489471436)
[2025-01-06 01:49:41,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:41,945][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.0001269435160793364, acc: 1.0)
[2025-01-06 01:49:42,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42,252][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.07473281025886536, acc: 0.9615384340286255)
[2025-01-06 01:49:42,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42,550][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.001028776285238564, acc: 1.0)
[2025-01-06 01:49:42,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42,916][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.017513196915388107, acc: 1.0)
[2025-01-06 01:49:43,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43,288][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.02775946818292141, acc: 1.0)
[2025-01-06 01:49:43,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43,660][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.018159693107008934, acc: 1.0)
[2025-01-06 01:49:43,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43,992][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.0006953873089514673, acc: 1.0)
[2025-01-06 01:49:44,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44,362][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.004771877080202103, acc: 1.0)
[2025-01-06 01:49:44,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44,736][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.2497062385082245, acc: 0.9756097793579102)
[2025-01-06 01:49:44,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45,053][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.019603382796049118, acc: 1.0)
[2025-01-06 01:49:45,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45,350][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.00025277090026065707, acc: 1.0)
[2025-01-06 01:49:45,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45,645][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.027462242171168327, acc: 0.9677419066429138)
[2025-01-06 01:49:45,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45,959][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.01896912418305874, acc: 1.0)
[2025-01-06 01:49:46,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46,314][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.012288237921893597, acc: 1.0)
[2025-01-06 01:49:46,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46,718][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.0014467529254034162, acc: 1.0)
[2025-01-06 01:49:46,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47,280][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.07832694798707962, acc: 0.9905660152435303)
[2025-01-06 01:49:47,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47,860][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.07237011939287186, acc: 0.9916666746139526)
[2025-01-06 01:49:47,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48,221][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.0136287622153759, acc: 1.0)
[2025-01-06 01:49:48,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48,609][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.04214547947049141, acc: 1.0)
[2025-01-06 01:49:48,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48,964][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.11067694425582886, acc: 0.9466666579246521)
[2025-01-06 01:49:49,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:49,352][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.27173686027526855, acc: 0.9583333134651184)
[2025-01-06 01:49:49,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50,191][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.4203970432281494, acc: 0.8960000276565552)
[2025-01-06 01:49:50,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50,553][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.10584487020969391, acc: 0.966292142868042)
[2025-01-06 01:49:50,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50,912][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.23028641939163208, acc: 0.9459459185600281)
[2025-01-06 01:49:51,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51,374][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.05569716542959213, acc: 1.0)
[2025-01-06 01:49:51,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51,724][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.009891618974506855, acc: 1.0)
[2025-01-06 01:49:52,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:54,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:54,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:22,228][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4981, device='cuda:0') eval_epoch_loss=tensor(0.9155, device='cuda:0') eval_epoch_acc=tensor(0.8340, device='cuda:0')
[2025-01-06 01:50:22,229][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:50:22,230][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:50:22,467][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_270_loss_0.9155341982841492/model.pt
[2025-01-06 01:50:22,470][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:50:22,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:22,867][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.010326758027076721, acc: 1.0)
[2025-01-06 01:50:22,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23,247][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.0015931129455566406, acc: 1.0)
[2025-01-06 01:50:23,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23,622][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.01843799464404583, acc: 1.0)
[2025-01-06 01:50:23,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24,020][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.17333875596523285, acc: 0.9333333373069763)
[2025-01-06 01:50:24,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24,341][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.010880783200263977, acc: 1.0)
[2025-01-06 01:50:24,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24,673][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.09339175373315811, acc: 0.9666666388511658)
[2025-01-06 01:50:24,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24,983][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.00569754047319293, acc: 1.0)
[2025-01-06 01:50:25,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25,344][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.002490320708602667, acc: 1.0)
[2025-01-06 01:50:25,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25,689][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.01962130516767502, acc: 1.0)
[2025-01-06 01:50:25,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26,073][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.06325802952051163, acc: 0.9791666865348816)
[2025-01-06 01:50:26,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26,446][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.0014208705397322774, acc: 1.0)
[2025-01-06 01:50:26,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26,889][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.15091606974601746, acc: 0.9518072009086609)
[2025-01-06 01:50:27,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27,258][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.10316883772611618, acc: 0.9722222089767456)
[2025-01-06 01:50:27,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27,534][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.003362490562722087, acc: 1.0)
[2025-01-06 01:50:27,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27,837][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.01142089907079935, acc: 1.0)
[2025-01-06 01:50:27,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28,231][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.011951452121138573, acc: 1.0)
[2025-01-06 01:50:28,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28,573][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.029136260971426964, acc: 0.9921875)
[2025-01-06 01:50:28,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28,953][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.04674956202507019, acc: 0.984000027179718)
[2025-01-06 01:50:29,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29,337][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.010808857157826424, acc: 1.0)
[2025-01-06 01:50:29,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29,701][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.05178489908576012, acc: 0.9813664555549622)
[2025-01-06 01:50:29,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30,054][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.05572180822491646, acc: 0.9793814420700073)
[2025-01-06 01:50:30,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30,386][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.248528391122818, acc: 0.9545454382896423)
[2025-01-06 01:50:30,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30,722][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.028461305424571037, acc: 0.976190447807312)
[2025-01-06 01:50:30,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31,058][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.037491366267204285, acc: 0.9655172228813171)
[2025-01-06 01:50:31,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31,547][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.03433050960302353, acc: 0.9818181991577148)
[2025-01-06 01:50:31,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32,094][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.09349681437015533, acc: 0.9639175534248352)
[2025-01-06 01:50:32,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32,415][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.0459492988884449, acc: 0.9655172228813171)
[2025-01-06 01:50:32,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32,752][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.005638149566948414, acc: 1.0)
[2025-01-06 01:50:32,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33,107][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.024521617218852043, acc: 1.0)
[2025-01-06 01:50:33,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33,448][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.0027771475724875927, acc: 1.0)
[2025-01-06 01:50:33,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33,831][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.06959494948387146, acc: 0.96875)
[2025-01-06 01:50:33,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34,152][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.001992092002183199, acc: 1.0)
[2025-01-06 01:50:34,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34,518][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.00046529454994015396, acc: 1.0)
[2025-01-06 01:50:34,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34,874][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.009521961212158203, acc: 1.0)
[2025-01-06 01:50:34,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35,186][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.0022263091523200274, acc: 1.0)
[2025-01-06 01:50:35,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35,553][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.08956222981214523, acc: 0.9836065769195557)
[2025-01-06 01:50:35,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35,883][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.0017444471595808864, acc: 1.0)
[2025-01-06 01:50:35,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36,175][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.0002545354072935879, acc: 1.0)
[2025-01-06 01:50:36,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36,522][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.03178253397345543, acc: 0.9710144996643066)
[2025-01-06 01:50:36,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36,951][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.01219281367957592, acc: 1.0)
[2025-01-06 01:50:37,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37,298][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.009872376918792725, acc: 1.0)
[2025-01-06 01:50:37,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37,679][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.03902784362435341, acc: 0.9743589758872986)
[2025-01-06 01:50:37,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38,041][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.006573486141860485, acc: 1.0)
[2025-01-06 01:50:38,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38,342][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.017368502914905548, acc: 1.0)
[2025-01-06 01:50:38,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38,696][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.0030985279008746147, acc: 1.0)
[2025-01-06 01:50:38,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39,028][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.016281310468912125, acc: 1.0)
[2025-01-06 01:50:39,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39,320][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.18520548939704895, acc: 0.9354838728904724)
[2025-01-06 01:50:39,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39,675][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.23607897758483887, acc: 0.9104477763175964)
[2025-01-06 01:50:39,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40,048][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.1401982456445694, acc: 0.9711538553237915)
[2025-01-06 01:50:40,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40,394][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.03550601378083229, acc: 0.9777777791023254)
[2025-01-06 01:50:40,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40,773][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.01687837392091751, acc: 1.0)
[2025-01-06 01:50:40,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41,105][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.0017366845859214664, acc: 1.0)
[2025-01-06 01:50:41,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41,457][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.17824764549732208, acc: 0.8888888955116272)
[2025-01-06 01:50:41,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41,807][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.07403294742107391, acc: 0.9714285731315613)
[2025-01-06 01:50:41,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42,169][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.13043247163295746, acc: 0.9487179517745972)
[2025-01-06 01:50:42,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42,483][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.3842356503009796, acc: 0.8780487775802612)
[2025-01-06 01:50:42,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42,782][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.14911167323589325, acc: 0.9736841917037964)
[2025-01-06 01:50:42,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43,135][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.011496328748762608, acc: 1.0)
[2025-01-06 01:50:43,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43,471][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.019658701494336128, acc: 1.0)
[2025-01-06 01:50:43,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43,852][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.005360701121389866, acc: 1.0)
[2025-01-06 01:50:43,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44,235][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.0029788471292704344, acc: 1.0)
[2025-01-06 01:50:44,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44,561][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.08397827297449112, acc: 0.9838709831237793)
[2025-01-06 01:50:44,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44,928][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.015676258131861687, acc: 1.0)
[2025-01-06 01:50:45,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45,291][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.025998204946517944, acc: 1.0)
[2025-01-06 01:50:45,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45,659][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.0011264631757512689, acc: 1.0)
[2025-01-06 01:50:45,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46,021][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.007060479838401079, acc: 1.0)
[2025-01-06 01:50:46,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46,397][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.08061401546001434, acc: 0.9599999785423279)
[2025-01-06 01:50:46,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46,808][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.06643623858690262, acc: 0.977011501789093)
[2025-01-06 01:50:46,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47,154][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.12271087616682053, acc: 0.978723406791687)
[2025-01-06 01:50:47,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47,513][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.0644574761390686, acc: 0.9879518151283264)
[2025-01-06 01:50:47,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47,892][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.0005829180590808392, acc: 1.0)
[2025-01-06 01:50:47,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48,217][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.021461430937051773, acc: 1.0)
[2025-01-06 01:50:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48,543][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.021865282207727432, acc: 1.0)
[2025-01-06 01:50:48,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48,900][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.10914088040590286, acc: 0.9811320900917053)
[2025-01-06 01:50:49,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49,237][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.002125413855537772, acc: 1.0)
[2025-01-06 01:50:49,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49,587][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.0064727533608675, acc: 1.0)
[2025-01-06 01:50:49,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49,955][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.02870165929198265, acc: 0.9850746393203735)
[2025-01-06 01:50:50,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50,326][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.005863653961569071, acc: 1.0)
[2025-01-06 01:50:50,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50,653][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.0008121135178953409, acc: 1.0)
[2025-01-06 01:50:50,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51,037][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.03684663027524948, acc: 0.9722222089767456)
[2025-01-06 01:50:51,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51,441][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.045232173055410385, acc: 0.9767441749572754)
[2025-01-06 01:50:51,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51,823][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.00206637941300869, acc: 1.0)
[2025-01-06 01:50:51,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52,187][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.12640808522701263, acc: 0.9777777791023254)
[2025-01-06 01:50:52,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52,556][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.010082605294883251, acc: 1.0)
[2025-01-06 01:50:52,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52,884][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.11790502816438675, acc: 0.9615384340286255)
[2025-01-06 01:50:52,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53,253][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.17691947519779205, acc: 0.9560439586639404)
[2025-01-06 01:50:53,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53,766][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.05921531096100807, acc: 0.9739130139350891)
[2025-01-06 01:50:53,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54,114][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.08992274850606918, acc: 0.967391312122345)
[2025-01-06 01:50:54,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54,500][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.008530538529157639, acc: 1.0)
[2025-01-06 01:50:54,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54,849][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.00020606211910489947, acc: 1.0)
[2025-01-06 01:50:54,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55,189][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.010887705720961094, acc: 1.0)
[2025-01-06 01:50:55,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55,546][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.06500312685966492, acc: 0.9756097793579102)
[2025-01-06 01:50:55,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55,894][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.03434500843286514, acc: 0.9777777791023254)
[2025-01-06 01:50:55,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56,219][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.003998120781034231, acc: 1.0)
[2025-01-06 01:50:56,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56,574][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.006800409406423569, acc: 1.0)
[2025-01-06 01:50:56,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56,910][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.006526831537485123, acc: 1.0)
[2025-01-06 01:50:57,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57,275][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.0017060822574421763, acc: 1.0)
[2025-01-06 01:50:57,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57,593][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.0013440345646813512, acc: 1.0)
[2025-01-06 01:50:57,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57,903][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.10097811371088028, acc: 0.9642857313156128)
[2025-01-06 01:50:57,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58,255][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.010967168025672436, acc: 1.0)
[2025-01-06 01:50:58,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58,868][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.10626079887151718, acc: 0.9636363387107849)
[2025-01-06 01:50:59,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:59,698][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.027263309806585312, acc: 0.9905660152435303)
[2025-01-06 01:50:59,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:00,037][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.017939170822501183, acc: 0.9888888597488403)
[2025-01-06 01:51:00,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:00,411][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.008501706644892693, acc: 1.0)
[2025-01-06 01:51:00,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:00,753][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.0029728214722126722, acc: 1.0)
[2025-01-06 01:51:00,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:01,087][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.0003904554178006947, acc: 1.0)
[2025-01-06 01:51:01,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:01,423][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.0004752415989059955, acc: 1.0)
[2025-01-06 01:51:01,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:01,802][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.018742645159363747, acc: 1.0)
[2025-01-06 01:51:01,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02,188][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.0069579784758389, acc: 1.0)
[2025-01-06 01:51:02,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02,758][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.05257461592555046, acc: 0.976047933101654)
[2025-01-06 01:51:02,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:03,190][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.02185218408703804, acc: 0.9924812316894531)
[2025-01-06 01:51:03,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04,431][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.12718355655670166, acc: 0.9465240836143494)
[2025-01-06 01:51:04,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04,991][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.005442493595182896, acc: 1.0)
[2025-01-06 01:51:05,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05,347][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.0031544228550046682, acc: 1.0)
[2025-01-06 01:51:05,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05,651][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.00034555987804196775, acc: 1.0)
[2025-01-06 01:51:05,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05,956][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.018996749073266983, acc: 1.0)
[2025-01-06 01:51:06,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:06,296][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.000494284147862345, acc: 1.0)
[2025-01-06 01:51:06,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:06,637][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.018543265759944916, acc: 1.0)
[2025-01-06 01:51:06,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07,006][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.0005714903236366808, acc: 1.0)
[2025-01-06 01:51:07,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07,351][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.0006836258107796311, acc: 1.0)
[2025-01-06 01:51:07,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07,736][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.3447816073894501, acc: 0.9523809552192688)
[2025-01-06 01:51:07,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08,081][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.19986121356487274, acc: 0.9259259104728699)
[2025-01-06 01:51:08,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08,430][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.11662063747644424, acc: 0.9611650705337524)
[2025-01-06 01:51:08,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08,947][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.11298532783985138, acc: 0.9632353186607361)
[2025-01-06 01:51:09,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09,359][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.192502960562706, acc: 0.9466666579246521)
[2025-01-06 01:51:09,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09,748][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.07061588019132614, acc: 0.9652777910232544)
[2025-01-06 01:51:09,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10,140][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.3974442481994629, acc: 0.9767441749572754)
[2025-01-06 01:51:10,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10,520][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.0017474801279604435, acc: 1.0)
[2025-01-06 01:51:10,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10,939][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.012765713967382908, acc: 1.0)
[2025-01-06 01:51:11,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11,312][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.013714662753045559, acc: 1.0)
[2025-01-06 01:51:11,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11,844][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.04137549176812172, acc: 0.970588207244873)
[2025-01-06 01:51:11,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12,218][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.03392118215560913, acc: 0.9733333587646484)
[2025-01-06 01:51:12,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12,522][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.003982766531407833, acc: 1.0)
[2025-01-06 01:51:12,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12,861][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.07719550281763077, acc: 0.9696969985961914)
[2025-01-06 01:51:12,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13,228][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.10423976927995682, acc: 0.9677419066429138)
[2025-01-06 01:51:13,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13,610][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.0005437078652903438, acc: 1.0)
[2025-01-06 01:51:13,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13,983][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.003194563090801239, acc: 1.0)
[2025-01-06 01:51:14,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14,345][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.005290383938699961, acc: 1.0)
[2025-01-06 01:51:14,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14,691][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.0028487632516771555, acc: 1.0)
[2025-01-06 01:51:14,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15,048][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.07869850099086761, acc: 0.9615384340286255)
[2025-01-06 01:51:15,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15,381][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.003583957441151142, acc: 1.0)
[2025-01-06 01:51:15,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15,689][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.29534393548965454, acc: 0.9642857313156128)
[2025-01-06 01:51:15,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15,996][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.0004478724440559745, acc: 1.0)
[2025-01-06 01:51:16,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46,486][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3304, device='cuda:0') eval_epoch_loss=tensor(0.8460, device='cuda:0') eval_epoch_acc=tensor(0.8449, device='cuda:0')
[2025-01-06 01:51:46,487][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:51:46,487][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:51:46,839][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_413_loss_0.8460263609886169/model.pt
[2025-01-06 01:51:46,849][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:51:46,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47,297][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.00250973179936409, acc: 1.0)
[2025-01-06 01:51:47,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47,665][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.00626661442220211, acc: 1.0)
[2025-01-06 01:51:47,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48,024][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.01820741966366768, acc: 1.0)
[2025-01-06 01:51:48,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48,328][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.08899358659982681, acc: 0.9230769276618958)
[2025-01-06 01:51:48,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48,694][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.009173272177577019, acc: 1.0)
[2025-01-06 01:51:48,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49,091][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.0024548815563321114, acc: 1.0)
[2025-01-06 01:51:49,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49,462][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.014353032223880291, acc: 1.0)
[2025-01-06 01:51:49,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49,837][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.009961375966668129, acc: 1.0)
[2025-01-06 01:51:49,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50,187][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.0049168989062309265, acc: 1.0)
[2025-01-06 01:51:50,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50,479][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.004082581959664822, acc: 1.0)
[2025-01-06 01:51:50,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50,865][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.07600724697113037, acc: 0.9444444179534912)
[2025-01-06 01:51:50,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51,195][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.0009400390554219484, acc: 1.0)
[2025-01-06 01:51:51,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51,548][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.0034864379558712244, acc: 1.0)
[2025-01-06 01:51:51,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51,883][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.017112767323851585, acc: 1.0)
[2025-01-06 01:51:51,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52,212][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.0459129698574543, acc: 0.9729729890823364)
[2025-01-06 01:51:52,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52,586][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.002547015668824315, acc: 1.0)
[2025-01-06 01:51:52,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52,919][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.022489655762910843, acc: 1.0)
[2025-01-06 01:51:53,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53,266][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.0003312470798846334, acc: 1.0)
[2025-01-06 01:51:53,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53,653][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 8.788981358520687e-05, acc: 1.0)
[2025-01-06 01:51:53,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54,019][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.0038030443247407675, acc: 1.0)
[2025-01-06 01:51:54,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54,388][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.019142307341098785, acc: 1.0)
[2025-01-06 01:51:54,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54,729][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.004406125750392675, acc: 1.0)
[2025-01-06 01:51:54,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:55,100][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.0015329165617004037, acc: 1.0)
[2025-01-06 01:51:55,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:55,471][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.010952846147119999, acc: 1.0)
[2025-01-06 01:51:55,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:55,835][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.0007369770319201052, acc: 1.0)
[2025-01-06 01:51:55,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56,204][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0001789806119631976, acc: 1.0)
[2025-01-06 01:51:56,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56,539][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.0020729689858853817, acc: 1.0)
[2025-01-06 01:51:56,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57,032][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.005208233837038279, acc: 1.0)
[2025-01-06 01:51:57,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57,739][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.10617008805274963, acc: 0.9520000219345093)
[2025-01-06 01:51:57,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58,159][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.06368493288755417, acc: 0.9838709831237793)
[2025-01-06 01:51:58,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58,810][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.15800981223583221, acc: 0.9552238583564758)
[2025-01-06 01:51:58,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59,163][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.03765947371721268, acc: 0.9811320900917053)
[2025-01-06 01:51:59,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59,573][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.0021387210581451654, acc: 1.0)
[2025-01-06 01:51:59,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59,923][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.0027819043025374413, acc: 1.0)
[2025-01-06 01:52:00,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00,289][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.003916459158062935, acc: 1.0)
[2025-01-06 01:52:00,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00,710][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.0012903802562505007, acc: 1.0)
[2025-01-06 01:52:00,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01,094][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.0031821876764297485, acc: 1.0)
[2025-01-06 01:52:01,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01,441][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.0422988086938858, acc: 0.9722222089767456)
[2025-01-06 01:52:01,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01,769][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.0017571481876075268, acc: 1.0)
[2025-01-06 01:52:01,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02,144][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.0015799780376255512, acc: 1.0)
[2025-01-06 01:52:02,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02,504][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.03156723454594612, acc: 0.9868420958518982)
[2025-01-06 01:52:02,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02,875][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.011533940210938454, acc: 1.0)
[2025-01-06 01:52:02,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03,217][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.004655405413359404, acc: 1.0)
[2025-01-06 01:52:03,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03,559][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.006438530050218105, acc: 1.0)
[2025-01-06 01:52:03,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03,906][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.004082856234163046, acc: 1.0)
[2025-01-06 01:52:04,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04,277][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.03530976176261902, acc: 0.9883720874786377)
[2025-01-06 01:52:04,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04,613][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.027403151616454124, acc: 0.9821428656578064)
[2025-01-06 01:52:04,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04,985][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.0035880974028259516, acc: 1.0)
[2025-01-06 01:52:05,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:05,337][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.0006939673912711442, acc: 1.0)
[2025-01-06 01:52:05,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:05,682][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.0008344416273757815, acc: 1.0)
[2025-01-06 01:52:05,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06,001][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.0008532719220966101, acc: 1.0)
[2025-01-06 01:52:06,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06,314][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.001881068223156035, acc: 1.0)
[2025-01-06 01:52:06,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06,667][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.017972620204091072, acc: 0.988095223903656)
[2025-01-06 01:52:06,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07,033][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.1626819521188736, acc: 0.9518072009086609)
[2025-01-06 01:52:07,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07,404][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.008479774929583073, acc: 1.0)
[2025-01-06 01:52:07,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07,740][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.0855918824672699, acc: 0.9902912378311157)
[2025-01-06 01:52:07,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08,096][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.058916885405778885, acc: 0.9674796462059021)
[2025-01-06 01:52:08,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08,421][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.0016266020247712731, acc: 1.0)
[2025-01-06 01:52:08,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08,786][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.0017433026805520058, acc: 1.0)
[2025-01-06 01:52:08,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09,219][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.07473568618297577, acc: 0.9803921580314636)
[2025-01-06 01:52:09,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09,621][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.11543567478656769, acc: 0.9650654792785645)
[2025-01-06 01:52:09,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09,982][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.03463837131857872, acc: 0.9895833134651184)
[2025-01-06 01:52:10,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10,348][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.0333399772644043, acc: 0.987730085849762)
[2025-01-06 01:52:10,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10,676][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.03303827345371246, acc: 0.9928057789802551)
[2025-01-06 01:52:10,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11,064][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.11573300510644913, acc: 0.9648241400718689)
[2025-01-06 01:52:11,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11,439][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.01099095493555069, acc: 1.0)
[2025-01-06 01:52:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11,771][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.00919348280876875, acc: 1.0)
[2025-01-06 01:52:11,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12,106][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.005142726469784975, acc: 1.0)
[2025-01-06 01:52:12,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12,467][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.03032848611474037, acc: 1.0)
[2025-01-06 01:52:12,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12,836][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.023172203451395035, acc: 1.0)
[2025-01-06 01:52:12,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13,205][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.03194300830364227, acc: 0.982758641242981)
[2025-01-06 01:52:13,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13,545][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.001785099389962852, acc: 1.0)
[2025-01-06 01:52:13,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13,928][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.0025258404202759266, acc: 1.0)
[2025-01-06 01:52:14,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14,290][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.024011535570025444, acc: 1.0)
[2025-01-06 01:52:14,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14,679][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.02780253253877163, acc: 1.0)
[2025-01-06 01:52:14,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15,023][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.028139593079686165, acc: 1.0)
[2025-01-06 01:52:15,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15,374][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.033337462693452835, acc: 0.9846153855323792)
[2025-01-06 01:52:15,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15,742][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.012978744693100452, acc: 1.0)
[2025-01-06 01:52:15,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16,076][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.0012850489001721144, acc: 1.0)
[2025-01-06 01:52:16,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16,390][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.09483310580253601, acc: 0.9411764740943909)
[2025-01-06 01:52:16,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16,704][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.0012598385801538825, acc: 1.0)
[2025-01-06 01:52:16,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:17,008][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.011718346737325191, acc: 1.0)
[2025-01-06 01:52:17,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:17,383][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.05176949501037598, acc: 1.0)
[2025-01-06 01:52:17,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:17,803][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.14046761393547058, acc: 0.9642857313156128)
[2025-01-06 01:52:17,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:18,167][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.03053954988718033, acc: 0.9887640476226807)
[2025-01-06 01:52:18,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:18,494][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.08758804947137833, acc: 0.9775280952453613)
[2025-01-06 01:52:18,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:18,847][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.1098250076174736, acc: 0.9716312289237976)
[2025-01-06 01:52:18,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19,217][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.01788531243801117, acc: 1.0)
[2025-01-06 01:52:19,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19,612][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.00028149803983978927, acc: 1.0)
[2025-01-06 01:52:19,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19,933][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.0002384159161010757, acc: 1.0)
[2025-01-06 01:52:20,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20,299][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.09372380375862122, acc: 0.9629629850387573)
[2025-01-06 01:52:20,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20,618][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.00418301485478878, acc: 1.0)
[2025-01-06 01:52:20,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20,944][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.11824899911880493, acc: 0.9433962106704712)
[2025-01-06 01:52:21,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21,327][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.013553301803767681, acc: 1.0)
[2025-01-06 01:52:21,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21,917][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.13658836483955383, acc: 0.954954981803894)
[2025-01-06 01:52:22,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22,358][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.058141715824604034, acc: 0.98591548204422)
[2025-01-06 01:52:22,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22,689][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.001432062708772719, acc: 1.0)
[2025-01-06 01:52:22,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23,080][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.02270282432436943, acc: 1.0)
[2025-01-06 01:52:23,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23,452][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.0006599128246307373, acc: 1.0)
[2025-01-06 01:52:25,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27,000][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.11496125906705856, acc: 0.9714285731315613)
[2025-01-06 01:52:27,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27,761][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.007418385706841946, acc: 1.0)
[2025-01-06 01:52:27,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28,042][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.005284321028739214, acc: 1.0)
[2025-01-06 01:52:28,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28,358][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.08918964117765427, acc: 0.9833333492279053)
[2025-01-06 01:52:28,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29,051][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.018289577215909958, acc: 1.0)
[2025-01-06 01:52:29,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29,366][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.00012538990995381027, acc: 1.0)
[2025-01-06 01:52:29,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29,720][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.0049879723228514194, acc: 1.0)
[2025-01-06 01:52:29,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30,075][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.0009503521141596138, acc: 1.0)
[2025-01-06 01:52:30,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30,468][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.003591901157051325, acc: 1.0)
[2025-01-06 01:52:30,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31,455][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.1321638971567154, acc: 0.9533898234367371)
[2025-01-06 01:52:31,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31,823][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.007177669554948807, acc: 1.0)
[2025-01-06 01:52:31,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32,196][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.027626268565654755, acc: 0.985401451587677)
[2025-01-06 01:52:32,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32,756][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.09318351745605469, acc: 0.9700000286102295)
[2025-01-06 01:52:32,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33,110][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.004458137787878513, acc: 1.0)
[2025-01-06 01:52:33,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33,501][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.013070032000541687, acc: 1.0)
[2025-01-06 01:52:33,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33,836][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.007107219658792019, acc: 1.0)
[2025-01-06 01:52:33,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34,165][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.08168597519397736, acc: 0.9836065769195557)
[2025-01-06 01:52:34,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34,520][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.012549731880426407, acc: 1.0)
[2025-01-06 01:52:34,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34,848][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.20303168892860413, acc: 0.9767441749572754)
[2025-01-06 01:52:34,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35,161][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.08595750480890274, acc: 0.9545454382896423)
[2025-01-06 01:52:35,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35,485][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.019283683970570564, acc: 1.0)
[2025-01-06 01:52:35,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35,849][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.06751937419176102, acc: 0.9545454382896423)
[2025-01-06 01:52:35,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36,192][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.007493353448808193, acc: 1.0)
[2025-01-06 01:52:36,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36,565][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.0007489125127904117, acc: 1.0)
[2025-01-06 01:52:36,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36,845][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.00030663967481814325, acc: 1.0)
[2025-01-06 01:52:36,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37,227][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.014830228872597218, acc: 1.0)
[2025-01-06 01:52:37,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37,595][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.01878339797258377, acc: 1.0)
[2025-01-06 01:52:37,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37,962][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.01192639023065567, acc: 1.0)
[2025-01-06 01:52:38,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38,312][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.1116180270910263, acc: 0.9696969985961914)
[2025-01-06 01:52:38,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38,641][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.0001885856909211725, acc: 1.0)
[2025-01-06 01:52:38,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38,969][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.0013748055789619684, acc: 1.0)
[2025-01-06 01:52:39,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39,340][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 7.966999692143872e-05, acc: 1.0)
[2025-01-06 01:52:39,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39,701][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.002882526256144047, acc: 1.0)
[2025-01-06 01:52:39,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40,079][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.0013112809974700212, acc: 1.0)
[2025-01-06 01:52:40,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40,435][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.00044313955004327, acc: 1.0)
[2025-01-06 01:52:40,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40,797][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.008569256402552128, acc: 1.0)
[2025-01-06 01:52:40,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41,158][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.007790464907884598, acc: 1.0)
[2025-01-06 01:52:41,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41,503][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 6.596343882847577e-05, acc: 1.0)
[2025-01-06 01:52:41,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41,866][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.0003662233939394355, acc: 1.0)
[2025-01-06 01:52:41,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42,250][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.009443074464797974, acc: 1.0)
[2025-01-06 01:52:42,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42,607][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.03455333039164543, acc: 0.9857142567634583)
[2025-01-06 01:52:42,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42,955][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.02605460211634636, acc: 0.9927007555961609)
[2025-01-06 01:52:43,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43,277][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.005436011124402285, acc: 1.0)
[2025-01-06 01:52:43,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43,650][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.04570702463388443, acc: 0.9928571581840515)
[2025-01-06 01:52:44,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14,060][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3443, device='cuda:0') eval_epoch_loss=tensor(0.8520, device='cuda:0') eval_epoch_acc=tensor(0.8467, device='cuda:0')
[2025-01-06 01:53:14,061][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:53:14,061][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:53:14,277][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_556_loss_0.8519677519798279/model.pt
[2025-01-06 01:53:14,281][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:53:14,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14,625][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.041201863437891006, acc: 0.9867549538612366)
[2025-01-06 01:53:14,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14,935][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.003367386292666197, acc: 1.0)
[2025-01-06 01:53:15,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15,296][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.008254889398813248, acc: 1.0)
[2025-01-06 01:53:15,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15,657][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.001680163200944662, acc: 1.0)
[2025-01-06 01:53:15,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,024][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.002552745398133993, acc: 1.0)
[2025-01-06 01:53:16,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,347][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.0008339118794538081, acc: 1.0)
[2025-01-06 01:53:16,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,697][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.0640798807144165, acc: 0.9888888597488403)
[2025-01-06 01:53:16,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16,980][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.011230145581066608, acc: 1.0)
[2025-01-06 01:53:17,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:17,332][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.10989297181367874, acc: 0.9583333134651184)
[2025-01-06 01:53:17,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:17,695][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.0019118025666102767, acc: 1.0)
[2025-01-06 01:53:17,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18,066][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.004154922440648079, acc: 1.0)
[2025-01-06 01:53:18,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18,425][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.001556669594720006, acc: 1.0)
[2025-01-06 01:53:18,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18,787][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.0007623964338563383, acc: 1.0)
[2025-01-06 01:53:18,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19,226][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.010691424831748009, acc: 1.0)
[2025-01-06 01:53:19,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19,572][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.0005357257323339581, acc: 1.0)
[2025-01-06 01:53:19,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19,938][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.038576193153858185, acc: 0.9829059839248657)
[2025-01-06 01:53:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20,269][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.06725715845823288, acc: 0.9846938848495483)
[2025-01-06 01:53:20,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20,606][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.04017625376582146, acc: 0.9874213933944702)
[2025-01-06 01:53:20,998][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.0554, train_epoch_loss=0.0539, epoch time 355.56035758554935s
[2025-01-06 01:53:20,999][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:53:20,999][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2025-01-06 01:53:20,999][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:53:20,999][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 25
[2025-01-06 01:53:20,999][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:53:21,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21,869][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.0029369371477514505, acc: 1.0)
[2025-01-06 01:53:21,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22,239][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.0015467180637642741, acc: 1.0)
[2025-01-06 01:53:22,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22,631][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.19833588600158691, acc: 0.9729729890823364)
[2025-01-06 01:53:22,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22,967][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.05357402190566063, acc: 0.9736841917037964)
[2025-01-06 01:53:23,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23,361][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.012631895020604134, acc: 1.0)
[2025-01-06 01:53:23,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23,719][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.003936867229640484, acc: 1.0)
[2025-01-06 01:53:23,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24,080][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.015076788142323494, acc: 1.0)
[2025-01-06 01:53:24,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24,423][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.16870489716529846, acc: 0.9333333373069763)
[2025-01-06 01:53:24,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24,781][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.0011538135586306453, acc: 1.0)
[2025-01-06 01:53:24,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25,102][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.00031564306118525565, acc: 1.0)
[2025-01-06 01:53:25,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25,463][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.0004317212151363492, acc: 1.0)
[2025-01-06 01:53:25,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25,803][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.0026764050126075745, acc: 1.0)
[2025-01-06 01:53:25,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26,186][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.001108884229324758, acc: 1.0)
[2025-01-06 01:53:26,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26,574][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.008046210743486881, acc: 1.0)
[2025-01-06 01:53:26,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26,923][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.0025035578291863203, acc: 1.0)
[2025-01-06 01:53:27,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27,249][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.047826189547777176, acc: 0.9795918464660645)
[2025-01-06 01:53:27,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27,612][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.0010933547746390104, acc: 1.0)
[2025-01-06 01:53:27,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27,996][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.04639187827706337, acc: 0.9583333134651184)
[2025-01-06 01:53:28,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28,357][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.1693691611289978, acc: 0.9166666865348816)
[2025-01-06 01:53:28,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28,655][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.0001472974690841511, acc: 1.0)
[2025-01-06 01:53:28,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28,960][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.0017680040327832103, acc: 1.0)
[2025-01-06 01:53:29,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:29,270][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.466810405254364, acc: 0.8965517282485962)
[2025-01-06 01:53:29,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:29,604][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.002184020821005106, acc: 1.0)
[2025-01-06 01:53:29,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:29,969][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.001863132114522159, acc: 1.0)
[2025-01-06 01:53:30,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30,352][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.00028688766178674996, acc: 1.0)
[2025-01-06 01:53:30,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30,723][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.010092739947140217, acc: 1.0)
[2025-01-06 01:53:30,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:31,126][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.0389663390815258, acc: 0.9863013625144958)
[2025-01-06 01:53:31,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32,412][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.248093381524086, acc: 0.9130434989929199)
[2025-01-06 01:53:32,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32,714][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.03324653208255768, acc: 1.0)
[2025-01-06 01:53:32,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33,113][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.0852813795208931, acc: 0.9638554453849792)
[2025-01-06 01:53:33,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33,491][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.041376397013664246, acc: 0.9876543283462524)
[2025-01-06 01:53:33,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33,844][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.06268567591905594, acc: 0.9642857313156128)
[2025-01-06 01:53:33,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34,255][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.0016194136114791036, acc: 1.0)
[2025-01-06 01:53:34,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34,659][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0016312143998220563, acc: 1.0)
[2025-01-06 01:53:34,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35,061][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.1003522276878357, acc: 0.9663865566253662)
[2025-01-06 01:53:35,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35,443][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.06728069484233856, acc: 0.9836065769195557)
[2025-01-06 01:53:35,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35,844][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.019469404593110085, acc: 1.0)
[2025-01-06 01:53:35,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36,186][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.05488436669111252, acc: 0.9830508232116699)
[2025-01-06 01:53:36,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36,567][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.053571343421936035, acc: 0.977011501789093)
[2025-01-06 01:53:36,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36,889][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.008490840904414654, acc: 1.0)
[2025-01-06 01:53:36,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37,267][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.0024237600155174732, acc: 1.0)
[2025-01-06 01:53:37,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37,640][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.012705975212156773, acc: 1.0)
[2025-01-06 01:53:37,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37,983][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.11351669579744339, acc: 0.9384615421295166)
[2025-01-06 01:53:38,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:38,449][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.03294120356440544, acc: 0.9898989796638489)
[2025-01-06 01:53:38,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:38,873][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.1267559826374054, acc: 0.9587628841400146)
[2025-01-06 01:53:39,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39,300][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.06655995547771454, acc: 0.970588207244873)
[2025-01-06 01:53:39,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39,631][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.0050719198770821095, acc: 1.0)
[2025-01-06 01:53:39,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39,944][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.004163473378866911, acc: 1.0)
[2025-01-06 01:53:40,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40,286][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.004916488192975521, acc: 1.0)
[2025-01-06 01:53:40,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40,678][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.010193384252488613, acc: 1.0)
[2025-01-06 01:53:40,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41,024][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.03789353370666504, acc: 0.9824561476707458)
[2025-01-06 01:53:41,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41,428][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.049991317093372345, acc: 0.9841269850730896)
[2025-01-06 01:53:41,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41,783][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.11890307813882828, acc: 0.9577465057373047)
[2025-01-06 01:53:41,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42,242][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.2510673403739929, acc: 0.9200000166893005)
[2025-01-06 01:53:42,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42,600][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.025983735918998718, acc: 1.0)
[2025-01-06 01:53:42,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42,923][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.012225131504237652, acc: 1.0)
[2025-01-06 01:53:44,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:45,919][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.48515021800994873, acc: 0.8430033922195435)
[2025-01-06 01:53:46,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:47,112][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.6332116723060608, acc: 0.8235294222831726)
[2025-01-06 01:53:47,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:47,735][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.2881629467010498, acc: 0.9090909361839294)
[2025-01-06 01:53:47,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48,304][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.07672756910324097, acc: 0.9558823704719543)
[2025-01-06 01:53:48,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48,863][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.15977327525615692, acc: 0.9492753744125366)
[2025-01-06 01:53:48,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49,270][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.06877533346414566, acc: 0.9750000238418579)
[2025-01-06 01:53:49,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49,588][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.6028721332550049, acc: 0.8823529481887817)
[2025-01-06 01:53:49,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49,924][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.008150864392518997, acc: 1.0)
[2025-01-06 01:53:50,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50,326][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.009115797467529774, acc: 1.0)
[2025-01-06 01:53:50,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50,701][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.008030044846236706, acc: 1.0)
[2025-01-06 01:53:50,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51,072][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.10146220773458481, acc: 0.9642857313156128)
[2025-01-06 01:53:51,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51,468][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.01165917981415987, acc: 1.0)
[2025-01-06 01:53:51,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51,829][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.0011033922201022506, acc: 1.0)
[2025-01-06 01:53:51,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52,223][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.11483757942914963, acc: 0.9722222089767456)
[2025-01-06 01:53:52,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52,577][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.035113658756017685, acc: 1.0)
[2025-01-06 01:53:52,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52,908][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.1750110685825348, acc: 0.9338235259056091)
[2025-01-06 01:53:52,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53,267][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.40859636664390564, acc: 0.9047619104385376)
[2025-01-06 01:53:53,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53,646][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.2816126048564911, acc: 0.8871794939041138)
[2025-01-06 01:53:53,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53,976][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.12122111767530441, acc: 0.9489796161651611)
[2025-01-06 01:53:54,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54,349][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.20618151128292084, acc: 0.9552238583564758)
[2025-01-06 01:53:54,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54,763][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.3557737171649933, acc: 0.8722627758979797)
[2025-01-06 01:53:54,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55,137][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.0008231888641603291, acc: 1.0)
[2025-01-06 01:53:55,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55,460][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.009971002116799355, acc: 1.0)
[2025-01-06 01:53:55,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55,779][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.1483173668384552, acc: 0.9696969985961914)
[2025-01-06 01:53:55,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56,152][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.004322175867855549, acc: 1.0)
[2025-01-06 01:53:56,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56,533][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.09311067312955856, acc: 0.9807692170143127)
[2025-01-06 01:53:56,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56,899][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.09179837256669998, acc: 0.9807692170143127)
[2025-01-06 01:53:57,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57,264][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.17722652852535248, acc: 0.96875)
[2025-01-06 01:53:57,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57,659][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.031788475811481476, acc: 1.0)
[2025-01-06 01:53:57,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58,021][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.14397497475147247, acc: 0.9399999976158142)
[2025-01-06 01:53:58,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58,290][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.0060478500090539455, acc: 1.0)
[2025-01-06 01:53:58,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58,756][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.015568830072879791, acc: 1.0)
[2025-01-06 01:53:58,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59,146][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.18370705842971802, acc: 0.9417475461959839)
[2025-01-06 01:53:59,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00,246][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.2482244223356247, acc: 0.9271844625473022)
[2025-01-06 01:54:00,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01,069][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.250973641872406, acc: 0.9247311949729919)
[2025-01-06 01:54:01,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01,876][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.2772587537765503, acc: 0.931034505367279)
[2025-01-06 01:54:02,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02,625][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.12469594180583954, acc: 0.9684210419654846)
[2025-01-06 01:54:02,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03,615][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.23358233273029327, acc: 0.9306930899620056)
[2025-01-06 01:54:03,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03,866][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.1124773919582367, acc: 0.9677419066429138)
[2025-01-06 01:54:04,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04,302][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.14412562549114227, acc: 0.95652174949646)
[2025-01-06 01:54:04,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04,664][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.16206905245780945, acc: 0.9831932783126831)
[2025-01-06 01:54:04,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05,031][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.22757218778133392, acc: 0.9519230723381042)
[2025-01-06 01:54:05,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05,422][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.16040554642677307, acc: 0.970802903175354)
[2025-01-06 01:54:05,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05,769][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.15454241633415222, acc: 0.9402984976768494)
[2025-01-06 01:54:05,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06,132][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.19124501943588257, acc: 0.8999999761581421)
[2025-01-06 01:54:06,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06,500][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.004447187762707472, acc: 1.0)
[2025-01-06 01:54:06,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06,807][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.003407147480174899, acc: 1.0)
[2025-01-06 01:54:06,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07,182][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.006019987631589174, acc: 1.0)
[2025-01-06 01:54:07,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07,535][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.027768122032284737, acc: 1.0)
[2025-01-06 01:54:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07,866][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.02063242718577385, acc: 0.9767441749572754)
[2025-01-06 01:54:07,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08,201][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.0825691819190979, acc: 0.9599999785423279)
[2025-01-06 01:54:08,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08,535][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.04450448229908943, acc: 0.9411764740943909)
[2025-01-06 01:54:08,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08,916][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.021408196538686752, acc: 1.0)
[2025-01-06 01:54:09,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09,296][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.33139505982398987, acc: 0.976190447807312)
[2025-01-06 01:54:09,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09,653][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.005534104071557522, acc: 1.0)
[2025-01-06 01:54:09,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10,053][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.08174901455640793, acc: 0.9649122953414917)
[2025-01-06 01:54:10,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10,411][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.12474896013736725, acc: 0.9649122953414917)
[2025-01-06 01:54:10,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10,752][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.2947307527065277, acc: 0.9487179517745972)
[2025-01-06 01:54:10,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11,116][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.11359567195177078, acc: 0.9795918464660645)
[2025-01-06 01:54:11,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11,437][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.012177086435258389, acc: 1.0)
[2025-01-06 01:54:11,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11,799][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.17864513397216797, acc: 0.920634925365448)
[2025-01-06 01:54:11,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12,172][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.0546652115881443, acc: 0.9918699264526367)
[2025-01-06 01:54:12,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12,569][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.04230303689837456, acc: 0.9838709831237793)
[2025-01-06 01:54:12,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13,430][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.24725736677646637, acc: 0.9163498282432556)
[2025-01-06 01:54:13,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13,806][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.08442696183919907, acc: 0.9733333587646484)
[2025-01-06 01:54:13,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14,229][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.10454095155000687, acc: 0.942307710647583)
[2025-01-06 01:54:14,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14,540][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.0035907693672925234, acc: 1.0)
[2025-01-06 01:54:14,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14,865][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.05253498628735542, acc: 1.0)
[2025-01-06 01:54:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:15,259][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.18553565442562103, acc: 0.9509202241897583)
[2025-01-06 01:54:16,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45,976][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2642, device='cuda:0') eval_epoch_loss=tensor(0.8172, device='cuda:0') eval_epoch_acc=tensor(0.8357, device='cuda:0')
[2025-01-06 01:54:45,978][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:54:45,978][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:54:46,188][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_125_loss_0.8172417283058167/model.pt
[2025-01-06 01:54:46,193][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:54:46,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46,613][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.16857537627220154, acc: 0.9444444179534912)
[2025-01-06 01:54:46,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46,916][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.19131912291049957, acc: 0.925000011920929)
[2025-01-06 01:54:47,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47,236][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.12894323468208313, acc: 0.976190447807312)
[2025-01-06 01:54:47,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47,577][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.27576714754104614, acc: 0.8974359035491943)
[2025-01-06 01:54:47,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47,975][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.2018844038248062, acc: 0.9338235259056091)
[2025-01-06 01:54:48,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48,291][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.19163672626018524, acc: 0.9230769276618958)
[2025-01-06 01:54:48,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48,579][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.021210506558418274, acc: 1.0)
[2025-01-06 01:54:48,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48,920][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.07985052466392517, acc: 0.9375)
[2025-01-06 01:54:49,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49,241][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.2792234718799591, acc: 0.95652174949646)
[2025-01-06 01:54:49,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49,587][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.3456108868122101, acc: 0.9714285731315613)
[2025-01-06 01:54:49,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49,907][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.020931780338287354, acc: 1.0)
[2025-01-06 01:54:50,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50,264][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.024429040029644966, acc: 1.0)
[2025-01-06 01:54:50,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50,620][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.13512635231018066, acc: 0.9333333373069763)
[2025-01-06 01:54:50,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50,975][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.023221895098686218, acc: 1.0)
[2025-01-06 01:54:51,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51,311][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.026102174073457718, acc: 1.0)
[2025-01-06 01:54:51,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51,669][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.24274827539920807, acc: 0.9230769276618958)
[2025-01-06 01:54:51,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52,070][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.02807653695344925, acc: 1.0)
[2025-01-06 01:54:52,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52,437][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.5593087673187256, acc: 0.9459459185600281)
[2025-01-06 01:54:52,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52,992][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.21079444885253906, acc: 0.9473684430122375)
[2025-01-06 01:54:53,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53,376][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.10025397688150406, acc: 0.9701492786407471)
[2025-01-06 01:54:53,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53,738][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.05941396206617355, acc: 0.9795918464660645)
[2025-01-06 01:54:53,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54,176][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.13029882311820984, acc: 0.978723406791687)
[2025-01-06 01:54:54,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54,491][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.08133114129304886, acc: 0.9571428298950195)
[2025-01-06 01:54:54,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54,773][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.049021653831005096, acc: 0.9642857313156128)
[2025-01-06 01:54:54,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55,116][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.20807108283042908, acc: 0.95652174949646)
[2025-01-06 01:54:55,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55,492][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.4231494665145874, acc: 0.8965517282485962)
[2025-01-06 01:54:55,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55,836][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.014474381692707539, acc: 1.0)
[2025-01-06 01:54:55,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56,216][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.03605297580361366, acc: 0.9830508232116699)
[2025-01-06 01:54:56,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56,574][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.11190374940633774, acc: 0.9473684430122375)
[2025-01-06 01:54:56,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56,932][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.06902976334095001, acc: 0.9864864945411682)
[2025-01-06 01:54:57,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57,305][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.018515128642320633, acc: 1.0)
[2025-01-06 01:54:57,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57,640][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.022834908217191696, acc: 1.0)
[2025-01-06 01:54:57,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57,951][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.23702140152454376, acc: 0.9473684430122375)
[2025-01-06 01:54:58,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59,531][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.20812426507472992, acc: 0.9054054021835327)
[2025-01-06 01:54:59,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59,817][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.06619856506586075, acc: 0.9814814925193787)
[2025-01-06 01:54:59,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00,216][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.17472586035728455, acc: 0.9534883499145508)
[2025-01-06 01:55:00,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00,799][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.23542137444019318, acc: 0.929411768913269)
[2025-01-06 01:55:00,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01,353][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.24840226769447327, acc: 0.9213483333587646)
[2025-01-06 01:55:01,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01,717][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.11814546585083008, acc: 0.9772727489471436)
[2025-01-06 01:55:01,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02,019][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.005766895599663258, acc: 1.0)
[2025-01-06 01:55:02,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02,293][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.4014769196510315, acc: 0.9655172228813171)
[2025-01-06 01:55:02,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02,661][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.0023359479382634163, acc: 1.0)
[2025-01-06 01:55:02,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02,995][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.02034205012023449, acc: 1.0)
[2025-01-06 01:55:03,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03,417][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.03718020021915436, acc: 1.0)
[2025-01-06 01:55:03,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03,750][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.2753172218799591, acc: 0.9215686321258545)
[2025-01-06 01:55:04,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04,774][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.10368107259273529, acc: 0.9726027250289917)
[2025-01-06 01:55:04,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05,037][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.011422461830079556, acc: 1.0)
[2025-01-06 01:55:05,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05,399][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.012655309401452541, acc: 1.0)
[2025-01-06 01:55:05,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05,750][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.20822405815124512, acc: 0.9285714030265808)
[2025-01-06 01:55:05,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06,285][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.22426816821098328, acc: 0.9380530714988708)
[2025-01-06 01:55:06,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06,590][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.05689537897706032, acc: 0.9855072498321533)
[2025-01-06 01:55:06,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06,965][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.020539050921797752, acc: 1.0)
[2025-01-06 01:55:07,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07,878][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.21233943104743958, acc: 0.9465649127960205)
[2025-01-06 01:55:08,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08,556][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.1523573398590088, acc: 0.9777777791023254)
[2025-01-06 01:55:08,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08,929][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.21064111590385437, acc: 0.9672130942344666)
[2025-01-06 01:55:09,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09,302][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.0311067346483469, acc: 1.0)
[2025-01-06 01:55:09,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09,646][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.0014859922230243683, acc: 1.0)
[2025-01-06 01:55:09,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10,021][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.12747550010681152, acc: 0.9642857313156128)
[2025-01-06 01:55:10,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10,340][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.025896389037370682, acc: 0.9878048896789551)
[2025-01-06 01:55:10,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10,672][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.09738237410783768, acc: 0.9607250690460205)
[2025-01-06 01:55:10,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10,999][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.14248481392860413, acc: 0.9625360369682312)
[2025-01-06 01:55:11,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11,481][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.11566100269556046, acc: 0.9624999761581421)
[2025-01-06 01:55:11,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12,006][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.2343498170375824, acc: 0.9287054538726807)
[2025-01-06 01:55:12,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12,418][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.1507086604833603, acc: 0.9537366628646851)
[2025-01-06 01:55:12,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12,776][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.07646014541387558, acc: 0.9599999785423279)
[2025-01-06 01:55:12,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13,323][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.19080685079097748, acc: 0.930232584476471)
[2025-01-06 01:55:13,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:14,123][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.23632222414016724, acc: 0.920634925365448)
[2025-01-06 01:55:14,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15,037][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.2843417823314667, acc: 0.9090909361839294)
[2025-01-06 01:55:15,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15,776][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.08827156573534012, acc: 0.9764705896377563)
[2025-01-06 01:55:16,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16,855][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.22877107560634613, acc: 0.9135802388191223)
[2025-01-06 01:55:17,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17,812][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.07675711065530777, acc: 0.9516128897666931)
[2025-01-06 01:55:17,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18,161][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.006214431952685118, acc: 1.0)
[2025-01-06 01:55:18,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18,497][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.029926244169473648, acc: 1.0)
[2025-01-06 01:55:18,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18,875][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.2385483682155609, acc: 0.8970588445663452)
[2025-01-06 01:55:18,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19,223][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.07226437330245972, acc: 0.9779411554336548)
[2025-01-06 01:55:19,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19,616][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.07260408997535706, acc: 0.9830508232116699)
[2025-01-06 01:55:19,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19,995][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.1809970885515213, acc: 0.9328358173370361)
[2025-01-06 01:55:20,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:20,361][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.25854527950286865, acc: 0.9320388436317444)
[2025-01-06 01:55:20,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:20,733][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.015506225638091564, acc: 1.0)
[2025-01-06 01:55:20,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21,076][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.013906127773225307, acc: 1.0)
[2025-01-06 01:55:21,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21,455][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.0415964350104332, acc: 0.9820627570152283)
[2025-01-06 01:55:21,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21,882][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.10485807061195374, acc: 0.960629940032959)
[2025-01-06 01:55:21,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22,271][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.0327489860355854, acc: 0.9913793206214905)
[2025-01-06 01:55:22,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22,634][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.061218101531267166, acc: 0.9818840622901917)
[2025-01-06 01:55:22,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22,993][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.04477052390575409, acc: 0.9805447459220886)
[2025-01-06 01:55:23,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23,323][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.02396564930677414, acc: 1.0)
[2025-01-06 01:55:23,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23,649][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.004248607903718948, acc: 1.0)
[2025-01-06 01:55:23,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23,951][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.020042115822434425, acc: 1.0)
[2025-01-06 01:55:24,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24,255][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.0022742380388081074, acc: 1.0)
[2025-01-06 01:55:24,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24,928][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.04253729060292244, acc: 0.9846153855323792)
[2025-01-06 01:55:25,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:25,225][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.011410189792513847, acc: 1.0)
[2025-01-06 01:55:25,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:25,626][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.02903801016509533, acc: 0.9883720874786377)
[2025-01-06 01:55:25,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26,157][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.010098196566104889, acc: 1.0)
[2025-01-06 01:55:26,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26,573][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.009111756458878517, acc: 1.0)
[2025-01-06 01:55:26,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26,915][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.0010321050649508834, acc: 1.0)
[2025-01-06 01:55:27,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27,260][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.0038839306216686964, acc: 1.0)
[2025-01-06 01:55:27,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27,650][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.03791205585002899, acc: 0.9599999785423279)
[2025-01-06 01:55:27,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28,047][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.11787737905979156, acc: 0.9807692170143127)
[2025-01-06 01:55:28,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28,808][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.0320531465113163, acc: 0.989130437374115)
[2025-01-06 01:55:28,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29,349][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.1346920281648636, acc: 0.9431818127632141)
[2025-01-06 01:55:29,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29,779][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.13662761449813843, acc: 0.957446813583374)
[2025-01-06 01:55:29,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30,162][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.10006213933229446, acc: 0.9622641801834106)
[2025-01-06 01:55:30,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30,537][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.025510210543870926, acc: 0.9833333492279053)
[2025-01-06 01:55:30,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30,867][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.06408718228340149, acc: 0.9767441749572754)
[2025-01-06 01:55:30,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31,156][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.12699292600154877, acc: 0.9666666388511658)
[2025-01-06 01:55:31,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31,527][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.19386455416679382, acc: 0.9052631855010986)
[2025-01-06 01:55:31,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31,926][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.17973293364048004, acc: 0.9444444179534912)
[2025-01-06 01:55:32,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32,348][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.2643616199493408, acc: 0.9111111164093018)
[2025-01-06 01:55:32,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32,841][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 0.38760921359062195, acc: 0.857798159122467)
[2025-01-06 01:55:32,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33,311][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.3682307302951813, acc: 0.8999999761581421)
[2025-01-06 01:55:33,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33,607][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.011793112382292747, acc: 1.0)
[2025-01-06 01:55:33,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33,907][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.004458076786249876, acc: 1.0)
[2025-01-06 01:55:34,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34,252][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.03917492926120758, acc: 0.9545454382896423)
[2025-01-06 01:55:34,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34,592][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.10975364595651627, acc: 0.9629629850387573)
[2025-01-06 01:55:34,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34,959][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.015967819839715958, acc: 1.0)
[2025-01-06 01:55:35,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35,300][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.02158300392329693, acc: 1.0)
[2025-01-06 01:55:35,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35,641][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.01920510083436966, acc: 1.0)
[2025-01-06 01:55:35,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36,218][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.1272139847278595, acc: 0.9354838728904724)
[2025-01-06 01:55:36,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36,746][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.12041544914245605, acc: 0.9772727489471436)
[2025-01-06 01:55:36,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37,069][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.00037418221472762525, acc: 1.0)
[2025-01-06 01:55:37,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37,409][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.002027768874540925, acc: 1.0)
[2025-01-06 01:55:37,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37,766][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.01152857020497322, acc: 1.0)
[2025-01-06 01:55:37,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38,128][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.1183541864156723, acc: 0.949999988079071)
[2025-01-06 01:55:38,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38,484][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.01839873008430004, acc: 1.0)
[2025-01-06 01:55:38,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38,887][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.009552511386573315, acc: 1.0)
[2025-01-06 01:55:39,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39,252][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.0014830719446763396, acc: 1.0)
[2025-01-06 01:55:39,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39,630][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.014495547860860825, acc: 1.0)
[2025-01-06 01:55:39,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39,963][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.08738905936479568, acc: 0.9512194991111755)
[2025-01-06 01:55:40,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40,277][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.044749654829502106, acc: 0.9599999785423279)
[2025-01-06 01:55:40,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40,593][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.0002195485431002453, acc: 1.0)
[2025-01-06 01:55:40,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40,968][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.0013091200962662697, acc: 1.0)
[2025-01-06 01:55:41,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41,362][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.018978960812091827, acc: 0.9824561476707458)
[2025-01-06 01:55:41,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41,705][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.005642104893922806, acc: 1.0)
[2025-01-06 01:55:41,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42,089][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.004451964050531387, acc: 1.0)
[2025-01-06 01:55:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42,654][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.029995283111929893, acc: 0.9811320900917053)
[2025-01-06 01:55:42,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43,234][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.06606677174568176, acc: 0.9916666746139526)
[2025-01-06 01:55:43,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43,536][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.006497444584965706, acc: 1.0)
[2025-01-06 01:55:43,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43,820][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.08795583993196487, acc: 0.9354838728904724)
[2025-01-06 01:55:43,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44,162][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.09305752068758011, acc: 0.9599999785423279)
[2025-01-06 01:55:44,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44,516][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.04464079439640045, acc: 0.9791666865348816)
[2025-01-06 01:55:44,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45,357][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.2412308007478714, acc: 0.8960000276565552)
[2025-01-06 01:55:45,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45,656][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.25767946243286133, acc: 0.932584285736084)
[2025-01-06 01:55:45,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45,961][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.06851757317781448, acc: 0.9729729890823364)
[2025-01-06 01:55:46,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15,442][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3542, device='cuda:0') eval_epoch_loss=tensor(0.8562, device='cuda:0') eval_epoch_acc=tensor(0.8367, device='cuda:0')
[2025-01-06 01:56:15,443][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:56:15,443][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:56:15,660][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_268_loss_0.8561995625495911/model.pt
[2025-01-06 01:56:15,663][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:56:15,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16,172][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.04974689334630966, acc: 0.982758641242981)
[2025-01-06 01:56:16,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16,533][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.01180043164640665, acc: 1.0)
[2025-01-06 01:56:16,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16,877][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.08873064070940018, acc: 0.9545454382896423)
[2025-01-06 01:56:16,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17,200][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.10582813620567322, acc: 0.96875)
[2025-01-06 01:56:17,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17,511][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.002067785244435072, acc: 1.0)
[2025-01-06 01:56:17,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17,908][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.07950938493013382, acc: 0.9666666388511658)
[2025-01-06 01:56:18,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18,247][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.003057696158066392, acc: 1.0)
[2025-01-06 01:56:18,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18,624][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.001121282228268683, acc: 1.0)
[2025-01-06 01:56:18,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19,002][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.002997845644131303, acc: 1.0)
[2025-01-06 01:56:19,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19,362][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.035964980721473694, acc: 0.9599999785423279)
[2025-01-06 01:56:19,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19,734][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.003547720843926072, acc: 1.0)
[2025-01-06 01:56:19,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20,065][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.008053376339375973, acc: 1.0)
[2025-01-06 01:56:20,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20,373][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.016169309616088867, acc: 1.0)
[2025-01-06 01:56:20,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20,818][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.053867436945438385, acc: 0.9759036302566528)
[2025-01-06 01:56:20,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21,180][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.14608080685138702, acc: 0.9351851940155029)
[2025-01-06 01:56:21,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21,500][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.0077332318760454655, acc: 1.0)
[2025-01-06 01:56:21,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21,817][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.03847416117787361, acc: 0.970588207244873)
[2025-01-06 01:56:21,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22,217][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.24897277355194092, acc: 0.949999988079071)
[2025-01-06 01:56:22,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22,582][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.03347201645374298, acc: 0.984375)
[2025-01-06 01:56:22,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22,983][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.032881755381822586, acc: 0.9919999837875366)
[2025-01-06 01:56:23,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23,350][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.010809012688696384, acc: 1.0)
[2025-01-06 01:56:23,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23,691][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.032206155359745026, acc: 0.9875776171684265)
[2025-01-06 01:56:23,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24,082][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.09078062325716019, acc: 0.969072163105011)
[2025-01-06 01:56:24,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24,423][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.004222743678838015, acc: 1.0)
[2025-01-06 01:56:24,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24,785][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.009676818735897541, acc: 1.0)
[2025-01-06 01:56:24,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25,173][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.0037401681765913963, acc: 1.0)
[2025-01-06 01:56:25,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25,663][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.12109221518039703, acc: 0.9818181991577148)
[2025-01-06 01:56:25,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26,207][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.10793378949165344, acc: 0.9484536051750183)
[2025-01-06 01:56:26,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26,510][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.04776451364159584, acc: 0.982758641242981)
[2025-01-06 01:56:26,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26,863][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.012950396165251732, acc: 1.0)
[2025-01-06 01:56:26,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27,227][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.008272063918411732, acc: 1.0)
[2025-01-06 01:56:27,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27,538][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.06283443421125412, acc: 0.9821428656578064)
[2025-01-06 01:56:27,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27,908][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.026062536984682083, acc: 1.0)
[2025-01-06 01:56:28,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28,258][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.030744444578886032, acc: 0.9811320900917053)
[2025-01-06 01:56:28,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28,644][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.0011964469449594617, acc: 1.0)
[2025-01-06 01:56:28,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29,012][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.02024485170841217, acc: 1.0)
[2025-01-06 01:56:29,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29,337][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.0009142106864601374, acc: 1.0)
[2025-01-06 01:56:29,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29,701][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.08504187315702438, acc: 0.9836065769195557)
[2025-01-06 01:56:29,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30,077][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.12546692788600922, acc: 0.9333333373069763)
[2025-01-06 01:56:30,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30,415][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.0002289762196596712, acc: 1.0)
[2025-01-06 01:56:30,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30,753][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.015814315527677536, acc: 1.0)
[2025-01-06 01:56:30,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31,222][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.0145728075876832, acc: 1.0)
[2025-01-06 01:56:31,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31,583][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.031179962679743767, acc: 0.9879518151283264)
[2025-01-06 01:56:31,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31,919][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.012639830820262432, acc: 1.0)
[2025-01-06 01:56:32,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32,268][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.03820883855223656, acc: 0.9897959232330322)
[2025-01-06 01:56:32,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32,571][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.0003852272348012775, acc: 1.0)
[2025-01-06 01:56:32,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32,863][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.046146322041749954, acc: 0.9583333134651184)
[2025-01-06 01:56:32,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33,168][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.0030404343269765377, acc: 1.0)
[2025-01-06 01:56:33,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33,476][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.03117748536169529, acc: 1.0)
[2025-01-06 01:56:33,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33,813][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.016267042607069016, acc: 1.0)
[2025-01-06 01:56:33,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34,206][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.06306000798940659, acc: 0.9615384340286255)
[2025-01-06 01:56:34,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34,569][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.10138452053070068, acc: 0.9777777791023254)
[2025-01-06 01:56:34,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34,888][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.01746898703277111, acc: 1.0)
[2025-01-06 01:56:34,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35,221][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.01365992147475481, acc: 1.0)
[2025-01-06 01:56:35,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35,602][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.20510746538639069, acc: 0.9259259104728699)
[2025-01-06 01:56:35,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35,964][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.16716523468494415, acc: 0.9714285731315613)
[2025-01-06 01:56:36,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36,328][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.10749946534633636, acc: 0.9743589758872986)
[2025-01-06 01:56:36,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36,695][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.08034245669841766, acc: 0.9756097793579102)
[2025-01-06 01:56:36,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37,056][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.013786116614937782, acc: 1.0)
[2025-01-06 01:56:37,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37,391][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.007365919183939695, acc: 1.0)
[2025-01-06 01:56:37,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37,742][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.015598916448652744, acc: 1.0)
[2025-01-06 01:56:37,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38,138][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.0009133673156611621, acc: 1.0)
[2025-01-06 01:56:38,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38,536][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.001926404656842351, acc: 1.0)
[2025-01-06 01:56:38,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38,885][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.0032712318934500217, acc: 1.0)
[2025-01-06 01:56:39,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39,264][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.013847194612026215, acc: 1.0)
[2025-01-06 01:56:39,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39,632][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.0038282047025859356, acc: 1.0)
[2025-01-06 01:56:39,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39,972][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.0005897677619941533, acc: 1.0)
[2025-01-06 01:56:40,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40,347][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.004404235631227493, acc: 1.0)
[2025-01-06 01:56:40,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40,741][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.28314635157585144, acc: 0.9399999976158142)
[2025-01-06 01:56:40,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:41,111][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.15636380016803741, acc: 0.977011501789093)
[2025-01-06 01:56:41,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:41,472][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.22883819043636322, acc: 0.936170220375061)
[2025-01-06 01:56:41,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:41,830][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.0689842700958252, acc: 0.9759036302566528)
[2025-01-06 01:56:41,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:42,151][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.0021729934960603714, acc: 1.0)
[2025-01-06 01:56:42,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:42,488][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.07106254994869232, acc: 0.9743589758872986)
[2025-01-06 01:56:42,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:42,887][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.009261559695005417, acc: 1.0)
[2025-01-06 01:56:42,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43,257][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.02696065790951252, acc: 1.0)
[2025-01-06 01:56:43,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43,573][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.00443960539996624, acc: 1.0)
[2025-01-06 01:56:43,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43,898][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.015508615411818027, acc: 1.0)
[2025-01-06 01:56:43,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44,262][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.032715361565351486, acc: 0.9850746393203735)
[2025-01-06 01:56:44,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44,604][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.0026455537881702185, acc: 1.0)
[2025-01-06 01:56:44,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44,975][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.026805350556969643, acc: 1.0)
[2025-01-06 01:56:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45,363][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.004405782092362642, acc: 1.0)
[2025-01-06 01:56:45,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45,727][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.012688903138041496, acc: 1.0)
[2025-01-06 01:56:45,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46,070][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.031018737703561783, acc: 0.9743589758872986)
[2025-01-06 01:56:46,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46,427][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.024238867685198784, acc: 1.0)
[2025-01-06 01:56:46,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46,762][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.00025102385552600026, acc: 1.0)
[2025-01-06 01:56:46,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:47,152][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.11678077280521393, acc: 0.9615384340286255)
[2025-01-06 01:56:47,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:47,546][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.07717341929674149, acc: 0.9670329689979553)
[2025-01-06 01:56:47,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48,055][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.09351008385419846, acc: 0.9652174115180969)
[2025-01-06 01:56:48,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48,420][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.03967699781060219, acc: 0.989130437374115)
[2025-01-06 01:56:48,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48,776][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.008762581273913383, acc: 1.0)
[2025-01-06 01:56:48,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49,112][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.00033042370341718197, acc: 1.0)
[2025-01-06 01:56:49,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49,483][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.04666519537568092, acc: 0.9615384340286255)
[2025-01-06 01:56:49,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49,789][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.0341765433549881, acc: 0.9756097793579102)
[2025-01-06 01:56:49,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50,167][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.020853325724601746, acc: 1.0)
[2025-01-06 01:56:50,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50,483][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.0028938674367964268, acc: 1.0)
[2025-01-06 01:56:50,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50,817][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.001574972178786993, acc: 1.0)
[2025-01-06 01:56:50,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51,180][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.0179872028529644, acc: 1.0)
[2025-01-06 01:56:51,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51,552][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.00028862591716460884, acc: 1.0)
[2025-01-06 01:56:51,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51,914][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.31579816341400146, acc: 0.95652174949646)
[2025-01-06 01:56:52,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52,295][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.0038414180744439363, acc: 1.0)
[2025-01-06 01:56:52,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52,635][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.007435758598148823, acc: 1.0)
[2025-01-06 01:56:52,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53,233][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.16482584178447723, acc: 0.9454545378684998)
[2025-01-06 01:56:53,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54,065][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.30453428626060486, acc: 0.9150943160057068)
[2025-01-06 01:56:54,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54,442][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.08333894610404968, acc: 0.9666666388511658)
[2025-01-06 01:56:54,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54,805][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.0032566965091973543, acc: 1.0)
[2025-01-06 01:56:54,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55,138][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.03983330726623535, acc: 0.9714285731315613)
[2025-01-06 01:56:55,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55,503][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.00016075893654488027, acc: 1.0)
[2025-01-06 01:56:55,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55,835][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.0005186046473681927, acc: 1.0)
[2025-01-06 01:56:55,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56,228][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.1705799102783203, acc: 0.9583333134651184)
[2025-01-06 01:56:56,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56,558][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.0006649066926911473, acc: 1.0)
[2025-01-06 01:56:56,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57,129][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.22249890863895416, acc: 0.9580838084220886)
[2025-01-06 01:56:57,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57,533][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.09267041087150574, acc: 0.9624060392379761)
[2025-01-06 01:56:57,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58,757][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.4234820306301117, acc: 0.866310179233551)
[2025-01-06 01:56:58,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59,319][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.13252736628055573, acc: 0.9729729890823364)
[2025-01-06 01:56:59,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59,621][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.22380460798740387, acc: 0.9285714030265808)
[2025-01-06 01:56:59,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59,960][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.042549069970846176, acc: 1.0)
[2025-01-06 01:57:00,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00,293][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.0016023332718759775, acc: 1.0)
[2025-01-06 01:57:00,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00,634][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.00043978754547424614, acc: 1.0)
[2025-01-06 01:57:00,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00,968][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.015559215098619461, acc: 1.0)
[2025-01-06 01:57:01,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01,321][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.0019108392298221588, acc: 1.0)
[2025-01-06 01:57:01,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01,701][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.0035348087549209595, acc: 1.0)
[2025-01-06 01:57:01,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02,037][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.05951718986034393, acc: 0.9523809552192688)
[2025-01-06 01:57:02,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02,373][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.4749755859375, acc: 0.8518518805503845)
[2025-01-06 01:57:02,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02,746][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.13958413898944855, acc: 0.9514563083648682)
[2025-01-06 01:57:02,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03,267][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.24260257184505463, acc: 0.9485294222831726)
[2025-01-06 01:57:03,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03,635][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.17908351123332977, acc: 0.9399999976158142)
[2025-01-06 01:57:03,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04,012][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.10348773747682571, acc: 0.9652777910232544)
[2025-01-06 01:57:04,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04,370][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.35274767875671387, acc: 0.9069767594337463)
[2025-01-06 01:57:04,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04,735][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.05196629464626312, acc: 0.9583333134651184)
[2025-01-06 01:57:04,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05,087][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.07269832491874695, acc: 0.9534883499145508)
[2025-01-06 01:57:05,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05,470][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.03359493613243103, acc: 0.9599999785423279)
[2025-01-06 01:57:05,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06,002][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.06459391862154007, acc: 0.970588207244873)
[2025-01-06 01:57:06,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06,379][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.0698658674955368, acc: 0.9733333587646484)
[2025-01-06 01:57:06,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06,739][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.09570543467998505, acc: 0.9696969985961914)
[2025-01-06 01:57:06,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07,099][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.17522434890270233, acc: 0.939393937587738)
[2025-01-06 01:57:07,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07,439][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.0022212257608771324, acc: 1.0)
[2025-01-06 01:57:07,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07,749][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.0009897778509184718, acc: 1.0)
[2025-01-06 01:57:07,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08,084][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.0017001793021336198, acc: 1.0)
[2025-01-06 01:57:08,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08,419][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.0009872573427855968, acc: 1.0)
[2025-01-06 01:57:08,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08,794][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.014955838210880756, acc: 1.0)
[2025-01-06 01:57:08,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09,161][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.001880465541034937, acc: 1.0)
[2025-01-06 01:57:09,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09,548][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.00420594634488225, acc: 1.0)
[2025-01-06 01:57:10,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:36,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:36,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:37,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:37,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:37,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:38,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:38,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:38,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:39,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:39,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:40,101][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3421, device='cuda:0') eval_epoch_loss=tensor(0.8510, device='cuda:0') eval_epoch_acc=tensor(0.8320, device='cuda:0')
[2025-01-06 01:57:40,102][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:57:40,102][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:57:40,319][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_411_loss_0.8510420918464661/model.pt
[2025-01-06 01:57:40,323][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:57:40,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:40,723][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.002109107095748186, acc: 1.0)
[2025-01-06 01:57:40,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:41,072][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.004000147804617882, acc: 1.0)
[2025-01-06 01:57:41,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:41,412][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.016528796404600143, acc: 1.0)
[2025-01-06 01:57:41,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:41,763][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.01021659281104803, acc: 1.0)
[2025-01-06 01:57:41,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:42,142][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.053619105368852615, acc: 0.9803921580314636)
[2025-01-06 01:57:42,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:42,487][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.19456185400485992, acc: 0.9230769276618958)
[2025-01-06 01:57:42,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:42,813][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.010898488573729992, acc: 1.0)
[2025-01-06 01:57:42,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:43,138][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.038052696734666824, acc: 0.9750000238418579)
[2025-01-06 01:57:43,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:43,464][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.0013654439244419336, acc: 1.0)
[2025-01-06 01:57:43,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:43,827][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.002002798253670335, acc: 1.0)
[2025-01-06 01:57:43,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:44,200][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.00951576791703701, acc: 1.0)
[2025-01-06 01:57:44,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:44,576][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.27009204030036926, acc: 0.96875)
[2025-01-06 01:57:44,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:44,957][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.18600144982337952, acc: 0.9166666865348816)
[2025-01-06 01:57:45,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:45,327][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.00251159630715847, acc: 1.0)
[2025-01-06 01:57:45,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:45,687][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.13576026260852814, acc: 0.939393937587738)
[2025-01-06 01:57:45,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:46,043][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.0022060777992010117, acc: 1.0)
[2025-01-06 01:57:46,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:46,397][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.09111285954713821, acc: 0.9729729890823364)
[2025-01-06 01:57:46,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:46,755][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.002433589193969965, acc: 1.0)
[2025-01-06 01:57:46,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:47,113][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.004723924677819014, acc: 1.0)
[2025-01-06 01:57:47,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:47,448][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.0005540368147194386, acc: 1.0)
[2025-01-06 01:57:47,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:47,763][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.00027353523182682693, acc: 1.0)
[2025-01-06 01:57:47,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:48,091][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.21824541687965393, acc: 0.95652174949646)
[2025-01-06 01:57:48,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:48,482][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.005152626428753138, acc: 1.0)
[2025-01-06 01:57:48,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:48,870][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.0033042002469301224, acc: 1.0)
[2025-01-06 01:57:48,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:49,210][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.0009390856721438468, acc: 1.0)
[2025-01-06 01:57:49,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:49,536][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.0025286227464675903, acc: 1.0)
[2025-01-06 01:57:49,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:49,902][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.0035489776637405157, acc: 1.0)
[2025-01-06 01:57:50,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:50,247][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.0003746102738659829, acc: 1.0)
[2025-01-06 01:57:50,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:50,634][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.010146459564566612, acc: 1.0)
[2025-01-06 01:57:50,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:51,132][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.0547964908182621, acc: 0.9848484992980957)
[2025-01-06 01:57:51,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:51,810][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.1959177702665329, acc: 0.9359999895095825)
[2025-01-06 01:57:51,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:52,209][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.16387560963630676, acc: 0.9516128897666931)
[2025-01-06 01:57:52,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:52,858][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.1431778371334076, acc: 0.9552238583564758)
[2025-01-06 01:57:52,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:53,226][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.007753957062959671, acc: 1.0)
[2025-01-06 01:57:53,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:53,651][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.019235355779528618, acc: 1.0)
[2025-01-06 01:57:53,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:54,023][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.07700688391923904, acc: 0.95652174949646)
[2025-01-06 01:57:54,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:54,352][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.025265436619520187, acc: 1.0)
[2025-01-06 01:57:54,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:54,704][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.018931355327367783, acc: 1.0)
[2025-01-06 01:57:54,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:55,072][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.017036978155374527, acc: 0.9850746393203735)
[2025-01-06 01:57:55,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:55,467][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.07191751152276993, acc: 0.9861111044883728)
[2025-01-06 01:57:55,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:55,833][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.004773770458996296, acc: 1.0)
[2025-01-06 01:57:55,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:56,198][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.054149381816387177, acc: 0.9871794581413269)
[2025-01-06 01:57:56,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:56,533][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.023553315550088882, acc: 1.0)
[2025-01-06 01:57:56,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:56,900][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.04506249353289604, acc: 0.9795918464660645)
[2025-01-06 01:57:57,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:57,279][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.11869139224290848, acc: 0.9696969985961914)
[2025-01-06 01:57:57,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:57,648][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.062334172427654266, acc: 0.9793814420700073)
[2025-01-06 01:57:57,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:57,995][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.005217660218477249, acc: 1.0)
[2025-01-06 01:57:58,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:58,354][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.048737648874521255, acc: 0.9883720874786377)
[2025-01-06 01:57:58,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:58,696][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.007335221860557795, acc: 1.0)
[2025-01-06 01:57:58,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:59,032][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.009782346896827221, acc: 1.0)
[2025-01-06 01:57:59,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:59,392][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.007317913230508566, acc: 1.0)
[2025-01-06 01:57:59,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:59,730][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.0424436517059803, acc: 0.96875)
[2025-01-06 01:57:59,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:00,087][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.00670046079903841, acc: 1.0)
[2025-01-06 01:58:00,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:00,448][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.007721847854554653, acc: 1.0)
[2025-01-06 01:58:00,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:00,829][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.05105957016348839, acc: 0.976190447807312)
[2025-01-06 01:58:00,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:01,207][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.13002358376979828, acc: 0.9518072009086609)
[2025-01-06 01:58:01,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:01,608][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.020584305748343468, acc: 1.0)
[2025-01-06 01:58:01,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:01,944][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.07880942523479462, acc: 0.9514563083648682)
[2025-01-06 01:58:02,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:02,330][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.05930594727396965, acc: 0.9837398529052734)
[2025-01-06 01:58:02,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:02,643][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.006343680899590254, acc: 1.0)
[2025-01-06 01:58:02,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:02,993][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.01743675023317337, acc: 1.0)
[2025-01-06 01:58:03,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:03,396][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.09251676499843597, acc: 0.970588207244873)
[2025-01-06 01:58:03,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:03,754][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.2191888839006424, acc: 0.9170305728912354)
[2025-01-06 01:58:03,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:04,111][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.058819547295570374, acc: 1.0)
[2025-01-06 01:58:04,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:04,429][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.04784340038895607, acc: 0.987730085849762)
[2025-01-06 01:58:04,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:04,791][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.058490313589572906, acc: 0.9784172773361206)
[2025-01-06 01:58:04,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:05,157][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.14232325553894043, acc: 0.9547738432884216)
[2025-01-06 01:58:05,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:05,557][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.08375479280948639, acc: 0.9722222089767456)
[2025-01-06 01:58:05,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:05,911][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.012264139950275421, acc: 1.0)
[2025-01-06 01:58:06,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:06,273][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.04347110167145729, acc: 0.9629629850387573)
[2025-01-06 01:58:06,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:06,593][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.0271021518856287, acc: 1.0)
[2025-01-06 01:58:06,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:06,983][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.006005284376442432, acc: 1.0)
[2025-01-06 01:58:07,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:07,376][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.0893520638346672, acc: 0.982758641242981)
[2025-01-06 01:58:07,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:07,759][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.008695847354829311, acc: 1.0)
[2025-01-06 01:58:07,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:08,122][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.008623845875263214, acc: 1.0)
[2025-01-06 01:58:08,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:08,508][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.025880327448248863, acc: 1.0)
[2025-01-06 01:58:08,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:08,888][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.07194610685110092, acc: 0.9523809552192688)
[2025-01-06 01:58:08,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:09,200][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.013401065021753311, acc: 1.0)
[2025-01-06 01:58:09,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:09,566][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.05913960933685303, acc: 0.9846153855323792)
[2025-01-06 01:58:09,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:09,939][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.014709905721247196, acc: 1.0)
[2025-01-06 01:58:10,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:10,319][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.02228948101401329, acc: 1.0)
[2025-01-06 01:58:10,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:10,697][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.013465720228850842, acc: 1.0)
[2025-01-06 01:58:10,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:11,065][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.10733041167259216, acc: 0.9655172228813171)
[2025-01-06 01:58:11,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:11,403][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.017769604921340942, acc: 1.0)
[2025-01-06 01:58:11,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:11,763][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.00252496893517673, acc: 1.0)
[2025-01-06 01:58:11,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:12,136][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.14095909893512726, acc: 0.9553571343421936)
[2025-01-06 01:58:12,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:12,509][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.0115726413205266, acc: 1.0)
[2025-01-06 01:58:12,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:12,884][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.07636206597089767, acc: 0.9775280952453613)
[2025-01-06 01:58:13,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:13,252][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.11364364624023438, acc: 0.9432623982429504)
[2025-01-06 01:58:13,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:13,524][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.07883641868829727, acc: 0.967391312122345)
[2025-01-06 01:58:13,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:13,821][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.00013919472985435277, acc: 1.0)
[2025-01-06 01:58:13,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:14,121][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0008944747969508171, acc: 1.0)
[2025-01-06 01:58:14,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:14,468][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.012607288546860218, acc: 1.0)
[2025-01-06 01:58:14,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:14,843][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.001443596207536757, acc: 1.0)
[2025-01-06 01:58:14,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:15,199][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.12905873358249664, acc: 0.9622641801834106)
[2025-01-06 01:58:15,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:15,544][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.14532221853733063, acc: 0.931034505367279)
[2025-01-06 01:58:15,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16,127][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.13600145280361176, acc: 0.9459459185600281)
[2025-01-06 01:58:16,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16,564][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.08007891476154327, acc: 0.98591548204422)
[2025-01-06 01:58:16,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16,873][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.031647514551877975, acc: 1.0)
[2025-01-06 01:58:16,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:17,197][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.0020125515293329954, acc: 1.0)
[2025-01-06 01:58:17,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:17,608][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.000721406249795109, acc: 1.0)
[2025-01-06 01:58:18,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:20,274][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.28609976172447205, acc: 0.9214285612106323)
[2025-01-06 01:58:20,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:21,031][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.02040845900774002, acc: 0.9920634627342224)
[2025-01-06 01:58:21,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:21,422][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.00730228703469038, acc: 1.0)
[2025-01-06 01:58:21,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:21,824][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.0021403480786830187, acc: 1.0)
[2025-01-06 01:58:21,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:22,515][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.004158306401222944, acc: 1.0)
[2025-01-06 01:58:22,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:22,892][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 5.040785254095681e-05, acc: 1.0)
[2025-01-06 01:58:22,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:23,229][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.0008419225341640413, acc: 1.0)
[2025-01-06 01:58:23,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:23,571][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.009192727506160736, acc: 1.0)
[2025-01-06 01:58:23,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:23,835][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.002271819394081831, acc: 1.0)
[2025-01-06 01:58:24,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:24,812][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.18172813951969147, acc: 0.944915235042572)
[2025-01-06 01:58:24,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:25,231][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.03302884101867676, acc: 0.9850746393203735)
[2025-01-06 01:58:25,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:25,615][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.039782747626304626, acc: 0.9927007555961609)
[2025-01-06 01:58:25,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:26,176][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.13866929709911346, acc: 0.9649999737739563)
[2025-01-06 01:58:26,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:26,578][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.0017695740098133683, acc: 1.0)
[2025-01-06 01:58:26,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:26,963][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.05858074128627777, acc: 0.9807692170143127)
[2025-01-06 01:58:27,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:27,315][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.0016014764551073313, acc: 1.0)
[2025-01-06 01:58:27,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:27,667][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.026750581339001656, acc: 1.0)
[2025-01-06 01:58:27,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:27,990][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.1772567182779312, acc: 0.9661017060279846)
[2025-01-06 01:58:28,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:28,295][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.09581182152032852, acc: 0.930232584476471)
[2025-01-06 01:58:28,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:28,610][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.028864651918411255, acc: 0.9772727489471436)
[2025-01-06 01:58:28,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:28,931][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.019939696416258812, acc: 1.0)
[2025-01-06 01:58:29,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:29,282][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.30958291888237, acc: 0.9545454382896423)
[2025-01-06 01:58:29,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:29,664][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.042422033846378326, acc: 0.9599999785423279)
[2025-01-06 01:58:29,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:30,041][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.003029387444257736, acc: 1.0)
[2025-01-06 01:58:30,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:30,378][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.0006382535793818533, acc: 1.0)
[2025-01-06 01:58:30,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:30,774][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.021715480834245682, acc: 1.0)
[2025-01-06 01:58:30,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:31,160][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.07283084839582443, acc: 0.96875)
[2025-01-06 01:58:31,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:31,555][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.05815725773572922, acc: 0.96875)
[2025-01-06 01:58:31,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:31,897][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.00432848185300827, acc: 1.0)
[2025-01-06 01:58:32,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:32,264][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.01606176793575287, acc: 1.0)
[2025-01-06 01:58:32,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:32,598][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.0007287028711289167, acc: 1.0)
[2025-01-06 01:58:32,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:32,895][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.0001487671979703009, acc: 1.0)
[2025-01-06 01:58:32,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:33,186][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.0014621264999732375, acc: 1.0)
[2025-01-06 01:58:33,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:33,548][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.0007576695643365383, acc: 1.0)
[2025-01-06 01:58:33,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:33,844][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.0007865703082643449, acc: 1.0)
[2025-01-06 01:58:33,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:34,203][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.002601534826681018, acc: 1.0)
[2025-01-06 01:58:34,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:34,562][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.0009483133908361197, acc: 1.0)
[2025-01-06 01:58:34,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:34,930][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.00015567913942504674, acc: 1.0)
[2025-01-06 01:58:35,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:35,286][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.0020707580260932446, acc: 1.0)
[2025-01-06 01:58:35,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:35,627][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.0005306691164150834, acc: 1.0)
[2025-01-06 01:58:35,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:35,962][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.03954950347542763, acc: 0.9857142567634583)
[2025-01-06 01:58:36,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:36,369][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.039643991738557816, acc: 0.9927007555961609)
[2025-01-06 01:58:37,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:37,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:37,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:38,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:38,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:38,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:39,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:39,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:39,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:40,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:40,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:40,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:41,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:41,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:42,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:42,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:42,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:43,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:43,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:43,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:45,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:45,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:46,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:46,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:46,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:47,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:47,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:47,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:48,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:48,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:48,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:49,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:49,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:49,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:50,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:50,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:50,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:51,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:51,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:51,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:52,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:52,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:52,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:53,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:53,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:55,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:55,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:57,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:57,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:58,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:58,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:58,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:59,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:59,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:00,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:00,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:00,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:01,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:01,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:03,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:03,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:05,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:05,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:05,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:07,537][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4237, device='cuda:0') eval_epoch_loss=tensor(0.8853, device='cuda:0') eval_epoch_acc=tensor(0.8429, device='cuda:0')
[2025-01-06 01:59:07,538][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:59:07,539][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:59:07,797][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_554_loss_0.8852918744087219/model.pt
[2025-01-06 01:59:07,800][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:59:07,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:08,157][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.008330055512487888, acc: 1.0)
[2025-01-06 01:59:08,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:08,541][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.021630913019180298, acc: 0.9928571581840515)
[2025-01-06 01:59:08,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:08,926][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.07589615881443024, acc: 0.9801324605941772)
[2025-01-06 01:59:09,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:09,277][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.01634647138416767, acc: 1.0)
[2025-01-06 01:59:09,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:09,552][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.00013440000475384295, acc: 1.0)
[2025-01-06 01:59:09,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:09,882][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.019170226529240608, acc: 1.0)
[2025-01-06 01:59:10,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10,295][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.0612415112555027, acc: 0.9615384340286255)
[2025-01-06 01:59:10,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10,556][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.0008099832921288908, acc: 1.0)
[2025-01-06 01:59:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10,873][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.03619910404086113, acc: 0.9888888597488403)
[2025-01-06 01:59:10,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11,194][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.01364351250231266, acc: 1.0)
[2025-01-06 01:59:11,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11,556][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.3654511868953705, acc: 0.9375)
[2025-01-06 01:59:11,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11,926][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.01631811633706093, acc: 1.0)
[2025-01-06 01:59:12,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:12,301][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.029778102412819862, acc: 0.988095223903656)
[2025-01-06 01:59:12,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:12,684][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.002479459159076214, acc: 1.0)
[2025-01-06 01:59:12,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:13,054][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.0003889691724907607, acc: 1.0)
[2025-01-06 01:59:13,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:13,473][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.05322834849357605, acc: 0.9839572310447693)
[2025-01-06 01:59:13,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:13,800][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.00031588340061716735, acc: 1.0)
[2025-01-06 01:59:13,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:14,160][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.007834614254534245, acc: 1.0)
[2025-01-06 01:59:14,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:14,485][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.0667801946401596, acc: 0.9744898080825806)
[2025-01-06 01:59:14,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:14,789][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.03310428932309151, acc: 0.9874213933944702)
[2025-01-06 01:59:15,213][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0792, train_epoch_loss=0.0762, epoch time 354.213001050055s
[2025-01-06 01:59:15,214][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:59:15,214][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 20 GB
[2025-01-06 01:59:15,214][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:59:15,214][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 28
[2025-01-06 01:59:15,214][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:59:15,218][root][INFO] - Key: avg_train_prep, Value: 1.3758896589279175
[2025-01-06 01:59:15,220][root][INFO] - Key: avg_train_loss, Value: 0.2635788321495056
[2025-01-06 01:59:15,220][root][INFO] - Key: avg_train_acc, Value: 0.9292565584182739
[2025-01-06 01:59:15,220][root][INFO] - Key: avg_eval_prep, Value: 2.1604232788085938
[2025-01-06 01:59:15,220][root][INFO] - Key: avg_eval_loss, Value: 0.7636927962303162
[2025-01-06 01:59:15,220][root][INFO] - Key: avg_eval_acc, Value: 0.8368111848831177
[2025-01-06 01:59:15,220][root][INFO] - Key: avg_epoch_time, Value: 356.1918168451637
[2025-01-06 01:59:15,220][root][INFO] - Key: avg_checkpoint_time, Value: 0.2639585996046662
